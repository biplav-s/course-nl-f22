{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised/ Clustering Illustration for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/doc1.txt\"\n",
    "\n",
    "with open(filename) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for tokenization \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for removing frequently occuring words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could have cleaned using stop words\n",
    "text_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean word tokens\n",
    "clean_word_tokens = [w for w in text_tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have vector representation before we can do classification\n",
    "# Do imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the object\n",
    "vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit training data to the count vectorizer\n",
    "data_tfidf = vectorizer.fit_transform(clean_word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now clustering setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to run and print clusters using vectorizer\n",
    "##  K-means parameters explain here\n",
    "#   - https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html \n",
    "def run_kmeans(k, data_tfidf_format, vectorizer):\n",
    "    model = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=10)\n",
    "    # Fit model to data\n",
    "    model.fit(data_tfidf)\n",
    "    # Explain the clusters, i.e., their centroids\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(k):\n",
    "        print(\"Cluster %d:\" % i),\n",
    "        for word in order_centroids[i, :10]:\n",
    "            print(\"\\t%s\" % terms[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "\ttest\n",
      "\ttests\n",
      "\tmodels\n",
      "\tchecklist\n",
      "\tmft\n",
      "\tinv\n",
      "\t2019\n",
      "\tcapabilities\n",
      "\tal\n",
      "\tusers\n",
      "Cluster 1:\n",
      "\tet\n",
      "\tzimmermann\n",
      "\tenvironment\n",
      "\tencourages\n",
      "\tencouraging\n",
      "\tend\n",
      "\tengine\n",
      "\tengineering\n",
      "\tengland\n",
      "\tenjoyed\n",
      "Cluster 2:\n",
      "\tcapability\n",
      "\tzimmermann\n",
      "\tequal\n",
      "\tencouraging\n",
      "\tend\n",
      "\tengine\n",
      "\tengineering\n",
      "\tengland\n",
      "\tenjoyed\n",
      "\tensures\n",
      "Cluster 3:\n",
      "\tbert\n",
      "\tbase\n",
      "\tlarge\n",
      "\tzimmermann\n",
      "\tengine\n",
      "\tengineering\n",
      "\tengland\n",
      "\tenjoyed\n",
      "\tensures\n",
      "\tentities\n",
      "Cluster 4:\n",
      "\tmodel\n",
      "\tagnostic\n",
      "\tencouraging\n",
      "\tend\n",
      "\tengine\n",
      "\tengineering\n",
      "\tengland\n",
      "\tenjoyed\n",
      "\tensures\n",
      "\tentities\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "run_kmeans(k, data_tfidf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc1 is the text of paper -\n",
    "# \"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList\", ACL 2020\n",
    "#  https://aclanthology.org/2020.acl-main.442/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try  doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/doc2.txt\"\n",
    "\n",
    "with open(filename) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could have cleaned using stop words\n",
    "text_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean word tokens\n",
    "clean_word_tokens = [w for w in text_tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the object\n",
    "vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit training data to the count vectorizer\n",
    "data_tfidf = vectorizer.fit_transform(clean_word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "\tdrinking\n",
      "\twater\n",
      "\thazards\n",
      "\tborne\n",
      "\tderived\n",
      "\tquality\n",
      "\tdisadvantage\n",
      "\tdisasters\n",
      "\tdischarges\n",
      "\tdischarged\n",
      "Cluster 1:\n",
      "\thealth\n",
      "\tguideline\n",
      "\tvalue\n",
      "\ttreatment\n",
      "\tquality\n",
      "\tconcentrations\n",
      "\tused\n",
      "\tbased\n",
      "\texposure\n",
      "\tμg\n",
      "Cluster 2:\n",
      "\tpotential\n",
      "\tμm\n",
      "\tdisaster\n",
      "\tdischarging\n",
      "\tdischarges\n",
      "\tdischarged\n",
      "\tdischarge\n",
      "\tdiscernible\n",
      "\tdiscarding\n",
      "\tdiscarded\n",
      "Cluster 3:\n",
      "\twater\n",
      "\tbased\n",
      "\tusing\n",
      "\tμm\n",
      "\tdischarges\n",
      "\tdischarged\n",
      "\tdischarge\n",
      "\tdiscernible\n",
      "\tdiscarding\n",
      "\tdiscarded\n",
      "Cluster 4:\n",
      "\toccurrence\n",
      "\tμm\n",
      "\tdisasters\n",
      "\tdisclaimer\n",
      "\tdischarging\n",
      "\tdischarges\n",
      "\tdischarged\n",
      "\tdischarge\n",
      "\tdiscernible\n",
      "\tdiscarding\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "run_kmeans(k, data_tfidf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doc2 has the text of: \n",
    "#  - https://github.com/biplav-s/course-nl-f22/blob/main/sample-code/common-data/WHO-Water/WHO-Drinking-4ed-9789240045064-eng.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: how to evaluate clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: using labeled data (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try clustering notebook on fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL: https://github.com/biplav-s/course-nl/blob/master/l9-ml-review/Clustering%20-%20Fake%20news%20Illustration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

Fourth editionincorporatingthe first andsecond addendaGuidelines fordrinking-waterqualityFourth editionincorporatingthe first andsecond addendaGuidelines fordrinking-waterqualityGuidelines for drinking‑water quality Fourth edition incorporating the first and second addenda Guidelines for drinking-water quality: fourth edition incorporating the first and second addenda ISBN 978-92-4-004506-4 (electronic version) ISBN 978-92-4-004507-1 (print version) © World Health Organization 2022 Some rights reserved. This work is available under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO licence (CC BY-NC-SA 3.0 IGO; https://creativecommons.org/licenses/by-nc-sa/3.0/igo). Under the terms of this licence, you may copy, redistribute and adapt the work for non-commercial purposes, provided the work is appropriately cited, as indicated below. In any use of this work, there should be no suggestion that WHO endorses any specific organization, products or services. The use of the WHO logo is not permitted. If you adapt the work, then you must license your work under the same or equivalent Creative Commons licence. If you create a translation of this work, you should add the following disclaimer along with the suggested citation: “This translation was not created by the World Health Organization (WHO). WHO is not responsible for the content or accuracy of this translation. The original English edition shall be the binding and authentic edition”. Any mediation relating to disputes arising under the licence shall be conducted in accordance with the mediation rules of the World Intellectual Property Organization (http://www.wipo.int/amc/en/mediation/ rules/). Suggested citation. Guidelines for drinking-water quality: fourth edition incorporating the first and second addenda. Geneva: World Health Organization; 2022. Licence: CC BY-NC-SA 3.0 IGO. Cataloguing-in-Publication (CIP) data. CIP data are available at http://apps.who.int/iris. Sales, rights and licensing. To purchase WHO publications, see http://apps.who.int/bookorders. To submit requests for commercial use and queries on rights and licensing, see https://www.who.int/copyright. Third-party materials. If you wish to reuse material from this work that is attributed to a third party, such as tables, figures or images, it is your responsibility to determine whether permission is needed for that reuse and to obtain permission from the copyright holder. The risk of claims resulting from infringement of any third-party-owned component in the work rests solely with the user. General disclaimers. The designations employed and the presentation of the material in this publication do not imply the expression of any opinion whatsoever on the part of WHO concerning the legal status of any country, territory, city or area or of its authorities, or concerning the delimitation of its frontiers or boundaries. Dotted and dashed lines on maps represent approximate border lines for which there may not yet be full agreement. The mention of specific companies or of certain manufacturers’ products does not imply that they are endorsed or recommended by WHO in preference to others of a similar nature that are not mentioned. Errors and omissions excepted, the names of proprietary products are distinguished by initial capital letters. All reasonable precautions have been taken by WHO to verify the information contained in this publication. However, the published material is being distributed without warranty of any kind, either expressed or implied. The responsibility for the interpretation and use of the material lies with the reader. In no event shall WHO be liable for damages arising from its use. Cover design by WHO Graphics, Switzerland Typeset by Interligar, Brazil Contents Preface xv Acknowledgements xix Abbreviations used in text xxii 1. Introduction 1 1.1 General considerations and principles 1 1.1.1 Framework for safe drinking-water 3 1.1.2 Microbial aspects 4 1.1.3 Disinfection 5 1.1.4 Chemical aspects 6 1.1.5 Radiological aspects 7 1.1.6 Acceptability aspects: taste, odour and appearance 7 1.2 Roles and responsibilities in drinking-water safety management 8 1.2.1 Surveillance and quality control 8 1.2.2 Public health authorities 10 1.2.3 Local authorities 11 1.2.4 Water resource management 12 1.2.5 Drinking-water supply agencies 13 1.2.6 Community management 14 1.2.7 Water vendors 15 1.2.8 Individual consumers 15 1.2.9 Certification agencies 16 1.2.10 Plumbing 17 1.3 Supporting resources to the Guidelines 18 1.3.1 Published documents 18 2. A conceptual framework for implementing the Guidelines 19 2.1 Health-based targets 20 2.2 Water safety plans 22 2.2.1 System assessment and design 22 2.2.2 Operational monitoring 23 2.2.3 Management plans, documentation and communication 24 2.3 Surveillance 25 iii 2.4 Verification of drinking-water quality 25 2.4.1 Microbial water quality 26 2.4.2 Chemical water quality 26 2.5 Identifying priority concerns 27 2.5.1 Undertaking a drinking-water quality assessment 28 2.5.2 Assessing microbial priorities 29 2.5.3 Assessing chemical priorities 29 2.6 Developing drinking-water quality standards 30 2.6.1 Adapting guideline values to locally relevant standards 31 2.6.2 Periodic review and revision of standards 31 2.7 Drinking-water regulations and supporting policies and programmes 32 2.7.1 Regulations 32 2.7.2 Supporting policies and programmes 33 3. Health-based targets 35 3.1 Setting health-based targets 36 3.2 Disability-adjusted life years, tolerable disease burden and reference level of risk 37 3.3 Types of health-based targets 38 3.3.1 Health outcome targets 41 3.3.2 Water quality targets 42 3.3.3 Performance targets 43 3.3.4 Specified technology targets 44 4. Water safety plans 45 4.1 System assessment and design 49 4.1.1 New systems 50 4.1.2 Collecting and evaluating available data 51 4.1.3 Resource and source protection 53 4.1.4 Treatment 55 4.1.5 Piped distribution systems 56 4.1.6 Non-piped, community and household systems 58 4.1.7 Validation 60 4.1.8 Upgrade and improvement 61 4.2 Operational monitoring and maintaining control 61 4.2.1 Determining system control measures 62 4.2.2 Selecting operational monitoring parameters 62 4.2.3 Establishing operational and critical limits 63 4.2.4 Non-piped, community and household systems 63 4.3 Verification 65 4.3.1 Microbial water quality 66 4.3.2 Chemical water quality 66 4.3.3 Source waters 67 4.3.4 Piped distribution systems 68 4.3.5 Community-managed supplies 68 4.3.6 Quality assurance and quality control 69 4.3.7 Water safety plans 69 4.4 Management procedures for piped distribution systems 69 4.4.1 Predictable incidents (“deviations”) 72 4.4.2 Unplanned events 72 4.4.3 Emergencies 72 4.4.4 Preparing a monitoring plan 73 4.4.5 Supporting programmes 73 4.5 Management of community and household water supplies 75 4.6 Documentation and communication 76 4.7 Planned review 77 4.7.1 Periodic review 77 4.7.2 Post-incident review 77 5. Surveillance 79 5.1 Types of approaches 81 5.1.1 Audit 81 5.1.2 Direct assessment 82 5.2 Adapting approaches to specific circumstances 83 5.2.1 Urban areas in developing countries 83 5.2.2 Community drinking-water supplies 83 5.2.3 Household treatment and storage systems 84 5.3 Adequacy of supply 85 5.3.1 Accessibility 85 5.3.2 Quantity 86 5.3.3 Continuity 88 5.3.4 Affordability 89 5.4 Planning and implementation 90 5.5 Reporting and communicating 92 5.5.1 Interaction with community and consumers 92 5.5.2 Regional use of data 93 6. Application of the Guidelines in specific circumstances 97 6.1 Climate change: increasing threats from water scarcity, heavy rainfall and extreme events 98 6.2 Rainwater harvesting 99 6.3 Vended water 100 6.4 Bulk water supply 101 6.5 Desalination systems 102 6.6 Potable reuse systems 103 6.7 Dual piped water supply systems 104 6.8 Emergencies and disasters 105 6.9 Temporary water supplies 108 6.10 Buildings 110 6.11 Health-care facilities 113 6.12 Safe drinking-water for travellers 114 6.13 Aircraft and airports 118 6.14 Ships 119 6.15 Packaged drinking-water 121 6.16 Food production and processing 122 7. Microbial aspects 125 7.1 Microbial hazards associated with drinking-water 126 7.1.1 Waterborne infections 126 7.1.2 Emerging issues 130 7.1.3 Persistence and growth in water 133 7.1.4 Public health aspects 134 7.2 Health-based target setting 135 7.2.1 Health-based targets applied to microbial hazards 135 7.2.2 Reference pathogens 136 7.2.3 Quantitative microbial risk assessment 138 7.2.4 Risk-based performance target setting 143 7.2.5 Presenting the outcome of performance target development 144 7.2.6 Adapting risk-based performance target setting to local circumstances 144 7.2.7 Health outcome targets 145 7.3 Occurrence and treatment of pathogens 147 7.3.1 Occurrence 147 7.3.2 Treatment 149 7.4 Microbial monitoring 159 7.5 Methods of detection of faecal indicator organisms 162 7.6 Identifying local actions in response to microbial water quality problems and emergencies 163 7.6.1 Boil water advisories 164 7.6.2 Actions following an incident 166 8. Chemical aspects 167 8.1 Chemical hazards in drinking-water 168 8.2 Derivation of chemical guideline values and health-based values 170 8.2.1 Approaches taken 171 8.2.2 Threshold chemicals 173 8.2.3 Non-threshold chemicals 178 8.2.4 Data quality 178 8.2.5 Provisional guideline values 179 8.2.6 Chemicals with effects on acceptability 179 8.2.7 Chemicals not included in the Guidelines 180 8.2.8 Mixtures 180 8.2.9 Adapting guideline values to local circumstances 181 8.3 Analytical achievability 182 8.4 Treatment 184 8.4.1 Treatment performance 184 8.4.2 Process control measures for disinfection by-products 186 8.4.3 Treatment for corrosion control 188 8.4.4 Household treatment 189 8.5 Guideline values for individual chemicals, by source category 189 8.5.1 Naturally occurring chemicals 189 8.5.2 Chemicals from industrial sources and human dwellings 190 8.5.3 Chemicals from agricultural activities 195 8.5.4 Chemicals used in water treatment or from materials in contact with drinking-water 195 8.5.5 Chemicals of emerging concern 204 8.6 Pesticides used in water for public health purposes 205 8.7 Identifying local actions in response to chemical water quality problems and emergencies 207 8.7.1 Trigger for action 210 8.7.2 Investigating the situation 210 8.7.3 Talking to the right people 210 8.7.4 Informing the public 210 8.7.5 Evaluating the significance to public health and individuals 211 8.7.6 Determining appropriate action 214 8.7.7 Consumer acceptability 215 8.7.8 Ensuring remedial action, preventing recurrence and updating the water safety plan 215 8.7.9 Mixtures 215 8.7.10 Water avoidance advisories 215 9. Radiological aspects 219 9.1 Sources and health effects of radiation exposure 220 9.1.1 Radiation exposure through ingestion of drinking-water 221 9.1.2 Radiation-induced health effects through drinking-water 222 9.2 Rationale for screening levels and guidance levels 223 9.3 Monitoring and assessment for dissolved radionuclides 224 9.3.1 Screening of drinking-water supplies 224 9.3.2 Strategy for assessing drinking-water if screening levels are exceeded 227 9.3.3 Strategy for assessing drinking-water if guidance levels are exceeded 227 9.3.4 Sampling frequency 229 9.4 Guidance levels for radionuclides commonly found in drinking-water 230 9.5 Analytical methods 231 9.5.1 Measuring gross alpha and gross beta activity concentrations 231 9.5.2 Measuring specific radionuclides 231 9.6 Remedial measures 231 9.7 Radon 232 9.7.1 Radon in air and water 232 9.7.2 Health risks from radon 234 9.7.3 Guidance on radon in drinking-water supplies 234 9.7.4 Measuring radon in drinking-water 234 9.7.5 Decreasing radon concentrations in drinking-water 235 9.8 Risk communication 235 9.8.1 Reporting results 235 9.8.2 Communicating risks 236 10. Acceptability aspects: Taste, odour and appearance 237 10.1 Biologically derived contaminants 239 Actinomycetes and fungi 239 Cyanobacteria and algae 239 Invertebrate animal life 239 Iron bacteria 240 10.2 Chemically derived contaminants 240 Aluminium 240 Ammonia 241 Chloramines 241 Chloride 241 Chlorine 241 Chlorobenzenes 242 Chlorophenols 242 Colour 242 Copper 242 Dissolved oxygen 243 Ethylbenzene 243 Hardness 243 Hydrogen sulfide 244 Iron 244 Manganese 244 Petroleum oils 244 pH and corrosion 245 Sodium 245 Styrene 245 Sulfate 245 Synthetic detergents 246 Toluene 246 Total dissolved solids 246 Turbidity 246 Xylenes 247 Zinc 247 10.3 Treatment of taste, odour and appearance problems 247 10.4 Temperature 248 11. Microbial fact sheets 249 11.1 Bacterial pathogens 250 Acinetobacter 250 Aeromonas 252 Burkholderia pseudomallei 253 Campylobacter 254 Enterobacter sakazakii 256 Escherichia coli pathogenic strains 257 Helicobacter pylori 258 Klebsiella 259 Legionella 261 Leptospira 262 Mycobacterium 264 Pseudomonas aeruginosa 266 Salmonella 267 Shigella 269 Staphylococcus aureus 270 Tsukamurella 271 Vibrio 272 Yersinia 274 11.2 Viral pathogens 275 Adenoviruses 275 Astroviruses 277 Caliciviruses 278 Enteroviruses 280 Hepatitis A virus 281 Hepatitis E virus 282 Rotaviruses and orthoreoviruses 284 11.3 Protozoan pathogens 285 Acanthamoeba 286 Balantidium coli 287 Blastocystis 288 Cryptosporidium 290 Cyclospora cayetanensis 291 Entamoeba histolytica 293 Giardia intestinalis 294 Isospora belli 296 Microsporidia 297 Naegleria fowleri 299 Toxoplasma gondii 300 11.4 Helminth pathogens 302 Dracunculus medinensis 302 Fasciola spp. 304 Free-living nematodes 305 Schistosoma spp. 307 11.5 Toxic cyanobacteria 310 11.6 Indicator organisms 315 Total coliform bacteria 315 Escherichia coli and thermotolerant coliform bacteria 316 Heterotrophic plate counts 317 Intestinal enterococci 319 Clostridium perfringens 320 Coliphages 321 Bacteroides fragilis phages 324 Enteric viruses 325 12. Chemical fact sheets 329 12.1 Chemical contaminants in drinking-water 329 Acrylamide 329 Alachlor 331 Aldicarb 331 Aldrin and dieldrin 332 Aluminium 333 Ammonia 335 Anatoxins (cyanobacterial toxins) 336 Antimony 339 Arsenic 340 Asbestos 343 Atrazine and its metabolites 345 Barium 346 Bentazone 347 Benzene 349 Beryllium 350 Boron 350 Bromate 351 Bromide 352 Brominated acetic acids 353 Cadmium 354 Carbaryl 355 Carbofuran 356 Carbon tetrachloride 356 Chloral hydrate 357 Chloramines (monochloramine, dichloramine, trichloramine) 358 Chlordane 360 Chloride 361 Chlorine 361 Chlorine dioxide, chlorite and chlorate 362 Chloroacetones 364 Chlorophenols (2-chlorophenol, 2,4-dichlorophenol, 2,4,6-trichlorophenol) 364 Chloropicrin 365 Chlorotoluron 366 Chlorpyrifos 366 Chromium 367 Copper 369 Cyanazine Cyanide 371 Cyanogen chloride 372 Cylindrospermopsins (cyanobacterial toxins) 373 2,4-D 376 2,4-DB 377 DDT and metabolites 377 1,2-Dibromo-3-chloropropane 379 1,2-Dibromoethane 379 Dichloroacetic acid Dichlorobenzenes (1,2-dichlorobenzene, 1,3-dichlorobenzene, 1,4-dichlorobenzene) 381 1,1-Dichloroethane 383 1,2-Dichloroethane 383 1,1-Dichloroethene 384 1,2-Dichloroethene 384 Dichloromethane 386 1,2-Dichloropropane 386 1,3-Dichloropropane 387 1,3-Dichloropropene 388 Dichlorprop 388 Dichlorvos 389 Dicofol Di(2-ethylhexyl)adipate 392 Di(2-ethylhexyl)phthalate 393 Dimethoate 394 1,4-Dioxane 394 Diquat 395 Edetic acid 396 Endosulfan 397 Endrin 398 Epichlorohydrin 399 Ethylbenzene Fenitrothion 400 Fenoprop 401 Fluoride 402 Formaldehyde 405 Glyphosate and AMPA 406 Halogenated acetonitriles (dichloroacetonitrile, dibromoacetonitrile, bromochloroacetonitrile, trichloroacetonitrile) 407 Hardness 408 Heptachlor and heptachlor epoxide 409 Hexachlorobenzene Hexachlorobutadiene 411 Hydrogen sulfide 412 Inorganic tin 412 Iodine 413 Iron 414 Isoproturon 415 Lead 415 Lindane 418 Malathion 419 Manganese 420 MCPA 423 Mecoprop 425 Mercury 425 Methoxychlor 426 Methyl parathion 427 Methyl tertiary-butyl ether 428 Metolachlor 429 Microcystins (cyanobacterial toxins) 430 Molinate 433 Molybdenum 433 Monochloroacetic acid 434 Monochlorobenzene 435 MX 435 Nickel 436 Nitrate and nitrite 438 Nitrilotriacetic acid 444 Nitrobenzene 445 N-Nitrosodimethylamine 446 Organotins 447 Parathion 448 Pendimethalin 449 Pentachlorophenol 450 Perchlorate 450 Petroleum products 451 pH 452 2-Phenylphenol and its sodium salt 453 Polynuclear aromatic hydrocarbons 453 Potassium 455 Propanil 456 Saxitoxins (cyanobacterial toxins) 456 Selenium 459 Silver 461 Simazine 462 Sodium 463 Sodium dichloroisocyanurate 464 Styrene 465 Sulfate 466 2,4,5-T 466 Terbuthylazine 467 Tetrachloroethene 468 Toluene 470 Total dissolved solids 470 Trichloroacetic acid 471 Trichlorobenzenes (total) 472 1,1,1-Trichloroethane 472 Trichloroethene 473 Trifluralin 474 Trihalomethanes (bromoform, bromodichloromethane, chloroform, dibromochloromethane) 475 Uranium 478 Vinyl chloride 480 Xylenes 481 Zinc 482 12.2 Pesticides used for vector control in drinking-water sources and containers 482 Bacillus thuringensis israelensis 482 Diflubenzuron 483 Methoprene 484 Novaluron 485 Permethrin 486 Pirimiphos-methyl 487 Pyriproxyfen 488 Spinosad 488 Temephos 489 Annex 1 Supporting documentation to the Guidelines 491 Annex 2 References cited 499 Annex 3 Chemical summary tables 520 Annex 4 Analytical methods and achievability 529 Annex 5 Treatment methods and performance 539 Annex 6 Supporting information on radionuclides 558 Annex 7 Contributors to the development of the Guidelines for drinking-water quality: fourth edition incorporating the first and second addenda 563 Preface AAccess to safe drinking-water is essential to health, a basic human right and a component of effective policy for health protection. The importance of water, sanitation and hygiene for health and development has been reflected in the outcomes of a series of international policy forums. This includes, most recently, the adoption of the Sustainable Development Goals by countries, in 2015, which include a target and indicator on safe drinking-water. Further, the United Nations (UN) General Assembly declared in 2010 that safe and clean drinking-water and sanitation is a human right, essential to the full enjoyment of life and all other human rights. These commitments build on a long history of support including the UN General Assembly adopting the Millennium Development Goals in 2000 and declaring the period 2005–2015 as the International Decade for Action, “Water for Life”. Access to safe drinking-water is important as a health and development issue at national, regional and local levels. In some regions, it has been shown that investments in water supply and sanitation can yield a net economic benefit, because the reductions in adverse health effects and health-care costs outweigh the costs of undertaking the interventions. This is true for investments ranging from major water supply infrastructure through to water treatment in the home. Experience has also shown that interventions in improving access to safe water favour the poor in particular, whether in rural or urban areas, and can be an effective part of poverty alleviation strategies. The World Health Organization (WHO) published four editions of the Guidelines for drinking-water quality (in 1983–1984, 1993–1997, 2004, and 2011), as successors to the previous WHO International standards for drinking water, which were published in 1958, 1963 and 1971. Since 1995, the Guidelines have been updated through a process of rolling revision, whereby a limited number of sections within each edition are updated as feasible, including in response to new evidence, uncertainty about best practice, or requests from stakeholders. New editions of the Guidelines usually introduce major new recommendations and are published following comprehensive review. Leading the process of the development of the fourth edition was the Water, Sanitation, Hygiene and Health Unit within WHO headquarters. The Chemical Safety Unit and the Risk Assessment and Management Unit provided input on chemical xv hazards, and the Radiation Programme provided input on radiological hazards. All six WHO regional offices participated in the process, in consultation with Member States. This version of the Guidelines, Guidelines for drinking-water quality: fourth edition incorporating the first and second addenda,1 supersedes previous editions of the Guidelines, including the fourth edition incorporating the first addendum, published in 2017; the fourth edition, published in 2011; and the previous International Standards. These Guidelines integrate into the fourth edition the updates of 2017 and subsequent updates, as listed below. The primary goal of the Guidelines is to protect public health associated with drinking-water quality. The overall objectives of the Guidelines are to: • provide an authoritative basis for the effective consideration of public health in setting national or regional drinking-water policies and actions; • provide a comprehensive preventive risk management framework for health protection, from catchment to consumer, that covers policy formulation and standard setting, risk-based management approaches and surveillance; • emphasize achievable practices and the formulation of sound regulations that are applicable to low-income, middle-income and industrialized countries alike; • summarize the health implications associated with contaminants in drinking-water, and the role of risk assessment and risk management in disease prevention and control; • summarize effective options for drinking-water management; and • provide guidance on hazard identification and risk assessment. The fourth edition of the Guidelines, including its updates, further develops concepts, approaches and information introduced in previous editions, such as the comprehensive preventive risk management approach for ensuring drinking-water quality that was introduced in the third edition. It considers: • drinking-water safety, including minimum procedures and specific guideline values, and how these are intended to be used; • approaches used in deriving the Guidelines, including guideline values; • microbial hazards, which continue to be the primary concern in both developing and developed countries. Experience has shown the value of a systematic approach to securing microbial safety. This edition builds on the preventive principles introduced in the third edition on ensuring the microbial safety of drinking-water through a multiple-barrier approach, highlighting the importance of source water protection; • climate change, which results in changing water temperature and rainfall patterns, severe and prolonged drought or increased flooding, and its implications for water quality and water scarcity, recognizing the importance of managing these impacts as part of water management strategies; 1 Up to 2017, the Guidelines incorporating the addenda were accompanied by separate addenda publications that detailed the updates made to the Guidelines. Separate addenda are no longer published, but will be referenced in the naming of the Guidelines between editions up to the fifth edition. • chemical contaminants in drinking-water, including information on chemicals not considered previously (e.g. pesticides used for vector control in drinking-water); revisions of existing chemical fact sheets, taking into account new scientific information; and reduced coverage in the Guidelines in cases where new information suggests a lesser priority; • key chemicals responsible for large-scale health effects through drinking-water exposure (e.g. arsenic, fluoride, lead, nitrate, selenium and uranium), with the Guidelines providing guidance on identifying local priorities and on management; • the important roles of many different stakeholders in ensuring drinking-water safety; this edition furthers the discussion introduced in the third edition of the roles and responsibilities of key stakeholders in ensuring drinking-water safety; and • guidance in situations other than traditional community supplies or managed utilities, such as rainwater harvesting and other non-piped supplies or dual-piped systems. The Guidelines are accompanied by a series of supporting publications. These include internationally peer-reviewed risk assessments for specific chemicals (see list of chapter 12 background documents in Annex 2) and other publications explaining the scientific basis of the development of the Guidelines and providing guidance on good practice in their implementation (see Annex 1). The publication Guidelines for drinking-water quality Volume 3—Surveillance and control of community supplies (1997, revision forthcoming) provides guidance on good practice in surveillance, monitoring and assessment of drinking-water quality in community supplies. Supporting publications have also informed the update of the Guidelines, including the updates to the fourth edition, and are referenced throughout. Key updates to the Guidelines in 2017 were: • new guidance on microbial risk assessment, aggregating multiple barriers for overall water treatment performance and microbial detection methods (chapter 7);• new or updated fact sheets for barium; bentazone; chlorine dioxide, chlorate and chlorite; dichlorvos; dicofol; diquat; MCPA; nitrate and nitrite; and perchlorate, with corresponding updates to guideline values or health-based values (chapter 12); and • additional guidance on risk management considerations and monitoring of lead (chapter 12).2 Key updates included in the current version of the Guidelines are: • clarification that manganese can be a concern in some areas because of the potential extent of exposure at concentrations of human health significance, considering the updated WHO guideline value (section 2.5.3);• updated information on the adequacy of water supply (section 5.3); 2 See Guidelines for drinking-water quality, fourth edition: first addendum (WHO, 2017) for the detailed list of changes made to the fourth edition of the Guidelines. • updated information on climate change, emergencies, and water used in food production and processing (chapter 6);• a new section on reuse of wastewater (chapter 6);• an explanation of reference values, which are a new type of value included in the Guidelines (section 8.2);• additional guidance on assessing chemical mixtures (section 8.2.8);• updated guidance on management of radionuclides, including interpretation and application of the WHO screening values and guidance levels, and management of radon (chapter 9);• updated information on cyanobacteria, including considerations for use of an alert level framework (sections 8.5.1, 10.1 and 11.5);• new or updated fact sheets for anatoxin-a variants, asbestos, bentazone, chromium, cylindrospermopsins, iodine, manganese, microcystins, nickel, organotins, saxitoxins, silver, tetrachloroethene and trichloroethene. These either reaffirm or update the guideline values and health-based values and, in some instances, establish reference values (chapter 12). Corresponding updates have been made to the chemical summary tables (chapter 8 and Annex 3), aesthetic considerations for manganese (section 10.2), factors influencing leaching of nickel in nickel-containing pipes and fittings (Annex 5), and analytical achievability and treatment performance tables for cyanobacteria, cyanotoxins and manganese (Annexes 4 and 5); and • updated references in chapters 1–3 and in the above-mentioned sections, WHO web links throughout the Guidelines and web links in Annex 1. The Guidelines are addressed primarily to water and health regulators, policymakers and their advisors, to assist in the development of national policies and regulations. The Guidelines and associated documents are also used by many others as a source of information on water quality and health, and on effective management approaches. The Guidelines are recognized as representing the position of the UN system on issues of drinking-water quality and health by “UN-Water”, the body that coordinates among the 24 UN agencies and programmes concerned with water issues. Acknowledgements TThe preparation of the fourth edition of the Guidelines for drinking-water quality, the updates to the fourth edition and supporting documentation covered a period of more than 10 years. It involved the participation of hundreds of experts from a wide range of developing and developed countries. The contributions of all who participated in the preparation and finalization of the fourth edition and the updates to the fourth edition, including those individuals listed in Annex 7, are gratefully acknowledged. The work of the following working group coordinators and other Drinking-water Quality Committee members was crucial to the development of the fourth edition: Dr F. Ahmed, Bangladesh University of Engineering and Technology, Bangladesh (Small systems) Dr I. Chorus, Federal Environment Agency, Germany (Resource and source protection) Dr J. Cotruvo, Joseph Cotruvo & Associates/NSF International Collaborating Centre, United States of America (USA) (Materials and chemicals used in the production and distribution of drinking-water) Dr D. Cunliffe, Department of Health, Australia (Public health) Dr A.M. de Roda Husman, National Institute for Public Health and the Environment (RIVM), the Netherlands (Viruses and risk assessment) Dr T. Endo, Ministry of Health, Labour and Welfare, Japan (Parasites) Mr J.K. Fawell, Independent Consultant, the United Kingdom of Great Britain and Northern Ireland (Naturally occurring and industrial contaminants and Pesticides) Ms M. Giddings, Health Canada, Canada (Disinfectants and disinfection byproducts) Dr G. Howard, British High Commission, India (Monitoring and assessment) Mr P. Jackson, WRc-NSF Ltd, United Kingdom (Chemicals – Practical aspects) Dr S. Kumar, University of Malaya, Malaysia (Protozoa and risk management) Dr S. Kunikane, Institute for Environmental Sciences, Japan (Operations and Maintenance Network) Professor Y. Magara, Hokkaido University, Japan (Analytical aspects) Dr A.V.F. Ngowi, Muhimbili University of Health and Allied Sciences, United Republic of Tanzania (Pesticides) Dr E. Ohanian, Environmental Protection Agency, USA (Disinfectants and disinfection by-products) Dr C.N. Ong, National University of Singapore, Singapore (Emerging chemical hazards) Mr O. Schmoll, Federal Environment Agency, Germany (Water safety plan capacity building and monitoring) Professor M. Sobsey, University of North Carolina, USA (Risk management) The WHO coordinator was Mr B. Gordon, WHO headquarters, with support from Mr P. Callan from the National Health and Medical Research Council, Australia. Ms C. Vickers and Dr A. Tritscher provided important liaisons with the international chemical risk assessment programmes at WHO headquarters. Dr M. Perez contributed on behalf of the Radiation and Environmental Health Programme, WHO headquarters. Dr M. Zaim, Pesticide Evaluation Scheme, WHO headquarters, provided input on pesticides added to drinking-water for public health purposes. The Coordinator of Water, Sanitation, Hygiene and Health, WHO headquarters (formerly Jamie Bartram and, since 2009, Robert Bos), provided strategic direction throughout the process. With reference to the updates to the fourth edition, the following experts contributed in the Guideline Development Group or chemical, microbial or protection and control working groups, supporting the development and finalization of the addenda: Dr D. Cunliffe (Chair), Dr S.H. Abedelrahman, Dr M. Asami, Dr R. Bevan, Mrs J. Brown, Mr E. Calderon, Mr R. Carrier, Dr I. Chorus, Dr J. Cotruvo, Dr. L. d’Anglada, Dr A.M. de Roda Husman, Dr A. Eckhardt, Professor J. Fawell, Ms M. Giddings, Dr A. Hirose, Dr A. Humpage, Dr P. Hunter, Dr P. Labhasetwar, Professor K. Linden, Dr P. Marsden, Dr Y. Matsui, Dr G. Medema, Dr M.E. Meek, Dr E. Ohanian, Professor C.N. Ong, Dr S. Ramasamy, Professor S. Snyder, Dr J. Strong, Professor M. Sobsey and Dr E. Testai. The WHO Steering Group for the updates included: Mr H. Bakir, Mr R. Brown, Ms J. De France, Mr B. Gordon, Ms Payden, Dr M. Perez, Dr A. Prüss-Üstün, Mr O. Schmoll, Dr J. Simon, Dr P. Verger and Dr R. Yadav. The contributions from additional WHO staff are also acknowledged: Dr R. Alemam, Dr M. Bagayoko, Dr S. Boisson, Dr N. Hassan, Dr S. Madsen, Dr G. Mbayo, Dr T. Monteiro, Dr G. Peralta, Dr K. Petersen, Dr H. Rasheed, Dr P. Segurado, Dr A. Tritscher, and Ms C. Vickers. The coordinator for the updates was Ms J. De France, WHO headquarters, with support from Mr P. Callan, Australia, for the first addendum. Strategic direction was provided by Mr B. Gordon, WHO headquarters. Many individuals from various countries contributed to the development of the Guidelines. The efforts of all who contributed to the preparation of this document and in particular those who provided peer or public domain review comments are greatly appreciated. The generous financial and technical support of the following is gratefully acknowledged: Agence Française de Développement of France; Department of Foreign Affairs and Trade of Australia; Federal Environment Agency of Germany; Health Canada; Ministry of Health, Labour and Welfare of Japan; Ministry of Environment and Water Resources, Singapore; Ministry of Development Cooperation and Humanitarian Affairs of Luxembourg; Ministry of Foreign Affairs of the Netherlands; Norwegian Agency for Development Cooperation; United Kingdom Foreign, Commonwealth & Development Office; and United States Environmental Protection Agency. Abbreviations used in text 2,4-D 2,4-dichlorophenoxyacetic acid 2,4-DB 2,4-dichlorophenoxybutyric acid 2,4-DP dichlorprop 2,4,5-T 2,4,5-trichlorophenoxyacetic acid 2,4,5-TP 2,4,5-trichlorophenoxy propionic acid; fenoprop AAS atomic absorption spectrometry Absor absorptiometry ADI acceptable daily intake AES atomic emission spectrometry AIDS acquired immunodeficiency syndrome AMPA aminomethylphosphonic acid ARfD acute reference dose ATX anatoxin BDCM bromodichloromethane BMD benchmark dose BMDL lower confidence limit on the benchmark dose BMDLx lower 95% confidence limit on the benchmark dose for an x% response BTEX benzene, toluene, ethylbenzene and xylenes Bti Bacillus thuringiensis israelensis bw body weight CAS Chemical Abstracts Service Col colorimetry CPVC chlorinated polyvinyl chloride CSAF chemical-specific adjustment factor Ct product of disinfectant concentration and contact time CYN cylindrospermopsin DAEC diffusely adherent E. coli DALY disability-adjusted life year DBCM dibromochloromethane DBCP 1,2-dibromo-3-chloropropane DBP disinfection by-product DCA dichloroacetic acid DCB dichlorobenzene DCP dichloropropane DDT dichlorodiphenyltrichloroethane DEHA di(2-ethylhexyl)adipate DEHP di(2-ethylhexyl)phthalate DNA deoxyribonucleic acid DPD N,N-diethyl-1,4-phenylenediamine sulfate EAAS electrothermal atomic absorption spectrometry EAEC enteroaggregative E. coli ECD electron capture detector EDTA ethylenediaminetetraacetic acid; edetic acid EHEC enterohaemorrhagic E. coli EIEC enteroinvasive E. coli ELISA enzyme-linked immunosorbent assay EPEC enteropathogenic E. coli ETEC enterotoxigenic E. coli F0 parental generation F1 first filial generation FAAS flame atomic absorption spectrometry FAO Food and Agriculture Organization of the United Nations FD fluorescence detector FID flame ionization detector FPD flame photodiode detector GAC granular activated carbon GC gas chromatography GL guidance level (used for radionuclides in drinking-water) GV guideline value HAA haloacetic acid HAV hepatitis A virus HCB hexachlorobenzene HCBD hexachlorobutadiene HCH hexachlorocyclohexane HEV hepatitis E virus HIV human immunodeficiency virus HPC heterotrophic plate count HPLC high-performance liquid chromatography IARC International Agency for Research on Cancer IC ion chromatography ICP inductively coupled plasma ICRP International Commission on Radiological Protection IDC individual dose criterion IPCS International Programme on Chemical Safety IQ intelligence quotient ISO International Organization for Standardization JECFA Joint FAO/WHO Expert Committee on Food Additives JMPR Joint FAO/WHO Meeting on Pesticide Residues LC liquid chromatography LOAEL lowest-observed-adverse-effect level LRV log10 reduction value MC microcystin MCB monochlorobenzene MCPA 4-(2-methyl-4-chlorophenoxy)acetic acid MCPB 2,4-MCPB; 4-(4-chloro-o-tolyloxy)butyric acid; 4-(4-chloro2-methylphenoxy)butanoic acid MCPP 2(2-methyl-chlorophenoxy) propionic acid; mecoprop MDL method detection limit MMT methylcyclopentadienyl manganese tricarbonyl MS mass spectrometry MS/MS tandem mass spectrometry MTBE methyl tertiary-butyl ether MX 3-chloro-4-dichloromethyl-5-hydroxy-2(5H)-furanone NDMA N-nitrosodimethylamine NOAEL no-observed-adverse-effect level NOEL no-observed-effect level NTA nitrilotriacetic acid NTP National Toxicology Program (USA) NTU nephelometric turbidity unit PAC powdered activated carbon PAH polynuclear aromatic hydrocarbon PCE tetrachloroethene PCP pentachlorophenol PCR polymerase chain reaction PD photoionization detector PDA photodiode array PMTDI provisional maximum tolerable daily intake PPA protein phosphatase assay PT purge and trap PTDI provisional tolerable daily intake PTMI provisional tolerable monthly intake PTWI provisional tolerable weekly intake PVC polyvinyl chloride QMRA quantitative microbial risk assessment RNA ribonucleic acid SI Système international d’unités (International System of Units) SODIS solar water disinfection STX saxitoxin sp. species (singular) spp. species (plural) subsp. subspecies (singular) TBA terbuthylazine TCB trichlorobenzene TCE trichloroethene TCU true colour unit TD05 tumorigenic dose05, the dose associated with a 5% excess incidence of tumours in experimental animal studies TDI tolerable daily intake TDS total dissolved solids THM trihalomethane TID thermal ionization detector; total indicative dose UF uncertainty factor UN United Nations UNICEF United Nations Children’s Fund UNSCEAR United Nations Scientific Committee on the Effects of Atomic Radiation USA United States of America UV ultraviolet UVPAD ultraviolet photodiode array detector WHO World Health Organization WHOPES World Health Organization Pesticide Evaluation Scheme WSP water safety plan YLD years of healthy life lost in states of less than full health (i.e. years lived with a disability) YLL years of life lost by premature mortality 1 Introduction TThe primary purpose of the Guidelines for drinking-water quality is the protection of public health. The Guidelines provide the recommendations of the World Health Organization (WHO) for managing the risk from hazards that may compromise the safety of drinking-water. The recommendations should be considered in the context of managing the risk from other sources of exposure to these hazards, such as waste, air, food and consumer products. 1.1 General considerations and principles Water is essential to sustain life, and a satisfactory (adequate, safe and accessible) supply must be available to all. Improving access to safe drinking-water can result in tangible benefits to health. Every effort should be made to achieve drinking-water that is as safe as practicable. Safe drinking-water, as defined by the Guidelines, does not represent any significant risk to health over a lifetime of consumption, including different sensitivities that may occur between life stages. Those at greatest risk of waterborne disease are infants and young children, people who are debilitated and the elderly, especially when living 1 under unsanitary conditions. Those who are generally at risk of waterborne illness Diseases related to contamination of drinking‑water constitute a major burden may need to take additional steps to pro-on human health. Interventions to imtect themselves against exposure to water-prove the quality of drinking‑water pro‑borne pathogens, such as boiling their vide significant benefits to health. drinking-water. Safe drinking-water is required for all usual domestic purposes, including drinking, food preparation and personal hygiene. The Guidelines are applicable to packaged water and ice intended for human consumption. However, water of higher quality may be required for some special purposes, such as renal dialysis and cleaning of contact lenses, or for certain purposes in food production and pharmaceutical use. The Guidelines may not be suitable for the protection of aquatic life or for some industries. The Guidelines are intended to support the development and implementation of risk management strategies that will ensure the safety of drinking-water supplies through the control of hazardous constituents of water. These strategies may include national or regional standards developed from the scientific basis provided in the Guidelines. The Guidelines describe reasonable minimum requirements of safe practice to protect the health of consumers and derive numerical “guideline values” for constituents of water or indicators of water quality. When defining mandatory limits, it is preferable to consider the Guidelines in the context of local or national environmental, social, economic and cultural conditions. The Guidelines should also be part of an overall health protection strategy that includes sanitation and other strategies, such as managing food contamination. This strategy would also normally be incorporated into a legislative and regulatory framework that adapts the Guidelines to address local requirements and circumstances (see also section 2.6). The main reason for not promoting the adoption of international standards for drinking-water quality is the advantage provided by the use of a risk–benefit approach (qualitative or quantitative) in the establishment of national standards and regulations. Further, the Guidelines are best used to promote an integrated preventive management framework for safety applied from catchment to consumer. The Guidelines provide a scientific point of departure for national authorities to develop drinking-water regulations and standards appropriate for the national situation. In developing standards and regulations, care should be taken to ensure that scarce resources are not unnecessarily diverted to the development of standards and the monitoring of substances of relatively minor importance to public health. The approach followed in these Guidelines is intended to lead to national standards and regulations that can be readily implemented and enforced and are protective of public health. The nature and form of drinking-water standards may vary among countries and regions. There is no single approach that is universally applicable. It is essential in the development and implementation of standards that the current or planned legislation relating to water, health and local government is taken into account and that the capacity of regulators in the country is assessed. Approaches that may work in one country or region will not necessarily transfer to other countries or regions. It is essential that each country review its needs and capacities in developing a regulatory framework. The judgement of safety—or what is an acceptable level of risk in particular circumstances—is a matter in which society as a whole has a role to play. The final judgement as to whether the benefit resulting from the adoption of any of the Guidelines or guideline values as national or local standards justifies the cost is for each country to decide. Although the Guidelines describe a quality of water that is acceptable for lifelong consumption, the establishment of these Guidelines, including guideline values, should not be regarded as implying that the quality of drinking-water may be degraded to the recommended level. Indeed, a continuous effort should be made to maintain drinking-water quality at the highest possible level. An important concept in the allocation of resources to improving drinking-water safety is that of incremental improvement towards long-term health-based targets. Priorities set to remedy the most urgent problems (e.g. protection from patho-An important concept in the allocation gens; see section 1.1.2) may be linked to of resources to improving drinking‑water long-term targets of further water qual-safety is that of incremental improvement ity improvements (e.g. improvements in towards long‑term water quality targets. the acceptability of drinking-water in terms of its taste, odour and appearance; see section 1.1.6). 1.1.1 Framework for safe drinking-water The basic and essential requirements to ensure the safety of drinking-water are a “framework” for safe drinking-water, comprising health-based targets established by a competent health authority, adequate and properly managed systems (adequate infrastructure, proper monitoring and effective planning and management) and a system of independent surveillance. A holistic approach to the risk assessment and risk management of a drinking-water supply increases confidence in the safety of the drinking-water. This approach entails systematic assessment of risks throughout a drinking-water supply—from the catchment and its source water through to the consumer—and identification of the ways in which these risks can be managed, including In Stockholm, in 1999, it was agreed that future guidelines for methods to ensure that condrinking‑water, wastewater and recreational water1 should trol measures are working integrate assessment of risk, risk management options and effectively. It incorporates exposure control elements within a single framework with embedded quality targets (see the supporting document strategies to deal with day-Water quality—Guidelines, standards and health; Annex 1).to-day management of Following this approach, the assessment of risk is not a goal water quality, including up-in its own right, but rather a basis for decision‑making. The sets and failures. In this re-framework for safe drinking‑water and the recommended approach for regulations, policies and programmes are spect, climate change—in based on this overall framework, known as the Stockholm Framework (see chapter 2). 1 See WHO (2006) and WHO (2021), respectively, for Guidelines for the safe use of wastewater, excreta and greywater in agriculture and aquaculture, Volumes 1–4 and Guidelines on recreational water quality. WHO advice on the safe management of excreta, which is a primary source of contamination of drinking-water, is covered in the WHO Guidelines on sanitation and health (WHO, 2018). the form of increased and more severe periods of drought or more intense rainfall events leading to flooding—can have an impact on both the quality and the quantity of water and will require planning and management to minimize adverse impacts on drinking-water supplies. Climate change also needs to be considered in the light of demographic change, such as the continuing growth of cities, which itself brings significant challenges for drinking-water supply. In support of the framework for safe drinking-water, the Guidelines provide a range of supporting information, including microbial aspects (chapters 7 and 11), chemical aspects (chapters 8 and 12), radiological aspects (chapter 9) and acceptability aspects (chapter 10). Figure 1.1 provides an overview of the interrelationships among the individual chapters of the Guidelines in ensuring drinking-water safety. The Guidelines are applicable to large metropolitan and small community piped drinking-water systems and to non-piped drinking-water systems in communities and in individual dwellings. The Guidelines are also applicable to a range of specific circumstances (chapter 6), including buildings, travellers and conveyances. 1.1.2 Microbial aspects Securing the microbial safety of drinking-water supplies is based on the use of multiple barriers, from catchment to consumer, to prevent the contamination of drinking-water or to reduce contamination to levels not injurious to health. Safety is increased if multiple barriers are in place, including protection of water resources, proper selection and operation of a series of treatment steps and management of distribution systems (piped or otherwise) to maintain and protect treated water quality. The preferred strategy is a management approach that places the primary emphasis on preventing or reducing the entry of pathogens into water sources and reducing reliance on treatment processes for removal of pathogens. In general terms, the greatest microbial risks are associated with ingestion of water that is contaminated with faeces from humans or animals (including birds). Faeces can be a source of pathogenic bacteria, viruses, protozoa and helminths. Faecally derived pathogens are the principal concerns in setting health-based targets for microbial safety. Microbial water quality often varies rapidly and over a wide range. Short-term The potential health consepeaks in pathogen concentration may increase disease quences of microbial con‑risks considerably and may trigger outbreaks of water-tamination are such that borne disease. Furthermore, by the time microbial its control must always be of paramount importance contamination is detected, many people may have and must never be com‑been exposed. For these reasons, reliance cannot be promised. placed solely on end-product testing, even when frequent, to determine the microbial safety of drinking-water. Particular attention should be directed to a water safety framework and implementing comprehensive water safety plans to consistently ensure drinking-water safety and thereby protect public health (see chapter 4). Failure to ensure drinking-water safety may expose the community to the risk of outbreaks of intestinal and other infectious diseases. Outbreaks of waterborne disease are particularly to be avoided Figure 1.1 Interrelationships among the individual chapters of the Guidelines for drinking-water quality in ensuring drinking-water safety because of their capacity to result in the simultaneous infection of a large number of persons and potentially a high proportion of the community. In addition to faecally borne pathogens, other microbial hazards, such as guinea worm (Dracunculus medinensis), toxic cyanobacteria and Legionella, may be of public health importance under specific circumstances. Although water can be a very significant source of infectious organisms, many of the diseases that may be waterborne may also be transmitted by other routes, including person-to-person contact, food intake and droplets and aerosols. Depending on the circumstances and in the absence of waterborne outbreaks, these routes may be more important than waterborne transmission. Microbial aspects of water quality are considered in more detail in chapter 7, with fact sheets on specific microorganisms provided in chapter 11. 1.1.3 Disinfection Disinfection is of unquestionable importance in the supply of safe drinking-water. The destruction of pathogenic microorganisms is essential and very commonly involves the use of reactive chemical agents such as chlorine. Disinfection is an effective barrier to many pathogens (especially bacteria) during drinking-water treatment and should be used for surface waters and for groundwater subject to faecal contamination. Residual disinfection is used to provide a partial safeguard against low-level contamination and growth within the distribution system. Chemical disinfection of a drinking-water supply that is faecally contaminated will reduce the overall risk of disease but may not necessarily render the supply safe. For example, chlorine disinfection of drinking-water has limitations against the protozoan pathogens—in particular Cryptosporidium—and some viruses. Disinfection efficacy may also be unsatisfactory against pathogens within flocs or particles, which protect them from the action of disinfectants. High levels of turbidity can protect microorganisms from the effects of disinfection, stimulate the growth of bacteria and give rise to a significant chlorine demand. It is essential that an overall management strategy is implemented in which multiple barriers, including source water protection and appropriate treatment processes, as well as protection during storage and distribution, are used in conjunction with disinfection to prevent or remove microbial contamination. The use of chemical disinfectants in water treatment usually results in the formation of chemical by-products. However, the risks to health from these by-products Disinfection should not be compromised are extremely small in comparison with the in attempting to control disinfection by‑risks associated with inadequate disinfec-products. tion, and it is important that disinfection efficacy not be compromised in attempting to control such by-products. Some disinfectants, such as chlorine, can be easily monitored and controlled as a drinking-water disinfectant, and frequent monitoring is recommended wherever chlorination is practised. Disinfection of drinking-water is considered in more detail in chapter 7 and Annex 5, with fact sheets on specific disinfectants and disinfection by-products provided in chapter 12. 1.1.4 Chemical aspects The health concerns associated with chemical constituents of drinking-water differ from those associated with microbial contamination and arise primarily from the ability of chemical constituents to cause adverse health effects after The great majority of evident water‑related health problems are the result of microbial (bacterial, viral, protozoan or other bio‑prolonged periods of logical) contamination. Nevertheless, an appreciable number of exposure. There are few serious health concerns may occur as a result of the chemical chemical constituents contamination of drinking‑water. of water that can lead to health problems resulting from a single exposure, except through massive accidental contamination of a drinking-water supply. Moreover, experience shows that in many, but not all, such incidents, the water becomes undrinkable owing to unacceptable taste, odour and appearance. In situations where short-term exposure is not likely to lead to health impairment, it is often most effective to concentrate the available resources for remedial action on finding and eliminating the source of contamination, rather than on installing expensive drinking-water treatment for the removal of the chemical constituent. There are many chemicals that may occur in drinking-water; however, only a few are of immediate health concern in any given circumstance. The priority given to both monitoring and remedial action for chemical contaminants in drinking-water should be managed to ensure that scarce resources are not unnecessarily directed towards those of little or no health concern (see the supporting documents Chemical safety of drinking-water and Developing drinking-water quality regulations and standards; Annex 1). There are few chemicals for which the contribution from drinking-water to overall intake is an important factor in preventing disease. One example is the effect of fluoride in drinking-water in protecting against dental caries. The Guidelines do not attempt to define minimum desirable concentrations for chemicals in drinking-water. Guideline values are derived for many chemical constituents of drinking-water. A guideline value normally represents the concentration of a constituent that does not result in any significant risk to health over a lifetime of consumption. A number of provisional guideline values have been established based on the practical level of treatment performance or analytical achievability. In these cases, the guideline value is higher than the calculated health-based value. The chemical aspects of drinking-water quality are considered in more detail in chapter 8, with fact sheets on specific chemical contaminants provided in chapter 12. 1.1.5 Radiological aspects The health risks associated with the presence of naturally occurring radionuclides in drinking-water should also be taken into consideration, although the contribution of drinking-water to total exposure to radionuclides is very small under normal circumstances. Formal guideline values are not set for individual radionuclides in drinking-water. Rather, the approach used is based on screening drinking-water for gross alpha and gross beta radiation activity. Although finding levels of activity above screening values does not indicate any immediate risk to health, it should trigger further investigation to determine the radionuclides responsible and the possible risks, taking local circumstances into account. The guidance levels for radionuclides recommended in these Guidelines do not apply to drinking-water supplies contaminated during emergencies arising from accidental releases of radioactive substances to the environment. Radiological aspects of drinking-water quality are considered in more detail in chapter 9. 1.1.6 Acceptability aspects: taste, odour and appearance Water should be free of tastes and odours that would be objectionable to the majority of consumers. In assessing the quality of drinking-water, consumers rely principally upon their senses. Microbial, chemical and physical constituents of water may affect the appearance, odour or taste of the water, and the consumer will evaluate the quality and acceptability of the water on the basis of these criteria. Although these constituents may have no direct health effects, water that is highly turbid, is highly coloured or has an objectionable taste or odour may be regarded by consumers as unsafe and rejected. In extreme cases, consumers may avoid aesthetically unacceptable but otherwise safe drinking-water in favour of more pleasant but potentially unsafe sources. It is therefore wise to be aware of consumer perceptions and to take into account both health-related guideline values and aesthetic criteria when assessing drinking-water supplies and developing regulations and standards. Changes in the normal appearance, taste or odour of a drinking-water supply may signal changes in the quality of the raw water source or deficiencies in the treatment process and should be investigated. Acceptability aspects of drinking-water quality are considered in more detail in chapter 10. 1.2 Roles and responsibilities in drinking-water safety management Preventive management is the preferred approach to ensuring drinking-water safety and should take account of the characteristics of the A preventive integrated management approach withdrinking-water supply from collaboration from all relevant agencies is the preferred catchment and source to its approach to ensuring drinking‑water safety use by consumers. As many aspects of drinking-water quality management are often outside the direct responsibility of the water supplier, it is essential that a collaborative multiagency approach be adopted to ensure that agencies with responsibility for specific areas within the water cycle are involved in the management of water quality. One example is where catchments and source waters are beyond the drinking-water supplier’s jurisdiction. Consultation with other authorities will generally be necessary for other elements of drinking-water quality management, such as monitoring and reporting requirements, emergency response plans and communication strategies. Major stakeholders that could affect or be affected by decisions or activities of the drinking-water supplier should be encouraged to coordinate their planning and management activities where appropriate. These could include, for example, health and resource management agencies, consumers, industry and plumbers. Appropriate mechanisms and documentation should be established for stakeholder commitment and involvement. 1.2.1 Surveillance and quality control In order to protect public health, a dual-role approach, differentiating the roles and responsibilities of service providers from those of an authority responsible for independent oversight protective of public health (“drinking-water supply surveillance”), has proven to be effective. Organizational arrangements for the maintenance and improvement of drinking-water supply services should therefore take into account the vital and complementary roles of the agency responsible for surveillance and of Drinking‑water suppliers are responsible at all times for the quality and safety of the water that they produce the water supplier. The two functions of surveillance and quality control are best performed by separate and independent entities because of the conflict of interest that arises when the two are combined. In this: • national agencies provide a framework of targets, standards and legislation to enable and require suppliers to meet defined obligations; • agencies involved in supplying water for consumption by any means should be required to ensure and verify that the systems they administer are capable of delivering safe water and that they routinely achieve this; • a surveillance agency is responsible for independent (external) surveillance through periodic audit of all aspects of safety and/or verification testing. In practice, there may not always be a clear division of responsibilities between the surveillance and drinking-water supply agencies. In some cases, the range of professional, governmental, nongovernmental and private institutions may be wider and more complex than that discussed above. Whatever the existing framework, it is important that clear strategies and structures be developed for implementing water safety plans, quality control and surveillance, collating and summarizing data, reporting and disseminating the findings and taking remedial action. Clear lines of accountability and communication are essential. Surveillance is an investigative activity undertaken to identify and evaluate potential health risks associated with drinking-water. Surveillance contributes to the protection of Surveillance of drinking‑water public health by promoting improvement of the quality can be defined as “the quality, quantity, accessibility, coverage (i.e. popu-continuous and vigilant public health assessment and review lations with reliable access), affordability and of the safety and acceptabilcontinuity of drinking-water supplies (termed ity of drinking‑water supplies” “service indicators”). The surveillance authority (WHO, 1976). must have the authority to determine whether a water supplier is fulfilling its obligations. In most countries, the agency responsible for the surveillance of drinking-water supply services is the ministry of health (or public health) and its regional or departmental offices. In some countries, it may be an environmental protection agency; in others, the environmental health departments of local government may have some responsibility. Surveillance requires a systematic programme of surveys, which may include auditing, analysis, sanitary inspection and institutional and community aspects. It should cover the whole of the drinking-water system, including sources and activities in the catchment, transmission infrastructure, treatment plants, storage reservoirs and distribution systems (whether piped or unpiped). Ensuring timely action to prevent problems and ensure the correction of faults should be one aim of a surveillance programme. There may at times be a need for penalties to encourage and ensure compliance. The surveillance agency must therefore be supported by strong and enforceable legislation. However, it is important that the agency develops a positive and supportive relationship with suppliers, with the application of penalties used as a last resort. The surveillance agency should be empowered by law to compel water suppliers to recommend the boiling of water or other measures when microbial contamination that could threaten public health is detected. 1.2.2 Public health authorities In order to effectively support the protection of public health, a national entity with responsibility for public health will normally act in four areas: 1) surveillance of health status and trends, including outbreak detection and investigation, generally directly but in some instances through a decentralized body; 2) directly establishing drinking-water norms and standards. National public health authorities often have the primary responsibility for setting norms on drinking-water supply, which may include the setting of water quality targets, performance and safety targets and directly specified requirements (e.g. treatment). Normative activity is not restricted to water quality but also includes, for example, regulation and approval of materials and chemicals used in the production and distribution of drinking-water (see section 8.5.4) and establishing minimum standards in areas such as domestic plumbing (see section 1.2.10). Nor is it a static activity, because as changes occur in drinking-water supply practice, in technologies and in materials available (e.g. in plumbing materials and treatment processes), so health priorities and responses to them will also change; 3) representing health concerns in wider policy development, especially health policy and integrated water resource management (see section 1.2.4). Health concerns will often suggest a supportive role towards resource allocation to those concerned with drinking-water supply extension and improvement, will often involve lobbying for the primary requirement to satisfy drinking-water needs above other priorities and may imply involvement in conflict resolution; 4) direct action, generally through subsidiary bodies (e.g. regional and local environmental health administrations) or by providing guidance to other local entities (e.g. local government) in surveillance of drinking-water supplies. These roles vary widely according to national and local structures and responsibilities and frequently include a supportive role to community suppliers, where local authorities often intervene directly. Public health surveillance (i.e. surveillance of health status and trends) contributes to verifying drinking-water safety. It takes into consideration disease in the entire population, which may be exposed to pathogenic microorganisms from a range of sources, not only drinking-water. National public health authorities may also undertake or direct research to evaluate the role of water as a risk factor in disease, through case–control, cohort or intervention studies, for example. Public health surveillance teams typically operate at national, regional and local levels, as well as in cities and rural health centres. Routine surveillance includes: • ongoing monitoring of reportable diseases, many of which can be caused by waterborne pathogens; • outbreak detection; • long-term trend analysis; • geographic and demographic analysis; • feedback to water authorities. Public health surveillance can be enhanced in a variety of ways to identify possible waterborne outbreaks in response to suspicion about unusual disease incidence or following deterioration of water quality. Epidemiological investigations include: • outbreak investigations; • intervention studies to evaluate intervention options; • case–control or cohort studies to evaluate the role of water as a risk factor in disease. However, public health surveillance cannot be relied upon to provide information in a timely manner to enable short-term operational response to control waterborne disease. Limitations include: • outbreaks of non-reportable disease; • time delay between exposure and illness; • time delay between illness and reporting; • low level of reporting; • difficulties in identifying causative pathogens and sources. The public health authority operates reactively, as well as proactively, against the background of overall public health policy and in interaction with all stakeholders. In accounting for public health context, priority will normally be afforded to disadvantaged groups. This will generally entail balancing drinking-water safety management and improvement with the need to ensure access to reliable supplies of safe drinking-water in adequate quantities. In order to develop an understanding of the national drinking-water situation, the national public health authority should periodically produce reports outlining the state of national water quality and highlighting public health concerns and priorities in the context of overall public health priorities. This implies the need for effective exchange of information between local, regional and national agencies. National health authorities should lead or participate in the formulation and implementation of policy to ensure access to some form of reliable, safe drinking-water supply. Where this has not been achieved, appropriate tools and education should be made available to implement individual or household-level treatment and safe storage. 1.2.3 Local authorities Local environmental health authorities often play an important role in managing water resources and drinking-water supplies. This may include catchment inspection and authorization of activities in the catchment that may have an impact on source water quality. It can also include verifying and auditing (surveillance) of the management of formal drinking-water systems. Local environmental health authorities will also give specific guidance to communities or individuals in designing and implementing community and household drinking-water systems and correcting deficiencies, and they may also be responsible for surveillance of community and household drinking-water supplies. They have an important role to play in educating consumers where household water treatment is necessary. Management of household and small community drinking-water supplies generally requires education programmes about drinking-water supply and water quality. Such programmes should normally include: • water hygiene awareness raising; • basic technical training and technology transfer in drinking-water supply and management; • consideration of and approaches to overcoming sociocultural barriers to acceptance of water quality interventions; • motivation, mobilization and social marketing activities; • a system of continued support, follow-up and dissemination of the water quality programme to achieve and maintain sustainability. These programmes can be administered at the community level by local health authorities or other entities, such as nongovernmental organizations and the private sector. If the programme arises from other entities, the involvement of the local health authority in the development and implementation of the water quality education and training programme is strongly encouraged. Behaviour change approaches for sanitation and hygiene to be implemented by local authorities are summarized in Table 5.1 of the WHO Guidelines on sanitation and health (WHO, 2018). 1.2.4 Water resource management Water resource management is an integral aspect of the preventive management of drinking-water quality. Prevention of microbial and chemical contamination of source water is the first barrier against drinking-water contamination of public health concern. Water resource management and potentially polluting human activity in the catchment will influence water quality downstream and in aquifers. This will have an impact on the treatment steps required to ensure safe water, and preventive action may be preferable to upgrading treatment. The influence of land use on water quality should be assessed as part of water resource management. This assessment is not normally undertaken by health authorities or drinking-water supply agencies alone and should take into consideration: • land cover modification; • extraction activities; • construction/modification of waterways; • application of fertilizers, herbicides, pesticides and other chemicals; • livestock density and application of manure; • road construction, maintenance and use; • various forms of recreation; • urban or rural residential development, with particular attention to excreta disposal, sanitation, landfill and waste disposal; • other potentially polluting human activities, such as industry, mining and military sites. Water resource management may be the responsibility of catchment management agencies and/or other entities controlling or affecting water resources, such as industrial, agricultural, navigation and flood control entities. The extent to which the responsibilities of health or drinking-water supply agencies include water resource management varies greatly between countries and communities. Regardless of government structures and sector responsibilities, it is important that health authorities liaise and collaborate with sectors managing the water resource and regulating land use in the catchment. Establishing close collaboration between the public health authority, water supplier and resource management agency assists recognition of the health hazards potentially occurring in the system. It is also important for ensuring that the protection of drinking-water resources is considered in decisions for land use or regulations to control contamination of water resources. Depending on the setting, this may include involvement of further sectors, such as agriculture, traffic, tourism or urban development. To ensure the adequate protection of drinking-water sources, national authorities will normally interact with other sectors in formulating national policy for integrated water resource management. Regional and local structures for implementing the policy will be set up, and national authorities will guide regional and local authorities by providing tools. Regional environmental or public health authorities have an important task in participating in the preparation of integrated water resource management plans to ensure the best available drinking-water source quality. For further information, see the supporting documents Protecting groundwater for health and Protecting surface water for health (Annex 1). 1.2.5 Drinking-water supply agencies Drinking-water supplies vary from very large urban systems servicing large populations with tens of millions of people to small community systems providing water to very small populations. In most countries, they include community sources as well as piped means of supply. Drinking-water supply agencies are responsible for quality assurance and quality control (see section 1.2.1). Their key responsibilities are to prepare and implement water safety plans (for more information, see chapter 4). In many cases, the water supplier is not responsible for the management of the catchment feeding the sources of its supplies. The roles of the water supplier with respect to catchments are to participate in interagency water resource management activities, to understand the risks arising from potentially contaminating activities and incidents and to use this information in assessing risks to the drinking-water supply and developing and applying appropriate management. Although drinking-water suppliers may not undertake catchment surveys and pollution risk assessment alone, their role is to recognize the need for them and to initiate multiagency collaboration— for example, with health and environmental authorities. Experience has shown that an association of stakeholders in drinking-water supply (e.g. operators, managers and specialist groups such as small suppliers, scientists, sociologists, legislators and politicians) can provide a valuable non-threatening forum for the interchange of ideas. For further information, see the supporting document Water safety plans (Annex 1). 1.2.6 Community management Community-managed drinking-water systems, with both piped and non-piped distribution, are common worldwide in both developed and developing countries. The precise definition of a community drinking-water system will vary. Although a definition based on population size or the type of supply may be appropriate under many conditions, approaches to administration and management provide a distinction between the drinking-water systems of small communities and those of larger towns and cities. This includes the increased reliance on often untrained and sometimes unpaid community members in the administration and operation of community drinking-water systems. Drinking-water systems in periurban areas—the communities surrounding major towns and cities—in developing countries may also have the characteristics of community systems. Effective and sustainable programmes for the management of community drinking-water quality require the active support and involvement of local communities. These communities should be involved at all stages of such programmes, including initial surveys; decisions on siting of wells, siting of intakes or establishing protection zones; monitoring and surveillance of drinking-water supplies; reporting faults, carrying out maintenance and taking remedial action; and supportive actions, including sanitation and hygiene practices. A community may already be highly organized and taking action on health or drinking-water supply issues. Alternatively, it may lack a well-developed drinking-water system; some sectors of the community, such as women, may be poorly represented; and there may be disagreements or factional conflicts. In these situations, achieving community participation will take more time and effort to bring people together, resolve differences, agree on common aims and take action. Visits, possibly over several years, will often be needed to provide support and encouragement and to ensure that the structures created for safe drinking-water supply continue to operate. This may involve setting up hygiene and health educational programmes to ensure that the community: • is aware of the importance of drinking-water quality and its relationship with health and of the need for safe drinking-water in sufficient quantities for domestic use for drinking, cooking and hygiene; • recognizes the importance of surveillance and the need for a community response; • understands and is prepared to play its role in the surveillance process; • has the necessary skills to perform that role; • is aware of requirements for the protection of drinking-water supplies from pollution. For further information, see the 1997 volume entitled Surveillance and control of community supplies (WHO, 1997); the supporting document Water safety planning for small community water supplies (Annex 1); Simpson-Hébert, Sawyer & Clarke (1996); Sawyer, Simpson-Hébert & Wood (1998); and Brikké (2000). 1.2.7 Water vendors Vendors selling water to households or at collection points are common in many parts of the world where scarcity of water or faults in or lack of infrastructure limits access to suitable quantities of drinking-water. Water vendors use a range of modes of transport to carry drinking-water for sale directly to the consumer, including tanker trucks and wheelbarrows or trolleys. In the context of these Guidelines, water vending does not include bottled or packaged water (which is considered in section 6.15) or water sold through vending machines. There are a number of health concerns associated with water supplied to consumers by water vendors. These include access to adequate volumes and concern regarding inadequate treatment or transport in inappropriate containers, which can result in contamination. More detailed information on treatment of vended water, undertaking a risk assessment of vended water supplies, operational monitoring of control measures, management plans and independent surveillance is included in section 6.3. 1.2.8 Individual consumers Everyone consumes water from one source or another, and consumers often play important roles in the collection, treatment and storage of water. Consumer actions may help to ensure the safety of the water they consume and may also contribute to improvement or contamination of the water consumed by others. Consumers have the responsibility for ensuring that their actions do not have an adverse impact on water quality. Installation and maintenance of household plumbing systems should be undertaken preferably by qualified and authorized plumbers (see section 1.2.10) or other persons with appropriate expertise to ensure that cross-connections or backflow events do not result in contamination of local water supplies. In most countries, there are populations whose water is derived from household sources, such as private wells and rainwater. In households using non-piped water supplies, appropriate efforts are needed to ensure safe collection, storage and perhaps treatment of their drinking-water. In some circumstances, households and individuals may wish to treat water in the home to increase their confidence in its safety. This would be relevant where community supplies are absent or where community supplies are known to be contaminated or causing waterborne disease (see chapter 7). Public health surveillance or other local authorities may provide guidance to support households and individual consumers in ensuring the safety of their drinking-water. Such guidance is best provided in the context of a community education and training programme. 1.2.9 Certification agencies Certification is used to verify that devices and materials used in the drinking-water supply meet a given level of quality and safety. Certification is a process in which an independent organization validates the claims of the manufacturers against a formal standard or criterion or provides an independent assessment of possible risks of contamination from a material or process. The certification agency may be responsible for seeking data from manufacturers, generating test results, conducting inspections and audits and possibly making recommendations on product performance. Certification has been applied to technologies used at household and community levels, such as hand pumps; materials used by water supplies, such as treatment chemicals; and devices used in the household for collection, treatment and storage. Certification of products or processes involved in the collection, treatment, storage and distribution of water can be overseen by government agencies or private organizations. Certification procedures will depend on the standards against which the products are certified, certification criteria and the party that performs the certification. Certification can also be applied to the implementation of water safety plans. This can take the form of an independent organization or party undertaking audits to verify that plans have been properly designed, are being implemented correctly and are effective. National, local government or private (third-party auditing) certification pro-grammes have a number of possible objectives: • certification of products to ensure that their use does not threaten the safety of the user or the general public, such as by causing contamination of drinking-water with toxic substances, substances that could affect consumer acceptability or substances that support the growth of microorganisms; • product testing, to avoid retesting at local levels or prior to each procurement; • ensuring uniform quality and condition of products; • certification and accreditation of analytical and other testing laboratories; • control of materials and chemicals used for the treatment of drinking-water, including the performance of devices for household use; • ensuring that water safety plans are effective. An important step in any certification procedure is the establishment of standards, which must form the basis of assessment of the products. These standards should also—as far as possible—contain the criteria for approval. In procedures for certification on technical aspects, these standards are generally developed in cooperation with the manufacturers, the certifying agency and the consumers. The national public health authorities should have responsibility for developing the parts of the approval process or criteria relating directly to public health. For further information on the control of materials and chemicals used for the treatment of drinking-water, see section 8.5.4. 1.2.10 Plumbing Significant adverse health effects have been associated with inadequate plumbing systems within public and private buildings arising from poor design, incorrect installation, alterations and inadequate maintenance. Numerous factors influence the quality of water within a building’s piped distribution system and may result in microbial or chemical contamination of drinking-water. Outbreaks of gastrointestinal disease can occur through faecal contamination of drinking-water within buildings arising from deficiencies in roof storage tanks and cross-connections with wastewater pipes, for example. Poorly designed plumbing systems can cause stagnation of water and provide a suitable environment for the proliferation of Legionella. Plumbing materials, pipes, fittings and coatings can result in elevated heavy metal (e.g. lead) concentrations in drinking-water, and inappropriate materials can be conducive to bacterial growth. Potential adverse health effects may not be confined to the individual building. Exposure of other consumers to contaminants is possible through contamination of the local public distribution system, beyond the particular building, through cross-contamination of drinking-water and backflow. The delivery of water that complies with relevant standards within buildings generally relies on a plumbing system that is not directly managed by the water supplier. Reliance is therefore placed on proper installation of plumbing and, for larger buildings, on building-specific water safety plans (see section 6.10). To ensure the safety of drinking-water supplies within the building system, plumbing practices must prevent the introduction of hazards to health. This can be achieved by ensuring that: • pipes carrying either water or wastes are watertight, durable, of smooth and unobstructed interior and protected against anticipated stresses; • cross-connections between the drinking-water supply and the wastewater removal systems do not occur; • roof storage systems are intact and not subject to intrusion of microbial or chemical contaminants; • hot and cold water systems are designed to minimize the proliferation of Legionella (see also sections 6.10, 6.11 and 11.1);• appropriate protection is in place to prevent backflow; • the system design of multistorey buildings minimizes pressure fluctuations; • waste is discharged without contaminating drinking-water; • plumbing systems function efficiently. It is important that plumbers are appropriately qualified, have the competence to undertake necessary servicing of plumbing systems to ensure compliance with local regulations and use only materials approved as safe for use with drinking-water. Design of the plumbing systems of new buildings should normally be approved prior to construction and be inspected by an appropriate regulatory body during construction and prior to commissioning of the buildings. For more information on the essential roles of proper drinking-water system and waste system plumbing in public health, see the supporting document Health aspects of plumbing (Annex 1). 1.3 Supporting resources to the Guidelines 1.3.1 Published documents These Guidelines are accompanied by separate texts that provide background information substantiating the derivation of the Guidelines and providing guidance on good practice towards their effective implementation. These are available as published texts and for download from the WHO website. Reference details are provided in Annex 1. 2 A conceptual framework for implementing the Guidelines TThe basic and essential requirement to ensure the safety of drinking-water is the implementation of a “framework for safe drinking-water” based on the Guidelines. This framework provides a preventive, risk-based approach to managing water quality. It would be composed of health-based targets established by a competent health authority using the Guidelines as a starting point, adequate and properly managed systems (adequate infrastructure, proper monitoring and effective planning and management) and a system of independent surveillance. Such a framework would normally be enshrined in national standards, regulations, or guidelines, in conjunction with relevant policies and programmes (see sections 2.6 and 2.7). Resultant regulations and policies should be appropriate to local circumstances, taking into consideration environmental, social, economic and cultural issues and priority setting. The framework for safe drinking-water is a preventive management approach comprising three key components: 19 1) health-based targets based on an evaluation of health risks (section 2.1 and chapter 3); 2) water safety plans (WSPs), comprising (section 2.2 and chapter 4):• a system assessment to determine whether the drinking-water supply (from source through treatment to the point of consumption) as a whole can deliver water of a quality that meets the health-based targets (section 4.1);• operational monitoring of the control measures in the drinking-water supply that are of particular importance in securing drinking-water safety (section 4.2);• management plans documenting the system assessment and monitoring plans and describing actions to be taken in normal operation and incident conditions, including upgrade and improvement, documentation and communication (sections 4.4–4.6); 3) a system of independent surveillance that verifies that the above are operating properly (section 2.3 and chapter 5). Verification to determine whether the performance of the drinking-water supply is in compliance with the health-based targets and whether the WSP itself is effective may be undertaken by the supplier, surveillance agencies or a combination of the two (see section 4.3). 2.1 Health-based targets Health-based targets are an essential component of the drinking-water safety framework. They should be established by a high-level authority responsible for health in consultation with others, including water suppliers and affected communities. They should take account of the overall public health situation and contribution of drinking-water quality to disease due to waterborne microbes and chemicals, as a part of overall water and health policy. They must also take account of the importance of ensuring access to water for all consumers. Health-based targets provide the basis for the application of the Guidelines to all types of drinking-water suppliers. Some constituents of drinking-water may cause adverse health effects from single exposures (e.g. pathogenic microorganisms) or longterm exposures (e.g. many chemicals). Because of the range of constituents in water, their mode of action and the nature of fluctuations in their concentrations, there are four principal types of health-based targets used as a basis for identifying safety requirements: 1) Health outcome targets: Where waterborne disease contributes to a measurable and significant burden, reducing exposure through drinking-water has the potential to appreciably reduce the risks and incidence of disease. In such circumstances, it is possible to establish a health-based target in terms of a quantifiable reduction in the overall level of disease. This is most applicable where adverse effects follow shortly after exposure, where such effects are readily and reliably monitored and where changes in exposure can also be readily and reliably monitored. This type of health outcome target is primarily applicable to some microbial hazards in developing countries and chemical hazards with clearly defined health effects largely attributable to water (e.g. fluoride, nitrate/nitrite and arsenic). In other circumstances, health outcome targets may be the basis for evaluation of results through quantitative risk assessment models. In these cases, health outcomes are estimated based on information concerning high-dose exposure and dose– response relationships. The results may be employed directly as a basis for the specification of water quality targets or provide the basis for development of the other types of health-based targets. Health outcome targets based on information on the impact of tested interventions on the health of real populations are ideal, but rarely available. More common are health outcome targets based on defined levels of tolerable risk, either absolute or fractions of total disease burden, usually based on toxicological studies in experimental animals and occasionally based on epidemiological evidence. 2) Water quality targets: Water quality targets are established for individual drinking-water constituents that represent a health risk from long-term exposure and where fluctuations in concentration are small. They are typically expressed as guideline values (concentrations) of the substances or chemicals of concern. 3) Performance targets: Performance targets are employed for constituents where short-term exposure represents a public health risk or where large fluctuations in numbers or concentration can occur over short periods with significant health implications. These are typically technology based and expressed in terms of required reductions of the substance of concern or effectiveness in preventing contamination. 4) Specified technology targets: National regulatory agencies may establish other recommendations for specific actions for smaller municipal, community and household drinking-water supplies. Such targets may identify specific permissible devices or processes for given situations and/or for generic drinking-water system types. It is important that health-based targets are realistic under local operating conditions and are set to protect and improve public health. Health-based targets underpin the development of WSPs, provide information with which to evaluate the adequacy of existing installations and assist in identifying the level and type of inspection and analytical verifications that are appropriate. Most countries apply several types of targets for different types of supplies and different contaminants. In order to ensure that they are relevant and supportive, representative scenarios should be developed, including description of assumptions, management options, control measures and indicator systems for performance tracking and verification, where appropriate. These should be supported by general guidance addressing the identification of national, regional or local priorities and progressive implementation, thereby helping to ensure that best use is made of limited resources. Health-based targets are considered in more detail in chapter 3. For guidance on how to prioritize constituents based on greatest risk to public health, the reader should refer to section 2.5 and the supporting document Chemical safety of drinking-water (Annex 1). 2.2 Water safety plans Overall control of the microbial and chemical quality of drinking-water requires the development of management plans that, when implemented, provide the basis for system protection and process control to ensure that numbers of pathogens and concentrations of chemicals present a negligible risk to public health and that water is acceptable to consumers. The management plans developed by water suppliers are WSPs. A WSP comprises system assessment and design, operational monitoring and management plans, including documentation and communication. The elements of a WSP build on the multiple-barrier principle, the principles of hazard analysis and critical control points and other systematic management approaches. The plans should address all aspects of the drinking-water supply and focus on the control of abstraction, treatment and delivery of drinking-water. Many drinking-water supplies provide adequate safe drinking-water in the absence of formalized WSPs. Major benefits of developing and implementing a WSP for these supplies include the systematic and detailed assessment and prioritization of hazards, the operational monitoring of barriers or control measures and improved documentation. In addition, a WSP provides for an organized and structured system to minimize the chance of failure through oversight or lapse of management and for contingency plans to respond to system failures or unforeseen events that may have an impact on water quality, such as increasing severe droughts, heavy rainfall or flood events. 2.2.1 System assessment and design Assessment of the drinking-water system is applicable, with suitable modifications, to large utilities with piped distribution systems, piped and non-piped community supplies, including hand pumps, and individual domestic supplies, including rainwater. The complexity of a WSP varies with the circumstances. Assessment can be of existing infrastructure or of plans for new supplies or for upgrading existing supplies. As drinking-water quality varies throughout the system, the assessment should aim to determine whether the final quality of water delivered to the consumer will routinely meet established health-based targets. Understanding source quality and changes throughout the system requires expert input. The assessment of systems should be reviewed periodically. The system assessment needs to take into consideration the behaviour of selected constituents or groups of constituents that may influence water quality. After actual and potential hazards, including events and scenarios that may affect water quality, have been identified and documented, the level of risk for each hazard can be estimated and ranked, based on the likelihood and severity of the consequences. Validation is an element of system assessment. It is undertaken to ensure that the information supporting the plan is correct and is concerned with the assessment of the scientific and technical inputs into the WSP. Evidence to support the WSP can come from a wide variety of sources, including scientific literature, regulation and legislation departments, historical data, professional bodies and supplier knowledge. The WSP is the management tool that should be used to assist in actually meeting the health-based targets, and it should be developed following the steps outlined in chapter 4. If the system is unlikely to be capable of meeting the health-based targets, a programme of upgrading (which may include capital investment or training) should be initiated to ensure that the drinking-water supply would meet the targets. The WSP is an important tool in identifying deficiencies and where improvements are most needed. In the interim, the WSP should be used to assist in making every effort to supply water of the highest achievable quality. Where a significant risk to public health exists, additional measures may be appropriate, including notification, information on compensatory options (e.g. boiling or disinfection at the point of use) and availability of alternative and emergency supplies when necessary. System assessment and design are considered in more detail in section 4.1 (see also the supporting document Upgrading water treatment plants; Annex 1). 2.2.2 Operational monitoring Operational monitoring is the conduct of planned observations or measurements to assess whether the control measures in a drinking-water system are operating properly. It is possible to set limits for control measures, monitor those limits and take corrective action in response to a detected deviation before the water becomes unsafe. Operational monitoring would include actions, for example, to rapidly and regularly assess whether the structure around a hand pump is complete and undamaged, the turbidity of water following filtration is below a certain value or the chlorine residual after disinfection plants or at the far point of the distribution system is above an agreed value. Operational monitoring is usually carried out through simple observations and tests, in order to rapidly confirm that control measures are continuing to work. Control measures are actions implemented in the drinking-water system that prevent, reduce or eliminate contamination and are identified in system assessment. They include, for example, management actions related to the catchment, the immediate area around a well, filters and disinfection infrastructure and piped distribution systems. If collectively operating properly, they would ensure that health-based targets are met. The frequency of operational monitoring varies with the nature of the control measure—for example, checking structural integrity monthly to yearly, monitoring turbidity online or very frequently and monitoring disinfectant residual at multiple points daily or continuously online. If monitoring shows that a limit does not meet specifications, then there is the potential for water to be, or to become, unsafe. The objective is timely monitoring of control measures, with a logically based sampling plan, to prevent the delivery of potentially unsafe water. Operational monitoring includes observing or testing parameters such as turbidity, chlorine residual or structural integrity. More complex or costly microbial or chemical tests are generally applied as part of validation and verification activities (discussed in sections 4.1.7 and 4.3, respectively) rather than as part of operational monitoring. In order not only to have confidence that the chain of supply is operating properly, but to confirm that safe water quality is being achieved and maintained, it is necessary to carry out verification, as outlined in section 4.3. The use of indicator organisms (see section 11.6) in the monitoring of water quality is discussed in the supporting document Assessing microbial safety of drinking water (Annex 1), and operational monitoring is considered in more detail in section 4.2. 2.2.3 Management plans, documentation and communication A management plan documents system assessment and operational monitoring and verification plans and describes actions in both normal operation and during “incidents” where a loss of control of the system may occur. The management plan should also outline procedures and other supporting programmes required to ensure optimal operation of the drinking-water system. As the management of some aspects of the drinking-water system often falls outside the responsibility of a single agency, it is essential that the roles, accountabilities and responsibilities of the various agencies involved be defined in order to coordinate their planning and management. Appropriate mechanisms and documentation should therefore be established for ensuring stakeholder involvement and commitment. This may include establishing working groups, committees or task forces, with appropriate representatives, and developing partnership agreements, including, for example, signed memoranda of understanding (see also section 1.2). Documentation of all aspects of drinking-water quality management is essential. Documents should describe activities that are undertaken and how procedures are performed. They should also include detailed information on: • assessment of the drinking-water system (including flow diagrams and potential hazards); • control measures and operational monitoring and verification plans and performance consistency; • routine operation and management procedures; • incident and emergency response plans; • supporting measures, including: — training programmes; — research and development; — procedures for evaluating results and reporting; — performance evaluations, audits and reviews; — communication protocols; • community consultation. Documentation and record systems should be kept as simple and focused as possible. The level of detail in the documentation of procedures should be sufficient to provide assurance of operational control when coupled with suitably qualified and competent operators. Mechanisms should be established to periodically review and, where necessary, revise documents to reflect changing circumstances. Documents should be assembled in a manner that will enable any necessary modifications to be made easily. A document control system should be developed to ensure that current versions are in use and obsolete documents are discarded. Appropriate documentation and reporting of incidents or emergencies should also be established. The organization should learn as much as possible from an incident to improve preparedness and planning for future events. Review of an incident may indicate necessary amendments to existing protocols. Effective communication to increase community awareness and knowledge of drinking-water quality issues and the various areas of responsibility helps consumers to understand and contribute to decisions about the service provided by a drinking-water supplier or land use constraints imposed in catchment areas. It can encourage the willingness of consumers to generate funds to finance needed improvements. A thorough understanding of the diversity of views held by individuals or groups in the community is necessary to satisfy community expectations. Management, documentation and communication are considered in more detail in sections 4.4, 4.5 and 4.6. 2.3 Surveillance Surveillance agencies are responsible for an independent (external) and periodic review of all aspects of quality and public health safety and should have the power to investigate and to compel action to respond to and rectify incidents of contamination-caused outbreaks of waterborne disease or other threats to public health. The act of surveillance includes identifying potential drinking-water contamination and waterborne illness events and, more proactively, assessing compliance with WSPs and promoting improvement of the quality, quantity, accessibility, coverage, affordability and continuity of drinking-water supplies. Surveillance of drinking-water requires a systematic programme of data collection and surveys that may include auditing of WSPs, analysis, sanitary inspection and institutional and community aspects. It should cover the whole of the drinking-water system, including sources and activities in the catchment, transmission infrastructure, whether piped or unpiped, treatment plants, storage reservoirs and distribution systems. As incremental improvement and prioritizing action in systems presenting greatest overall risk to public health are important, there are advantages to adopting a grading scheme for the relative safety of drinking-water supplies (see chapter 4). More sophisticated grading schemes may be of particular use in community supplies where the frequency of testing is low and exclusive reliance on analytical results is particularly inappropriate. Such schemes will typically take account of both analytical findings and sanitary inspection through approaches such as those presented in section 4.1.2. The role of surveillance is discussed in section 1.2.1 and chapter 5. 2.4 Verification of drinking-water quality Drinking-water safety is secured by application of a WSP, which includes monitoring the efficiency of control measures using appropriately selected determinants. In addition to this operational monitoring, a final verification of quality is required. Verification is the use of methods, procedures or tests in addition to those used in operational monitoring to determine whether the performance of the drinking-water supply is in compliance with the stated objectives outlined by the health-based targets and whether the WSP needs modification or revalidation. Verification of drinking-water may be undertaken by the supplier, surveillance agencies or a combination of the two (see section 4.3). Although verification is most commonly carried out by the surveillance agency, a utility-led verification programme can provide an additional level of confidence, supplementing regulations that specify monitoring parameters and frequencies. 2.4.1 Microbial water quality For microbial water quality, verification is likely to be based on the analysis of faecal indicator microorganisms, with the organism of choice being Escherichia coli or, alternatively, thermotolerant coliforms (see sections 4.3.1, 7.4 and 11.6). Monitoring of specific pathogens may be included on very limited occasions to verify that an outbreak was waterborne or that a WSP has been effective. Escherichia coli provides conclusive evidence of recent faecal pollution and should not be present in drinking-water. Under certain circumstances, additional indicators, such as bacteriophages or bacterial spores, may be used. However, water quality can vary rapidly, and all systems are at risk of occasional failure. For example, rainfall can greatly increase the levels of microbial contamination in source waters, and waterborne outbreaks often occur following rainfall. Results of analytical testing must be interpreted taking this into account. 2.4.2 Chemical water quality Assessment of the adequacy of the chemical quality of drinking-water relies on comparison of the results of water quality analysis with guideline values. These Guidelines provide guideline values for many more chemical contaminants than will actually affect any particular water supply, so judicious choices for monitoring and surveillance should be made prior to initiating an analytical chemical assessment. For additives (i.e. chemicals deriving primarily from materials and chemicals used in the production and distribution of drinking-water), emphasis is placed on the direct control of the quality of these commercial products. In controlling drinking-water additives, testing procedures typically assess whether the product meets the specifications (see section 8.5.4). As indicated in chapter 1, most chemicals are of concern only following longterm exposure; however, some hazardous chemicals that occur in drinking-water are of concern because of effects arising from sequences of exposures over a short period. Where the concentration of the chemical of interest (e.g. nitrate/nitrite, which is associated with methaemoglobinaemia in bottle-fed infants) varies widely, even a series of analytical results may fail to fully identify and describe the public health risk. In controlling such hazards, attention must be given to both knowledge of causal factors such as fertilizer use in agriculture and trends in detected concentrations, as these will indicate whether a significant problem may arise in the future. Other hazards may arise intermittently, often associated with seasonal activity or seasonal conditions. One example is the occurrence of blooms of toxic cyanobacteria in surface water. A guideline value represents the concentration of a constituent that does not exceed tolerable risk to the health of the consumer over a lifetime of consumption. Guideline values for some chemical contaminants (e.g. lead, nitrate) are set to be protective for susceptible subpopulations. These guideline values are also protective of the general population over a lifetime. It is important that recommended guideline values are scientifically justified, practical and feasible to implement as well as protective of public health. Guideline values are not normally set at concentrations lower than the detection limits achievable under routine laboratory operating conditions. Moreover, some guideline values are established taking into account available techniques for controlling, removing or reducing the concentration of the contaminant to the desired level. In some instances, therefore, provisional guideline values have been set for contaminants for which calculated health-based values are not practically achievable. 2.5 Identifying priority concerns These Guidelines cover a large number of potential constituents in drinking-water in order to meet the varied needs of countries worldwide. Generally, however, only a few constituents will be of public health concern under any given circumstances. It is essential that the national regulatory agency and local water authorities identify and respond to the constituents of relevance to the local circumstances. This will ensure that efforts and investments can be directed to those constituents that have the greatest risk or public health significance. Health-based targets are established for potentially hazardous water constituents and provide a basis for assessing drinking-water quality. Different parameters may require different priorities for management to improve and protect public health. In general, the priorities, in decreasing order, are to: • ensure an adequate supply of microbially safe water and maintain acceptability to discourage consumers from using potentially less microbially safe water; • manage key chemical hazards known to cause adverse health effects; • address other chemical hazards, particularly those that affect the acceptability of drinking-water in terms of its taste, odour and appearance; • apply appropriate technologies to reduce contaminant concentrations in the source to below the guideline or regulated values. The two key features in choosing hazards for which Many microbial and chemical constituents of drinking‑setting a standard is desir-water can potentially cause adverse human health effects. The detection of these constituents in both raw able on health grounds are water and water delivered to consumers is often slow, the health impacts (severity) complex and costly, which limits early warning capabilassociated with the substance ity and affordability. Reliance on water quality determiand the probability of signifi-nation alone is insufficient to protect public health. As it is neither physically nor economically feasible to test for cant occurrence (exposure). all drinking‑water quality parameters, the use of moni‑Combined, these elements toring effort and resources should be carefully planned determine the risk associated and directed at significant or key characteristics. with a particular hazard. For microbial hazards, the setting of targets will be influenced by occurrence and concentrations in source waters and the relative contribution of waterborne organisms to disease. For chemical hazards, the factors to be considered are the severity of health effects and the frequency of exposure of the population in combination with the concentration to which they will be exposed. The probability of health effects clearly depends on the toxicity and the concentration, but it also depends on the period of exposure. For most chemicals, health impacts are associated with long-term exposure. Hence, in the event that exposure is occasional, the risk of an adverse health effect is likely to be low, unless the concentration is extremely high. The substances of highest priority will therefore be those that occur widely, are present in drinking-water sources or drinking-water all or most of the time and are present at concentrations that are of health concern. Guidance on determining which chemicals are of importance in a particular situation is given in the supporting document Chemical safety of drinking-water (Annex 1). Although WHO does not set formal guideline values for substances on the basis of consumer acceptability (i.e. substances that affect the appearance, taste or odour of drinking-water), it is not uncommon for standards to be set for substances and parameters that relate to consumer acceptability. Although exceeding such a standard is not a direct issue for health, it may be of great significance for consumer confidence and may lead consumers to obtain their water from an alternative, less safe source. Such standards are usually based on local considerations of acceptability. Priority setting should be undertaken on the basis of a systematic assessment based on collaborative effort among all relevant agencies and may be applied at national and system-specific levels. At the national level, priorities need to be set in order to identify the relevant hazards, based on an assessment of risk—i.e. severity and exposure. At the level of individual water supplies, it may be necessary to also prioritize constituents for effective system management. These processes may require the input of a broad range of stakeholders, including health, water resources, drinking-water supply, environment, agriculture and geological services/mining authorities, to establish a mechanism for sharing information and reaching consensus on drinking-water quality issues. 2.5.1 Undertaking a drinking-water quality assessment In order to determine which constituents are, indeed, of concern, it will be necessary to undertake a drinking-water quality assessment. It is important to identify what types of drinking-water systems are in place in the country (e.g. piped water supplies, non-piped water supplies, vended water) and the quality of drinking-water sources and supplies. Additional information that should be considered in the assessment includes catchment type (protected, unprotected), wastewater discharges, geology, topography, agricultural land use, industrial activities, sanitary surveys, records of previous monitoring, inspections and local and community knowledge. The wider the range of data sources used, the more useful the results of the process will be. In many situations, authorities or consumers may have already identified a number of drinking-water quality problems, particularly where they cause obvious health effects or acceptability problems. These existing problems would normally be assigned a high priority. Drinking-water supplies that represent the greatest risks to public health should be identified, with resources allocated accordingly. 2.5.2 Assessing microbial priorities The most common and widespread health risk associated with drinking-water is microbial contamination, the consequences of which mean that its control The most common and widespread health must always be of paramount impor-risk associated with drinking‑water is mitance. Priority needs to be given to crobial contamination, the consequences of which mean that its control must always improving and developing the drinking-be of paramount importance. water supplies that represent the greatest public health risk. Health-based targets for microbial contaminants are discussed in section 3.2, and a comprehensive consideration of microbial aspects of drinking-water quality is contained in chapter 7. 2.5.3 Assessing chemical priorities Not all of the chemicals with guideline values will be present in all water supplies or, indeed, all countries. If they do exist, they may not be found at levels of concern. Conversely, some chemicals without guideline values or not addressed in the Guidelines may nevertheless be of legitimate local concern under special circumstances. Risk management strategies (as reflected in national standards and monitoring activities) and commitment of resources should give priority to those chemicals that pose a risk to human health or to those with significant impacts on the acceptability of water. Only a few chemicals have been shown to cause widespread health effects in humans as a consequence of exposure through drinking-water when they are present in excessive quantities. These include fluoride, arsenic and nitrate. Human health effects associated with lead (from domestic plumbing) have also been demonstrated in some areas, and there is concern because of the potential extent of exposure to manganese, selenium and uranium in some areas at concentrations of human health significance. Manganese and iron are of widespread significance also because of their effects on acceptability through discoloration. These constituents should be taken into consideration as part of any priority-setting process. In some cases, assessment will indicate that no risk of significant exposure exists at the national, regional or system level. Drinking-water may be only a minor contributor to the overall exposure to a particular chemical, and in some circumstances controlling the levels in drinking-water, at potentially considerable expense, may have little impact on overall exposure. Drinking-water risk management strategies should therefore be considered in conjunction with other potential sources of human exposure. The process of “short-listing” chemicals of concern may initially be a simple classification of high and low risk to identify broad issues. This may be refined using data from more detailed assessments and analysis and may take into consideration rare events, variability and uncertainty. Guidance on how to undertake prioritization of chemicals in drinking-water is provided in the supporting documents Chemical safety of drinking-water and Developing drinking-water quality regulations and standards (Annex 1). These deal with issues including: • the probability of exposure (including the period of exposure) of the consumer to the chemical; • the concentration of the chemical that is likely to give rise to health effects (see also section 8.5);• the evidence of health effects or exposure arising through drinking-water, as opposed to other sources, and relative ease of control of the different sources of exposure. Additional information on the hazards and risks of many chemicals not included in these Guidelines is available from several sources, including WHO Environmental Health Criteria monographs and Concise International Chemical Assessment Documents, reports by the Joint Food and Agriculture Organization of the United Nations (FAO)/WHO Meeting on Pesticide Residues and the Joint FAO/WHO Expert Committee on Food Additives and information from competent national authorities. These information sources have been peer reviewed and provide readily accessible information on toxicology, hazards and risks of many less common contaminants. They can help water suppliers and health officials to decide upon the significance (if any) of a detected chemical and on the response that might be appropriate. 2.6 Developing drinking-water quality standards Health-based targets, including numeric guideline values and other targets described in the Guidelines for drinking-water quality, are not intended to be mandatory limits, but are provided as the scientific point of departure for development of national or regional numerical drinking-water quality standards. No single approach is universally applicable, and the nature and form of drinking-water standards may vary among countries and regions. In developing national drinking-water standards based on these Guidelines, it will be necessary to take account of a variety of environmental, social, cultural, economic, dietary and other conditions affecting potential exposure. This may lead to national standards that differ appreciably from these Guidelines, both in scope as well as in risk targets. A programme based on modest but realistic goals—including fewer water quality parameters of priority health concern at attainable levels consistent with providing a reasonable degree of public health protection in terms of reduction of disease or disease risk within the population—may achieve more than an overambitious one, especially if targets are upgraded periodically. To ensure that standards are acceptable to consumers, communities served, together with the major water users, should be involved in the standards-setting process. Public health agencies may be closer to the community than those responsible for its drinking-water supply. At a local level, they also interact with other sectors (e.g. education), and their combined action is essential to ensure active community involvement. Further guidance on considerations for developing drinking-water quality standards and drinking-water regulations (see section 2.7), including adapting guideline values (see section 2.6.1) and periodic revision of standards (see section 2.6.2), is provided in the supporting document Developing drinking-water quality regulations and standards (Annex 1). 2.6.1 Adapting guideline values to locally relevant standards In order to account for variations in exposure from different sources (e.g. water, food) in different parts of the world, the proportion of the tolerable daily intake allocated to drinking-water in setting guideline values for many chemicals will vary. Where relevant exposure data are available, authorities are encouraged to develop context-specific guideline values that are tailored to local circumstances and conditions. For example, in areas where the intake of a particular contaminant in drinking-water is known to be much greater than that from other sources (e.g. air and food), it may be appropriate to allocate a greater proportion of the tolerable daily intake to drinking-water to derive a guideline value more suited to the local conditions. Daily water intake can vary significantly in different parts of the world, seasonally and particularly where consumers are involved in manual labour in hot climates. Local adjustments to the daily water consumption value may be needed in setting local standards, as in the case of fluoride, for example. Volatile substances in water may be released into the air during showering and through a range of other household activities. Under such circumstances, inhalation may become a significant route of exposure. Where such exposure is shown to be important for a particular substance (i.e. high volatility, low ventilation rates and high rates of showering/bathing), it may be appropriate to adjust the guideline value. For those substances that are particularly volatile, such as chloroform, the correction factor would be approximately equivalent to a doubling of exposure. For further details, the reader should refer to section 8.2.9. 2.6.2 Periodic review and revision of standards As knowledge increases, there may be changes to specific guideline values or consideration of new hazards for the safety of drinking-water. There may also be changes in the technology of drinking-water treatment and analytical methods for contaminants. Changes in the country context affecting public health risks, as well as changes in capacity for management and monitoring parameters, should also be considered. National or subnational standards must therefore be subjected to periodic review and should be structured in such a way that changes can be made readily. Changes may need to be made to modify standards, remove parameters or add new parameters, but no changes should be made without proper justification through risk assessment and prioritization of resources for protecting public health. Where changes are justified, it is important that they are communicated to all stakeholders. 2.7 Drinking-water regulations and supporting policies and programmes The incorporation of a preventive risk management and prioritization approach to drinking-water quality regulations, policies and programmes will: • ensure that regulations support the prioritization of drinking-water quality parameters to be tested, instead of making mandatory the testing of every parameter in these Guidelines; • ensure implementation of appropriate sanitation measures at community and household levels and encourage action to prevent or mitigate contamination at source; • identify drinking-water supplies that represent the greatest risks to public health and thus determine the appropriate allocation of resources. 2.7.1 Regulations The alignment of national drinking-water quality regulations with the principles outlined in these Guidelines will ensure that: • there is an explicit link between drinking-water quality regulations and the protection of public health; • regulations are designed to ensure safe drinking-water from source to consumer, using multiple barriers; • regulations are based on good practices that have been proven to be appropriate and effective over time; • a variety of tools are in place to build and ensure compliance with regulations, including education and training programmes, incentives to encourage good practices and penalties, if enforcement is required; • regulations are appropriate and realistic within national, subnational and local contexts, including specific provisions or approaches for certain contexts or types of supplies, such as small community water supplies; • stakeholder roles and responsibilities, including how they should work together, are clearly defined; • “what, when and how” information is shared between stakeholders—including consumers—and required action is clearly defined for normal operations and in response to incidents or emergencies; • regulations are adaptable to reflect changes in contexts, understanding and technological innovation and are periodically reviewed and updated; • regulations are supported by appropriate policies and programmes. The aim of drinking-water quality regulations should be to ensure that the consumer has access to sustainable, sufficient and safe drinking-water. Enabling legislation should provide broad powers and scope to related regulations and include public health protection objectives, such as the prevention of waterborne disease and the provision of an adequate supply of drinking-water. Drinking-water regulations should focus on improvements to the provision and safety of drinking-water through a variety of requirements, tools and compliance strategies. Although sanctions are needed within regulations, the principal aim is not to shut down deficient water supplies. Drinking-water quality regulations are not the only mechanism by which public health can be protected. Other regulatory mechanisms include those related to source water protection, infrastructure, water treatment and delivery, surveillance and response to potential contamination and waterborne illness events. Drinking-water quality regulations may also provide for interim standards, permitted deviations and exemptions as part of a national or regional policy, rather than as a result of local initiatives. This may take the form of temporary exemptions for certain communities or areas for defined periods of time. Short-term and medium-term targets should be set so that the most significant risks to human health are managed first. Regulatory frameworks should support long-term progressive improvements. 2.7.2 Supporting policies and programmes Developing and promulgating regulations alone will not ensure that public health is protected. Regulations must be supported by adequate policies and programmes. This includes ensuring that regulatory authorities, such as enforcement agencies, have sufficient resources to fulfil their responsibilities and that the appropriate policy and programme supports are in place to assist those required to comply with regulations. In other words, the appropriate supports need to be in place so that those being regulated and those who are responsible for regulating are not destined to fail. Implementation or modification of policies and programmes to provide safe drinking-water should not be delayed because of a lack of appropriate regulation. Even where drinking-water regulations do not yet exist, it may be possible to encourage, and even enforce, the supply of safe drinking-water through, for example, educational efforts or commercial, contractual arrangements between consumer and supplier (e.g. based on civil law). In countries where universal access to safe drinking-water at an acceptable level of service has not been achieved, policies should refer to expressed targets for increases in sustainable access to safe drinking-water. Such policy statements should be consistent with achievement of the United Nations Sustainable Development Goals (https://sdgs.un.org/goals/) as part of the 2030 Agenda for Sustainable Development and should take into account the recognition of the human right to water by the United Nations General Assembly (UNGA, 2010a, b) and criteria for adequate water supply outlined in General Comment 15 on the Right to Water of the United Nations Committee on Economic, Social and Cultural Rights (http://umn.edu/humanrts/ gencomm/escgencom15.htm) and associated documents. 3 Health-based targets HHealth-based targets are measurable health, water quality or performance objectives that are established based on a judgement of safety and on risk assessments of waterborne hazards. These Guidelines describe four distinct types of health-based targets, applicable to all types of hazards and water supplies: 1) health outcome targets (e.g. tolerable burdens of disease); 2) water quality targets (e.g. guideline values for chemical hazards); 3) performance targets (e.g. log reductions of specific pathogens); 4) specified technology targets (e.g. application of defined treatment processes). These targets are common components of existing drinking-water guidelines or standards that are used to protect and improve drinking-water quality and, consequently, human health. They provide benchmarks for water suppliers and regulators to confirm the adequacy of existing systems or the need for improvement. They Health‑based targets can be used to support increunderpin the development of water mental improvement by marking out milestones to guide progress towards water safety and public safety plans and verification of health goals. successful implementation. Where 35 required, health-based targets can be used to support incremental improvement by marking out milestones to guide progress towards water safety and public health goals. This normally requires periodic review and updating of priorities and targets. In turn, norms and standards should also be periodically updated (see section 2.6.2). Health-based targets should assist in determining specific interventions appropriate to delivering safe drinking-water, including control measures such as source protection and treatment processes. 3.1 Setting health-based targets The use of health-based targets is applicable in countries at all levels of development. To ensure effective health protection and improvement, targets need to be realistic, measurable, based on scientific data and relevant to local conditions (including economic, environmental, social and cultural conditions) and financial, technical and institutional resources. Health-based targets should be part of an overall public health policy, taking into account public health status and trends and the contribution of drinking-water to the transmission of infectious disease and to overall exposure to hazardous chemicals both in individual settings and within overall health management. Although water can be a source of microbial, chemical or radiological hazards, it is by no means the only source. In setting targets, consideration needs to be given to other sources, including food, air, person-to-person contact and consumer products, as well as poor sanitation and personal hygiene. Where the overall burden of disease from multiple exposure routes is very high, there is limited value in setting strict targets for drinking-water. For example, there is limited value in establishing a strict target for a chemical hazard if drinking-water provides only a small proportion of the total exposure to that chemical. The cost of meeting such targets could unnecessarily divert funding from other, more pressing health interventions and is not consistent with the public health objective of reducing overall levels of risk from all sources of exposure to environmental hazards (Prüss-Ustün et al., 2016; WHO, 2019). It is also important to take account of the impact of the proposed intervention on overall rates of disease. For some pathogens and their associated diseases, interventions in water quality may be ineffective and may therefore not be justified. This may be the case where other routes of exposure dominate. For others, long experience has shown the effectiveness of improving drinking-water supply and quality management in the control of waterborne diseases such as typhoid and dysentery. Meeting health-based targets should be viewed in the context of broader public health policy, including initiatives to improve sanitation, waste disposal, personal hygiene and public education on ways to reduce both personal exposure to hazards and im-The judgement of safety—or what is a tolerable bur‑pacts of personal activity on den of disease in particular circumstances—is a matter in which society as a whole has a role to play. The final water resources. Improved judgement as to whether the benefit resulting from the public health, reduced carriage adoption of any of the health‑based targets justifies the of pathogens and reduced cost is for each country to decide. human impacts on water Table 3.1 Benefits of health-based targets Target development stage Benefit Formulation Provides insight into the health of the population Reveals gaps in knowledge Supports priority setting Increases the transparency of health policy Promotes consistency among national health programmes Stimulates debate Implementation Inspires and motivates collaborating authorities to take action Improves commitment Fosters accountability Guides the rational allocation of resources Evaluation Supplies established milestones for incremental improvements Provides opportunity to take action to correct deficiencies and/ or deviations Identifies data needs and discrepancies resources all contribute to drinking-water safety (Howard et al., 2002). Public health prioritization would normally indicate that the major contributors to disease should be dealt with preferentially, taking account of the costs and impacts of potential interventions. However, this does not mean ignoring lesser targets if they can be easily achieved for little cost, as long as this does not divert attention from major targets. An important concept in the allocation of resources to improving drinking-water safety is the possibility of establishing less stringent transitional targets supported by sound risk management systems in order to encourage incremental improvements of the quality of drinking-water. In this regard, health-based targets can be used as the basis for supporting and measuring incremental progress in water quality improvement. Improvements can relate to progression through increasingly tighter targets or evolution through target types that more precisely reflect the health protection goals (e.g. from specified technology targets to performance targets). The processes of formulating, implementing, communicating and evaluating health-based targets provide benefits to the overall preventive management of drinking-water quality. These benefits are outlined in Table 3.1. 3.2 Disability-adjusted life years, tolerable disease burden and reference level of risk At a national level, decisions about risk acceptance and tolerable burdens of disease are complex and need to take account of the probability and severity of impact in addition to the environmental, social, cultural, economic and political dimensions that play important roles in decision-making. Negotiations are an important part of these processes, and the outcome may very well be unique in each situation. Notwithstanding the complexity of these decisions, definitions of tolerable burdens of disease and reference levels of risk are required to provide a baseline for the development of health-based targets and as a point of departure for decisions in specific situations. Descriptions of tolerable burdens of disease relating to water are typically expressed in terms of specific health outcomes such as maximum frequencies of diarrhoeal disease or cancer incidence. However, these descriptions do not consider the severity of the outcomes. The various hazards that may be present in water are associated with very diverse health outcomes with different impacts ranging from mild diarrhoea to potentially severe outcomes such as typhoid, cancer or skeletal fluorosis. A common “metric” is needed that can be used to quantify and compare the burden of disease associated with different water-related hazards, taking into account varying probabilities, severities and duration of effects. Such a metric should be applicable regardless of the type of hazard (microbial, chemical or radiological) to enable the use of a consistent approach for each hazard. The metric used in these Guidelines is the disability-adjusted life year, or DALY (Box 3.1). The World Health Organization has used DALYs quite extensively to evaluate public health priorities and to assess the disease burden associated with environmental exposures, particularly for microbial hazards. A key advantage of using the DALY is its aggregation of “Tolerable burden of disease” represents an upper different impacts on the quality limit of the burden of health effects associated with waterborne disease that is established by national and quantity of life and its focus policy‑makers. “Reference level of risk” is an equivaon actual outcomes rather than lent term used in the context of quantitative risk potential risks; hence, it supports assessments. rational public health priority setting. DALYs can be used to define tolerable burden of disease and the related reference level of risk. In these Guidelines, the tolerable burden of disease is defined as an upper limit of 10−6 DALY per person per year. This upper-limit DALY is approximately equivalent to a 10−5 excess lifetime risk of cancer (i.e. 1 excess case of cancer per 100 000 people ingesting drinking-water at the water quality target daily over a 70-year period), which is the risk level used in these Guidelines to determine guideline values for genotoxic carcinogens. Expressing health-based targets for chemical hazards in DALYs has the advantage of enabling comparisons with microbial risks. However, use of the DALY approach for chemicals has been limited in practice due to gaps in knowledge. The 10−6 DALY tolerable burden of disease target may not be achievable or realistic in some locations and circumstances in the near term. Where the overall burden of disease by multiple exposure routes (water, food, air, direct personal contact, etc.) is very high, setting a 10−6 DALY per person per year level of disease burden from waterborne exposure alone will have little impact on the overall disease burden. Setting a less stringent level of acceptable risk, such as 10−5 or 10−4 DALY per person per year, from waterborne exposure may be more realistic, yet still consistent with the goals of providing high-quality, safer water. 3.3 Types of health-based targets The nature and typical application of health-based targets are presented in Table 3.2. Health-based targets differ considerably with respect to the amount of resources Box 3.1 Disability-adjusted life years The various hazards that can be present in water can have very different health outcomes. Some outcomes are mild (e.g. diarrhoea), whereas others can be severe (e.g. cholera, haemolytic uraemic syndrome associated with Escherichia coli O157 or cancer). Some are acute (e.g. diarrhoea), whereas others are delayed (e.g. infectious hepatitis or cancer). Some especially relate to certain age ranges and groups (e.g. skeletal fluorosis in older adults often arises from long‑term exposure to high levels of fluoride in childhood; infection with hepatitis E virus has a very high mortality rate among pregnant women). In addition, any one hazard may cause multiple effects (e.g. gastroenteritis, Guillain‑Barré syndrome, reactive arthritis and mortality associated with Campylobacter). In order to support public health priority setting, a common metric is required that can be applied to all types of hazard and takes into account different health outcomes, including probabilities, severities and duration of effects. The disability‑adjusted life year (DALY) provides this metric. The basic principle of the DALY is to weight each health impact in terms of severity within the range of 0 for good health to 1 for death. The weighting is then multiplied by duration of the effect and the number of people affected. In the case of death, duration is regarded as the years lost in relation to normal life expectancy. Using this approach, a mild diarrhoea with a severity weighting of 0.1 and lasting for 7 days results in a DALY of 0.002, whereas death resulting in a loss of 30 years of life equates to a DALY of 30. Hence, DALY = YLL (years of life lost) + YLD (years lived with a disability or illness). In this context, disability refers to a condition that detracts from good health. For example, infection with rotavirus (in developed countries) causes: • mild diarrhoea (severity rating of 0.1) lasting 7 days in 97.5% of cases; • severe diarrhoea (severity rating of 0.23) lasting 7 days in 2.5% of cases; • rare deaths of very young children in 0.015% of cases. The DALY per case can then be calculated as follows: DALY = (0.1 × 7/365 × 0.975) + (0.23 × 7/365 × 0.025) + (1 × 70 × 0.00015) = 0.0019 + 0.0001 + 0.0105 = 0.0125 Infection with Cryptosporidium can cause watery diarrhoea (severity weighting of 0.067) lasting for 7 days with extremely rare deaths in 0.0001% of cases. This equates to a DALY per case of 0.0015. Further information on the use of DALYs in establishing health‑based targets is included in the supporting document Quantifying public health risk in the WHO Guidelines for drinking-water quality (Annex 1). needed for their development and implementation and in relation to the precision with which the public health benefits of risk management actions can be defined. The most precise are health outcome targets, which underpin the derivation of the remaining targets, as shown in Figure 3.1. Each target type is based on those above it in Table 3.2, and assumptions with default values are introduced in moving down between target types. The targets towards the top of the table require greater scientific and technical inputs and are therefore more precisely related to the level of health protection. Target types at the bottom of Table 3.2 require the least interpretation by practitioners in implementation, but depend on a number of assumptions (e.g. establishing specified technology targets in the absence of sufficient source water quality data to apply performance targets for microbial pathogens). Efforts should be made to collect additional information when critical data for applying the next stage of target setting may not be available. This incremental improvement will ensure that the health-based targets will be as pertinent as possible to local circumstances. Table 3.2 Nature and application of health-based targets Type of Nature of target Typical applications Notes target Health Defined tolerable outcome burden of disease No adverse effect or negligible risk High‑level policy target set at national level, used to inform derivation of performance, water quality and specified technology targets Chemical or radiological hazards These Guidelines define a tolerable burden of disease of 10−6 DALY per person per year Derived from international chemical or radionuclide risk assessments Water quality Guideline values Chemical hazards Based on individual chemical risk assessments Microbial water quality Escherichia coli is used as an indicator targets are not normally of faecal contamination and to verify applied water quality Radiological water Radiological screening levels are quality targets are not applied normally applied Performance Specified removal of hazards Microbial hazards (expressed as log reductions) Chemical hazards (expressed as percentage removal) Specific targets set by water supplier based on quantitative microbial risk assessment and health outcome targets or generic targets set at national level Specific targets set by water supplier based on chemical guideline values or generic targets set at national level Specified Defined Control of microbial and Set at national level; based on technology technologies chemical hazards assessments of source water quality, frequently underpinned by established or validated performance of the specified technology (e.g. requirement of filtration for surface water) When establishing health-based targets, care should be taken to account for short-term events and fluctuations in water quality along with “steady-state” conditions. This is particularly important when developing performance and specified technology targets. Short-term water quality can significantly deteriorate, for example, following heavy rain and during maintenance. Catastrophic events can result in periods of very degraded source water quality and greatly decreased efficiency in many processes, or even system failure, greatly increasing the likelihood of a disease outbreak, Events like these provide additional justification for the long-established “multiple-barrier principle” in water safety. For chemical hazards, health-based targets most commonly take the form of water quality targets, using the guideline values outlined in section 8.5. Performance targets expressed as percentage removals or specified technology targets can also be applied to chemical hazards. Figure 3.1 Examples of how to set health-based targets for various hazards For microbial hazards, health-based targets usually take the form of performance or specified technology targets. The choice of target will be influenced by the number of data available on source water quality, with performance targets requiring more information. Water quality targets are typically not developed for pathogens, because monitoring finished drinking-water for pathogens is not considered a feasible or cost-effective option. Concentrations of pathogens equivalent to a health outcome target of 10−6 DALY per person per year are typically less than 1 organism per 104–105 litres. Therefore, it is more feasible and cost-effective to monitor for indicator organisms such as E. coli. In practice, risks to public health from drinking-water are often attributable to a single hazard at a time; therefore, in deriving targets, the reference level of risk is applied independently to each hazard. 3.3.1 Health outcome targets The most direct descriptions of drinking-water safety are health outcome targets, such as upper limits on frequencies of diarrhoeal disease or cancer incidence. These upper limits represent tolerable burdens of disease and are typically set at the national level. They underpin the derivation of water quality, performance and specified technology targets (Figure 3.1). These Guidelines define a tolerable burden of disease of 10−6 DALY per person per year. For threshold chemicals, the health outcome target is based on no-observed-adverse-effect levels (see section 8.2). Health outcome targets must be translated into water quality, performance or specified technology targets in order to be actioned by the water supplier as part of the water safety plan. 3.3.2 Water quality targets Water quality targets are the most common form of health-based target applied to chemicals that may be found in drinking-water. The guideline values for individual chemicals described in section 8.5 provide water quality targets that can be used to verify that water safety plans have been effective in managing risks from chemicals in drinking-water. Guideline values are established on the basis of international risk assessments of the health effects associated with exposure to the chemical in water. In developing national drinking-water standards (or health-based targets) based on these guideline values, it will be necessary to take into consideration a variety of environmental, social, cultural, economic, dietary and other conditions affecting potential exposure, as well as the default assumptions that are used to derive the guideline values. Exposure from chemicals in drinking-water is typically minor in comparison with that from other sources (e.g. food, consumer products and air), with a few important exceptions (e.g. arsenic and fluoride). This may lead to national targets that differ appreciably from the guideline values. In some cases, it may be appropriate to take action to prevent exposure to a chemical from sources other than drinking-water (e.g. lead from soldered cans and from petrol). One example is that of the health-based target for fluoride in drinking-water. A guideline value of 1.5 mg/l is recommended in Table A3.3 of Annex 3, with a comment that “Volume of water consumed and intake from other sources should be considered when setting national standards”. Thus, in a country with a warm climate year-round and where piped water is the preferred source of drinking-water, authorities may select a health-based target for fluoride that is lower than this guideline value, as water consumption is expected to be higher. On a similar note, the health-based target should be reviewed in terms of its impact on the most vulnerable section of the population. Where water treatment processes have been put in place to remove or reduce specific chemicals (see section 8.4 and Annex 5), water quality targets should be used to determine appropriate treatment requirements. It is important that water quality targets are established only for those chemicals that, following rigorous assessment, have been determined to be of health concern or of concern for the acceptability of the drinking-water to consumers. There is little value in undertaking measurements for chemicals that are unlikely to be in the system, that will be present only at concentrations much lower than the guideline value or that have no human health effects or effects on drinking-water acceptability. One example is that of radionuclides in drinking-water, which may be present in such minute quantities that their contribution to the overall health risks from drinking-water will be negligible. Analysis of individual radionuclides requires sophisticated and expensive procedures; hence, in such cases, measurements of gross alpha and gross beta activities may be adopted as the screening tests for the presence of radionuclides in drinking-water, as discussed in section 9.3. Water quality targets are also used in the certification process for chemicals that occur in water as a result of treatment processes or from materials in contact with water. In such applications, assumptions are made in order to derive standards for materials and chemicals that can be employed in their certification. Generally, allowance must be made for the incremental increase over levels found in water sources. For some materials (e.g. domestic plumbing), assumptions must also account for the relatively high release of some substances for a short period following installation. Escherichia coli remains an important indicator of faecal contamination for verification of water quality, but measurements of E. coli do not represent a risk-based water quality target. The use of E. coli as an indicator organism is discussed in more detail in chapter 7. Further guidance on considerations for establishing water quality targets as part of drinking-water quality standards is included in the supporting document Developing drinking-water quality regulations and standards (Annex 1). 3.3.3 Performance targets Although performance targets can be applied to chemical hazards, the most common application is for control of microbial hazards in piped supplies. Performance targets assist in the selection and use of control measures that are capable of preventing pathogens from breaching the barriers of source protection, treatment and distribution systems or preventing growth within the distribution system. Performance targets define requirements in relation to source water quality. Ideally, this should be based on system-specific data; more commonly, however, targets will be specified in relation to broad categories of source water quality and type (see section 7.2). The derivation of performance targets requires the integration of factors such as tolerable disease burden (acceptable risk), including severity of disease outcomes, and, for pathogens, quantitative microbial risk assessment (see section 7.2). There are insufficient data, and it is not realistic, to derive performance targets for all potentially waterborne pathogens. The practical approach is to derive targets for reference pathogens representing groups of pathogens (e.g. bacteria, viruses and protozoa). Selection of reference pathogens should take into account variations in susceptibility to treatment as well as local conditions, including prevalence of waterborne transmission and source water characteristics. The most common application of performance targets is in identifying appropriate combinations of treatment processes to reduce pathogen concentrations in source water to a level that will meet health outcome targets and hence be safe. This is normally expressed in terms of log reductions. Selection of processes requires evidence that they will meet required performance targets (i.e. validation; see sections 2.2.2 and 4.1.7). Examples of treatment processes and pathogen reductions are given in section 7.3. Performance targets can be applied to catchment controls that are aimed at reducing pathogen concentrations through preventive measures and to measures to prevent ingress of contamination through distribution systems. Performance targets are also important in certification of point-of-use devices and specified technologies used for drinking-water treatment. Certification of devices is discussed elsewhere (see section 1.2.9). Performance targets can be applied to chemical hazards. In comparison with targets for microbial hazards, they are typically applied to specific chemicals, with performance measured in terms of percentage reduction (see section 8.4). 3.3.4 Specified technology targets Specified technology targets typically take the form of recommendations concerning technologies applicable in certain circumstances (e.g. filtration and disinfection of surface water). Selection of technologies is usually based on qualitative assessments of source water type and quality (e.g. impacted surface water, protected groundwater). Specified technology targets are most frequently applied to small community supplies and to devices used at the household level. They can be applied to both microbial and chemical hazards. Smaller municipal and community drinking-water suppliers often have limited resources and ability to develop individual system assessments and health-based targets. National regulatory agencies may therefore directly specify technology requirements or approved options. These may include, for example: • specific and approved treatment processes in relation to source types and characteristics; • providing guidance on requirements for protection of well heads; • requirements for protection of drinking-water quality in distribution systems. It is important to review specified targets on a regular basis to ensure that they are kept up to date in terms of the prevailing scientific knowledge about the technology and its application. 4 Water safety plans TThe most effective means of consistently ensuring the safety of a drinking-water supply is through the use of a comprehensive risk assessment and risk management approach that encompasses all steps in the water supply from catchment to consumer. In these Guidelines, such approaches are termed water safety plans (WSPs). The WSP approach has been developed to organize and systematize a long history of management practices applied to drinking-water and to ensure the applicability of these practices to the management of drinking-water quality. WSPs represent an evolution of the concept of sanitary surveys and vulnerability assessments that include and encompass the whole of the water supply system and its operation. The WSP approach draws on many of the principles and concepts from other risk management approaches, in particular the multiple-barrier approach and hazard assessment and critical control points (as used in the food industry). This chapter focuses on the key principles of WSPs and is not a comprehensive guide to their application in practice. Practical information on how to develop and implement a WSP is available in the supporting documents Water safety plan manual and Water safety planning for small community water supplies (Annex 1). A guide to equitable water safety planning (Annex 1) provides information to ensure improvements through WSPs benefit all users. 45 WSPs vary in complexity, as appropriate for the situation. In many cases, they will be quite simple, focusing on the key hazards identified for the specific drinking-water supply system. The wide range of examples of control measures given in the following text does not imply that all of these are appropriate in all cases. WSPs should, by preference, be developed for individual drinking-water systems. For smaller systems, it may be possible to develop generic WSPs by a statutory body or accredited third-party organization. In these settings, guidance on household water storage, handling and use may also be required. Plans dealing with household water should be linked to a hygiene education programme and advice to households in maintaining water safety. A WSP has three key compo-A WSP comprises, as a minimum, the three key nents, which are guided by health-components that are the responsibility of the drinking‑water supplier in order to ensure that based targets (see chapter 3) and drinking‑water is safe. These are: overseen through drinking-water • a system assessment; supply surveillance (see chapter 5). • effective operational monitoring; They are: • management and communication. 1) a system assessment to determine whether the drinking-water supply chain (up to the point of consumption) as a whole can deliver water of a quality that meets identified targets. This also includes the assessment of design criteria of new systems; 2) identifying control measures in a drinking-water system that will collectively control identified risks and ensure that the health-based targets are met. For each control measure identified, an appropriate means of operational monitoring should be defined that will ensure that any deviation from required performance is rapidly detected in a timely manner; 3) management and communication plans describing actions to be taken during normal operation or incident conditions and documenting the system assessment, including upgrade and improvement planning, monitoring and communication plans and supporting programmes. The primary objectives of a WSP in ensuring good drinking-water supply practice are the prevention or minimization of contamination of source waters, the reduction or removal of contamination through treatment processes and the prevention of contamination during storage, distribution and handling of drinking-water. These objectives are equally applicable to large piped drinking-water supplies, small community supplies (see section 1.2.6) and household systems and are achieved through: • development of an understanding of the specific system and its capability to supply water that meets water quality targets; • identification of potential sources of contamination and how they can be controlled; • validation of control measures employed to control hazards; • implementation of a system for operational monitoring of the control measures within the water system; • timely corrective actions to ensure that safe water is consistently supplied; • undertaking verification of drinking-water quality to ensure that the WSP is being implemented correctly and is achieving the performance required to meet relevant national, regional and local water quality standards or objectives. WSPs are a powerful tool for the drinking-water supplier to manage the supply safely. They also assist surveillance by public health authorities. Key benefits for water suppliers implementing WSPs include: • demonstration of “due diligence”; • improved compliance; • rationalizing and documenting existing operational procedures, leading to gains in efficiency, improvement of performance and quicker response to incidents; • better targeted and justification for long-term capital investments based on risk assessment; • improved management of existing staff knowledge and identification of critical gaps in skills for staff; • improved stakeholder relationships. One of the challenges and responsibilities of water suppliers and regulators is to anticipate, plan for and provide for climate variations and weather extremes. WSPs are an effective tool to manage such variations and extremes (see Climate resilient water safety plans (Annex 1) and also section 6.1). Where a defined entity is responsible for a drinking-water supply, its responsibility should include the preparation and implementation of a WSP. This plan should normally be reviewed and agreed upon with the authority responsible for protection of public health to ensure that it will deliver water of a quality consistent with the defined targets. Where there is no formal service provider, the competent national or regional authority should act as a source of information and guidance on the adequacy of appropriate management of community and individual drinking-water supplies. This will include defining requirements for operational monitoring and management. Approaches to verification in these circumstances will depend on the capacity of local authorities and communities and should be defined in national policy. Many water suppliers may face practical challenges in initiating, developing and implementing a WSP. These include mistaken perceptions that one prescribed methodology must be followed; that WSP steps must be undertaken with risks managed from source to tap in a defined order; that developing a WSP always requires external expertise; that WSPs supersede, rather than build on, existing good practices; and that WSPs are necessarily complicated and are not appropriate for small supplies. Although WSP implementation demands a certain minimum standard in terms of the steps involved (Figure 4.1), it is a flexible approach that should rely on the water supplier’s existing practices and fit the way that a supplier is organized. The WSP is a vital step in identifying the hazards and risks associated with the source water catchment, particularly where the water supplier does not manage the catchment, or with established treatment and distribution systems. Starting with existing treatment to ensure that it is operating at its optimum at all times is a 48 GUIDELINES FOR DRINKING-WATER QUALITY: FOURTH EDITION INCORPORATING THE FIRST AND SECOND ADDENDA Figure 4.1 Overview of the steps in developing a water safety plan vital component, as this is often the key barrier that prevents hazards from reaching drinking-water. It must be recognized that even if other hazards are identified in the catchment, remediation may take time, and this should not be a reason for delaying the start of WSP preparation and implementation. Similarly, initiating the process of ensuring that the distribution system is intact and managed appropriately is a vital step that is under the control of the water supplier. Many of the procedures inherent in the WSP, such as documenting the system and ensuring that standard operating procedures are established for each of the treatment processes and the operation of the distribution system, are simply normal good practice in drinking-water supply. The WSP should therefore build on and improve existing practice. WSPs should also not be seen as a competing initiative to existing programmes already being undertaken. For example, a programme that addresses non-revenue water (e.g. leakage), although primarily addressing a water quantity issue, is also part of a WSP. A non-revenue water programme would address issues such as intermittent supply and low water pressure, both of which are contributing factors to contamination of drinking-water in the distribution system. It is recognized that it will not be possible to fully establish a WSP all at once, but the mapping of the system, the identification of the hazards and the assessment of the risks will provide a framework for prioritizing actions and will identify the requirements for continuing improvement as resources become available. They will also identify and help make the case for resource allocation and investment so that they can be targeted to provide the greatest benefit, thus optimizing resources and investment. In some countries, the regulatory system is relatively complex. A vital component of WSPs and the delivery of safe drinking-water is proper communication and exchange of information between regulators, including environmental authorities, and between regulators or authorities and water suppliers. This is particularly important if resources are to be optimized, and shared information can lead to savings on all sides, while ensuring that drinking-water supplies are improved. Small supplies remain a significant challenge for many countries, partly because human, technical and financial resources are limited. The introduction of WSPs helps to identify simple and cost-effective steps that can be taken to protect and improve such supplies. It is important that health authorities emphasize the importance of safe drinking-water to the local community and raise the status of the operator’s role in the community. It would also be helpful for the relevant authorities to provide a resource or point of contact where operators can obtain advice on and help for WSP implementation. 4.1 System assessment and design The first stage in developing a WSP is to form a multidisciplinary team of experts with a thorough understanding of the drinking-water system involved. The team should be led by the drinking-water supplier and have sufficient expertise in abstraction, treatment and distribution of drinking-water. Typically, such a team would include individuals involved in each stage of the supply of drinking-water and in many cases representatives from a wider group of stakeholders with collective responsibility for the water supply system from catchment to consumer. Teams could include engineers, catchment and water managers, water quality specialists, environmental or public health or hygienist professionals, operational staff and representatives of consumers or from the community. In most settings, the team will include members from external agencies, including the relevant regulatory agency. For small water supplies, additional external expertise may be useful in addition to operational personnel. Effective management of the drinking-water system requires a comprehensive understanding of the system, the range and magnitude of hazards and hazardous events that may affect the system and the ability of existing processes and infrastructure to manage actual or potential risks (otherwise known as a sanitary survey). It also requires an assessment of capabilities to meet targets. When a new system or an upgrade of an existing system is being planned, the first step in developing a WSP is the collection and evaluation of all available relevant information and consideration of what risks may arise during delivery of water to the consumer. Assessment of the drinking-water system supports subsequent steps in the WSP in which effective strategies for control of hazards are planned and implemented. The assessment and evaluation of a drinking-water system are enhanced through an accurate system description, including a flow diagram. The system description should provide an overview of the drinking-water system, including characterization of the source, identification of potential pollution sources in the catchment, measures for resource and source protection, treatment processes, storage and mechanisms for distribution (including piped and non-piped systems). It is essential that the description and the flow diagram of the drinking-water system are conceptually accurate. If the description is not correct, it is possible to Effective risk management requires the identification of potential overlook potential haz-hazards and hazardous events and an assessment of the level of ards that may be sig-risk presented by each. In this context: nificant. To ensure ac-• a hazard is a biological, chemical, physical or radiological agent that has the potential to cause harm; curacy, the system a hazardous event is an incident or situation that can lead description should be to the presence of a hazard (what can happen and how); validated by visually • risk is the likelihood of identified hazards causing harm in checking against fea-exposed populations in a specified time frame, including the magnitude of that harm and/or the consequences. • tures observed on the ground. Data on the occurrence of pathogens and chemicals in source waters and in drinking-water combined with information concerning the effectiveness of existing controls enable an assessment of whether health-based targets can be achieved with the existing infrastructure. They also assist in identifying catchment management measures, treatment processes and distribution system It may often be more efficient to in‑operating conditions that would reasonably vest in preventive processes within the catchment than to invest in major treat‑be expected to achieve those health-based ment infrastructure to manage a hazard. targets if improvements are required. To ensure the accuracy of the assessment, including an overall estimate of risk, it is essential that all elements of the drinking-water system (catchment, treatment and distribution) are considered concurrently and that interactions among these elements are taken into consideration. 4.1.1 New systems When drinking-water supply sources are being investigated or developed, it is prudent to undertake a wide range of analyses in order to establish overall safety and to determine potential sources of contamination of the drinking-water supply source. These analyses would normally include hydrological analysis, geological assessment and land use inventories to determine potential chemical and radiological contaminants. When designing new systems, all water quality factors should be taken into account in selecting technologies for abstraction and treatment of new resources. Variations in the turbidity and other parameters of raw surface waters can be considerable, and allowance must be made for this. Treatment plants should be designed to take account of variations known or expected to occur with significant frequency rather than for average water quality; otherwise, for example, filters may rapidly become blocked or sedimentation tanks overloaded. The chemical aggressiveness of some groundwaters may affect the integrity of borehole casings and pumps, leading to unacceptably high levels of iron in the supply, eventual breakdown and expensive repair work. Both the quality and availability of drinking-water may be reduced and public health endangered. 4.1.2 Collecting and evaluating available data Areas that should be taken into consideration as part of the assessment of the drinking-water system include all real or potential hazards and hazardous events associated with each step in the drinking-water system that could result in contamination or interruption of supply. In most cases, consultation with public health and other sectors, including land and water users and all those who regulate activities in the catchment, will be required for the analysis of catchments. A structured approach is important to ensure that significant issues are not overlooked and that areas of greatest risk are identified. The overall assessment of the drinking-water system should take into consideration any historical water quality data that may assist in understanding source water characteristics and drinking-water system performance both over time and following specific events (e.g. heavy rainfall). For examples of information to consider in assessing components of the drinking-water system, see Module 3 in the supporting document Water safety plan manual (Annex 1). Prioritizing hazards for control Once potential hazards and their sources have been identified, the risk associated with each hazard or hazardous event should be compared so that priorities for risk management can be established and documented. Although there are numerous contaminants that can compromise drinking-water quality, not every hazard or hazardous event will require the same degree of attention. The risk associated with each hazard or hazardous event may be described by identifying the likelihood of occurrence (e.g. certain, possible, rare) and evaluating the severity of consequences if the hazard occurred (e.g. insignificant, major, catastrophic). The aim should be to distinguish between important and less important hazards or hazardous events. The approach used typically involves a semiquantitative matrix. Simple scoring matrices often apply technical information from guidelines, scientific literature and industry practice with well-informed “expert” judgement based on knowledge and experience of WSP team members, supported by peer review or benchmarking. Scoring is specific for each drinking-water system, as each system is unique. Where generic WSPs are developed for technologies used by small drinking-water systems, the scoring will be specific to the technology rather than the individual drinking-water system. By using risk ranking, control measures can be prioritized in relation to their significance. A variety of semiquantitative and qualitative approaches to ranking risk can be applied, and Module 3 of the supporting document Water safety plan manual (Annex 1) provides a series of practice-based examples. An example of a semiquantitative approach is given in Table 4.1. Application of this matrix relies to a significant Table 4.1 Example of a simple scoring matrix for ranking risks Likelihood Severity of consequences Insignificant Minor Moderate Major Catastrophic Almost certain 5 10 15 20 25 Likely 4 8 12 16 20 Moderately likely 3 6 9 12 15 Unlikely 2 4 6 8 10 Rare 1 2 3 4 5 Risk score < 6 6–9 10–15 > 15 Risk rating Low Medium High Very high extent on expert opinion to make judgements on the public health risk posed by hazards or hazardous events. An example of descriptors that can be used to rate the likelihood of occurrence and severity of consequences is given in Table 4.2. A “cut-off ” point must be determined, above which all risks will require immediate attention. There is little value in expending large amounts of effort to consider very low risks. Control measures The assessment and planning of control measures should ensure that health-based targets will be met and should be based on hazard identification and risk assessment. The level of control applied to a hazard should be proportional to the associated risk ranking. Assessment of control measures involves: • identifying existing control measures for each significant hazard or hazardous event from catchment to consumer; • evaluating whether the control measures, when considered together, are effective in reducing risk to acceptable levels; • if improvement is required, evaluating alternative and additional control measures that could be applied. Identification and implementation of control measures should be based on the multiple-barrier principle. The strength of this approach is that a failure of one barrier may be compensated by effective operation of the remaining barriers, thus minimizing the Control measures are activities or likelihood of contaminants passing through processes within the drinking‑water the entire system and being present in suffi-supply used to eliminate or significantly reduce the occurrence of a cient amounts to cause harm to consumers. water safety hazard. These measures Many control measures may contribute to are applied collectively to ensure that control more than one hazard, whereas some drinking‑water consistently meets hazards may require more than one control health‑based targets. measure for effective control. Examples of control measures are provided in the following sections. Table 4.2 Examples of definitions of likelihood and severity categories that can be used in risk scoring Item Rating Definition Likelihood categories Almost certain 5 Once per day Likely 4 Once per week Moderately likely 3 Once per month Unlikely 2 Once per year Rare 1 Once every 5 years Severity categories Catastrophic 5 Public health impact Major 4 Regulatory impact Moderate 3 Aesthetic impact Minor 2 Compliance impact Insignificant 1 No impact or not detectable All control measures are important and should be afforded ongoing attention. They should be subject to operational monitoring and control, with the means of monitoring and frequency of data collection based on the nature of the control measure and the rapidity with which change may occur (see section 4.2). 4.1.3 Resource and source protection Effective catchment management has many benefits. By decreasing the contamination of the source water, the amount of treatment required is reduced. This may reduce the production of treatment by-products and minimize operational costs. Hazard identification Understanding the reasons for variations in raw water quality is important, as it will influence the requirements for treatment, treatment efficiency and the resulting health risk associated with the finished drinking-water. In general, raw water quality is influenced by both natural and human use factors. Important natural factors include wildlife, climate, topography, geology and vegetation. Human use factors include point sources (e.g. wastewater discharges) and non-point sources (e.g. surface runoff). For example, discharges of municipal wastewater can be a major source of pathogens; urban runoff and livestock can contribute substantial microbial load; body contact recreation can be a source of faecal contamination; and agricultural runoff, including agrochemicals and manure, can lead to increased challenges to treatment. Whether water is drawn from surface or underground sources, it is important that the characteristics of the local catchment or aquifer are understood and that the scenarios that could lead to water pollution are identified and managed. The extent to which potentially polluting activities in the catchment can be reduced may appear to be limited by competition for water and pressure for increased development in the catchment. However, introducing good practices in land use and in containment of hazards is often possible without substantially restricting activities, and collaboration between stakeholders may be a powerful tool to reduce pollution without reducing beneficial development. Resource and source protection provides the first barrier in protection of drinking-water quality. Where catchment management is beyond the jurisdiction of the drinking-water supplier, the planning and implementation of control measures will require coordination with other agencies. These may include planning authorities, catchment boards, environmental and water resource regulators, road authorities, emergency services and agricultural, industrial and other commercial entities whose activities have an impact on water quality. It may not be possible to apply all aspects of resource and source protection initially; nevertheless, priority should be given to catchment management. This will contribute to a sense of ownership and joint responsibility for drinking-water resources through multistakeholder bodies that assess pollution risks and develop plans for improving management practices for reducing these risks. Groundwater from deep and confined aquifers is usually microbially safe and chemically stable in the absence of direct contamination; however, shallow or unconfined aquifers can be subject to contamination from discharges or seepages associated with agricultural practices (e.g. pathogens, nitrates and pesticides), on-site sanitation and sewerage (e.g. pathogens and nitrates) and industrial wastes. For examples of hazards and hazardous situations that should be taken into consideration as part of a hazard analysis and risk assessment, see Module 4 in the supporting document Water safety plan manual and the supporting documents Protecting groundwater for health and Protecting surface water for health (Annex 1). Control measures Effective resource and source protection includes the following elements: • developing and implementing a catchment management plan, which includes control measures to protect surface water and groundwater sources; • ensuring that planning regulations include the protection of water resources (land use planning and watershed management) from potentially polluting activities and are enforced; • promoting awareness in the community of the impact of human activity on water quality. Where a number of water sources are available, there may be flexibility in the selection of water for treatment and supply. It may be possible to avoid taking water from rivers and streams when water quality is poor (e.g. following heavy rainfall) in order to reduce risk and prevent potential problems in subsequent treatment processes. Retention of water in reservoirs can reduce the number of faecal microorganisms through settling and inactivation, including solar (ultraviolet) disinfection, but also provides opportunities for the introduction of contamination. Most pathogenic microorganisms of faecal origin (enteric pathogens) do not survive indefinitely in the environment. Substantial die-off of enteric bacteria will occur over a period of weeks. Enteric viruses and protozoa will often survive for longer periods (weeks to months) but are often removed by settling and antagonism from indigenous microbes. Retention also allows suspended material to settle, which makes subsequent disinfection more effective and reduces the formation of disinfection by-products (DBPs). Control measures for groundwater sources should include protecting the aquifer and the local area around the borehead from contamination and ensuring the physical integrity of the bore (surface sealed, casing intact, etc.); further information can be found in the supporting document Protecting groundwater for health (Annex 1). For examples of control measures for effective protection of source water and catchments and of water extraction and storage systems, see Module 4 in the supporting document Water safety plan manual and the supporting document Protecting surface water for health (Annex 1). Further information on the use of indicator organisms in catchment characterization is also available in chapter 4 of the supporting document Assessing microbial safety of drinking water (Annex 1). 4.1.4 Treatment After source water protection, the next barriers to contamination of the drinking-water system are those of water treatment processes, including disinfection and physical removal of contaminants. Hazard identification Hazards may be introduced during treatment, or hazardous events may allow contaminants to pass through treatment in significant concentrations. Constituents of drinking-water can be introduced through the treatment process, including chemical additives used in the treatment process or products in contact with drinking-water. Sporadic high turbidity in source water can overwhelm treatment processes, allowing enteric pathogens into treated water and the distribution system. Similarly, suboptimal filtration following filter backwashing can lead to the introduction of pathogens into the distribution system. For examples of potential hazards and hazardous events that can have an impact on the performance of drinking-water treatment, see Module 3 in the supporting document Water safety plan manual (Annex 1). Control measures Control measures may include pretreatment, coagulation, flocculation, sedimentation, filtration and disinfection. Pretreatment includes processes such as roughing filters, microstrainers, off-stream storage and bankside filtration. Pretreatment options may be compatible with a variety of treatment processes ranging in complexity from simple disinfection to membrane processes. Pretreatment can reduce or stabilize the microbial, natural organic matter and particulate load. Coagulation, flocculation, sedimentation (or flotation) and filtration remove particles, including microorganisms (bacteria, viruses and protozoa). It is important that processes are optimized and controlled to achieve consistent and reliable performance. Chemical coagulation is the most important step in determining the removal efficiency of coagulation, flocculation and clarification processes. It also directly affects the removal efficiency of granular media filtration units and has indirect impacts on the efficiency of the disinfection process. While it is unlikely that the coagulation process itself introduces any new microbial hazards to finished water, a failure or inefficiency in the coagulation process could result in an increased microbial load entering drinking-water distribution. Various filtration processes are used in drinking-water treatment, including granular, slow sand, precoat and membrane (microfiltration, ultrafiltration, nanofiltration and reverse osmosis) filtration. With proper design and operation, filtration can act as a consistent and effective barrier for pathogenic microorganisms and may in some cases be the only treatment barrier (e.g. for removing Cryptosporidium oocysts by direct filtration when chlorine is used as the sole disinfectant). Application of an adequate concentration of disinfectant is an essential element for most treatment systems to achieve the necessary level of microbial risk reduction. Taking account of the level of microbial inactivation required for the more resistant microbial pathogens through the application of the Ct concept (product of disinfectant concentration and contact time) for a particular pH and temperature ensures that other, more sensitive microbes are also effectively controlled. Where disinfection is used, measures to minimize DBP formation should be taken into consideration. The most commonly used disinfection process is chlorination. Ozonation, ultraviolet irradiation, chloramination and application of chlorine dioxide are also used. These methods are very effective in killing bacteria and can be reasonably effective in inactivating viruses (depending on type), and some may inactivate many protozoa, including Giardia and Cryptosporidium. For effective removal or inactivation of protozoal cysts and oocysts, filtration with the aid of coagulation and flocculation (to reduce particles and turbidity) followed by disinfection (by one or a combination of disinfectants) is the most practical method. Storage of water after disinfection and before supply to consumers can improve disinfection by increasing disinfectant contact times. This can be particularly important for more resistant microorganisms, such as Giardia and some viruses. For examples of treatment control measures, see Module 4 in the supporting document Water safety plan manual (Annex 1). Further information can also be found in the supporting document Water treatment and pathogen control (Annex 1). 4.1.5 Piped distribution systems Water treatment should be optimized to prevent microbial growth, corrosion of pipe materials and the formation of deposits. Maintaining good water quality in the distribution system will depend on the design and operation of the system and on maintenance and survey procedures to prevent contamination and to prevent and remove the accumulation of internal deposits. Hazard identification The protection of the distribution system is essential for providing safe drinking-water. Because of the nature of the distribution system, which may include many kilometres of pipe, storage tanks, interconnections with industrial users and the potential for tampering and vandalism, opportunities for microbial and chemical contamination exist. For examples of hazards and hazardous events in piped distribution systems, see Module 3 in the supporting document Water safety plan manual (Annex 1). Further information can also be found in the supporting document Water safety in distribution systems (Annex 1). When contamination by enteric pathogens or hazardous chemicals occurs within the distribution system, it is likely that consumers will be exposed. to the pathogens or chemicals. In the case of pathogen ingress, even where disinfectant residuals are employed to limit microbial occurrence, they may be inadequate to overcome the contamination or may be ineffective against some or all of the pathogen types introduced. As a result, pathogens may occur in concentrations that could lead to infection and illness. Where water is supplied intermittently, the resulting low water pressure will allow the ingress of contaminated water into the system through breaks, cracks, joints and pinholes. Intermittent supplies are not desirable but are very common in many countries and are frequently associated with contamination. The control of water quality in intermittent supplies represents a significant challenge, as the risks of infiltration and backflow increase significantly. The risks may be elevated seasonally as soil moisture conditions increase the likelihood of a pressure gradient developing from the soil to the pipe. Where contaminants enter the pipes in an intermittent supply, the charging of the system when supply is restored may increase risks to consumers, as a concentrated “slug” of contaminated water can be expected to flow through the system. Where household storage is used to overcome intermittent supply, localized use of disinfectants to reduce microbial proliferation may be warranted. Drinking-water entering the distribution system may contain free-living amoebae and environmental strains of various heterotrophic bacterial and fungal species. Under favourable conditions, amoebae and heterotrophs, including strains of Citrobacter, Enterobacter and Klebsiella, may colonize distribution systems and form bio-films. There is no evidence to implicate the occurrence of most microorganisms from biofilms (one exception is Legionella, which can colonize water systems in buildings) with adverse health effects in the general population through drinking-water, with the possible exception of severely immunocompromised people (see the supporting document Heterotrophic plate counts and drinking-water safety; Annex 1). Water temperatures and nutrient concentrations are not generally elevated enough within the distribution system to support the growth of E. coli (or enteric pathogenic bacteria) in biofilms. Thus, the presence of E. coli should be considered as evidence of recent faecal contamination. Natural disasters, including flood, drought and earth tremors, may significantly affect piped water distribution systems. Control measures Water entering the distribution system must be microbially safe and ideally should also be biologically stable. The distribution system itself must provide a secure barrier to contamination as the water is transported to the user. Maintaining a disinfectant residual throughout the distribution system can provide some protection against recontamination and limit microbial growth problems. Chloramination has proved successful in controlling Naegleria fowleri in water and sediments in long pipelines and may reduce the regrowth of Legionella within buildings. Residual disinfectant will provide partial protection against microbial contamination, but it may also mask the detection of contamination through the use of conventional faecal indicator bacteria such as E. coli, particularly by resistant organisms. Where a disinfectant residual is used within a distribution system, measures to minimize DBP production should be taken into consideration. Water distribution systems should be fully enclosed, and storage reservoirs and tanks should be securely roofed with external drainage to prevent contamination. Control of short-circuiting and prevention of stagnation in both storage and distribution contribute to prevention of microbial growth. A number of strategies can be adopted to maintain the quality of water within the distribution system, including use of backflow prevention devices, maintaining positive pressure throughout the system and implementation of efficient maintenance procedures. It is also important that appropriate security measures be put in place to prevent unauthorized access to or interference with the drinking-water system infrastructure. Control measures may include using a more stable secondary disinfecting chemical (e.g. chloramines instead of free chlorine), undertaking a programme of pipe replacement, flushing and relining and maintaining positive pressure in the distribution system. Reducing the time that water is in the system by avoiding stagnation in storage tanks, loops and dead-end sections will also contribute to maintaining drinking-water quality. For other examples of distribution system control measures, see Module 4 in the supporting document Water safety plan manual (Annex 1). Further information is also available in the supporting document Water safety in distribution systems and Safe piped water (Annex 1). 4.1.6 Non-piped, community and household systems Hazard identification For non-piped, community and household drinking-water systems, hazard identification would ideally be performed on a case-by-case basis. In practice, however, reliance is typically placed on general assumptions of hazardous conditions that are relevant for technologies or system types and that may be defined at a national or regional level. For examples of hazards and hazardous situations potentially associated with various non-piped sources of water, see Module 3 in the supporting documents Water safety plan manual and Water safety planning for small community water supplies (Annex 1). Further guidance is also provided in the supporting document Water safety plans (Annex 1) and in the 1997 volume entitled Surveillance and control of community supplies (WHO, 1997). Control measures The control measures required ideally depend on the characteristics of the source water and the associated catchment; in practice, standard approaches may be applied for each of these, rather than customized assessment of each system. For examples of control measures for various non-piped sources, see Module 4 in the supporting documents Water safety plan manual and Water safety planning for small community water supplies (Annex 1) and the 1997 report entitled Surveillance and control of community supplies (WHO, 1997). In most cases, contamination of groundwater supplies can be controlled by a combination of simple measures. In the absence of fractures or fissures, which may allow rapid transport of contaminants to the source, groundwater in confined or deep aquifers will generally be free of pathogenic microorganisms. Bores should be encased to a reasonable depth, and boreheads should be sealed to prevent ingress of surface water or shallow groundwater. Rainwater harvesting systems, particularly those involving storage in aboveground tanks, can be a relatively safe supply of water (see section 6.2). The principal sources of contamination are birds, small mammals and debris collected on roofs. The impact of these sources can be minimized by simple measures: guttering should be cleared regularly, overhanging branches should be kept to a minimum (because they can be a source of debris and can increase access to roof catchment areas by birds and small mammals) and inlet pipes to tanks should include leaf litter strainers. First-flush diverters, which prevent the initial roof-cleaning wash of water (20–25 litres) from entering tanks, are recommended. If first-flush diverters are not available, a detachable downpipe can be used manually to provide the same result. In general, surface waters will require at least disinfection, and usually also filtration, to ensure microbial safety. The first barrier is based on minimizing contamination from human waste, livestock and other hazards at the source. The greater the protection of the water source, the less the reliance on treatment or disinfection. Water should be protected during storage and delivery to consumers by ensuring that the distribution and storage systems are enclosed. This applies to both community piped systems and vendor-supplied water (section 6.3). For water stored in the home, protection from contamination can be achieved by use of enclosed or otherwise safely designed storage containers that prevent the introduction of hands, dippers or other extraneous sources of contamination. For control of chemical hazards, reliance may be placed primarily on initial screening of sources and on ensuring the quality and performance of treatment chemicals, materials and devices available for this use, including water storage systems. Model WSPs may be developed generically for the following types of water supply: • groundwater from protected boreholes or wells with mechanized pumping; • conventional treatment of water; • multistage filtration; • storage and distribution through supplier-managed piped systems; • storage and distribution through community-managed piped systems; • water vendors; • water on conveyances (planes, ships and trains); • tubewells from which water is collected by hand; • springs from which water is collected by hand; • simple protected dug wells; • rainwater catchments. Guidance is available regarding how water safety may be ensured for household water collection, transport and storage (see the supporting document Managing water in the home; Annex 1). This should be used in conjunction with hygiene education programmes to support health promotion in order to reduce water-related disease. 4.1.7 Validation For the WSP to be relied on for anticipating and managing the hazards and hazardous events for which it was set in place, it needs to be supported by accurate and reliable technical information. Validation is concerned with obtaining evidence on the performance of control measures. Depending on the type of control, validation can be done by site inspection, using existing data and literature or targeted monitoring programmes to demonstrate performance under normal and exceptional circumstances. Validation of treatment processes is required to show that the Validation is an investigative activity to identify the treatment processes can operate effectiveness of a control measure. It is typically an intensive activity when a system is initially con‑as required and achieve required structed or rehabilitated. It provides information on levels of hazard reduction. In the reliably achievable water quality in preference to case of microbial hazards, these assumed values and also to define the operational required levels commonly take the criteria required to ensure that the control measure contributes to effective control of hazards. form of performance targets based on the use of reference pathogens (see section 7.2). Validation can be undertaken during pilot stage studies or during initial implementation of a new or modified water treatment system. It is also a useful tool in the optimization of existing treatment processes. The first stage of validation is to consider data and information that already exist. Sources include the scientific literature, relevant industry bodies, partnering and benchmarking with larger authorities, manufacturers’ specifications and historical data. This stage will inform the testing requirements. It is important that data used in validation are relevant for system-specific conditions, as variations in water composition and quality, for example, may have a large impact on the efficacy of control measures. Validation is not used for day-to-day management of drinking-water supplies; as a result, microbial parameters that may be inappropriate for operational monitoring can be used, and the lag time for return of results and additional costs from pathogen measurements can often be tolerated. Parameters should be chosen to reflect the microorganisms being targeted by treatment (see section 7.2). Increasingly, indicator parameters are being used in validation. For example, coliphage can be used to assess the effectiveness of virus removal by filtration processes or to measure the effectiveness of disinfection processes, whereas Clostridium perfringens can be used to measure the effectiveness of the removal of protozoa by filtration processes. Validation should not be confused with routine operational monitoring, which is designed to show that validated control measures continue to work effectively (see section 4.2). The validation process often leads to improvements in operating performance through the identification of the most effective and robust operating modes. Additional benefits of the validation process may include identification of more suitable operational monitoring parameters for unit performance. 4.1.8 Upgrade and improvement The assessment of the drinking-water system may indicate that existing practices and control measures may not ensure drinking-water safety. In some instances, all that may be needed is to review, document and formalize these practices and address any areas where improvements are required; in others, major infrastructure changes may be needed. The assessment of the system should be used as a basis to develop a plan to address identified needs for full implementation of a WSP. Improvement of the drinking-water system may encompass a wide range of issues, such as: • capital works; • training; • enhanced operational procedures; • community consultation programmes; • research and development; • developing incident protocols; • communication and reporting. Upgrade and improvement plans can include short-term (e.g. 1 year) or longterm programmes. Short-term improvements might include, for example, improvements to community consultation and the development of community awareness programmes. Long-term capital works projects could include covering of water storages or enhanced coagulation and filtration. Implementation of improvement plans may have significant budgetary implications and therefore may require detailed analysis and careful prioritization in accord with the outcomes of risk assessment. Implementation of plans should be monitored to confirm that improvements have been made and are effective. Control measures often require considerable expenditure, and decisions about water quality improvements cannot be made in isolation from other aspects of drinking-water supply that compete for limited financial resources. Priorities will need to be established, and improvements may need to be phased in over a period of time. 4.2 Operational monitoring and maintaining control Operational monitoring is a planned and routine set of activities used to determine that control measures continue to work effectively. In operational monitoring, the drinking-water supplier monitors each control measure in a timely manner with the objectives to enable effective system management and to ensure that health-based targets are achieved. 4.2.1 Determining system control measures The identity and number of control measures are system specific and will be determined by the number and nature of hazards and hazardous events as well as the magnitude of associated risks. Control measures should reflect the likelihood and consequences of loss of control. Control measures have a number of operational requirements, including the following: • operational monitoring parameters that can be measured and for which limits can be set to define the operational effectiveness of the activity; • operational monitoring parameters that can be monitored with sufficient frequency to reveal failures in a timely fashion; • procedures for corrective action that can be implemented in response to deviation from limits. 4.2.2 Selecting operational monitoring parameters Operational monitoring can include measurement of parameters or observational activities. The parameters selected for operational monitoring should reflect the effectiveness of each control measure, provide a timely indication of performance, be readily Operational monitoring assesses the measured and provide the opportunity for performance of control measures at appropriate time intervals. The inter‑an appropriate response. Examples include vals may vary widely—for example, measurable variables, such as chlorine residfrom online control of residual chlorine uals, pH and turbidity, or observable factors, to quarterly verification of the integrity such as the integrity of vermin-proof of the plinth surrounding a well. screens. Enteric pathogens or indicator organisms are often of limited use for operational monitoring, because the time taken to process and analyse water samples does not allow operational adjustments to be made prior to supply. A range of parameters can be used in operational monitoring: • For source waters, these include turbidity, ultraviolet absorbency, algal growth, flow and retention time, colour, conductivity, local meteorological events and integrity of protective (e.g. fences) or abstraction infrastructures (e.g. well seals) (see the supporting documents Protecting groundwater for health and Protecting surface water for health; Annex 1).• For treatment, parameters may include disinfectant concentration and contact time, ultraviolet intensity, pH, light absorbency, membrane integrity, turbidity and colour (see the supporting document Water treatment and pathogen control; Annex 1).• In piped distribution systems, operational monitoring parameters may include the following: — Chlorine residual monitoring provides a rapid indication of problems that will direct measurement of microbial parameters. A sudden disappearance of an otherwise stable residual can indicate ingress of contamination. Alternatively, difficulties in maintaining residuals at points in a distribution system or a gradual disappearance of residual may indicate that the water or pipework has a high oxidant demand due to growth of bacteria. — Oxidation–reduction potential (or redox potential) measurement can also be used in the operational monitoring of disinfection efficacy. It is possible to define a minimum level of oxidation–reduction potential necessary to ensure effective disinfection. This value has to be determined on a case-by-case basis; universal values cannot be recommended. Further research and evaluation of oxidation–reduction potential as an operational monitoring technique are highly desirable. — Heterotrophic bacteria present in a supply can be a useful indicator of changes, such as increased microbial growth potential, increased biofilm activity, extended retention times or stagnation and a breakdown of integrity of the system. The numbers of heterotrophic bacteria present in a supply may reflect the presence of large contact surfaces within the treatment system, such as in-line filters, and may not be a direct indicator of the condition within the distribution system (see the supporting document Heterotrophic plate counts and drinking-water safety; Annex 1). — Pressure measurement and turbidity are also useful operational monitoring parameters in piped distribution systems (see the supporting document Turbidity: information for regulators and operators of water supplies; Annex 1). Guidance for management of distribution system operation and maintenance is available (see the supporting document Safe piped water; Annex 1) and includes the development of a monitoring programme for water quality and other parameters such as pressure. Examples of operational monitoring parameters are provided in Table 4.3. 4.2.3 Establishing operational and critical limits Control measures need to have defined limits for operational acceptability—termed operational limits—that can be applied to operational monitoring parameters. Operational limits should be defined for parameters applying to each control measure. If monitoring shows that an operational limit has been exceeded, then predetermined corrective actions (see section 4.4) need to be applied. The detection of the deviation and implementation of corrective action should be possible in a time frame adequate to maintain performance and water safety. For some control measures, a second series of “critical limits” may also be defined, outside of which confidence in water safety would be lost. Deviations from critical limits will usually require urgent action, including immediate notification of the appropriate health authority. Operational and critical limits can be upper limits, lower limits, a range or an “envelope” of performance measures. 4.2.4 Non-piped, community and household systems Generally, surface water or shallow groundwater should not be used as a source of drinking-water without sanitary protection or treatment. Table 4.3 Examples of operational monitoring parameters that can be used to monitor control measures Operational parameter Raw water Coagulation Sedimentation Filtration Disinfection Distribution system pH Turbidity (or particle count) Dissolved oxygen Stream/river flow Rainfall Colour Conductivity (total dissolved solids) Organic carbon Chemical dosage Flow rate Net charge Streaming current value Headloss Ct (disinfectant concentration × contact time) Disinfectant residual Oxidation–reduction potential DBPs Heterotrophic bacteria Hydraulic pressure • • • • • • • • • • • • • Monitoring of water sources (including rainwater tanks) by community operators or households will typically involve periodic sanitary inspection (for details, see the 1997 volume entitled Surveillance and control of community supplies; WHO, 1997). The sanitary inspection forms used should be comprehensible and easy to use; for instance, the forms may be pictorial. The risk factors included should be preferably related to activities that are under the control of the operator and that may affect water quality. The links to action from the results of operational monitoring should be clear, and training will be required. Operators should also undertake regular physical assessments of the water, especially after heavy rains, to monitor whether any obvious changes in water quality have occurred (e.g. changes in colour, odour, taste or turbidity). Maintaining the quality of water during collection and manual transport is the responsibility of the household. Good hygiene practices are required and should be supported through hygiene education. Hygiene education programmes should provide households and communities with skills to monitor and manage their water hygiene. If treatment is applied to water from community sources (such as boreholes, wells and springs) as well as household rainwater collection, then operational monitoring is advisable. When household treatment is introduced, it is essential that information (and, where appropriate, training) be provided to users to ensure that they understand basic operational monitoring requirements. 4.3 Verification Verification provides a final check on the overall performance of the drinking-water supply chain and the safety of drinking-water being supplied to consumers. Verification should be undertaken by the surveillance agency; water suppliers may also undertake internal verification programmes. For microbial verification, testing is typically for faecal indicator bacteria in treated water and water in distribution. For verification of chemical safety, testing for chemicals of concern may be at the end of treatment, In addition to operational monitoring of the performance of in distribution or at the the individual components of a drinking‑water system, it is point of consumption necessary to undertake final verification for reassurance that (depending on whether the system as a whole is operating safely. Verification may be undertaken by the supplier, by an independent authority the concentrations are or by a combination of these, depending on the administralikely to change in distri-tive regime in a given country. It typically includes testing for bution). Trihalomethanes faecal indicator organisms and hazardous chemicals, as well as auditing that WSPs are being implemented as intended and haloacetic acids are and are working effectively. the most common DBPs and occur at among the highest concentrations in drinking-water. Under many circumstances, they can serve as a suitable measure that will reflect the concentration of a wide range of related chlorinated DBPs. Frequencies of sampling should reflect the need to balance the benefits and costs of obtaining more information. Sampling frequencies are usually based on the population served or on the volume of water supplied, to reflect the increased population risk. Frequency of testing for individual characteristics will also depend on variability. Sampling and analysis are required most frequently for microbial and less often for chemical constituents. This is because even brief episodes of microbial contamination can lead directly to illness in consumers, whereas episodes of chemical contamination that would constitute an acute health concern, in the absence of a specific event (e.g. chemical overdosing at a treatment plant), are rare. Sampling frequencies for water leaving treatment depend on the quality of the water source and the type of treatment. Plans should be developed to respond to results that do not meet water quality targets. These should include investigation of the cause of non-compliance and, where necessary, corrective action, such as boil water advisories. Repeated failure to meet targets should lead to review of the WSP and development of improvement plans. 4.3.1 Microbial water quality Verification of the microbial quality of drinking-water typically includes testing for Escherichia coli as an indicator of faecal pollution. In practice, testing for thermotolerant coliform bacteria can be an acceptable alternative in many circumstances. Although E. coli is useful, it has limitations. Enteric viruses and protozoa are more resistant to disinfection; consequently, the absence of E. coli will not necessarily indicate freedom from these organisms. Under certain circumstances, the inclusion of more resistant indicators, such as bacteriophages and/or bacterial spores, should be considered (see section 7.4). Verification of the microbial quality of water in supply must be designed to ensure the best possible chance of detecting contamination. Sampling should therefore account for potential variations of water quality in distribution. This will normally mean taking account of locations and of times of increased likelihood of contamination. Faecal contamination will not be distributed evenly throughout a piped distribution system. In systems where water quality is good, this significantly reduces the probability of detecting faecal indicator bacteria in the relatively few samples collected. The chances of detecting contamination in systems reporting predominantly negative results for faecal indicator bacteria can be increased by using more frequent presence/absence testing. Presence/absence testing can be simpler, faster and less expensive than quantitative methods. Comparative studies of the presence/absence and quantitative methods demonstrate that the presence/absence methods can maximize the detection of faecal indicator bacteria. However, presence/absence testing is appropriate only in a system where the majority of tests for indicator organisms provide negative results. The more frequently the water is examined for faecal indicator organisms, the more likely it is that contamination will be detected. Frequent examination by a simple method is more valuable than less frequent examination by a complex test or series of tests. The nature and likelihood of contamination can vary seasonally, with rainfall and with other local conditions. Sampling should normally be random but should be increased at times of epidemics, flooding or emergency operations or following interruptions of supply or repair work. Recommended minimum sample numbers for verification of the microbial quality of drinking-water are shown in Table 4.4. 4.3.2 Chemical water quality Issues that need to be addressed in developing chemical verification include the availability of appropriate analytical facilities, the cost of analyses, the possible deterioration of samples, the stability of the contaminant, the likely occurrence of the contaminant in various supplies, the most suitable point for monitoring and the frequency of sampling. For a given chemical, the location and frequency of sampling will be determined by its principal sources (see chapter 8) and variability in its concentration. Substances that do not change significantly in concentration over time require less frequent sampling than those that might vary significantly. Table 4.4 Recommended minimum sample numbers for faecal indicator testing in distribution systemsa Type of water supply Total number of samples per year and population Point sources Progressive sampling of all sources over 3‑ to 5‑year cycles (maximum) Piped supplies < 5000 12 5000–100 000 12 per 5000 population > 100 000–500 000 12 per 10 000 population plus an additional 120 samples > 500 000 12 per 50 000 population plus an additional 600 samples a Parameters such as chlorine, turbidity and pH should be tested more frequently as part of operational and verification monitoring. In many cases, analysis of source water quality once per year, or even less, may be adequate, particularly in stable groundwaters, where the concentrations of naturally occurring substances of concern will vary very slowly over time. Concentrations of naturally occurring substances are likely to be more variable in surface waters, and surface waters therefore may require a greater number of samples, depending on the contaminant and its importance. Sampling locations will depend on the water quality characteristic being examined. Sampling at the treatment plant or at the head of the distribution system may be sufficient for constituents whose concentrations do not change during delivery. However, for those constituents whose concentrations can change during distribution, sampling should be undertaken following consideration of the behaviour or source of the specific substance. Samples should include points near the extremities of the distribution system and taps connected directly to the mains in houses and large multioccupancy buildings. Lead, for example, should be sampled at consumers’ taps, as the source of lead is usually service connections or plumbing in buildings. For further information, see the supporting documents Chemical safety of drinking-water and Developing drinking-water quality regulations and standards (Annex 1). 4.3.3 Source waters Verification testing of source waters is particularly important where there is no water treatment. It will also be useful following failure of the treatment process or as part of an investigation of a waterborne disease outbreak. The frequency of testing will depend on the reason for carrying out the sampling. Testing frequency may be: • on a regular basis (the frequency of verification testing will depend on several factors, including the size of the community supplied, the reliability of the quality of the drinking-water or degree of treatment and the presence of local risk factors); • on an occasional basis (e.g. random or during visits to community-managed drinking-water supplies); • increased following degradation of source water quality resulting from predictable incidents, emergencies or unplanned events considered likely to increase the potential for a breakthrough in contamination (e.g. following a flood, upstream spills). Prior to commissioning a new drinking-water supply, a wider range of analyses should be carried out, including parameters identified as potentially being present from a review of data from similar supplies or from a risk assessment of the source. 4.3.4 Piped distribution systems The choice of sampling points will be dependent on the individual water supply. The nature of the public health risk posed by pathogens and the contamination potential throughout distribution systems mean that collection of samples for microbial analysis (and associated parameters, such as chlorine residual, pH and turbidity) will typically be done frequently and from dispersed sampling sites. Careful consideration of sampling points and frequency is required for chemical constituents that arise from piping and plumbing materials and that are not controlled through their direct regulation and for constituents whose concentrations change in distribution, such as trihalomethanes. The use of stratified random sampling in distribution systems has proven to be effective. 4.3.5 Community-managed supplies If the performance of a community drinking-water system is to be properly evaluated, a number of factors must be considered. Some countries that have developed national strategies for the surveillance and quality control of drinking-water systems have adopted quantitative service indicators (i.e. quality, quantity, accessibility, coverage, affordability and continuity) for application at community, regional and national levels. Usual practice would be to include the critical parameters for microbial quality (normally E. coli, chlorine, turbidity and pH) and for a sanitary inspection to be carried out. Methods for these tests must be standardized and approved. It is recommended that field test kits be validated for performance against reference or standard methods and approved for use in verification testing. Together, service indicators provide a basis for setting targets for community drinking-water supplies. They serve as a quantitative guide to the adequacy of drinking-water supplies and provide consumers with an objective measure of the quality of the overall service and thus the degree of public health protection afforded. Periodic testing and sanitary inspection of community drinking-water supplies should typically be undertaken by the surveillance agency and should assess microbial hazards and known problem chemicals (see also chapter 5). Frequent sampling is unlikely to be possible, and one approach is therefore a rolling programme of visits to ensure that each supply is visited once every 3–5 years. The primary purpose is to inform strategic planning and policy rather than to assess compliance of individual drinking-water supplies. Comprehensive analysis of the chemical quality of all sources is recommended prior to commissioning as a minimum and preferably every 3–5 years thereafter. Advice on the design of sampling programmes and on the frequency of sampling for community supplies is given in the 1997 volume, Surveillance and control of community supplies (WHO, 1997). 4.3.6 Quality assurance and quality control Appropriate quality assurance and analytical quality control procedures should be implemented for all activities linked to the production of drinking-water quality data. These procedures will ensure that the data are fit for purpose—in other words, that the results produced are of adequate accuracy. Fit for purpose, or adequate accuracy, will be defined in the water quality monitoring programme, which will include a statement about accuracy and precision of the data. Because of the wide range of substances, methods, equipment and accuracy requirements likely to be involved in the monitoring of drinking-water, many detailed, practical aspects of analytical quality control are concerned. These are beyond the scope of this publication. The design and implementation of a quality assurance programme for analytical laboratories are described in detail in Water quality monitoring: A practical guide to the design and implementation of freshwater quality studies and monitoring programmes (Bartram & Ballance, 1996). The relevant chapter relates to standard ISO/IEC 17025:2005, General requirements for the competence of testing and calibration laboratories, which provides a framework for the management of quality in analytical laboratories. Guidance on sampling is given in the International Organization for Standardization (ISO) standards listed in Table 4.5. 4.3.7 Water safety plans In addition to testing of water quality, verification should include audits of WSPs to demonstrate that the plans have been properly designed, are being implemented correctly and are effective. Factors to consider include the following: • all significant hazards and hazardous events have been identified; • appropriate control measures have been included; • appropriate operational monitoring procedures have been established; • appropriate operational limits have been defined; • corrective actions have been identified; • appropriate verification monitoring procedures have been established. Audits can be undertaken as part of internal or external reviews and may form part of surveillance by independent authorities. Auditing can have both an assessment and a compliance-checking function. Further information can be found in the supporting document A practical guide to auditing water safety plans (Annex 1). 4.4 Management procedures for piped distribution systems Much of a management plan will describe actions to be taken to maintain optimal operation under normal operating conditions. These will include both responses to normal Table 4.5 International Organization for Standardization (ISO) standards for water quality giving guidance on samplinga ISO standard no. Title (water quality) 5667‑1:2006 Sampling—Part 1: Guidance on the design of sampling programmes and sampling techniques 5667‑3:2003 Sampling—Part 3: Guidance on the preservation and handling of water samples 5667‑4:1987 Sampling—Part 4: Guidance on sampling from lakes, natural and man‑made 5667‑5:2006 Sampling—Part 5: Guidance on sampling of drinking water and water from treatment works and piped distribution systems 5667‑6:2005 Sampling—Part 6: Guidance on sampling of rivers and streams 5667‑11:2009 Sampling—Part 11: Guidance on sampling of groundwaters 5667‑13:1997 Sampling—Part 13: Guidance on sampling of sludges from sewage and water treatment works 5667‑14:1998 Sampling—Part 14: Guidance on quality assurance of environmental water sampling and handling 5667‑16:1998 Sampling—Part 16: Guidance on biotesting of samples 5667‑20:2008 Sampling—Part 20: Guidance on the use of sampling data for decision making—Compliance with thresholds and classification systems 5667‑21:2010 Sampling—Part 21: Guidance on sampling of drinking water distributed by tankers or means other than distribution pipes 5667‑23:2011 Sampling—Part 23: Guidance on passive sampling in surface waters 5668‑17:2008 Sampling—Part 17: Guidance on sampling of bulk suspended sediments 13530:2009 Guidance on analytical quality control for chemical and physicochemical water analysis 17381:2003 Selection and application of ready‑to‑use test kit methods in water analysis a ISO has also established quality management standards relating to drinking‑water supply, including ISO 24510:2007, Activities relating to drinking water and wastewater services—Guidelines for the assessment and for the improvement of the service to users; and ISO 24512:2007, Activities relating to drinking water and wastewater services—Guidelines for the management of drinking water utilities and for the assessment of drinking water services. variations in operational monitoring parameters and responses Effective management implies definition of actions to be taken during normal operational conditions, when operational monitoring of actions to be taken in specific “incident” situations parameters reach critical limits. where a loss of control of the system may occur and of All activities, including standard procedures to be followed in unforeseen (emergency) operating procedures applied situations. Management procedures should be documented alongside system assessment, monitoring during normal conditions and plans, supporting programmes and communication planned responses to incidents required to ensure safe operation of the system. and emergencies, should be documented. A significant deviation in operational monitoring where a critical limit is exceeded (or in verification) is often referred to as an “incident”. An incident is any situation in which there is reason to suspect that water being supplied for drinking may be, or may become, unsafe (i.e. confidence in water safety is lost). As part of a WSP, management procedures should be defined for response to predictable incidents as well as unpredictable incidents and emergencies. Incident response plans can have a range of alert levels. These can be minor early warning, necessitating no more than additional investigation, through to emergency. Emergencies are likely to require the resources of organizations beyond the drinking-water supplier, particularly the public health authorities. Incident response plans typically comprise: • accountabilities and contact details for key personnel, often including several organizations and individuals; • lists of measurable indicators and limit values/conditions that would trigger incidents, along with a scale of alert levels; • clear description of the actions required in response to alerts; • location and identity of the standard operating procedures and required equipment; • location of backup equipment; • relevant logistical and technical information; • checklists and quick reference guides. The plan may need to be followed at very short notice, so standby rosters, effective communication systems and up-to-date training and documentation are required. Staff should be trained in response procedures to ensure that they can manage incidents or emergencies effectively. Incident and emergency response plans should be periodically reviewed and practised. This improves preparedness and provides opportunities to improve the effectiveness of plans before an emergency occurs. Following any incident or emergency, an investigation should be undertaken involving all concerned staff. The investigation should consider factors such as: • the cause of the problem; • how the problem was first identified or recognized; • the most essential actions required; • any communication problems that arose, and how they were addressed; • the immediate and longer-term consequences; • how well the emergency response plan functioned. Appropriate documentation and reporting of the incident or emergency should also be established. The organization should learn as much as possible from the incident or emergency to improve preparedness and planning for future incidents. Review of the incident or emergency may indicate necessary amendments to the WSP and existing protocols. The preparation of clear procedures, definition of accountability and provision of equipment for the sampling and storing of water in the event of an incident can be valuable for follow-up epidemiological or other investigations, and the sampling and storage of water from early on during a suspected incident should be part of the response plan. 4.4.1 Predictable incidents (“deviations”) Many incidents (e.g. exceedance of a critical limit) can be foreseen, and management plans can specify resulting actions. Actions may include, for example, temporary change of water sources (if possible), increasing coagulation dose, use of backup disinfection or increasing disinfectant concentrations in distribution systems. 4.4.2 Unplanned events Some scenarios that lead to water being considered potentially unsafe might not be specifically identified within incident response plans. This may be either because the events were unforeseen or because they were considered too unlikely to justify preparing detailed corrective action plans. To allow for such events, a general incident response plan should be developed. The plan would be used to provide general guidance on identifying and handling of incidents along with specific guidance on responses that would be applied to many different types of incident. A protocol for situation assessment and declaring incidents would be provided in a general incident response plan that includes personal accountabilities and categorical selection criteria. The selection criteria may include time to effect, population affected and nature of the suspected hazard. The success of general incident responses depends on the experience, judgement and skill of the personnel operating and managing the drinking-water supply. However, generic activities that are common in response to many incidents can be incorporated within general incident response plans. For example, for piped systems, emergency flushing standard operating procedures can be prepared and tested for use in the event that contaminated water needs to be flushed from a piped system. Similarly, standard operating procedures for rapidly changing or bypassing reservoirs can be prepared, tested and incorporated. The development of such a “toolkit” of supporting material limits the likelihood of error and speeds up responses during incidents. 4.4.3 Emergencies Water suppliers should develop plans to be invoked in the event of an emergency. These plans should consider potential natural disasters (e.g. earthquakes, floods, damage to electrical equipment by lightning strikes), accidents (e.g. spills in the watershed, interruptions in electricity supply), damage to treatment plant and distribution system and human actions (e.g. strikes, sabotage). Emergency plans should clearly specify responsibilities for coordinating measures to be taken, a communication plan to alert and inform users of the drinking-water supply and plans for providing and distributing emergency supplies of drinking-water. Plans should be developed in consultation with relevant regulatory authorities and other key agencies and should be consistent with national and local emergency response arrangements. Key areas to be addressed in emergency response plans include: • response actions, including increased monitoring; • responsibilities of authorities internal and external to the organization; • plans for emergency drinking-water supplies; • communication protocols and strategies, including notification procedures (internal, regulatory body, media and public); • mechanisms for increased public health surveillance. Response plans for emergencies and unforeseen events involving microorganisms or chemicals should also include the basis for issuing boil water advisories (see section 7.6.1) and water avoidance advisories (see section 8.7.10). The objective of the advisory should be taken in the public interest. Therefore, the advisory should be issued after rapid, but careful, consideration of available information and conclusion that there is an ongoing risk to public health that outweighs any risk from the advice to boil or avoid water. The advisory will typically be managed by public health authorities. A decision to close a drinking-water supply carries an obligation to provide an alternative safe supply and is very rarely justifiable because of the adverse effects, especially to health, of restricting access to water. Specific actions in the event of a guideline exceedance or an emergency are discussed in section 7.6 (microbial hazards) and section 8.7 (chemical hazards); more general considerations are discussed in section 6.8. “Practice” emergencies are an important part of the maintenance of readiness for emergencies. They help to determine the potential actions that can be taken in different circumstances for a specific water supply. 4.4.4 Preparing a monitoring plan Programmes should be developed for operational and verification monitoring and documented as part of a WSP, detailing the strategies and procedures to follow for monitoring the various aspects of the drinking-water system. The monitoring plans should be fully documented and should include the following information: • parameters to be monitored; • sampling location and frequency; • sampling methods and equipment; • schedules for sampling; • references to corrective action procedures, including responsibilities; • qualifications and certification requirements for testing laboratories; • methods for quality assurance and validation of sampling results; • requirements for checking and interpreting results; • responsibilities and necessary qualifications of staff; • requirements for documentation and management of records, including how monitoring results will be recorded and stored; • requirements for reporting and communication of results. 4.4.5 Supporting programmes Many actions are important in ensuring drinking-water safety but do not directly affect drinking-water quality and are therefore not control measures. These are referred to as “supporting programmes” and should also be documented in a WSP. Supporting programmes could involve: • controlling access to treatment plants, catchments and reservoirs and implementing the Actions that are important in appropriate security measures to prevent ensuring drinking‑water safety but do not directly affect drink‑transfer of hazards from people when they do ing‑water quality are referred enter source water; to as supporting programmes. • developing verification protocols for the use of chemicals and materials in the drinking-water supply—for instance, to ensure the use of suppliers that participate in quality assurance programmes; • using designated equipment for attending to incidents such as mains bursts (e.g. equipment should be designated for potable water work only and not for sewage work); • training and educational programmes for personnel involved in activities that could influence drinking-water safety; training should be implemented as part of induction programmes and frequently updated; • research and development to improve understanding of water quality, including the quality of source waters, and treatment. Supporting programmes will consist almost entirely of items that drinking-water suppliers and handlers will ordinarily have in place as part of their normal operation. For most, the implementation of supporting programmes will involve: • collation of existing operational and management practices; • initial and, thereafter, periodic review and updating to continually improve practices; • promotion of good practices to encourage their use; • audit of practices to check that they are being used, including taking corrective actions in case of non-conformance. Codes of good operating and management practice and hygienic working practice are essential elements of supporting programmes. These are often captured within standard operating procedures. They include, but are not limited to: • hygienic working practices in maintenance; • attention to personal hygiene; • training and competence of personnel involved in drinking-water supply; • tools for managing the actions of staff, such as quality assurance systems; • securing stakeholder commitment, at all levels, to the provision of safe drinking-water; • education of communities whose activities may influence drinking-water quality; • calibration of monitoring equipment; • record keeping. Comparison of one set of supporting programmes with the supporting pro-grammes of other suppliers, through peer review, benchmarking and personnel or document exchange, can stimulate ideas for improved practice. Supporting programmes can be extensive, be varied and involve multiple organizations and individuals. Many supporting programmes involve water resource protection measures and typically include aspects of land use control. Some water resource protection measures are engineered, such as effluent treatment processes and stormwater management practices that may be used as control measures. 4.5 Management of community and household water supplies Community-managed drinking-water supplies worldwide are more frequently contaminated than larger drinking-water supplies, may be more prone to operating discontinuously (or intermittently) and break down or fail more frequently. To ensure safe drinking-water, the focus in small supplies should be on: • informing the public; • assessing the water supply to determine whether it is able to meet identified health-based targets (see section 4.1);• monitoring identified control measures and training operators to ensure that all likely hazards can be controlled and that risks are maintained at a tolerable level (see section 4.2);• operational monitoring of the drinking-water system (see section 4.2);• implementing systematic water quality management procedures (see section 4.4), including documentation and communication (see section 4.6);• establishing appropriate incident response protocols (usually encompassing actions at the individual supply, backed by training of operators, and actions required by local or national authorities) (see sections 4.4.2 and 4.4.3); and • developing programmes to upgrade and improve existing water delivery (usually defined at a national or regional level rather than at the level of individual supplies) (see section 4.1.8). For small point sources serving communities or individual households, the emphasis should be on selecting source water of the best available quality and on protecting its quality by the use of multiple barriers (usually within source protection) and maintenance programmes. Whatever the source (groundwater, surface water or rainwater tanks), communities and householders should assure themselves that the water is safe to drink. Generally, surface water and shallow groundwater under the direct influence of surface water (which includes shallow groundwater with preferential flow paths) should receive treatment. The parameters recommended for the minimum monitoring of community supplies are those that best establish the hygienic state of the water and thus the risk of waterborne disease. The essential parameters of water quality are E. coli—thermotolerant (faecal) coliforms are accepted as suitable substitutes—and chlorine residual (if chlorination is practised). These should be supplemented, where appropriate, by pH adjustment (if chlorination is practised) and measurement of turbidity. These parameters may be measured on site using relatively unsophisticated testing equipment, and improved and relatively low cost systems continue to be developed. On-site testing is essential for the determination of turbidity and chlorine residual, which change rapidly during transport and storage; it is also important for the other parameters where laboratory support is lacking or where transportation problems would render conventional sampling and analysis impractical. Other health-related parameters of local significance should also be measured. The overall approach to control of chemical contamination is outlined in chapter 8. 4.6 Documentation and communication Documentation of a WSP should include: • description and assessment of the drinking-water system (see section 4.1), including programmes to upgrade and improve existing water delivery (see section 4.1.8);• the plan for operational monitoring and verification of the drinking-water system (see sections 4.2 and 4.3);• water safety management procedures for normal operation, incidents (specific and general) and emergency situations (see sections 4.4.1, 4.4.2 and 4.4.3), including communication plans; and • description of supporting programmes (see section 4.4.5). Records are essential to review the adequacy of the WSP and to demonstrate the adherence of the drinking-water system to the WSP. Several types of records are generally kept: • supporting documentation for developing the WSP, including validation; • records and results generated through operational monitoring and verification; • outcomes of incident investigations; • documentation of methods and procedures used; • records of employee training programmes. By tracking records generated through operational monitoring and verification, an operator or manager can detect that a process is approaching its operational or critical limit. Review of records can be instrumental in identifying trends and in making operational adjustments. Periodic review of WSP records is recommended so that trends can be noted and appropriate actions decided upon and implemented. Records are also essential when surveillance is implemented through auditing-based approaches. Communication strategies should include: • procedures for promptly advising of any significant incidents within the drinking-water supply, including notification of the public health authority; • summary information to be made available to consumers—for example, through annual reports and on the Internet; • establishment of mechanisms to receive and actively address community complaints in a timely fashion. The right of consumers to health-related information on the water supplied to them for domestic purposes is fundamental. However, in many communities, the simple right of access to information will not ensure that individuals are aware of the quality of the water supplied to them; furthermore, the probability of consuming unsafe water may be relatively high. The agencies responsible for monitoring should therefore develop strategies for disseminating and explaining the significance of health-related information. Further information on communication is provided in section 5.5. 4.7 Planned review 4.7.1 Periodic review WSPs should not be regarded as static documents. They need to be regularly reviewed and revised to ensure that they are functioning correctly and that they are kept up to date in light of changes in water systems or new developments. Reviews should consider: • data collected as part of monitoring processes; • changes to water sources and catchments; • changes to treatment, demand and distribution; • implementation of improvement and upgrade programmes; • revised procedures; • emerging hazards and risks. 4.7.2 Post-incident review WSPs should also be reviewed following incidents and emergencies to ensure that, where possible, incidents do not recur and, where this is not possible (e.g. floods), to reduce impacts. Post-incident reviews may identify areas for improvement and the need for revision of WSPs. 5 Surveillance DDrinking-water supply surveillance is “the continuous and vigilant public health assessment and review of the safety and acceptability of drinking-water supplies” (WHO, 1976). This surveillance contributes to the protection of public health by promoting improvement of the quality, quantity, accessibility, coverage, afford-ability and continuity of water supplies (known as service indicators) and is complementary to the quality control function of the drinking-water supplier. Drinking-water supply surveillance does not remove or replace the responsibility of the drinking-water supplier to ensure that a drinking-water supply is of acceptable quality and meets predetermined health-based targets. All members of the population receive drinking-water by some means—including the use of piped supplies with or without treatment and with or without pumping (supplied via domestic connection or public standpipe), delivery by tanker truck or carriage by beasts of burden or collection from groundwater sources (springs or wells) or surface sources (lakes, rivers and streams). It is important for the surveillance agency to build up a picture of the frequency of use of the different types of supply, especially as a preliminary step in the planning of a surveillance programme. There 79 is little to be gained from surveillance of piped water supplies alone if these are available to only a small proportion of the population or if they represent a minority of supplies. Information alone does not lead to improvement. Instead, the effective management and use of the information generated by surveillance make possible the rational improvement of water supplies—where “rational” implies that available resources are used for maximum public health benefit. Surveillance is an important element in the development of strategies for incremental improvement of the quality of drinking-water supply services. It is important that strategies be developed for implementing surveillance, collating, analysing and summarizing data and reporting and disseminating the findings and that the strategies are accompanied by recommendations for remedial action. Follow-up will be required to ensure that remedial action is taken. Surveillance extends beyond drinking-water supplies operated by a discrete drinking-water supplier to include drinking-water supplies that are managed by communities and includes assurance of good hygiene in the collection and storage of household water. The surveillance agency must have, or have access to, legal expertise in addition to expertise on drinking-water and water quality. Drinking-water supply surveillance is also used to ensure that any transgressions that may occur are appropriately investigated and resolved. In many cases, it will be more appropriate to use surveillance as a mechanism for collaboration between public health agencies and drinking-water suppliers to improve drinking-water supply than to resort to enforcement, particularly where the problem lies mainly with community-managed drinking-water supplies. The authorities responsible for drinking-water supply surveillance may be the public health ministry or other agency (see section 1.2.1), and their roles encompass four areas of activity: 1) public health oversight of organized drinking-water supplies; 2) public health oversight and information support to populations without access to organized drinking-water supplies, including communities and households; 3) consolidation of information from diverse sources to enable understanding of the overall drinking-water supply situation for a country or region as a whole as an input to the development of coherent public health–centred policies and practices; 4) participation in the investigation, reporting and compilation of outbreaks of waterborne disease. A drinking-water supply surveillance programme should normally include processes for approval of water safety plans (WSPs). This approval will normally involve review of the system assessment, of the identification of appropriate control measures and supporting programmes and of operational monitoring and management plans. It should ensure that the WSP covers normal operating conditions and predictable incidents (deviations) and has contingency plans in case of an emergency or unplanned event. The surveillance agency may also support or undertake the development of WSPs for community-managed drinking-water supplies and household water treatment and storage. Such plans may be generic for particular technologies rather than specific for individual systems. 5.1 Types of approaches There are two types of approaches to surveillance of drinking-water quality: audit-based approaches and approaches relying on direct assessment. Implementation of surveillance will generally include a mixture of these approaches according to supply type and may involve using rolling programmes whereby systems are addressed progressively. Often it is not possible to undertake extensive surveillance of all community or household supplies. In these cases, well-designed surveys should be undertaken in order to understand the situation at the national or regional level. 5.1.1 Audit In the audit approach to surveillance, assessment activities, including verification testing, are undertaken largely by the supplier, with third-party auditing to verify compliance. It is increasingly common that analytical services are procured from accredited external laboratories. Some authorities are also experimenting with the use of such arrangements for services such as sanitary inspection, sampling and audit reviews. An audit approach requires the existence of a stable source of expertise and capacity within the surveillance agency in order to: • review and approve new WSPs; • undertake or oversee auditing of the implementation of individual WSPs as a programmed routine activity; • respond to, investigate and provide advice on receipt of reports on significant incidents. Periodic audit of the implementation of WSPs is required: • at intervals (the frequency of routine audits will be dependent on factors such as the size of the population served and the nature and quality of source water and treatment facilities); • following substantial changes to the source, the distribution or storage system or treatment processes; • following significant incidents. Periodic audit would normally include the following elements: • examination of records to ensure that system management is being carried out as described in the WSP; • ensuring that operational monitoring parameters are kept within operational limits and that compliance is being maintained; • ensuring that verification programmes are operated by the water supplier (either through in-house expertise or through a third-party arrangement); • assessment of supporting programmes and of strategies for improving and updating the WSP; • in some circumstances, sanitary inspection, which may cover the whole of the drinking-water system, including sources, transmission infrastructure, treatment plants, storage reservoirs and distribution systems. In response to reports of significant incidents, it is necessary to ensure that: • the event is investigated promptly and appropriately; • the cause of the event is determined and corrected; • the incident and corrective action are documented and reported to appropriate authorities; • the WSP is reassessed to avoid the occurrence of a similar situation. The implementation of an audit-based approach places responsibility on the drinking-water supplier to provide the surveillance agency with information regarding system performance against agreed indicators. In addition, a programme of announced and unannounced visits by auditors to drinking-water suppliers should be implemented to review documentation and records of operational practice in order to ensure that data submitted are reliable. Such an approach does not necessarily imply that water suppliers are likely to falsify records, but it does provide an important means of reassuring consumers that there is true independent verification of the activities of the water supplier. The surveillance agency will normally retain the authority to undertake some analysis of drinking-water quality to verify performance or enter into a third-party arrangement for such analysis. 5.1.2 Direct assessment It may be appropriate for the drinking-water supply surveillance agency to carry out independent testing of water supplies. Such an approach often implies that the agency has access to analytical facilities with staff trained to carry out sampling, analysis and sanitary inspection. Direct assessment also implies that surveillance agencies have the capacity to assess findings and to report to and advise suppliers and communities. A surveillance programme based on direct assessment would normally include: • specified approaches to large municipality/small municipality/community supplies and individual household supplies; • sanitary inspections to be carried out by qualified personnel; • sampling to be carried out by qualified personnel; • tests to be conducted using suitable methods by accredited laboratories or using approved field testing equipment and qualified personnel; • procedures on reporting findings and follow-up to ensure that they have been acted on. For community-managed drinking-water supplies and where the development of in-house verification or third-party arrangements is limited, direct assessment may be used as the principal system of surveillance. This may apply to drinking-water supplies in small towns by small-scale private sector operators or local government. Direct assessment may lead to the identification of requirements to amend or update the WSP, and the process to be followed when undertaking such amendments should be clearly identified. Where direct assessment is carried out by the surveillance agency, it complements other verification testing of the water supplier. General guidance on verification testing, which is also applicable to surveillance through direct assessment, is provided in section 4.3. 5.2 Adapting approaches to specific circumstances 5.2.1 Urban areas in developing countries Drinking-water supply arrangements in urban areas of developing countries are typically complex. There can often be one or more large piped supplies with household and public connections, in combination with a range of alternative drinking-water supplies, including point sources and vended water. In these situations, the surveillance programme should take account of the different sources of drinking-water and the potential for deterioration in quality during collection, storage and use. Furthermore, the population will vary in terms of socioeconomic status and vulnerability to water-related disease. In many situations, zoning the urban area on the basis of vulnerability and drinking-water supply arrangements is required. The zoning system should include all populations within the urban area, including informal and periurban settlements, regardless of their legal status, in order to direct resources to where greatest improvements (or benefits) to public health will be achieved. This provides a mechanism to ensure that non-piped drinking-water sources are also included within drinking-water supply surveillance activities. Experience has shown that zoning can be developed using qualitative and quantitative methods and is useful in identifying vulnerable groups and priority communities where drinking-water supply improvements are required. 5.2.2 Community drinking-water supplies Small community-managed drinking-water supplies are found in most countries and may be the predominant form of drinking-water supply for large sections of the population. The precise definition of a “community drinking-water supply” will vary, but administration and management arrangements are often what set community supplies apart, especially in developing countries. Community-managed supplies may include simple piped water systems or a range of point sources, such as boreholes with hand pumps, dug wells and protected springs. The control of water quality and implementation of surveillance programmes for such supplies often face significant constraints. These typically include: • limited capacity and skills within the community to undertake process control and verification; this may increase the need both for surveillance to assess the state of drinking-water supplies and for surveillance staff to provide training and support to community members; • the very large number of widely dispersed supplies, which significantly increases overall costs in undertaking surveillance activities. Furthermore, it is often small community-managed water supplies that present the greatest water quality problems. Experience from both developing and developed countries has shown that surveillance of community-managed drinking-water supplies can be effective when well designed and when the objectives are geared more towards a supportive role to enhance community management than towards enforcement of compliance. Surveillance of community drinking-water supplies requires a systematic pro-gramme of surveys that encompass all aspects of the drinking-water supply to the population as a whole, including sanitary inspection (including catchment inspections) and institutional and community aspects. Surveillance should address variability in source water quality, treatment process efficacy and the quality of distributed or household-treated and household-stored water. Experience has also shown that the role of surveillance may include health education and health promotion activities to improve healthy behaviour towards management of drinking-water supply and sanitation. Participatory activities can include sanitary inspection by communities and, where appropriate, community-based testing of drinking-water quality using affordable field test kits and other accessible testing resources. In the evaluation of overall strategies, the principal aim should be to derive overall lessons for improving water safety for all community supplies, rather than relying on monitoring the performance of individual supplies. Frequent visits to every individual supply may be impractical because of the very large numbers of such supplies and the limitations of resources for such visits. However, surveillance of large numbers of community supplies can be achieved through a rolling programme of visits. Commonly, the aim will be to visit each supply periodically (once every 3–5 years at a minimum) using either stratified random sampling or cluster sampling to select specific supplies to be visited. During each visit, sanitary inspection and water quality analysis will normally be done to provide insight to contamination and its causes. During each visit, testing of water stored in the home may be undertaken in a sample of households. The objective for such testing is to determine whether contamination occurs primarily at the source or within the home. This will allow evaluation of the need for investment in supply improvement or education on good hygiene practices for household treatment and safe storage. Household testing may also be used to evaluate the impact of a specific hygiene education programme. 5.2.3 Household treatment and storage systems Where water is handled during storage in households, it may be vulnerable to contamination, and sampling of household-stored water is of interest in independent surveillance. It is often undertaken on a “survey” basis to develop insights into the extent and nature of prevailing problems. Surveillance systems managed by public health authorities for drinking-water supplies using household treatment and household storage containers are therefore recommended. The principal focus of surveillance of household-based interventions will be assessment of their acceptance and impact through sample surveys so as to evaluate and inform overall strategy development and refinement. Systematic determination of continued, correct and effective use and management is recommended so that deficiencies in use and management can be identified and corrected by those responsible. 5.3 Adequacy of supply As the drinking-water supply surveillance agency has an interest in the population at large, its interest extends beyond water quality in isolation to include all aspects of the adequacy of drinking-water supply for the protection of public health. In undertaking an assessment of the adequacy of the drinking-water supply, the following basic service parameters of a drinking-water supply should normally be taken into consideration: • Accessibility: the percentage of the population that has reasonable access to an improved drinking-water supply; • Quantity: the proportion of the population with access to different levels of drinking-water supply (inadequate access, basic access, intermediate access, optimal access) as a proxy for the quantity of water used; • Quality: whether the supply has regularly verified water quality and an approved WSP (see chapter 4) that has been validated and is subject to periodic audit to demonstrate compliance with relevant regulations (see chapters 3 and 4);• Continuity: the percentage of the time during which drinking-water is available (daily, weekly and seasonally); • Affordability: the price of water paid by domestic consumers. As the assessment of the quality of drinking-water is covered extensively throughout the Guidelines, including in section 5.5.2, this section focuses on the other basic service parameters. 5.3.1 Accessibility From the public health standpoint, the proportion of the population with sustained, reliable access to safe drinking-water is the most important single indicator of the overall success of a drinking-water supply programme. There are a number of definitions of access (or coverage), many with qualifications regarding safety or adequacy. Sustainable Development Goal (SDG) target 6.1 calls for universal and equitable access to safe and affordable drinking-water for all. The target is monitored using the indicator “proportion of population using safely managed drinking water services”, which is defined as the use of an improved water source that is accessible on premises, available when needed, and free from faecal and priority chemical contamination (WHO & UNICEF, 2017). SDG target 1.4 also references drinking-water, by calling for “all men and women” to have equal access to basic services, including “basic drinking water services”, which are defined as use of an improved water source, provided that collection time is not more than 30 minutes for a round trip, including queuing. An improved drinking-water source is one that has the potential to deliver safe water by the nature of its construction and design. Improved sources are more likely than unimproved sources to supply drinking-water free from microbiological contamination, although microbiological contamination does occur in all types of water supply, particularly when they are inadequately managed. Improved and unimproved water supply technologies are summarized below: • Improved drinking-water sources: — piped supplies (including household and yard connections, public taps and standpipes) — boreholes and tubewells — protected dug wells — protected springs — rainwater — water kiosks — packaged water — delivered water. • Unimproved drinking-water sources: — unprotected dug wells — unprotected springs — surface water (rivers, reservoirs, lakes, ponds, streams, canals and irrigation channels). Determining the proportion of a population with reliable access to drinking-water is an important function of a drinking-water surveillance agency. This task can be facilitated by establishing a common definition for reasonable access, appropriate to the local context, which may describe a minimum quantity of water supplied per person per day, together with a maximum tolerable distance or time to a source (see section 5.3.2). The global SDG indicators identify different levels of accessibility: the basic service indicator calls for improved water supplies to be located within a 30-minute round trip travel time, including queuing, while the safely managed services indicator requires that the water collection point be located on-premises (i.e. within the household, yard or plot). 5.3.2 Quantity The quantity of water used by households has an important influence on health. There is a basic human physiological requirement for water to maintain adequate hydration and an additional requirement for food preparation. There is a further requirement for water to support hygiene, both personal and household, which is necessary for health. Estimates of the volume of water needed for health purposes vary widely. In deriving World Health Organization (WHO) guideline values, it is assumed that the Table 5.1 Summary of water access, adequacy and level of health concern Access Likely volumes of Public health risk Intervention priority level Distance/time water collected from poor hygiene and actions Inadequate More than 1 km Very low: can Very high Very high access or more than be below Hygiene practice Provision of safely 30 min total minimum daily compromised managed water at least collection time requirements to intermediate access Basic consumption for hydration may be Hygiene education compromised Basic access 100 m to 1 km or Average quantity High High 5–30 min total collection time unlikely to exceed 20 litres Hygiene may be compromised Provision of safely managed water at least per person per day Laundry and bathing to intermediate access may occur off‑plot Hygiene education Intermediate Water provided Average quantity access on‑plot through approximately at least one tap, 50 litres per or within 100 m person per day or 5 min total collection time Medium Medium Personal hygiene should not be compromised under usual conditions but may be compromised under outbreak Hygiene promotion still yields health gains Encourage optimal access conditions Optimal Supply of water Average quanity Low Low access through multiple taps within the house will exceed 100 litres per person per day Personal hygiene should not be compromised Hygiene promotion still yields health gains Household cleaning is also likely assured Source: Domestic water quantity, service level and health, 2nd edition (supporting document in Annex 1) daily per capita consumption of drinking-water is approximately 2 litres for adults, although actual consumption varies according to climate, activity level and diet. Based on currently available data, a minimum volume of 5.3 litres per person per day will provide sufficient water for hydration under most conditions. There is insufficient empirical evidence to define a minimum quantity of water necessary for food preparation or for hygiene. Experience and expert opinion suggest that 20 litres per person per day is often sufficient for drinking, cooking, food hygiene, handwashing and face washing. However other hygiene practices, including bathing and laundry, may not be assured. Further, where demands for water are increased—for example, due to increased hand hygiene in response to outbreaks of disease—20 litres per person per day may be insufficient; in many cases, running water from a tap will be necessary to support sufficient handwashing. The quantity of water collected and used by households is primarily a function of the distance to the water supply or the total collection time required (including queuing). This broadly equates to access level. Four levels of access can be defined, as shown in Table 5.1. Access level is a useful and easily measured indicator that provides a valid proxy for the quantity of water collected by households and is the preferred indicator for surveillance. Available evidence indicates that health gains accrue from improving access level in three key stages: delivery of water within 1 km or 5–30 minutes of total collection time (“basic access”); water being supplied reliably on-plot, especially when running water is available (“intermediate access”); and water being available within the home through multiple taps (“optimal access”). The first and second categories of access broadly align with the access levels associated with basic and safely managed services, respectively. Although health gains do occur in these two categories, health concerns remain, particularly during outbreaks of disease when enhanced personal hygiene is needed. The volume of water collected may also depend on the continuity, reliability and cost of the water. Therefore, collection of data on these indicators is important. 5.3.3 Continuity Interruptions to drinking-water supply, as a result of either intermittent sources or engineering inefficiencies, are a major determinant of the quantity and quality of drinking-water available to households. Analysis of data on continuity of supply requires the consideration of several components. Continuity can be classified as follows: • year-round service from a reliable source with no interruption of flow at the tap or source; • year-round service with frequent (daily or weekly) interruptions, of which the most common causes are: — water production that is insufficient to meet demand, requiring rationing of water; — restricted pumping regimes in pumped systems, whether planned or due to power failure or sporadic failure; — peak demand exceeding the flow capacity of the transmission mains or the capacity of the reservoir; — excessive leakage within the distribution system; — excessive demands on community-managed point sources; — breakdown of components of point sources (e.g. handpump parts); • seasonal service variation resulting from source water fluctuation, which typically has three causes: — natural variation in source water volume during the year, which may be exacerbated by changes in climate (see section 6.1); — volume limitation because of competition with other uses, such as irrigation; — periods of high turbidity when the source water may be untreatable; • combined frequent and seasonal discontinuity. These classifications reflect broad categories of continuity, which are likely to affect the quantity and quality of water available and thereby public health in different ways. Any interruption of service is likely to result in an increased risk of degradation of water quality, an increased risk of exposure to contaminated water and therefore an increased risk of waterborne disease. Managed discontinuity often results in low supply pressure and a consequent risk of in-pipe recontamination. Other consequences include reduced availability and lower volume use, which adversely affect hygiene. This may be a problem both when the discontinuity is predictable and when it is highly unpredictable. Household water storage may be necessary, and this may lead to an increase in the risk of contamination during such storage and associated handling. Discontinuity often forces users to obtain water from inferior and distant sources. As a consequence, in addition to the obvious reduction in water quality and quantity, time is lost in water collection, and other risks (e.g. to musculoskeletal health) may arise. This applies particularly to women, who disproportionately bear the burden of collecting and hauling water from off-premises sources. The SDGs do not specify a minimum quantity of water to be available, but the safely managed drinking-water services indicator calls for water to be “available when needed”. For the purposes of global reporting, households are considered to have water available when needed if they report having “sufficient water” or that water is available “most of the time” (i.e. at least 12 hours per day or 4 days per week). However, although this metric is useful for global monitoring, it does not imply that this level of continuity is sufficient to realize public health gains, or that it represents a normative target. The indicators used for SDG monitoring are simplifications and do not capture all aspects of water services that are important from a normative perspective. 5.3.4 Affordability Affordability, or economic accessibility, implies that individuals or households should be able to purchase water without compromising the purchase of any other basic needs (UNICEF & WHO, 2021). A critical component in assessing affordability is the cost of water paid by individuals and further information related to costs is described in these Guidelines. The cost of water has an influence on the use of water and selection of water sources. Households with the lowest levels of access to safe water supply frequently pay more for their water than do households connected to a piped water system. The high cost of water may force households to use alternative sources of water of poorer quality that represent a greater risk to health, or to use multiple sources of water, which may have varying quality and therefore also pose a risk to health. Furthermore, high costs of water may reduce the volumes of water used by households, particularly where multiple sources are not available, which in turn may influence hygiene practices and increase risks of disease transmission. When assessing cost, it is important to collect data on the price at the point of purchase. Where households are connected to the drinking-water supplier, this will be the tariff applied. Where water is purchased from, for example, public standpipes or trucked water, the price at the point of purchase may be very different from the drinking-water supplier tariff. Many alternative water sources (notably vendors) also involve costs, and these costs should also be considered. In addition to recurrent costs, the costs for initial acquisition of a connection for households connected to the drinking-water supplier or where households have their own private point source should be considered. 5.4 Planning and implementation For drinking-water supply surveillance to lead to improvements in drinking-water supply, it is vital that the mechanisms for promoting improvement are recognized and used. The focus of drinking-water supply-related improvement activities (whether these are establishment of regional or national priorities, hygiene education programmes or enforcement compliance) will depend on the nature of the drinking-water supplies and the types of problems identified. A list of mechanisms for drinking-water supply improvement based on the output of surveillance is given below: • Establishing national priorities: When the most common problems and shortcomings in the drinking-water system have been identified, national strategies can be formulated for improvements and remedial measures; these might include changes in training (of managers, administrators, engineers or field staff), rolling programmes for rehabilitation or improvement or changes in funding strategies to target specific needs. • Establishing subnational/regional priorities: Regional offices of drinking-water supply agencies can decide in which communities to work and which remedial activities are priorities; public health criteria should be considered when priorities are set. • Establishing hygiene education programmes: Not all of the problems revealed by surveillance are technical in nature, and not all are solved by drinking-water suppliers; surveillance also looks at problems involving community and household supplies, water collection and transport and household treatment and storage. The solutions to many of these problems are likely to require educational and promotional activities. • Auditing of WSPs and upgrading: The information generated by surveillance can be used to audit WSPs and to assess whether these are in compliance. Drinking-water systems and their associated WSPs should be upgraded where they are found to be deficient, although feasibility must be considered, and enforcement of upgrading should be linked to strategies for progressive improvement. • Ensuring community operation and maintenance: Support should be provided by a designated authority to enable community members to be trained so that they are able to assume responsibility for the operation and maintenance of community drinking-water supplies. • Establishing public awareness and information channels: Publication of information on public health aspects of drinking-water supplies, water quality and the performance of suppliers can encourage suppliers to follow good practices, mobilize public opinion and response and reduce the need for regulatory enforcement, which should be an option of last resort. • Implementing programmes for household water treatment and safe storage: If information from surveillance reveals no or only basic access to water service, as defined in Table 5.1, or unsafe supplied water, the implementation of programmes to promote household water treatment and safe storage may be advised to improve water quality and promote hygienic water management at the household level. These may be effective interim measures for provision of safer water supported by appropriate outreach, education and training activities and creating supply chains for appropriate household water treatment and safe storage technologies. Further information is available in section 7.3.2 and the 1997 volume, Surveillance and control of community supplies (WHO, 1997). In order to make best use of limited resources where surveillance is not yet practised, it is advisable to start with a basic programme that develops in a planned manner. Activities in the early stages should generate enough useful data to demonstrate the value of surveillance. Thereafter, the objective should be to progress to more advanced surveillance as resources and conditions permit. The activities normally undertaken in the initial, intermediate and advanced stages of development of drinking-water supply surveillance are summarized as follows: • Initial phase: — Establish requirements for institutional development. — Provide training for staff involved in the programme. — Define the role of participants (e.g. quality assurance/quality control by supplier, surveillance by public health authority). — Develop methodologies suitable for the area. — Commence routine surveillance in priority areas (including inventories). — Limit verification to essential parameters and known problem substances. — Establish reporting, filing and communication systems. — Advocate improvements according to identified priorities. — Establish reporting to local suppliers, communities, media and regional authorities. — Establish liaison with communities; identify community roles in surveillance and means of promoting community participation. • Intermediate phase: — Train staff involved in the programme. — Establish and expand systematic routine surveillance. — Expand access to analytical capability (often by means of regional laboratories, national laboratories being largely responsible for analytical quality control and training of regional laboratory staff). — Undertake surveys for chemical contaminants using wider range of analytical methods. — Evaluate all methodologies (sampling, analysis, etc.). — Use appropriate standard methods (e.g. analytical methods, fieldwork procedures). — Develop capacity for statistical analysis of data. — Establish national database. — Identify common problems and improve activities to address them at regional and national levels. — Expand reporting to include interpretation at the national level. — Draft or revise health-based targets as part of a framework for safe drinking-water. — Use legal enforcement where necessary. — Involve communities routinely in surveillance implementation. • Advanced phase: — Provide further or advanced training for staff involved in the programme. — Establish routine surveillance for all health and acceptability parameters at defined frequencies. — Use a full network of national, regional and local laboratories (including analytical quality control). — Use national framework for drinking-water quality. — Improve water services on the basis of national and local priorities, hygiene education and enforcement of standards. — Establish regional database archives compatible with national database. — Disseminate data at all levels (local, regional and national). — Involve communities routinely in surveillance implementation. 5.5 Reporting and communicating An essential element of a successful surveillance programme is the reporting of results to stakeholders. It is important to establish appropriate systems of reporting to all relevant bodies. Proper reporting and feedback will support the development of effective remedial strategies. The ability of the surveillance programme to identify and advocate interventions to improve water supply is highly dependent on the ability to analyse and present information in a meaningful way to different target audiences. The target audiences for surveillance information will typically include: • public health officials at local, regional and national levels; • water suppliers; • local administrations; • communities and water users; • local, regional and national authorities responsible for development planning and investment. 5.5.1 Interaction with community and consumers Community participation is a desirable component of surveillance, particularly for community and household drinking-water supplies. As primary beneficiaries of improved drinking-water supplies, community members have a right to take part The right of consumers to information on in decision-making. The community the safety of the water supplied to them for represents a resource that can be drawn domestic purposes is fundamental. upon for local knowledge and experience. They are the people who are likely to first notice problems in the drinking-water supply and therefore can provide an indication of when immediate remedial action is required. Communication strategies should include: • provision of summary information to consumers (e.g. through annual reports or the Internet); • establishment and involvement of consumer associations at local, regional and national levels. In many communities, however, the simple right of access to information will not ensure that individuals are aware of the quality or safety of the water supplied to them. The agencies responsible for surveillance should develop strategies for disseminating and explaining the significance of results obtained. It may not be feasible for the surveillance agency to provide feedback information directly to the entire community. Thus, it may be appropriate to use community organizations, where these exist, to provide an effective channel for providing feedback information to users. Some local organizations (e.g. local councils and community-based organizations, such as women’s groups, religious groups and schools) have regular meetings in the communities that they serve and can therefore provide a mechanism of relaying important information to a large number of people within the community. Furthermore, by using local organizations, it is often easier to initiate a process of discussion and decision-making within the community concerning water quality. The most important element in working with local organizations is to ensure that the organization selected can access the whole community and can initiate discussion on the results of surveillance (see sections 7.6.1 and 8.7). 5.5.2 Regional use of data Strategies for regional prioritization are typically of a medium-term nature and have specific data requirements. While the management of information at a national level is aimed at highlighting common or recurrent problems, the objective at a regional level is to assign a degree of priority to individual interventions. It is therefore important to derive a relative measure of health risk. Although this information cannot be used on its own to determine which systems should be given immediate attention (which would also require the analysis of economic, social, environmental and cultural factors), it provides an extremely important tool for determining regional priorities. It should be a declared objective to ensure that remedial action is carried out each year on a predetermined proportion of the systems classified as high risk. At the regional level, it is also important to monitor the improvement in (or deterioration of) both individual drinking-water supplies and the supplies as a whole. In this context, simple measures, such as the mean sanitary inspection score of all systems, the proportion of systems with given degrees of faecal contamination, the population with different levels of service and the mean cost of domestic consumption, should be calculated yearly and changes monitored. As shown in Table 7.10 in section 7.4, the aim should be to provide drinking-water that contains no faecal indicator organisms, such as Escherichia coli. However, in many developing and developed countries, a high proportion of household and small community drinking-water systems, in particular, fail to meet requirements for water safety, including the absence of E. coli. In such circumstances, it is important that realistic goals for progressive improvement are agreed upon and implemented. It is practical to classify water quality results in terms of an overall grading for water safety linked to priority for action, as illustrated in Table 5.2. Grading schemes may be of particular use in community supplies where the frequency of testing is low and reliance on analytical results alone is especially inappropriate. Such schemes will typically take account of both analytical findings Table 5.2 Example of categorization of drinking-water systems on the basis of population size and quality rating in order to prioritize actions (see also Table 7.10) Proportion (%) of samples negative for E. coli Quality of drinking-water systema < 5000 population 5000–100 000 population > 100 000 population A 90 95 99 B 80 90 95 C 70 85 90 D 60 80 85 a Quality decreases from A to D. Table 5.3 Example of assessment of priority of remedial actions of community drinking-water supplies based on a grading system of microbial quality and sanitary inspection rating or scorea Sanitary inspection risk score (susceptibility of supply to contamination from human and animal faeces) 0–2 3–5 6–8 9–10 E .coli classificationbA B C D Low risk: Intermediate risk: low High risk: Very high risk: urgent no action required action priority higher action priority action required a Where there is a potential discrepancy between the results of the microbial water quality assessment and the sanitary inspection, further follow‑up or investigation is required. b Classifications based on those shown in Table 5.2. Quality decreases from A to D. Source: Adapted from Lloyd & Bartram (1991). See also the supporting document Rapid assessment of drinking-water quality (Annex 1). and results of the sanitary inspection through matrices such as the one illustrated in Table 5.3. Combined analysis of sanitary inspection and water quality data can be used to identify the most important causes of and control measures for contamination. This is important to support effective and rational decision-making. For instance, it will be important to know whether on-site or off-site sanitation could be associated with contamination of drinking-water, as the remedial actions required to address either source of contamination will be very different. This analysis may also identify other factors associated with contamination, such as heavy rainfall. As the data will be nonparametric, suitable methods for analysis include chi-square, odds ratios and logistic regression models. Combined analysis of sanitary inspection and water quality data is especially useful in assessing household water management systems. Microbial water quality data Table 5.4 Example of assessment of priority of remedial action for household drinking-water systems based on a grading system of microbial quality and sanitary inspection rating or scoresa Sanitary inspection risk score (susceptibility of supply to contamination from human and animal faeces) 0–2 3–5 6–8 9–10 E .coli classification (as decimalconcentration/100)< 1 1–10 11–100 > 100 Low risk: no action Intermediate risk: low High risk: higher Very high risk: urgent required action priority action priority action required a Where there is a potential discrepancy between the results of the microbial water quality assessment and the sanitary inspection, further follow‑up or investigation is required. are often limited, and sanitary inspection risk scoring therefore becomes an important consideration in assessing household water systems, their management and priority for remedial actions. An example of a combined system to assess risk and prioritize remedial actions for household water systems is shown in Table 5.4. 6 Application of the Guidelines in specific circumstances TThese Guidelines provide a generally applicable approach to ensuring the safety of drinking-water supplied through piped distribution and community supplies. This chapter describes the application of the Guidelines in some commonly encountered circumstances and specific issues that should be taken into account in each. The sections are not intended to stand alone, and reference is made to more comprehensive supporting documents that provide detailed guidance. In all the specific circumstances described below, the principles enshrined in water safety plans (WSPs) apply. However, the WSP should be tailored to the type of supply in each circumstance; for example, routine chemical and microbiological monitoring of rainwater may not be feasible at a household level, but preventive barriers are both applicable and achievable. As indicated in chapter 4, WSPs require careful consideration of possible hazards, and forward planning is one of the important requirements in ensuring that both the quantity and quality of water supplies are maintained. One of the significant concerns is climate change; all types of water supplies are being affected, including the specific circumstances discussed below. 97 6.1 Climate change: increasing threats from water scarcity, heavy rainfall and extreme events Regional or localized droughts and heavy precipitation events and floods have always occurred, but they are increasing in frequency, and in some cases intensity, as a consequence of human-caused climate change. Anticipating and planning for these extreme weather events, such that sufficient quantities of safe water can be delivered to consumers without disruptions, are growing challenges for water suppliers and regulators. The effects of these climate extremes on water quality and quantity will be especially acute in areas with growing populations or where water demands are increasing. In such areas, existing water supplies typically are already stressed, and there is little, if any, water supply margin available in the event of a major or extended weather event. This may be a particular problem in regions with arid and semi-arid climates, such as parts of the Mediterranean, the Middle East, Australia and the south-western United States of America. Regions affected by flooding are also at risk. In some regions, drought may be rapidly followed by flooding, creating multiple, cascading threats. Without action to reduce global heating, greater weather extremes will occur. Climate change is leading to more frequent and longer spells in extreme weather, with much higher peak temperatures, droughts, greater frequency of heavy precipitation, wildfires and violent storms, all of which may adversely affect drinking-water quality and, in some cases, water quantity. Changes in sea level from melting ice can affect coastal groundwater, causing salination, which may also occur as a result of over-abstraction. Windstorms will lead to storm surges that also degrade coastal water sources. With changes in water quantity come changes in water quality: greater or lesser runoff affects the sediment loading, chemical composition, total organic carbon content and microbial quality of water. These changes require modifications in water storage capacity and water treatment to ensure safe drinking-water. Changes in groundwater levels may also lead to altered mineral composition, and moves to deeper groundwater may tap into aquifers with high mineral content or high levels of specific constituents of concern for health. To provide for adequate water quantity and quality in the event of these changes and extremes, natural supplies may need to be augmented in some areas, and more climate-resilient technologies and processes may need to be used. Water treatment systems may need to be upgraded and obtain greater storage capacity to be able to cope with greater microbial, turbidity and chemical loadings, and to improve the efficiency of water use during treatment. New sources of water may need to be developed, such as recycled wastewater or desalinated brackish water or seawater, and new strategies may need to be implemented, such as aquifer storage and recovery. In parallel, action will be required to improve water management, including detecting and reducing leaks, and reducing consumer demand. WSPs should address climate change risks, particularly in the steps of assembling the team, assessing the system, preparing management procedures and developing supporting programmes (see Figure 4.1). For further information on the impacts of climate change on water supplies and practical information on how climate change impacts can be considered and addressed in the WSP process, see the supporting document Climate-resilient water safety plans (Annex 1). 6.2 Rainwater harvesting Rainwater harvesting is widely practised at a household level but is increasingly being used on a larger community scale. Rainwater can provide an important source of drinking-water in some circumstances as well as a useful source of water for blending with other sources to reduce the levels of contaminants of health concern, such as arsenic and fluoride. The development of formal WSPs at the household level may not always be practical, but promotion of sanitary inspection with simple good practice is important. Well-designed rainwater harvesting systems with clean catchments, covered cisterns and storage tanks, and treatment, as appropriate, supported by good hygiene at point of use, can offer drinking-water with very low health risk. Rainwater is initially relatively free from impurities, except those picked up by the rain from the atmosphere. However, the quality of rainwater may subsequently deteriorate during harvesting, storage and household use. Wind-blown dirt, leaves, faecal droppings from birds and other animals, insects and litter on the catchment areas, such as roofs and in cisterns, can contaminate rainwater, as can particles from the atmosphere, such as soot from burning materials such as old tyres. Regular cleaning of catchment surfaces and gutters should be undertaken to minimize the accumulation of debris. Wire meshes or inlet filters should be placed over the top of downpipes to prevent leaves and other debris from entering storage containers and cleaned regularly to prevent clogging. Materials used in the catchment and storage tank should be approved for use in contact with drinking-water and should not leach contaminants or cause taste, odour or discoloration. As rainwater is slightly acidic and very low in dissolved minerals, it can dissolve metals and other impurities from materials of the catchment and storage tank, resulting in unacceptably high concentrations of contaminants in the water. Most solid roof materials are suitable for collecting rainwater, but roofs with bitumen-based coatings are generally not recommended, as they may leach hazardous substances or cause taste problems. Care should be taken to ensure that lead-based paints are not used on roof catchments. Thatched roofs can cause discoloration or deposition of particles in collected water. Poor hygiene in water storage and abstraction from storage containers or at the point of use can also represent a health concern, but risks can be minimized by good design and practice. Faecal contamination is quite common, particularly in samples collected shortly after rainfall, but can be minimized by good practice. Higher microbial concentrations are generally found in the first flush of rainwater, decreasing as the rain continues; therefore, microbial contamination is less in rainy seasons when catchments are frequently washed with fresh rainwater. A system to divert the contaminated first flow of rainwater from roof surfaces is necessary, and automatic devices that prevent the first flush of runoff from being collected in storage are recommended. If diverters are not available, a detachable downpipe can be used manually to provide the same result. Storage tanks can present breeding sites for mosquitoes, including species that transmit dengue virus (see section 8.6). Covers discourage mosquito breeding and help to prevent faecal contaminants and sunlight from reaching the water (light can promote the growth of algae and cyanobacteria). Covers should be fitted, and openings need to be protected by mosquito-proof mesh. Cracks in the tank can result in contamination of stored water, whereas water withdrawal using contaminated containers is a potential cause of both faecal and chemical contamination. Storage containers should preferably be fitted with a mechanism such as a tap or outlet pipe that enables hygienic abstraction of water. Further treatment at the point of consumption may be applied to ensure better quality of drinking-water and reduce health risk. Solar water disinfection and pointof-use chlorination are examples of low-cost disinfection options for the treatment of stored rainwater. These and other household water treatment technologies are discussed in more detail in sections 7.3.2 (microbial) and 8.4.4 (chemical). 6.3 Vended water Vended water is common in many parts of the world where scarcity of supplies or lack of infrastructure limits access to suitable quantities of safe drinking-water. Although water vending is more common in developing countries, it also occurs in developed countries. In the context of these Guidelines, water vending implies private vending of drinking-water, but does not include bottled or packaged water (which is considered in section 6.15) or water sold in bottles through vending machines. Water vending may be undertaken by formal bodies, such as water utilities or registered associations, by contracted suppliers or by informal and independent suppliers. Where formal vending is practised, the water typically comes from treated utility supplies or registered sources and is supplied in tankers or from standpipes and water kiosks. Informal suppliers tend to use a range of sources, including untreated surface water, dug wells and boreholes, and deliver small volumes for domestic use, often in containers loaded onto small carts or tanker trucks. Both the quality and adequacy of vended supplies can vary significantly, and vended water has been associated with outbreaks of diarrhoeal disease (Hutin, Luby & Paquet, 2003). Water supplied to users should be suitable for drinking and comply with national or regional guidelines and regulatory requirements. The chemical and microbial quality of untreated or private sources of water should be tested to determine their suitability for use and to identify appropriate control measures, including treatment requirements. Surface water and some dug well and borehole waters are not suitable for drinking without treatment; disinfection is the minimum requirement, and filtration is often required when surface water is used. In many developing countries, consumers purchase water from kiosks and then carry the water home in a variety of containers of varying size. Measures should be taken to protect vended water from contamination during transport as well as storage in the home, including transporting and storing water in containers that are clean, free from both faecal and chemical contamination and either enclosed or with narrow openings, ideally fitted with a dispensing device such as a spigot that prevents hand access and other sources of extraneous contamination. Good hygiene is required and should be supported by educational programmes. In other cases, particularly in developed countries, vendors transport and deliver the water to users in tanker trucks. If large volumes are being transported, the addition of chlorine to provide a free residual concentration of at least 0.5 mg/l at the point of delivery to users is desirable. Tankers should also be used solely for water or, if this is not possible, should be thoroughly cleaned prior to use. All components of systems associated with supplying and delivering vended water need to be designed and operated in a manner that protects water quality. Water storage containers, pipework and fittings should not include defects such as structural faults that allow leakage and permit the entry of contaminants. Cleanliness of storage containers, standpipes, taps and hoses needs to be maintained. Hoses used to transfer water at kiosks or used on carts and tanker trucks should be protected from contamination (e.g. by preventing contact of the ends with the ground) and drained when not in use. The area around standpipes should include drainage or be constructed in a manner to prevent pooling of water. Materials used in all components, including pipework, containers and hoses, need to be suitable for use in contact with drinking-water and should not result in contamination of the water with hazardous chemicals or with substances that could adversely affect its taste. All components of water vending, including sources, methods of abstraction and transport, should be incorporated into a WSP. Where vendors are registered or have a contract with a water utility, implementation and operation of the WSP should be regularly checked by the utility. WSPs and the operation of water vendors should also be subject to independent surveillance. 6.4 Bulk water supply Bulk water supplies can be either untreated or treated water, but usually there is limited or no choice in the provision of such supplies. They may be provided where one agency or company controls a large raw water source, usually surface water, and provides water to one or several other water suppliers. Bulk water supplies can be delivered by pipeline or tanker or using ships or fleets of road or rail tankers. In all cases, it is important that the bulk supply is incorporated into the WSP of the receiving supply and treated as another source. Where bulk supplies of treated water have been used to provide support during a drought or emergency, it is vital that the receiving supplier takes steps to ensure that the water is safe before it is introduced into the receiving distribution system. At all stages, it is important that there is close communication between all parties involved and that the procedures and requirements are documented, understood and carried out with appropriate monitoring and verification. The potential hazards from bulk water are similar to those from any water supply, but there are additional sources of contamination, such as inappropriate containers and materials and lack of sanitation and hygiene at bulk water filling connections or transfer points. Pipelines may be vulnerable to contamination along the transmission route, particularly if there is the potential for unapproved connections into the system. Many of the requirements for bulk supply are the same as for any piped supply, such as using approved materials that will not adversely affect water quality. Where tankers are used, these should be of a suitable material and be clean and free from microbial and chemical contamination. To minimize contamination during filling of bulk water containers or water tankers and charging of water transmission pipelines, sanitary inspections and maintenance of sanitary conditions for water filling stations are necessary. These sites should have proper drainage to avoid standing water and flooding, should not be exposed to sources of contamination and should be secure, with access restricted to authorized personnel. At water filling and delivery points, nozzles and couplings should be protected from sources of contamination, including animals. Installation of protective coverings for filling and receiving connectors would help in this respect. Some plastic pipe materials are permeable to organic chemicals, and transfer of substances such as petroleum hydrocarbons could diminish the structural integrity of the pipe materials or render the water unpalatable to consumers. Such piping is most likely to be found in transfer hoses, so the cleanliness of the transfer points where tankers are used is vital, as is protection of the transfer area from spills of petroleum fuels. Implementation of security measures to guard against intentional contamination and theft may also be warranted. 6.5 Desalination systems Desalination is used to remove salts from brackish or saline surface water and groundwater to render it acceptable for human consumption or other uses. It is increasing employed to provide drinking-water because of a growing scarcity of fresh water driven by population growth, overexploitation of water resources and climate change. Desalination facilities exist all over the world, particularly in the eastern Mediterranean region, with use increasing on all continents. Small-scale desalination is used to supply fresh water on ships and to provide additional fresh water in some hot and arid regions. These Guidelines, including the use of WSPs, are fully applicable to desalinated water supply systems. Management of desalination includes consideration of specific aspects but these only represent variations in emphasis when developing and implementing WSPs, as described in these Guidelines. Desalinated water has a very low total organic carbon content and low disinfectant demand, so disinfection by-products are generally of little concern, although brominated organics may occur owing to the presence of bromide in seawater. Membrane and distillation desalination processes are very efficient at removing higher molecular weight organic chemicals and virtually all inorganic chemicals, and volatile organic compounds are vented during thermal desalination processes. Where membranes are used, boron and some smaller molecular weight organic substances may not be excluded, so it is important to establish the membrane capability. Because of the apparently high effectiveness of some of the processes used (especially distillation and reverse osmosis) in removing both microorganisms and chemical constituents, these processes may be employed as single-stage treatments or combined with only a low level of residual disinfectant. For further information, see the supporting document Water treatment and pathogen control (Annex 1). Pretreatment is largely in place to protect the desalination process, but it will also remove certain hazards present in brackish or saline waters. Water produced by desalination is low in minerals and usually aggressive towards materials with which it comes into contact, such as materials used for distribution pipes, storage and plumbing. During post-treatment, the water must be stabilized or remineralized prior to distribution to reduce its corrosive nature. Stabilization is commonly achieved by adding chemical constituents such as calcium and magnesium carbonate along with pH adjustment or through blending with small volumes of mineral-rich waters. Seawater and spent seawater that has undergone electrolysis to form hypochlorite have been used for this purpose, but the latter practice has essentially ended because of the formation of bromate in the distributed water. Blending waters should be pretreated to ensure their microbial safety, because the post-desalination residual disinfectant level may be insufficient to control pathogens present in the blending water. Desalinated water contains lower than usual concentrations of dissolved solids and essential elements such as calcium and magnesium, which are commonly found in water (see the supporting document Calcium and magnesium in drinking-water; Annex 1). Drinking-water typically contributes a small proportion to the recommended daily intake of essential elements, with most of the intake occurring through food. Fluoride would also be missing from desalinated water unless it were added prior to distribution, which may be considered by countries in which sugar consumption is high (WHO, 2005). High temperatures of distributed water in warm climate areas and difficulty in maintaining disinfectant residuals during transport over long distances may lead to microbial aftergrowth, depending on nutrient availability. Although such growth is likely to be without health significance (see the supporting document Heterotrophic plate counts and drinking-water safety; Annex 1), it can contribute to problems of acceptability. The use of chloramines constitutes an advantageous alternative to free chlorine in distribution systems with long residence times and elevated temperatures, although nitrite formation by organisms in biofilms needs to be considered where chloramination is practised and excess ammonia is present. Extensive information on desalination for safe drinking-water supply is available in the book Desalination technology: Health and environmental impacts (Cotruvo et al., 2010) and the supporting document Safe drinking-water from desalination (Annex 1). 6.6 Potable reuse systems Potable reuse of appropriately treated wastewater represents a source of drinking-water that is largely independent of the influence of climate change. Potable reuse, like desalination, is increasing in response to the growing scarcity of fresh water driven by population growth, overexploitation of water resources and climate change. Potable reuse can involve direct supply of treated wastewater into drinking-water supplies or indirect supply through the planned addition of treated wastewater into water bodies (rivers, lakes, reservoirs or aquifers) for the purpose of augmenting existing sources of drinking-water. These Guidelines, including the use of WSPs, are fully applicable to potable reuse schemes with no requirement for additional targets. Management of potable reuse includes consideration of specific aspects, but these only represent variations in emphasis when developing and implementing WSPs as described in these Guidelines. Specific aspects include the very poor quality of source waters; the associated complexity of potable reuse schemes, including the use of advanced water treatment processes; additional monitoring considerations; potential use of environmental buffers and engineered storages; and community acceptance. In some cases, wastewater management may be described in a separate sanitation safety plan, particularly where different entities are responsible for wastewater management and provision of drinking-water. Where both forms of safety plan are used, they will need to be coordinated to ensure that the overall scheme functions effectively. Wastewater, by its nature, typically contains high concentrations of enteric pathogens, requiring the setting and meeting of appropriate performance targets to achieve production of safe drinking-water. Combinations of established treatment technologies can achieve the required performance targets. Wastewater can also contain a broad range of chemicals, including domestic chemicals (e.g. cleaning compounds, garden products), chemicals excreted by people (e.g. pharmaceuticals, metabolites), personal care products, and chemicals used in wastewater and drinking-water treatment processes. Depending on the management and control of industrial discharges, wastewater can include significant concentrations of industrial chemicals. However, the impact of these chemicals can be minimized by applying appropriate industrial discharge controls. Metals and inorganic chemicals are generally present in moderate concentrations (μg/l to mg/l) in untreated wastewater, and pharmaceuticals, personal care products and organic chemicals are generally present in lower concentrations (ng/l to μg/l). Treatment trains used for potable reuse often include specific processes such as reverse osmosis, nanofiltration or ozone/biologically activated carbon to reduce concentrations of potential chemical hazards and to provide protection against emerging chemicals of concern. While potable reuse can be a practical source of drinking-water, potable reuse schemes are typically complex, and operators need to have sufficient resources and technical capabilities for successful operation. For further guidance, see the supporting document Potable reuse: Guidance for producing safe drinking-water (Annex 1). 6.7 Dual piped water supply systems In some locations, households and buildings served with a piped drinking-water supply may also receive piped water from an alternative source for non-potable purposes, creating a dual piped water supply system. The alternative water source is usually provided to reduce the use of high-quality water resources for non-potable uses (e.g. toilets, washing clothes, irrigation) or simply to conserve scarce water resources. Non-potable piped supplies can potentially introduce health hazards, commonly through accidental cross-connections between potable and non-potable piped supplies. Measures to control health risks from dual piped supply systems include: • use of good design practices that prevent cross-connections; • unambiguous labelling of both systems to ensure that the non-potable supply is not mistaken for the potable supply; • installation of the non-potable piped system only by qualified plumbers; • regulation of non-potable piped systems by the authority responsible for drinking-water surveillance; • public communication about the potential health risks from exposure to non-potable water through cross-connections and the dangers of modifying systems by inexperienced and non-certified individuals. Increasingly in developed countries, dual systems are being installed at a household level or in public buildings. Guidance should be provided on installation, particularly where this is by non-certified individuals. Potable water supplied into the building should be fitted with a non-return valve in order to prevent backflow into the public water supply. 6.8 Emergencies and disasters Safe water—for consumption and to enable adequate hygiene—is one of the most important public health requirements in most emergencies and disasters, along with adequate sanitation. The greatest waterborne risk to health comes from the transmission of faecal pathogens as a result of inadequate sanitation, hygiene and protection of drinking-water sources. Some important infectious diseases, such as those caused by the Ebola virus or SARS-CoV-2, do not have a faecal–oral transmission route but require sufficient and safe water for cleaning, caring for patients and good hand and environmental hygiene (WHO, 2020, 2021). Some disasters, including those caused by or involving damage to chemical or nuclear industrial installations, spillage in transport or volcanic activity, may result in contamination by chemical or radiological hazards of concern. The circumstances of most large-scale emergencies will vary, and each will present its own peculiar problems and challenges. Where a number of agencies are involved in disaster relief or overseeing an emergency, it is vital that there is good communication between the agencies and coordination of their activities. It is also important that the overall coordinators take advice from the experts in a particular field, such as water supply and sanitation. This section considers primarily large-scale disasters and emergencies, although much of the information will apply to smaller-scale emergencies as well. For microbiological and chemical emergencies on a smaller scale in piped supplies, the relevant sections in chapters 7 and 8 should be consulted. For emergencies involving radionuclides, see section 2 in the supporting document Management of radioactivity in drinking-water (Annex 1). People displaced by conflict and natural disaster may move to an area where unprotected water sources are contaminated. When population density is high and sanitation is inadequate, unprotected water sources in and around the temporary settlement are highly likely to become contaminated. A displaced population with low immunity due to malnutrition, as a consequence of food shortages or the burden of other diseases, is at an increased risk of an outbreak of waterborne disease. Emergency planning initiatives should include three phases: 1) vulnerability assessments (which should be part of a WSP for any large supply) to identify the critical elements of the existing systems that, if compromised, would result in major disruption of basic services; 2) mitigation plans to identify feasible actions to prevent or reduce the disruptive effects related to the loss of the vulnerable elements or facilities; 3) emergency preparedness plans to facilitate managing the crisis and restoring service should disruptions occur. The key is to anticipate probable events, have plans in place, prepare to respond when needed, and have backup materials and facilities. As well, simulations should be conducted before the event so that the organization and its staff will be effective in the event of an emergency. Available sources of water are limited in most emergency situations, and providing a sufficient quantity of water for personal and domestic hygiene as well as for drinking and cooking is important. Detailed information may be found in Sphere Association (2018) and the supporting document Domestic water quantity, service level and health (Annex 1). Furthermore, it may be important to prioritize drinking-water to particular settings, such as health-care facilities (see section 6.11) and/or specific geographic regions (e.g. cholera hotspots). National drinking-water quality standards should therefore be flexible, taking into consideration the risks and benefits to health in the short and long terms. They should not excessively restrict water availability for hygiene, as this would often result in an increased overall risk of disease transmission. There are a number of factors to take into consideration when providing drinking-water for a population affected by a disaster, including the following: • The quantity of water available and the reliability of supply: These are likely to be the overriding concerns in most emergency situations, as it is usually easier to improve water quality than to increase its availability or to move the affected population closer to another water source. However, since ensuring sufficient quantities of water for consumption and hygiene is important, provision of increased quantities of water is common in emergency settings. This may involve rehabilitation or construction of new water supplies (e.g. network rehabilitation or extensions, new boreholes) or, as a last resort, temporary transportation of water by trucks. • The equitability of access to water: Even if sufficient water is available to meet minimum needs, additional measures may be needed to ensure that access is equitable. Unless water points are sufficiently close to their dwellings, people will not be able to collect enough water for their needs. Water may need to be rationed to ensure that everyone’s basic needs are met. • Protecting the water source against contamination: This should always be a priority in emergencies, whether or not disinfection of the water supply is considered necessary. • The need for disinfection: Disinfection, maintaining an adequate disinfectant residual and, where necessary, pretreatment to reduce turbidity to as low as feasible in order to ensure the efficiency of disinfection are essential components in ensuring a safe drinking-water supply. • Acceptability: It is important to ensure that drinking-water provided in emergencies is acceptable to the consumers in terms of taste, odour and appearance, or the consumers may resort to water from unprotected or untreated supplies. • The need for containers to collect and store water: Containers that are hygienic and appropriate to local needs and habits are needed for the collection and storage of water to be used for washing, cooking and bathing. • The availability of bottled or packaged water: Provision of bottled or packaged water from a reliable source is often an effective way to quickly provide safe, potable water in emergencies and disasters. Brewers and soft drink producers, if they are part of the emergency response plan, are often capable of converting their processes to produce bottled or packaged water in emergencies. This is particularly valuable if they have water treatment plants for ensuring the quality of water used as an ingredient in their processes. • Longer-term planning for continuing emergency situations: When the first phase of an emergency or disaster is over, consideration needs to be given to the longer-term provision of safe water and sanitation. Long-term planning at an early stage of a response can save costs (e.g. replacing water trucking with more sustainable water supply solutions, such as rehabilitation/construction of water networks or new boreholes). Long-term planning can also build local capacities (e.g. training of local employees for more sustainable water quality monitoring systems). Finally, it can provide an opportunity to develop water, sanitation and hygiene services in under-privileged areas; for example, ensuring sustainable interventions in cholera hotspots can prevent future outbreaks, in addition to providing an immediate response. This is particularly valuable for countries and regions (e.g. fragile and conflict-affected countries) that are experiencing recurrent or longer-term, chronic emergencies that can last several years or more. In many emergency situations, water is collected from central water collection points, stored in containers and then transferred to cooking and drinking vessels by the affected people. It is important that people be aware of the risks to health from contamination of water from the point of collection to the moment of consumption and have the means to reduce or eliminate these risks. Detailed information may be found in Wisner & Adams (2002). Point-of-use treatment may be necessary; if so, ensuring that water treatment technologies meet WHO performance standards is important. A list of products that meet WHO standards can be found on the web pages of the WHO International Scheme to Evaluate Household Water Treatment Technologies.1 Further information on point-of-use treatment can be found in section 7.3.2, including Table 7.8. The information in section 6.12, Table 6.1, on drinking1 https://www.who.int/tools/international-scheme-to-evaluate-household-water-treatment-technologies/ products-evaluated water disinfection methods that can be used by travellers may also be applied for temporary use in emergency situations. Water quality should be assessed during emergencies. This includes conducting sanitary inspections; monitoring microbial water quality; monitoring water treatment processes, including disinfection; monitoring other water quality parameters; and assessing water quality in the investigation of disease outbreaks or the evaluation of hygiene promotion activities, as required. With respect to water quality monitoring, the priority should be ensuring adequate disinfection (e.g. monitoring chlorine residual) in distribution systems, storage reservoirs, water collection points and households, including in high-density areas such as camps and peri-urban areas, and during outbreaks of diarrhoeal disease. Monitoring and reporting systems should be designed and managed to ensure that action is swiftly taken to protect health. Health information should also be monitored to ensure that water quality can be rapidly investigated when it is suspected of contributing to a health problem so that treatment processes, particularly disinfection, can be modified as required. Where large numbers of water samples need testing or analysis of a broad range of parameters is of interest, laboratory analysis is usually most appropriate. If the drinking-water supplier’s laboratories or laboratories at environmental health offices and universities no longer function because of the disaster, a temporary laboratory may need to be set up. Where samples are transported to laboratories, appropriate handling is important to ensure meaningful results. Portable testing kits allow the determination in the field of key water quality parameters, such as thermotolerant coliform count, free residual chlorine, pH, turbidity and filterability. Workers should be trained in the correct procedures for collecting, labelling, packing and transporting samples and in supplying supporting information from the sanitary survey to help interpret laboratory results. For guidance on methods of water sampling and testing, see Bartram & Ballance (1996), WHO (1997) and APHA, AWWA & WEF (2005). For further information on considerations for providing safe drinking-water during emergencies and disasters, see Sphere Association (2018) and WHO/WEDC (2013). 6.9 Temporary water supplies A number of waterborne disease outbreaks have occurred as a result of poor management and design of temporary water supplies, which are distributed water supplies for planned seasonal or time-limited events (e.g. festivals, markets and summer camps). Water supplies for holiday towns are not covered, because they are permanent supplies, although substantial seasonal variations in demand bring specific problems. A systematic approach to drinking-water safety, including adequate quantity and quality, is needed for temporary water supplies. A WSP is an essential requirement in identifying the hazards and risks and developing good management procedures to deal with them. Chapter 4 and other sections in chapter 6 provide additional useful information. Where water is supplied through tankers, the requirements are the same as for vended water (section 6.3) and bulk water supplies (section 6.4). A temporary water supply may be independent (i.e. not connected to any other water supply system and with its own facilities from source to tap) or dependent (i.e. receiving treated water from an existing water supply system but with independent distribution facilities). The risk of drinking-water contamination is usually lower in dependent systems, provided there is access to the technologies, expertise and management of the permanent system. A contract is often made between the organizer of an event (e.g. a festival) and a water supply entity, which should include the water quantity and quality supplied by the entity, the roles and responsibilities of each party in water quality management, the locations and frequency of water quality monitoring, sanitary inspection and surveillance by a health authority and the provision of adequate and properly sited sanitation. Coordination between an event organizer, a water supply entity and the relevant health authority is very important for ensuring drinking-water safety. Temporary water supply systems can vary substantially in terms of their scale, period of operation, water use and fluctuations in demand, and these variations should be taken into consideration during the planning and design stages. In the case of an independent system, adequate consideration should also be given to the selection of a water source in terms of quantity, quality and treatment processes, and care should be taken not to adversely affect any other supply or water source. Where a temporary system is directly connected to a mains water supply, it is important to prevent the accidental contamination of the mains water supply through backflow during construction and operation of the temporary system. Water consumption for firefighting, hand washing and toilet flushing should be taken into account in estimating total and predictable variations in water demand where there are no other water sources available for such purposes. Water quality targets for temporary supplies should be the same as those for permanent water supplies. Disinfection should be considered indispensable in a temporary supply, and it is preferable to maintain a certain level of disinfectant (e.g. chlorine) residual at service taps. If the supply is not for potable uses, appropriate action should be taken to ensure that it is not used for drinking. If a temporary water supply is used recurrently, it is essential to fully flush the entire system with water containing a higher than normal disinfectant residual before restarting. When planning installation on site, positioning of pipes, hoses and connections should take risks of contamination into account—for example, by avoiding the placement of hosing and fittings on the ground near sites of potential faecal contamination or storage tanks in direct sunlight where rising temperatures support microbial growth. It is also important to ensure that the facility has no defects, including leakage, that could cause the deterioration of water quality and that water quality at every service tap satisfies the required quality target. Important control measures during dismantling and transport of installations include emptying hoses, preferably drying them and storing them so that ingress of contamination is avoided. In all cases, the materials should be approved for use in contact with potable water. Care should be taken in planning and designing wastewater management and disposal facilities, particularly to ensure that lavatories and disposal facilities are located so as to avoid any risk of adversely affecting source water quality or stored water. It is also important to prevent runoff from other areas, such as livestock pens, from entering the source. The source, treatment facilities and distribution reservoirs should be well protected from access by animals (e.g. bird faeces) and humans by covers or roofs. A temporary system is usually more vulnerable to accidental and deliberate contamination than an existing permanent water supply system, and attention needs to be paid to security. All water treatment facilities should be thoroughly inspected at least every day. All of these procedures and requirements should be included in the operational management documents that are at the core of the WSP. Signs are an important part of ensuring that water from taps is used appropriately and the protection of water sources and drinking-water infrastructure. The signs should be easily understood and used in conjunction with other barriers, such as fences. Water quality and appearance should be routinely monitored at the service taps of a temporary water supply system. At the very least, water temperature and disinfectant residual should be monitored every day as simple rapid tests that act as indicators of possible problems. Other basic parameters that should be regularly monitored, if possible, include pH, conductivity, turbidity, colour and Escherichia coli (or, alternatively, thermotolerant coliforms). Routine sanitary inspection of a temporary water supply by the appropriate health authority is very important. If any problem related to water quality arises, remedial actions that are included in the management documents supporting the WSP should be taken promptly. If a temporary water supply system is to be used for a period of more than a few weeks, regular surveillance by the appropriate health authority should be implemented. 6.10 Buildings2 Drinking-water systems in buildings can be a significant source of contamination, and poor management of these systems has contributed to outbreaks of disease and illness. One of the challenges in ensuring water safety is that responsibility for many actions essential to the control of drinking-water quality in buildings is often outside the mandate of the drinking-water supplier. Roles and responsibilities of different stakeholders relating to the safe management of drinking-water systems within buildings can be influenced by a number of factors, including ownership of assets and rights of access. WSPs established for management of public water supplies are not typically extended to buildings, although the water supplier WSP may include a number of initiatives to ensure that backflow prevention is in place or to provide information to consumers on protecting their own water quality. In many cases, owners, managers or maintenance personnel are responsible for managing building water supplies, but awareness and application of drinking-water guidelines are often limited, and so educational supporting programmes may be required. 2 Hospitals, nursing care homes and other health-care facilities are discussed in section 6.11. The design of water networks in buildings is variable, as influenced by the diversity of building types (e.g. schools, child-care facilities, residential buildings, hotels, sports facilities, factories, office blocks, museums, transport terminals), designs and water uses. Drinking-water systems in buildings are typically divided into hot and cold water networks and may be connected to water-based devices (e.g. cooling towers, boilers, swimming pools) or point-of-use equipment (e.g. washing machines). General drinking-water safety is ensured by good management practices, including sound design, routine maintenance protocols, regular cleaning, temperature management and flow management (avoidance of stagnation). These practices should be incorporated in WSPs developed by building owners or managers. WSPs for buildings should address cold and hot drinking-water networks and consider water-based devices and point-of-use equipment. Regulatory or other appropriate authorities may provide guidance on the development and application of WSPs for drinking-water systems in buildings. The regulator can specify compliance requirements for buildings in general or for specific types of buildings based on the level of risk. Schools, hotels and some other large buildings are high-risk environments because of both the complex nature of their drinking-water systems and the vulnerability of some users, occupants and visitors, and heightened vigilance in terms of operational monitoring, validation of control measures and verification is generally justified. Compliance may require that maintenance and monitoring programmes be carried out through a building-specific WSP. It may be appropriate to display maintenance and monitoring programmes and certification of compliance at a conspicuous location within the building. Compliance could be verified and certified by an independent auditor. The principal hazard that may threaten drinking-water systems of buildings is ingress of contamination from external water supplies or through faults in the distribution system (including storage tanks). Unapproved and inappropriate fittings and materials can lead to the release of chemical substances from tanks, piping, jointing and plumbing materials. The release may vary with the age of the material and the contact period; for example, first-draw water contains higher concentrations of lead or copper. Cross-connections with chemical storage containers, backflow from point-ofuse equipment and cross-connections with non-potable supplies can lead to a range of contaminants entering drinking-water. Where water is supplied directly to equipment in buildings, the potential for backflow into the mains network exists. This may be driven by high pressures generated in equipment connected to mains water supplies or by low pressures in the mains, but it can be prevented by fitting appropriate backflow prevention devices. An additional problem not directly related to drinking-water is microbial growth (e.g. Legionella) on surfaces and in water-based devices that may lead to an inhalation hazard from spray droplets. Growth of such bacteria can be controlled through basic measures (e.g. maintaining water outside the range at which Legionella proliferate, i.e. > 50 °C for hot water and < 25 °C for cold water, or maintaining a suitable disinfectant residual). Poor temperature control can occur in cold water systems through inadequate insulation and separation from hot water systems and in hot water systems in heating devices and storage containers, inappropriate location of tempering devices, long branch mains and dead ends (i.e. lengths of pipe, closed at one end, through which no water passes). In large buildings, there is increased potential for growth of Legionella in long water distribution systems, and maintenance of these systems needs particular attention. For further information on Legionella in drinking-water, see section 11.1 and the supporting document Legionella and the prevention of legionellosis (Annex 1). Effective assessment of potential health hazards and risks requires documentation of the physical structure of water systems in buildings. This should be kept up to date and include hot and cold water networks, including materials used; point-ofentry treatment; point-of-use treatment, equipment and systems (e.g. for firefighting) connected to the drinking-water supply; and water-based devices supplied by the drinking-water system. In undertaking an assessment of the building’s distribution system, a range of specific issues must be taken into consideration that relate to ingress, introduction and proliferation of contaminants, including: • the quality and management of external supplies; • use of independent water supplies; • intermittent supplies; • pressure of water within the system; • temperature of water (in both cold and hot water systems); • integrity of storage tanks; • areas subject to intermittent or seasonal use (e.g. hotels with seasonal occupancy, schools); • cross-connections, especially in mixed systems; • backflow prevention; • system design to minimize dead/blind ends and other areas of potential stagnation; • the use of materials and coatings approved for use with drinking-water. The aim of a distribution system within a large building is to supply safe drinking-water at adequate pressure and flow. The quality of water entering building supplies will be ensured by a water utility or by the installation of point-of-entry devices typically managed by the building owner or operator. To maintain drinking-water quality, it is important to minimize transit times, low flows and low pressures. Procedures should be established for repairs, renovations or extensions of systems to ensure that water safety is maintained, and all work, including changes to water systems, should be documented. Following work on the system, it would be appropriate to disinfect and flush. Monitoring should focus on ensuring that control measures are working effectively. Where possible, this should include monitoring by maintenance personnel using field kits for parameters such as temperature, pH and disinfectant residuals. The frequency will vary depending on the size and use of the building, but it should be weekly in large buildings. Monitoring of drinking-water quality will be more frequent when the building is new or recently commissioned. Independent surveillance is a desirable element in ensuring continued water safety within buildings and should be undertaken by the relevant health agency or other independent authority. To ensure the safety of drinking-water within buildings, supportive activities of national regulatory agencies include: • specific attention to application of codes of good practice (e.g. at commissioning and in contracting construction and rehabilitation); • suitable education and training programmes for building owners and managers, engineers, plumbers and operators of water-based devices (e.g. cooling towers and evaporative condensers); • regulation of the plumbing community and use of certified professionals; • effective certification and use of materials and devices in the marketplace; • codes of practice for design and operation of water-based devices; For further guidance, see the supporting document Water safety in buildings (Annex 1). 6.11 Health-care facilities Health-care facilities include hospitals, health centres and hospices, residential care, dental surgeries and dialysis units. Drinking-water in such facilities should be suitable for human consumption and for all usual domestic purposes, including personal hygiene. However, it may not be suitable for all uses or for some patients, and further processing or treatment or other safeguards may be required. Although microorganisms such as Pseudomonas aeruginosa and mycobacteria, Acinetobacter, Aeromonas and Aspergillus species do not appear to represent a health concern through water consumption by the general population, including most patients in health-care facilities, they may be of concern for severely immunosuppressed persons, such as those with neutrophil counts below 500 per microlitre (see the supporting document Heterotrophic plate counts and drinking-water safety; Annex 1). Some of these microorganisms also have the potential to cause infections if drinking-water is used to wash burns or medical devices such as endoscopes and catheters. Water used for such purposes may require additional processing, such as microfiltration or sterilization, depending on use. Health-care facilities may include environments that support the proliferation and dissemination of Legionella (see section 11.1 and the supporting document Legionella and the prevention of legionellosis; Annex 1). Some equipment, such as water-cooled high-speed drills in dental surgeries, is of particular concern for both inhalation of droplets and infection of wounds. Renal dialysis requires large volumes of water that is of higher quality than drinking-water. Water used for dialysis requires special processing to minimize the presence of microorganisms, endotoxins, toxins and chemical contaminants. There are special requirements regarding aluminium, which, in the past, has caused dialysis dementia, and dialysis patients are also sensitive to chloramines, which needs to be considered when chloramination is used to disinfect drinking-water supplies, particularly in areas where there are home dialysis patients. All health-care facilities should have specific WSPs as part of their infection control programme. These plans should address issues such as water quality and treatment requirements, cleaning of specialized equipment and control of microbial growth in water systems and ancillary equipment. 6.12 Safe drinking-water for travellers The most common sources of exposure to disease-causing organisms for travellers are contaminated drinking-water and food that has been washed with contaminated water. Diarrhoea is the most common symptom of waterborne infection, affecting 20–50% of all travellers or about 10 million people per year. Cases can occur even among people staying in high-quality resorts and hotels. In some parts of the world, tap or bottled water that has not been produced under proper conditions may not be safe, even if it is clear and colourless. No vaccine is capable of conferring general protection against infectious diarrhoea, which is caused by many different pathogens. It is important that travellers be aware of the possibility of illness and take appropriate steps to minimize the risks. Preventive measures while living or travelling in areas with questionable drinking-water quality include the following: • Drink only bottled water or other beverages (carbonated beverages, pasteurized juices and milk) provided in sealed tamper-proof containers and bottled/canned by known manufacturers (preferably certified by responsible authorities). Hotel personnel or local hosts are often good sources of information about which local brands are safe. • Drink water that has been treated effectively at point of use (e.g. through boiling, filtration or chemical disinfection) and stored in clean containers. • Drink hot beverages such as coffee and tea that are made with boiled water and are kept hot and stored in clean containers. • Avoid brushing teeth with unsafe water. • Do not use ice unless it has been made from safe water. • Avoid salads or other uncooked foods that may have been washed or prepared with unsafe water. Water can be treated in small quantities by travellers to significantly improve its safety. Numerous simple treatment approaches and commercially available technologies are available to travellers to disinfect drinking-water for single-person or family use. Travellers should select a water treatment approach that removes or inactivates all classes of pathogens. Technologies should be certified by a credible organization, and manufacturers’ instructions should be followed carefully. Bringing water to a rolling boil is the simplest and most effective way to kill all disease-causing pathogens, even in turbid water and at high altitudes. The hot water should be allowed to cool without the addition of ice. If the water is turbid and needs to be clarified for aesthetic reasons, this should be done before boiling. If it is not possible to boil water, chemical disinfection of clear, non-turbid water is effective for killing bacteria and most viruses and some protozoa (but not, for example, Cryptosporidium oocysts). Certain chlorine-based or iodine-based compounds are most widely used for disinfection of drinking-water by travellers. Following chlorination or iodination, an activated carbon (charcoal) filter may be used to remove excess taste and odour from the water. The use of iodine is not recommended for longterm use by infants, pregnant women, those with a history of thyroid disease and those with known hypersensitivity to iodine unless treatment includes an effective post-disinfection iodine removal device (e.g. activated carbon). Travellers intending to use iodine treatment daily for all water consumed for more than 3–4 weeks should consult a physician beforehand and not use it in excessive amounts. Silver is sometimes promoted as a disinfectant, but it is not recommended, as its efficacy is uncertain and it requires lengthy contact periods. Suspended particles in water can reduce the effectiveness of disinfectants, and turbid water should be clarified or filtered before disinfection. Chemical products that combine clarification (coagulation and flocculation to remove particles) with chlorine disinfection are available. Portable point-of-use filtration devices tested and rated to remove protozoa and some bacteria, such as ceramic, membrane (mainly reverse osmosis) and activated carbon block filters, are also available. A pore size rating of 1 μm or less is recommended to ensure the removal of Cryptosporidium oocysts. These filters may require a pre-filter to remove suspended particles in order to avoid clogging the final filter. Unless water is boiled, a combination of techniques (e.g. clarification and/or filtration followed by chemical disinfection) is recommended. This combination provides a multiple treatment barrier that removes significant numbers of protozoa in addition to killing bacteria and viruses. For people with weakened immune systems, pregnant women and infants, extra precautions are recommended to reduce the risk of infection from water contaminated with Cryptosporidium, for example. Boiling and storing water in a protected container are recommended, although internationally or nationally certified bottled or mineral water may also be acceptable. The treatment methods described here, with the exception of carbon filtration and reverse osmosis, will generally not reduce levels of most chemical contaminants in drinking-water. However, these are not usually of health concern in the short term. Further information on household water treatment of microbial and chemical contaminants of water can be found in sections 7.3.2 and 8.4.4, respectively. Table 6.1 provides a summary of drinking-water disinfection methods that can be used by travellers. Table 6.1 Drinking-water disinfection methods for use by travellers Method Recommendation What it does What it does not do Boiling Bring water to a rolling boil and allow Kills all pathogens Does not remove turbidity/cloudiness to cool Does not provide residual chemical disinfectant, such as chlorine, to protect against contamination Chlorine compounds: 1. Unscented household bleach (sodium hypochlorite) 2. Sodium dichloroisocyanurate tablet 3. Calcium hypochlorite For typical room temperature and water temperature of 25 °C, minimum contact time should be 30 min; increase contact time for colder water—e.g. double time for each 10 °C less than 25 °C Prepare according to instructions Should be added to clear water or after settling or clarification to be most effective Type and typical dosage: 1. Household bleach (5%)—4 drops per litre 2. Sodium dichloroisocyanurate—1 tablet (per package directions) 3. Calcium hypochlorite (1% stock solution)a—4 drops per litre Effective for killing most bacteria Not effective against Cryptosporidium; not as and viruses effective as iodine when using turbid water Longer contact time required to kill Giardia cysts, especially when water is cold Flocculant‑chlorine tablet or Dose per package directions Effective for killing or removing Flocculated water must be decanted into a clean sachet most waterborne pathogens container, preferably through a clean fabric filter (coagulant‑flocculants partially remove Cryptosporidium) Table 6.1 (continued) Method Recommendation What it does What it does not do Iodine: 25 °C—minimum contact for 30 min; increase contact time for colder water solution) Prepare according to package 1. Tincture of iodine (2% instructions 2. Iodine (10% solution) Type and typical dosage: 3. Iodine tablet 1. Tincture of iodine (2% solution)—5 pentaiodide) resin 4. Iodinated (triiodide or drops per litre 2. Iodine (10% solution)—8 drops per litre 3. Iodine tablet—1 or 2 tablets per litre 4. Iodinated (triiodide or pentaiodide) resin—room temperature according to directions and stay within rated capacity Caution: Not recommended for pregnant women, for people with thyroid problems or for more than a few months’ time. Excess iodine may be removed after iodine treatment through use of a carbon filter or other effective process. Kills most pathogens Not effective against Cryptosporidium Longer contact time is required to kill Giardia cysts, especially when water is cold Carbon filtration after an iodine resin will remove excess iodine from the water; replace the carbon filter regularly Portable filtering devices: 1. Ceramic filters 2. Carbon filters; some carbon block filters will remove Cryptosporidium—only if tested and certified for oocyst removal 3. Membrane filter (microfilter, ultrafilter, nanofilter and reverse osmosis) type devices Check pore size rating and reported removal efficiencies for different pathogens (viruses, bacteria and protozoa) provided by manufacturer and certified by a national or international certification agency. Filter media pore size must be rated at 1 μm (absolute) or less. Note that water must be clear to prevent clogging of pores. Filtration or settling of turbid water to clarify it is recommended before disinfection with chlorine or iodine if water is not boiled 1 μm or less filter pore size will remove Giardia, Cryptosporidium and other protozoa Approved reverse osmosis device can remove almost all pathogens Some filters include a chemical disinfectant such as iodine or chlorine to kill microbes; check for manufacturer’s claim and documentation from an independent national or international certification agency Most bacteria and viruses will not be removed by filters with a pore size larger than 1 μm Microfilters may not remove viruses, especially from clear waters; additional treatment such as chemical disinfection or boiling/pasteurization may be needed to reduce viruses Most carbon block filters do not remove pathogens, other than possibly protozoa, even if carbon is impregnated with silver, because pore size is too large (> 1 μm) a To make a 1% stock solution of calcium hypochlorite, add (to 1 litre of water) 28 g if chlorine content is 35%, 15.4 g if chlorine content is 65% or 14.3 g if chlorine content is 70%. 6.13 Aircraft and airports The importance of water as a potential vehicle for infectious disease transmission on aircraft has been well documented. In general terms, the greatest microbial risks are those associated with ingestion of water that is contaminated with human and animal excreta. If the source of water used to replenish aircraft supplies is contaminated and adequate precautions are not taken, disease can be spread through the aircraft water if it used for drinking or tooth cleaning. It is thus imperative that airports comply with the International Health Regulations (2005) and be provided with potable drinking-water from a source approved by the appropriate regulatory agency (WHO, 2016). Airports usually have special arrangements for managing water after it has entered the airport. A potable water source is not a safeguard if the water is subsequently contaminated during transfer, storage or distribution in aircraft. A WSP covering water management within airports from receipt of the water through to its transfer to the aircraft (e.g. by water servicing vehicles or water bowsers), complemented by measures to ensure that water quality is maintained on the aircraft (e.g. safe materials and good practices in design, construction, operation and maintenance of aircraft systems), provides a framework for water safety in aviation. In undertaking an assessment of the general airport/aircraft water distribution system, a range of specific issues must be taken into consideration, including: • quality of source water and the need for additional treatment; • design and construction of airport storage tanks and pipes; • design and construction of water servicing vehicles; • use of materials and fittings approved for contact with drinking-water at all stages; • water loading techniques; • any treatment systems on aircraft (e.g. ultraviolet disinfection); • maintenance of on-board plumbing; • prevention of cross-connections, including backflow prevention. The airport authority has responsibility for safe drinking-water supply, including operational monitoring, until water is transferred to the aircraft operator. The primary emphasis of monitoring is to ensure that management processes are operating efficiently—for example, the source water quality is not compromised; all parts of the system, including hydrants, hoses and bowsers, are clean and in good repair; backflow prevention is in place; and any filters are clean. In addition, the system should be disinfected and flushed after maintenance or repairs, and the microbiological quality of the water should be checked, preferably before the system is returned to service. Transfer of water into the aircraft and the aircraft drinking-water system also has the potential to introduce hazards, even if the water is of good quality up to this point. It is therefore important that staff involved be properly trained and understand the reasons for the precautions to be taken and the care required in preventing contamination. The precautions described in previous sections regarding transfer of drinking-water from a piped supply or from bowsers and tankers are essential, including maintaining the cleanliness of vehicles and transfer points. There is a significant potential for aviation fuel to contaminate the system, and only small quantities of low molecular weight hydrocarbons can cause the water to be unacceptable. In addition, staff employed in drinking-water supply must not be engaged in activities related to aircraft toilet servicing without first taking all necessary precautions (e.g. thorough hand washing, change of outer garments). All of these requirements and procedures should be properly documented as part of the WSP for the airport water transfer system and should be made clear to airlines using the airport to ensure that they play their part as key stakeholders. Independent surveillance is an important part of the WSP, because circumstances and equipment or staff may change, and the weakening of barriers or the introduction of new risks may not be noticed. This would include initial review and approval of the WSP, periodic review and direct assessment of the provisions and operation of the WSP, paying specific attention to the aircraft industry’s codes of practice, the supporting document Guide to hygiene and sanitation in aviation (Annex 1) and airport health or airline regulations. It is also important that the response to any incident is recorded and reviewed and any lessons learnt incorporated into the WSP. 6.14 Ships The importance of water as a vehicle for infectious disease transmission on ships has been clearly documented. In general terms, the greatest microbial risks are associated with ingestion of water that is contaminated with human and animal excreta. However, chemical contamination could also occur on ships as a result of contaminated bulk water being brought aboard in port, cross-connections on board or improper on-board treatment. The supporting document Guide to ship sanitation (Annex 1) describes the factors that can be encountered during water treatment, transfer, production, storage or distribution in ships and specific features of the organization of the supply and the regulatory framework. To this end, it is vital that all staff responsible for working with the potable water system are properly trained. The organization of water supply systems covering shore facilities and ships differs considerably from conventional water transfer on land but is similar to that for airports. The port authority has responsibility for providing safe potable water for loading onto vessels. If water is suspected to have come from an unsafe source, the ship’s master may have to decide if any additional treatment (e.g. hyperchlorination or filtration) is necessary. When treatment on board or prior to boarding is necessary, the treatment selected should be that which is best suited to the water and which is most easily operated and maintained by the ship’s officers and crew. Water is delivered to ships by hoses or transferred to the ship via water boats or barges. The transfer from shore to ship is a potential source of microbial or chemical contamination. In addition to shore-to-ship transfer of water and bulk storage on board ship, many ships use desalination (see section 6.5) to produce their own drinking-water. In contrast to a shore facility, plumbing aboard ships consists of numerous piping systems carrying potable water, seawater, sewage and fuel and fitted into a relatively confined space. Piping systems are normally extensive and complex, making them difficult to inspect, repair and maintain. A number of waterborne outbreaks on ships have been caused by contamination of potable water after it had been loaded onto the ship—for example, by sewage or bilge water when the water storage systems were not adequately designed and constructed. Potable water should be stored in one or more tanks that are constructed, located and protected so as to be safe against contamination. Potable water lines should be protected and located so that they will not be submerged in bilge water or pass through tanks storing non-potable liquids. It is important to design the system to prevent deterioration of water quality during distribution by minimizing stagnation and dead ends and to take into account ship movement, which increases the possibility of surge and backflow. An overall assessment of the operation of the ship’s water supply should be made, for which the final responsibility lies with the ship’s master, who must ensure that all of the management processes in place are functioning efficiently. An important part of this process is ensuring that those crew who are responsible for the fresh drinking-water supply are properly trained and receive refresher training as appropriate. In developing a WSP and ensuring that the system is capable of supplying safe water, the following need to be considered: • quality of source water if this is from a shore-based source along with the equipment and method of transfer from shore to ship; • desalination equipment and processes where these are used, taking into consideration the points raised in section 6.5;• design and construction of storage tanks and pipework, including the use of approved materials and chemicals and clear colour coding of pipes for different purposes; • minimization of dead ends and areas of stagnation, which may be managed by periodic flushing; • filtration systems and other treatment systems on board the ship, including disinfection and delivery of residual disinfection; • prevention of cross-connections and presence of working backflow prevention devices; • maintenance of adequate water pressure within the system; • presence of a disinfectant residual throughout the system. The system needs to be checked regularly for cleanliness and repair, and parameters such as pH and disinfectant residual need to be checked daily. Where possible, checks on microbiological quality such as plate counts and faecal coliforms, even if only in port, help to ensure that the supply continues to deliver safe water. There also need to be suitable procedures in place to ensure safety after maintenance or repair, including specific disinfection of the system or the affected zone. Any indication of a problem, such as illness or taste or odour problems, should be immediately investigated and the system corrected if it is shown to be the source. In confined communities such as on ships, person-to-person spread of infectious disease is a major issue. Someone who has been working on the latrines and sanitation system on ships should not transfer to work on the drinking-water system without thorough hand washing and a change of outer clothing. Independent surveillance is a desirable element in ensuring drinking-water safety on ships. This implies that there will be periodic audit and direct assessment and the review and approval of the WSP. Specific attention should be given to the shipping industry’s codes of practice, the supporting document Guide to ship sanitation (Annex 1) and port health and shipping regulations. Independent surveillance should also include ensuring that any specific incidents that affect or might have affected water quality have been properly investigated and the lessons to be learnt are incorporated in the WSP. 6.15 Packaged drinking-water Bottled water and water in containers are widely available in both industrialized and developing countries. Consumers purchase packaged drinking-water for reasons such as taste, convenience or fashion, but safety and potential health benefits are also important considerations. Water is packaged for consumption in a range of vessels, including cans, laminated boxes and plastic bags, but it is most commonly supplied in glass or plastic bottles. Bottled water also comes in various sizes, from single servings to large carbuoys holding up to 80 litres. Control of the quality of materials, containers and closures for bottled water is of special concern. Ozone is sometimes used for final disinfection prior to bottling because it does not impart a taste to the water. If the water contains naturally occurring bromide, this can lead to the formation of bromate unless care is taken to minimize its formation. The Guidelines provide a basis for derivation of standards for all packaged waters. As with other sources of drinking-water, safety is pursued through a combination of safety management and end product quality standards and testing and is more readily achievable because batches can be held until results are available. The international framework for packaged water regulation is provided by the Codex Alimentarius Commission of the World Health Organization and the Food and Agriculture Organization of the United Nations. The Codex Alimentarius Commission has developed a Standard for natural mineral waters—which describes the product and its compositional and quality factors, including prescribed treatments, limits for certain chemicals, hygiene, packaging and labelling—and an associated Code of Practice. It has also developed a Standard for bottled/packaged waters to cover packaged drinking-water other than natural mineral waters. Both relevant Codex standards refer directly to these Guidelines; the Codex standards for bottled/packaged water are directly equivalent to the guideline values established in these Guidelines. Under the Codex Standard for natural mineral waters and associated Code of Practice, natural mineral waters must conform to strict requirements, including collection and bottling without further treatment from a natural source, such as a spring or well. In comparison, the Codex Standard for bottled/packaged waters includes waters from other sources, in addition to springs and wells, and treatment to improve their safety and quality. The distinctions between these standards are especially relevant in regions where natural mineral waters have a long cultural history. For further information on the Codex Standard for natural mineral waters and its companion Code of Practice and the Codex Standard for bottled/packaged waters, readers are referred to the Codex web site (http://www.codexalimentarius.net/). The Codex Alimentarius Commission’s Code of practice for collecting, processing and marketing of natural mineral waters provides guidance on a range of good manufacturing practices and provides a generic WSP applied to packaged drinking-water. Some consumers believe that certain natural mineral waters have medicinal properties or offer other health benefits. Some such waters have higher mineral content, sometimes significantly higher than concentrations normally accepted in drinking-water. They often have a long tradition of use and are often accepted on the basis that they are considered foods rather than drinking-water per se. Although certain mineral waters may be useful in providing essential micronutrients, such as calcium and magnesium, these Guidelines do not make recommendations regarding minimum concentrations of essential elements because of the uncertainties surrounding mineral nutrition from drinking-water. Packaged waters with very low mineral content, such as distilled or demineralized waters, are also consumed. There is insufficient scientific information on the benefits or hazards of long-term consumption of very low mineral waters to allow any recommendations to be made (WHO, 2005; see also the supporting document Calcium and magnesium in drinking-water; Annex 1). Another form of packaged water is ice that is intended for adding to drinks and which may come into contact with food to be eaten without cooking. Ice prepared and sold in this manner should be treated the same as any packaged water for potable use. 6.16 Food production and processing The diverse uses of water in food production and processing have different water quality requirements. Uses include irrigation and livestock watering, as an ingredient or to wash or “refresh” foods (e.g. misting of salad vegetables in grocery stores, cleaning fish that are filleted), and in which contact between the water and foodstuff should be minimal (e.g. water for heating, cooling or cleaning). The quality of water defined by the Guidelines is such that it is suitable for many processes used in the food industry. Some processes, however, have special water quality requirements to secure the desired characteristics of the product, and the Guidelines do not necessarily guarantee that such special requirements are met. Poor quality drinking-water may have a severe impact in food processing and potentially on public health. The consequences of a failure to use water of suitable quality in food processing will depend on the use of the water and the subsequent processing of potentially contaminated materials. Variations in water quality that may be tolerated occasionally in drinking-water supply may be unacceptable for some uses in the food industry. These variations may result in a significant financial impact on food production—for example, through product recalls. Water of a different quality from water used for drinking or as an ingredient may be fit for some purposes, provided that it does not compromise the safety of the final product for the consumer. A risk-based approach should be used, and the fitness of the water for the intended purpose should be assessed. For guidance on determining whether water is “fit for purpose” for sourcing, use and reuse in different food production and processing scenarios, see FAO/WHO (2019, 2021). To reduce microbial contamination, specific treatments (e.g. heat, chemical disinfectants, preservatives) capable of minimizing or preventing proliferation of a range of pathogenic organisms of public health concern may be used in food processing. The effect of these treatments should be taken into account when assessing the impacts of deterioration in drinking-water quality on a food production or processing facility. For example, water that is used in canning will usually be heated to a temperature that is at least equivalent to pasteurization. Information on deterioration of the microbial or chemical quality of a drinking-water supply should be promptly communicated to food and beverage production facilities. For further information on disinfection of water for use in food production and processing, see FAO/WHO (2009). 7 Microbial aspects TThe greatest risk to public health from microbes in water is associated with consumption of drinking-water that is contaminated with human and animal excreta, although other sources and routes of exposure may also be significant. Waterborne outbreaks have been associated with inadequate treatment of water supplies and unsatisfactory management of drinking-water distribution. For example, in distribution systems, such outbreaks have been linked to cross-connections, contamination during storage, low water pressure and intermittent supply. Waterborne outbreaks are preventable if an integrated risk management framework based on a multiple-barrier approach from catchment to consumer is applied. Implementing an integrated risk management framework to keep the water safe from contamination in distribution systems includes the protection of water sources, the proper selection and operation of drinking-water treatment processes, and the correct management of risks within the distribution systems (for further information, see the supporting document Water safety in distribution systems; Annex 1). This chapter focuses on organisms for which there is evidence, from outbreak studies or from prospective studies in non-outbreak situations, of diseases being caused by ingestion of drinking-water, inhalation of water droplets or dermal contact 125 with drinking-water and their prevention and control. For the purpose of the Guidelines, these routes are considered waterborne. Chapter 11 (Microbial fact sheets) provides additional detailed information on individual waterborne pathogens, as well as on indicator microorganisms. 7.1 Microbial hazards associated with drinking-water Infectious diseases caused by pathogenic bacteria, viruses and parasites (e.g. protozoa and helminths) are the most common and widespread health risk associated with drinking-water. The public health burden is determined by the severity and incidence of the illnesses associated with pathogens, their infectivity and the population exposed. In vulnerable subpopulations, disease outcome may be more severe. Breakdown in water supply safety (source, treatment and distribution) may lead to large-scale contamination and potentially to detectable disease outbreaks. In some Infectious diseases caused by pathogenic cases, low-level, potentially repeated con-bacteria, viruses, protozoa and helminths tamination may lead to significant spor-are the most common and widespread adic disease, but public health surveillance health risk associated with drinking‑water. is unlikely to identify contaminated drinking-water as the source. Waterborne pathogens have several properties that distinguish them from other drinking-water contaminants: • Pathogens can cause acute and also chronic health effects. • Some pathogens can grow in the environment. • Pathogens are discrete. • Pathogens are often aggregated or adherent to suspended solids in water, and pathogen concentrations vary in time, so that the likelihood of acquiring an infective dose cannot be predicted from their average concentration in water. • Exposure to a pathogen resulting in disease depends upon the dose, invasiveness and virulence of the pathogen, as well as the immune status of the individual. • If infection is established, pathogens multiply in their host. • Certain waterborne pathogens are also able to multiply in food, beverages or warm water systems, perpetuating or even increasing the likelihood of infection. • Unlike many chemical agents, pathogens do not exhibit a cumulative effect. Quantitative microbial risk assessment (QMRA), a mathematical framework for evaluating infectious risks from human pathogens, can assist in understanding and managing waterborne microbial hazards, especially those associated with sporadic disease. 7.1.1 Waterborne infections The pathogens that may be transmitted through contaminated drinking-water are diverse in characteristics, behaviour and resistance. Table 7.1 provides general information on pathogens that are of relevance for drinking-water supply management. Waterborne transmission of the pathogens listed has been confirmed by epidemiological studies and case histories. Part of the demonstration of pathogenicity involves reproducing the disease in suitable hosts. Experimental studies in which healthy adult volunteers are exposed to known numbers of pathogens provide information, but these data are applicable to only a part of the exposed population; extrapolation to more vulnerable subpopulations is an issue that remains to be studied in more detail. Table 7.2 provides information on organisms that have been suggested as possible causes of waterborne disease but where evidence is inconclusive or lacking. The spectrum of pathogens may change as a result of host, pathogen and environmental changes such as fluctuations in human and animal populations, reuse of wastewater, changes in lifestyles and medical interventions, population movement and travel, selective pressures for new pathogens and mutants or recombinations of existing pathogens. The immunity of individuals also varies considerably, whether acquired by contact with a pathogen or influenced by such factors as age, sex, state of health and living conditions. For pathogens transmitted by the faecal–oral route, drinking-water is only one vehicle of transmission. Contamination of food, hands, utensils and clothing can also play a role, particularly when domestic sanitation and hygiene are poor. Improvements in the quality and availability of water, excreta disposal and general hygiene are all important in reducing faecal–oral disease transmission. Microbial drinking-water safety is not related only to faecal contamination. Some organisms grow in piped water distribution systems (e.g. Legionella), whereas others occur in source waters (e.g. guinea worm [Dracunculus medinensis]) and may cause outbreaks and individual cases. Some other microbes (e.g. toxic cyanobacteria) require specific management approaches, which are covered elsewhere in these Guidelines (see section 11.5). Although consumption of contaminated drinking-water represents the greatest risk, other routes of transmission can also lead to disease, with some pathogens transmitted by multiple routes (e.g. adenovirus) (Figure 7.1). Certain serious illnesses result from inhalation of water droplets (aerosols) in which the causative organisms have multiplied because of warm waters and the presence of nutrients. These include legionellosis, caused by Legionella spp., and illnesses caused by the amoebae Naegleria fowleri (primary amoebic meningoencephalitis) and Acanthamoeba spp. (amoebic meningitis, pulmonary infections). Schistosomiasis (bilharziasis) is a major parasitic disease of tropical and subtropical regions that is transmitted when the larval stage (cercariae), which is released by infected aquatic snails, penetrates the skin. It is primarily spread by contact with water. Ready availability of safe drinking-water contributes to disease prevention by reducing the need for contact with contaminated water resources—for example, when collecting water to carry to the home or when using water for bathing or laundry. It is conceivable that unsafe drinking-water contaminated with soil or faeces could act as a carrier of other infectious parasites, such as Balantidium coli (balantidiasis) and certain helminths (species of Fasciola, Fasciolopsis, Echinococcus, Spirometra, Ascaris, Trichuris, Toxocara, Necator, Ancylostoma, Strongyloides and Taenia solium). However, in most of these, the normal mode of transmission is ingestion of the eggs in food contaminated with faeces or faecally contaminated soil (in the case of Taenia solium, Table 7.1 Pathogens transmitted through drinking-watera Type species/ Health Persistence in Resistance to Relative Important Pathogen genus/groupb significancec water suppliesd chlorinee infectivityf animal source Bacteria Burkholderia B. pseudomallei High May multiply Low Low No Campylobacter C. coli High Moderate Low Moderate Yes C. jejuni Escherichia coli – High Moderate Low Low Yes Diarrhoeagenicg E. coli – E. coli O157 High Moderate Low High Yes Enterohaemorrhagic Francisella F. tularensis High Long Moderate High Yes Legionella L. pneumophila High May multiply Low Moderate No Mycobacteria (non‑Mycobacterium avium Low May multiply High Low No tuberculous) complex Salmonella typhi High Moderate Low Low No Other salmonellae S. enterica High May multiply Low Low Yes S. bongori Shigella S. dysenteriae High Short Low High No Vibrio V. cholerae O1 and High Short to longh Low Low No O139 Viruses Adenoviridae Adenoviruses Moderate Long Moderate High No Astroviridae Astroviruses Moderate Long Moderate High No Caliciviridae Noroviruses, High Long Moderate High Potentially Sapoviruses Hepeviridae Hepatitis E virus High Long Moderate High Potentially Picornaviridae Enteroviruses, High Long Moderate High No Parechoviruses, Hepatitis A virus Reoviridae Rotaviruses High Long Moderate High No Table 7.1 (continued) Type species/ Health Persistence in Resistance to Relative Important Pathogen genus/groupb significancec water suppliesd chlorinee infectivityf animal source Protozoa Acanthamoeba A. culbertsoni High May multiply High High No Cryptosporidium C. hominis/parvum High Long High High Yes Cyclospora C. cayetanensis High Long High High No Entamoeba E. histolytica High Moderate High High No Giardia G. intestinalis High Moderate High High Yes Naegleria N. fowleri High May multiply Low Moderate No Helminths Dracunculus D. medinensis High Moderate Moderate High No a This table contains pathogens for which there is some evidence of health significance related to their occurrence in drinking‑water supplies. More information on these and other pathogens is presented in chapter 11. b The type species listed (e.g. L. pneumophila) are those most commonly linked to waterborne transmission but other species may also cause disease. Health significance relates to the incidence and severity of disease, including association with outbreaks. d Detection period for infective stage in water at 20 °C: short, up to 1 week; moderate, 1 week to 1 month; long, over 1 month. e Within pathogen species and groups, there are likely to be variations in resistance, which could be further impacted by characteristics of the water supply and operating conditions. Resistance is based on 99% inactivation at 20 °C where, generally, low represents a Ct99 of < 1 min.mg/L, moderate 1–30 min.mg/L and high > 30 min.mg/L (where C = the concentration of free chlorine in mg/L and t = contact time in minutes) under the following conditions: the infective stage is freely suspended in water treated at conventional doses and contact times, and the pH is between 7 and 8. It should be noted that organisms that survive and grow in biofilms, such as Legionella and mycobacteria, will be protected from chlorination. f From experiments with human volunteers, from epidemiological evidence and from experimental animal studies. High means infective doses can be 1–102 organisms or particles, moderate 102–104 and low > 104. g Includes enteropathogenic, enterotoxigenic, enteroinvasive, diffusely adherent and enteroaggregative. h Vibrio cholerae may persist for long periods in association with copepods and other aquatic organisms. ingestion of the larval cysticercus stage in uncooked pork) rather than ingestion of contaminated drinking-water. Other pathogens that may be naturally present in the environment may be able to cause disease in vulnerable subpopulations: the elderly or the very young, patients with burns or extensive wounds, those undergoing immunosuppressive therapy or those with acquired immunodeficiency syndrome (AIDS). If water used by such persons for drinking or bathing contains sufficient numbers of these organisms, they can produce various infections of the skin and the mucous membranes of the eye, ear, nose and throat. Examples of such agents are Pseudomonas aeruginosa and species of Flavobacterium, Acinetobacter, Klebsiella, Serratia, Aeromonas and certain “slowgrowing” (non-tuberculous) mycobacteria (see the supporting document Pathogenic mycobacteria in water; Annex 1). A number of these organisms are listed in Table 7.2 (and described in more detail in chapter 11). Most of the human pathogens listed in Table 7.1 (which are also described in more detail in chapter 11) are distributed worldwide; some, however, such as those causing outbreaks of cholera or guinea worm disease, are regional. Eradication of Dracunculus medinensis is a recognized target of the World Health Assembly (1991). It is likely that there are pathogens not shown in Table 7.1 that are also transmitted by water. This is because the number of known pathogens for which water is a transmission route continues to increase as new or previously unrecognized pathogens continue to be discovered (WHO, 2003). 7.1.2 Emerging issues A number of developments are subsumed under the concept of “emerging issues” in drinking-water. Global changes, such as human development, population growth and movement and climate change (see section 6.1), exert pressures on the quality and quantity of water resources that may influence waterborne disease risks. Between 1972 and 1999, 35 new agents of disease were discovered, and many more have re-emerged after long periods of inactivity or are expanding into areas where they have not previously been reported (WHO, 2003). In 2003, a coronavirus was identified as the causative agent of severe acute respiratory syndrome, causing a multinational outbreak. Even more recently, influenza viruses originating from animal reservoirs have been transmitted to humans on several occasions, causing flu pandemics and seasonal epidemic influenza episodes (see the supporting document Review of latest available evidence on potential transmission of avian influenza (H5N1) through water and sewage and ways to reduce the risks to human health; Annex 1). Zoonotic pathogens make up 75% of the emerging pathogens and are of increasing concern for human health, along with pathogens with strictly human-to-human transmission. Zoonotic pathogens pose the greatest challenges to ensuring the safety of drinking-water and ambient water, now and in the future (see the supporting document Waterborne zoonoses; Annex 1). For each emerging pathogen, whether zoonotic or not, it should be considered whether it can be transmitted through water and, if so, which prevention and control measures can be suggested to minimize this risk. Table 7.2 Microorganisms for which transmission through drinking-water has been proposed but for which evidence is inconclusive or lackinga Waterborne transmission evidence Presence and Microorganism Type species/ genus/groupb (or epidemiological features) behaviour in water supplies Resistance to chlorinec Bacteria Acinetobacter A. calcoaceticus Possible issue in health‑Common and Low baumannii complex care facilities (nongastrointestinal) can multiply Aeromonas A. hydrophila Clinical isolates do not match environmental isolates Common and can multiply Low Enterobacter E. sakazakii Infection associated with infant formula; no evidence of waterborne Unlikely Low transmission Helicobacter H. pylori Suggested, but no direct evidence; familial transmission primary Detected, survives for limited time Low route Klebsiella K. pneumoniae Possible issue in healthcare facilities (nongastrointestinal) Can multiply Low Leptospira L. interrogans No evidence of transmission through drinking‑water ingestion. Primarily spread by contact with contaminated surface Can survive for months in water Low water; outbreaks associated with flooding Pseudomonas P. aeruginosa Possible issue in healthcare facilities (nongastrointestinal) Common and can multiply Moderate Staphylococcus S. aureus No evidence of transmission through drinking‑water; hands are the most important Common and can multiply Moderate source Tsukamurella T. paurometabola Possible issue in healthcare facilities (nongastrointestinal) Common and can multiply Unknown Yersinia Y. enterocolitica Species detected in water probably non‑pathogenic; food is the primary source Common and can multiply Low Table 7.2 (continued) Waterborne transmission evidence Presence and Type species/ (or epidemiological behaviour in Resistance Microorganism genus/groupb features) water supplies to chlorinec Viruses Filoviridae Ebola virus No evidence of transmission through drinking‑water Orthomyxoviridae Influenza viruses No evidence for waterborne transmission Coronaviridae Severe acute Some evidence for respiratory transmission via syndrome (SARS) inhalation of droplets coronaviruses Picornaviridae/ Aichivirus Present in fecal Kobuvirus wastes, wastewater and sometimes contaminated drinking water Unlikely Low Unlikely Low Unlikely Unknown Likely present in faecally contamined water Moderate Protozoa Balantidium B. coli Blastocystis B. hominis Isospora I. belli Microsporidia – Toxoplasma T. gondii One outbreak reported in 1971 Plausible, but limited evidence Plausible, but no evidence Plausible, but limited evidence; infections predominantly in persons with acquired immunodeficiency syndrome (AIDS) One outbreak reported in 1995 Detected High Unknown, High persistenced likely Unknown High Detected, Moderate persistence likely Long High Helminths Fasciola F. hepatica Plausible, detected in Detected High F. gigantica water in hyperendemic regions Free‑living – Plausible, but Detected and can High nematodes (other transmission primarily multiply than Dracunculus associated with food medinensis) or soil Table 7.2 (continued) Waterborne transmission evidence Presence and Type species/ (or epidemiological behaviour in Resistance Microorganism genus/groupb features) water supplies to chlorinec Schistosoma S. mansoni S. japonicum S. mekongi S. intercalatum S. haematobium No evidence of transmission through drinking‑water ingestion. Primarily spread by contact with contaminated surface water in communities with inadequate access to safe drinking‑water Life cycle Moderate involves animal and snail hosts; can be released into water following reproduction in freshwater snails a More information on these and other pathogens is presented in chapter 11. b The type species listed (e.g. H. pylori) are those most commonly linked to waterborne transmission but other species may also cause disease. Resistance is based on 99% inactivation at 20 °C where, generally, low represents a Ct99 of < 1 min.mg/L, moderate 1–30 min.mg/L and high > 30 min.mg/L (where C = the concentration of free chlorine in mg/L and t = contact time in minutes) under the following conditions: the infective stage is freely suspended in water treated at conventional doses and contact times, and the pH is between 7 and 8. It should be noted that organisms that survive and grow in biofilms, such as Pseudomonas aeruginosa, will be protected from chlorination. d Persistence means survival for 1 month or more. Ingestion (Drinking) Viruses Inhalation and aspiration (Aerosols) Respiratory Gastrointestinal Skin (especially if abraded), mucous membranes, wounds, eyes Contact (Bathing) Adenoviruses Acanthamoeba culbertsoni Bacteria Route of infection Sepsis and generalized infection may occur Protozoa and Campylobacter Adenoviruses helminths Enteroviruses Burkholderia pseudomallei jejuni/coli Astroviruses Cryptosporidium Legionella Leptospira interrogans E. coli – Enteroviruses hominis/parvum pneumophila Mycobacterium avium Diarrhoeagenic Hepatitis A Cyclospora Mycobacterium avium complex E. coli – virus cayetanensis complex Schistosoma mansoni Enterohaemorrhagic Hepatitis E Dracunculus Naegleria fowleri Francisella virus medinensis Parechoviruses tularensis Noroviruses Entamoeba Salmonella enterica, Parechoviruses histolytica S. bongori and Rotaviruses Giardia S. Typhi Sapoviruses intestinalis Shigella dysenteriae Toxoplasma Vibrio cholerae gondii O1 and O139 Figure 7.1 Transmission pathways for and examples of water-related pathogens 7.1.3 Persistence and growth in water Waterborne pathogens, such as Legionella, may grow in water, whereas other host-dependent waterborne pathogens, such as noroviruses and Cryptosporidium, cannot grow in water, but are able to persist. Host-dependent waterborne pathogens, after leaving the body of their host, gradually lose viability and the ability to infect. The rate of decay is usually exponential, and a pathogen will become undetectable after a certain period. Pathogens with low persistence must rapidly find new hosts and are more likely to be spread by person-toperson contact or poor personal hygiene than by drinking-water. Persistence is affected by several factors, of which temperature is the most important. Decay is usually faster at higher temperatures and may be mediated by the lethal effects of ultraviolet (UV) radiation in sunlight acting near the water surface. Relatively high amounts of biodegradable organic carbon, together with warm waters and low residual concentrations of chlorine, can permit growth of Legionella, Vibrio cholerae, Naegleria fowleri, Acanthamoeba and nuisance organisms in some surface waters and during water distribution (see also the supporting documents Heterotrophic plate counts and drinking-water safety and Legionella and the prevention of legionellosis; Annex 1). Microbial water quality may vary rapidly and widely. Short-term peaks in pathogen concentration may increase disease risks considerably and may also trigger outbreaks of waterborne disease. Microorganisms can accumulate in sediments and are mobilized when water flow increases. Results of water quality testing for microbes are not normally available in time to inform management action and prevent the supply of unsafe water. 7.1.4 Public health aspects Outbreaks of waterborne disease may affect large numbers of persons, and the first priority in developing and applying controls on drinking-water quality should be the control of such outbreaks. Available evidence also suggests that drinking-water can contribute to background rates of disease in non-outbreak situations, and control of drinking-water quality should therefore also address waterborne disease in the general community. Experience has shown that systems for the detection of waterborne disease outbreaks are typically inefficient in countries at all levels of socioeconomic development, and failure to detect outbreaks is not a guarantee that they do not occur; nor does it suggest that drinking-water should necessarily be considered safe. Some of the pathogens that are known to be transmitted through contaminated drinking-water lead to severe and sometimes life-threatening disease. Examples include typhoid, cholera, infectious hepatitis (caused by hepatitis A virus or hepatitis E virus) and disease caused by Shigella spp. and E. coli O157. Others are typically associated with less severe outcomes, such as self-limiting diarrhoeal disease (e.g. noroviruses, Cryptosporidium). The effects of exposure to pathogens are not the same for all individuals or, as a consequence, for all populations. Repeated exposure to a pathogen may be associated with a lower probability or severity of illness because of the effects of acquired immunity. For some pathogens (e.g. hepatitis A virus), immunity is lifelong, whereas for others (e.g. Campylobacter), the protective effects may be restricted to a few months to years. In contrast, vulnerable subpopulations (e.g. the young, the elderly, pregnant women, the immunocompromised) may have a greater probability of illness or the illness may be more severe, including mortality. Not all pathogens have greater effects in all vulnerable subpopulations. Not all infected individuals will develop symptomatic disease. The proportion of the infected population that is asymptomatic (including carriers) differs between pathogens and also depends on population characteristics, such as prevalence of immunity. Those with asymptomatic infections as well as patients during and after illness may all contribute to secondary spread of pathogens. 7.2 Health-based target setting 7.2.1 Health-based targets applied to microbial hazards General approaches to health-based target setting are described in section 2.1 and chapter 3. Sources of information on health risks may be from both epidemiology and QMRA, and typically both are employed as complementary sources. Development of health-based targets for many pathogens may be constrained by limitations in the data. Additional data, derived from both epidemiology and QMRA, are becoming progressively more available. Locally generated data will always be of great value in setting national targets. Health-based targets may be set using a direct health outcome approach, where the waterborne disease burden is believed to be sufficiently high to allow measurement of the impact of interventions—that is, epidemiological measurement of reductions in disease that can be attributed to improvements in drinking-water quality. Interpreting and applying information from analytical epidemiological studies to derive health-based targets for application at a national or local level require consideration of a number of factors, including the following questions: • Are specific estimates of disease reduction or indicative ranges of expected reductions to be provided? • How representative of the target population was the study sample in order to assure confidence in the reliability of the results across a wider group? • To what extent will minor differences in demographic or socioeconomic conditions affect expected outcomes? More commonly, QMRA is used as the basis for setting microbial health-based targets, particularly where the fraction of disease that can be attributed to drinking-water is low or difficult to measure directly through public health surveillance or analytical epidemiological studies. For the control of microbial hazards, the most frequent form of health-based target applied is performance targets (see section 3.3.3), which are anchored to a predetermined tolerable burden of disease and established by applying QMRA taking into account raw water quality. Water quality targets (see section 3.3.2) are typically not developed for pathogens; monitoring finished water for pathogens is not considered a feasible or cost-effective option because pathogen concentrations equivalent to tolerable levels of risk are typically less than 1 organism per 104–105 litres. 7.2.2 Reference pathogens It is not practical, and there are insufficient data, to set performance targets for all potentially waterborne pathogens, including bacteria, viruses, protozoa and helminths. A more practical approach is to identify reference pathogens that represent groups of pathogens, taking into account variations in characteristics, behaviours and susceptibilities of each group to different treatment processes. Typically, different reference pathogens will be identified to represent bacteria, viruses, protozoa and helminths. Selection criteria for reference pathogens include all of the following elements: • waterborne transmission established as a route of infection; • sufficient data available to enable a QMRA to be performed, including data on dose–response relationships in humans and disease burden; • occurrence in source waters; • persistence in the environment; • sensitivity to removal or inactivation by treatment processes; • infectivity, incidence and severity of disease. Some of the criteria, such as environmental persistence and sensitivity to treatment processes, relate to the specific characteristics of the reference pathogens. Other criteria can be subject to local circumstances and conditions. These can include waterborne disease burden, which can be influenced by the prevalence of the organism from other sources, levels of immunity and nutrition (e.g. rotavirus infections have different outcomes in high- and low-income regions); and occurrence of the organism in source waters (e.g. presence of toxigenic Vibrio cholerae and Entamoeba histolytica is more common in defined geographical regions, whereas Naegleria fowleri is associated with warmer waters). Selection of reference pathogens The selection of reference pathogens may vary between different countries and regions and should take account of local conditions, including incidence and severity of waterborne disease and source water characteristics (see section 7.3.1). Evidence of disease prevalence and significance should be used in selecting reference pathogens. However, the range of potential reference pathogens is limited by data availability, particularly in regard to human dose–response models for QMRA. Decision-making regarding selection of reference pathogens should be informed by all available data sources, including infectious disease surveillance and targeted studies, outbreak investigations and registries of laboratory-confirmed clinical cases. Such data can help identify the pathogens that are likely to be the biggest contributors to the burden of waterborne disease. It is these pathogens that may be suitable choices as reference pathogens and to consider when establishing health-based targets. Viruses Viruses are the smallest pathogens and hence are more difficult to remove by physical processes such as filtration. Specific viruses may be less sensitive to disinfection than bacteria and parasites (e.g. adenovirus is less sensitive to UV light). Viruses can persist for long periods in water. Infective doses are typically low. Viruses typically have a limited host range, and many are species specific. Most human enteric viruses are not carried by animals, although there are some exceptions, including specific strains of hepatitis E virus (Table 7.1). Rotaviruses, enteroviruses and noroviruses have been identified as potential reference pathogens. Rotaviruses are the most important cause of gastrointestinal infection in children and can have severe consequences, including hospitalization and death, with the latter being far more frequent in low-income regions. There is a dose– response model for rotaviruses, but there is no routine culture-based method for quantifying infectious units. Typically, rotaviruses are excreted in very large numbers by infected patients, and waters contaminated by human waste could contain high concentrations. Occasional outbreaks of waterborne disease have been recorded. In low-income countries, sources other than water are likely to dominate. Enteroviruses, including polioviruses and the more recently recognized parechoviruses, can cause mild febrile illness, but are also important causative agents of severe diseases, such as paralysis, meningitis and encephalitis, in children. There is a dose– response model for enteroviruses, and there is a routine culture-based analysis for measuring infective particles. Enteroviruses are excreted in very large numbers by infected patients, and waters contaminated by human waste could contain high concentrations. Noroviruses are a major cause of acute gastroenteritis in all age groups. Symptoms of illness are generally mild and rarely last longer than 3 days; however, infection does not yield lasting protective immunity. Hence, the burden of disease per case is lower than for rotaviruses. Numerous outbreaks have been attributed to drinking-water. A dose–response model has been developed to estimate infectivity for several norovirus strains, but no culture-based method is available. Bacteria Bacteria are generally the group of pathogens that is most sensitive to inactivation by disinfection. Some free-living pathogens, such as Legionella and non-tuberculous mycobacteria, can grow in water environments, but enteric bacteria typically do not grow in water and survive for shorter periods than viruses or protozoa. Many bacterial species that are infective to humans are carried by animals. There are a number of potentially waterborne bacterial pathogens with known dose–response models, including Vibrio, Campylobacter, E. coli O157, Salmonella and Shigella. Toxigenic Vibrio cholerae can cause watery diarrhoea. When it is left untreated, as may be the case when people are displaced by conflict and natural disaster, case fatality rates are very high. The infective dose is relatively high. Large waterborne outbreaks have been described and keep occurring. Campylobacter is an important cause of diarrhoea worldwide. Illness can produce a wide range of symptoms, but mortality is low. Compared with other bacterial pathogens, the infective dose is relatively low and can be below 1000 organisms. It is relatively common in the environment, and waterborne outbreaks have been recorded. Waterborne infection by E. coli O157 and other enterohaemorrhagic strains of E. coli is far less common than infection by Campylobacter, but the symptoms of infection are more severe, including haemolytic uraemic syndrome and death. The infective dose can be very low (fewer than 100 organisms). Shigella causes over 2 million infections each year, including about 60 000 deaths, mainly in developing countries. The infective dose is low and can be as few as 10–100 organisms. Waterborne outbreaks have been recorded. Although non-typhoidal Salmonella rarely causes waterborne outbreaks, S. Typhi causes large and devastating outbreaks of waterborne typhoid. Protozoa Protozoa are the group of pathogens that is least sensitive to inactivation by chemical disinfection. UV light irradiation is effective against Cryptosporidium, but Cryptosporidium is highly resistant to oxidizing disinfectants such as chlorine. Protozoa are of a moderate size (> 2 μm) and can be removed by physical processes. They can survive for long periods in water. They are moderately species specific. Livestock and humans can be sources of protozoa such as Cryptosporidium and Balantidium, whereas humans are the sole reservoirs of pathogenic Cyclospora and Entamoeba. Infective doses are typically low. There are dose–response models available for Giardia and Cryptosporidium. Giardia infections are generally more common than Cryptosporidium infections, and symptoms can be longer lasting. However, Cryptosporidium is smaller than Giardia and hence more difficult to remove by physical processes; it is also more resistant to oxidizing disinfectants, and there is some evidence that it survives longer in water environments. 7.2.3 Quantitative microbial risk assessment QMRA systematically combines available information on exposure (i.e. the number of pathogens ingested) and dose–response models to produce estimates of the probability of infection associated with exposure to pathogens in drinking-water. Epidemiological data on frequency of asymptomatic infections, duration and severity of illness can then be used to estimate disease burdens. QMRA can be used to determine performance targets and as the basis for assessing the effects of improved water quality on health in the population and subpopulations. Mathematical modelling can be used to estimate the effects of low doses of pathogens in drinking-water on health. Risk assessment, including QMRA, commences with problem formulation to identify all possible hazards and their pathways from sources to recipients. Human exposure to the pathogens (environmental concentrations and volumes ingested) and dose–response relationships for selected (or reference) organisms are then combined to characterize the risks. With the use of additional information (social, cultural, political, economic, environmental, etc.), management options can be prioritized. To encourage stakeholder support and participation, a transparent procedure and active risk communication at each stage of the process are important. An example of a risk assessment approach is outlined in Table 7.3 and described below. For more detailed Table 7.3 Risk assessment paradigm for pathogen health risks Step Aim 1. Problem formulation and To identify all possible hazards associated with drinking‑water that hazard identification would have an adverse public health consequence, as well as their pathways from source(s) to consumer(s) 2. Exposure assessment To determine the size and nature of the population exposed and the route, amount and duration of the exposure 3. Dose–response To characterize the relationship between exposure and the incidence of assessment the health effect 4. Risk characterization To integrate the information from exposure, dose–response and health interventions in order to estimate the magnitude of the public health problem and to evaluate variability and uncertainty Source: Adapted from Haas, Rose & Gerba (1999) information on QMRA in the context of drinking-water safety, see the supporting document Quantitative microbial risk assessment: application for water safety management; Annex 1). Problem formulation and hazard identification All potential hazards, sources and events that can lead to the presence of microbial pathogens (i.e. what can happen and how) should be identified and documented for each component of the drinking-water system, regardless of whether or not the component is under the direct control of the drinking-water supplier. This includes point sources of pollution (e.g. human and industrial waste discharges) as well as diffuse sources (e.g. those arising from agricultural and animal husbandry activities). Continuous, intermittent or seasonal pollution patterns should also be considered, as well as extreme and infrequent events, such as droughts and floods. The broader sense of hazards includes hazardous scenarios, which are events that may lead to exposure of consumers to specific pathogenic microorganisms. In this, the hazardous event (e.g. peak contamination of source water with domestic wastewater) may be referred to as the hazard. As a QMRA cannot be performed for each of the hazards identified, representative (or reference) organisms are selected that, if controlled, would ensure control of all pathogens of concern. Typically, this implies inclusion of at least one bacterium, virus, protozoan or helminth. In this section, Campylobacter, rotavirus and Cryptosporidium have been used as example reference pathogens to illustrate application of risk assessment and calculation of performance targets. Exposure assessment Exposure assessment in the context of drinking-water consumption involves estimation of the number of pathogens to which an individual is exposed, principally through ingestion. Exposure assessment inevitably contains uncertainty and must account for variability of such factors as concentrations of pathogens over time and volumes ingested. Exposure can be considered as a single dose of pathogens that a consumer ingests at a certain point in time or the total amount over several exposures (e.g. over a year). Exposure is determined by the concentration of pathogens in drinking-water and the volume of water consumed. It is rarely possible or appropriate to directly measure pathogens in drinking-water on a regular basis. More often, concentrations in raw waters are assumed or measured, and estimated reductions—for example, through treatment—are applied to estimate the concentration in the water consumed. Pathogen measurement, when performed, is generally best carried out at the location where the pathogens are at highest concentration (generally raw waters). Estimation of their removal by sequential control measures is generally achieved by the use of indicator organisms such as E. coli for enteric bacterial pathogens (see Table 7.4; see also the supporting document Water treatment and pathogen control in Annex 1). The other component of exposure assessment, which is common to all pathogens, is the volume of unboiled water consumed by the population, including person-toperson variation in consumption behaviour and especially consumption behaviour of vulnerable subpopulations. For microbial hazards, it is important that the unboiled volume of drinking-water, both consumed directly and used in food preparation, is used in the risk assessment, as heating will rapidly inactivate pathogens. This amount is lower than that used for deriving water quality targets, such as chemical guideline values. The daily exposure of a consumer to pathogens in drinking-water can be assessed by multiplying the concentration of pathogens in drinking-water by the volume of drinking-water consumed (i.e. dose). For the purposes of the example model calculations, drinking-water consumption was assumed to be 1 litre of unboiled water per day, but location-specific data on drinking-water consumption are preferred. Dose–response assessment The probability of an adverse health effect following exposure to one or more pathogenic organisms is derived from a dose–response model. Available dose–response data have been obtained mainly from studies using healthy adult volunteers. However, adequate data are lacking for vulnerable subpopulations, such as children, the elderly and the immunocompromised, who may suffer more severe disease outcomes. The conceptual basis for the dose–response model is the observation that exposure to the described dose leads to the probability of infection as a conditional event: for infection to occur, one or more viable pathogens must have been ingested. Furthermore, one or more of these ingested pathogens must have survived in the host’s body. An important concept is the single-hit principle (i.e. that even a single pathogen may be able to cause infection and disease). This concept supersedes the concept of (minimum) infectious dose that is frequently used in older literature (see the supporting document Hazard characterization for pathogens in food and water; Annex 1). In general, well-dispersed pathogens in water are considered to be Poisson distributed. When the individual probability of any organism surviving and starting infection is the same, the dose–response relationship simplifies to an exponential function. If, however, there is heterogeneity in this individual probability, this leads to the beta-Poisson dose–response relationship, where the “beta” stands for the distribution of the individual probabilities among pathogens (and hosts). At low exposures, such as would typically occur in drinking-water, the dose–response model is approximately linear and can be represented simply as the probability of infection resulting from exposure to a single organism (see the supporting document Hazard characterization for pathogens in food and water; Annex 1). Risk characterization Risk characterization brings together the data collected on exposure, dose–response and the incidence and severity of disease. The probability of infection can be estimated as the product of the exposure to drinking-water and the probability that exposure to one organism would result in infection. The probability of infection per day is multiplied by 365 to calculate the probability of infection per year. In doing so, it is assumed that different exposure events are independent, in that no protective immunity is built up. This simplification is justified for low risks only, such as those discussed here. Not all infected individuals will develop clinical illness; asymptomatic infection is common for most pathogens. The percentage of infected persons who will develop clinical illness depends on the pathogen, but also on other factors, such as the immune status of the host. Risk of illness per year is obtained by multiplying the probability of infection by the probability of illness given infection. The low numbers in Table 7.4 can be interpreted to represent the probability that a single individual will develop illness in a given year. For example, a risk of illness for Campylobacter of 2.2 × 10−4 per year indicates that, on average, 1 out of 4600 consumers would contract campylobacteriosis from consumption of drinking-water. To translate the risk of developing a specific illness to disease burden per case, the metric disability-adjusted life year, or DALY, is used (see Box 3.1 in chapter 3). This metric reflects not only the effects of acute end-points (e.g. diarrhoeal illness) but also mortality and the effects of more serious end-points (e.g. Guillain-Barré syndrome associated with Campylobacter). The disease burden per case varies widely. For example, the disease burden per 1000 cases of rotavirus diarrhoea is 480 DALYs in low-income regions, where child mortality frequently occurs. However, it is 14 DALYs per 1000 cases in high-income regions, where hospital facilities are accessible to the great majority of the population (see the supporting document Quantifying public health risk in the WHO Guidelines for drinking-water quality; Annex 1). This considerable difference in disease burden results in far stricter treatment requirements in low-income regions for the same raw water quality in order to obtain the same risk (expressed as DALYs per person per year). Ideally, the health outcome target of 10−6 DALY per person per year in Table 7.4 should be adapted to specific national situations. In Table 7.4, no accounting is made for effects on immunocompromised persons (e.g. cryptosporidiosis in patients with human immunodeficiency virus or AIDS), which is significant in some countries. Section 3.2 gives more information on the DALY metric and how it is applied to derive a reference level of risk. Only a proportion of the population may be susceptible to some pathogens, because immunity developed after an initial episode of infection or illness may provide lifelong protection. Examples include hepatitis A virus and rotaviruses. It is estimated Table 7.4 Linking tolerable disease burden and raw water quality for reference pathogens: example calculation River water (human and livestock pollution) Units Cryptosporidium Campylobacter Rotavirusa Raw water quality (CR) Organisms per litre 10 100 10 Treatment effect needed to Log reduction value 105.89 5.98 5.96 reach tolerable risk (PT) Drinking‑water quality (CD) Organisms per litre 1.3 × 10−5 1.05 × 10−4 1.1 × 10−5 Consumption of unheated Litres per day 1 1 1 drinking‑water (V) Exposure by drinking‑water Organisms per day 1.3 × 10−5 1.05 × 10−4 1.1 × 10−5 (E) Dose–response (r)b Probability of infection 2.0 × 10−1 1.9 × 10−2 5.9 × 10−1 per organism Risk of infection (Pinf,d ) Per day 2.6 × 10−6 2.0 × 10−6 6.5 × 10−6 Risk of infection (Pinf,y) Per year 9.5 × 10−4 7.3 × 10−4 2.4 × 10−3 Risk of (diarrhoeal) illness Probability of illness 0.7 0.3 0.5 given infection (Pill|inf) per infection Risk of (diarrhoeal) illness Per year 6.7 × 10−4 2.2 × 10−4 1.2 × 10−3 (Pill) Disease burden (db) DALY per case 1.5 × 10−3 4.6 × 10−3 1.4 × 10−2 Susceptible fraction (fs) Percentage of 100 100 6 population Health outcome target (HT) DALY per yearc 1 × 10−6 1 × 10−6 1 × 10−6 Formulas: CD = CR÷ 10PT Pinf,d = E × r HT = Pill × db × f ÷ 100s E = CD × V Pill = Pinf,y × Pill|inf DALY, disability‑adjusted life year a Data from high‑income regions. In low‑income regions, severity is typically higher (see the supporting document Quantifying public health risk in the WHO Guidelines for drinking-water quality; Annex 1). b Dose–response for Campylobacter and rotavirus from Haas, Rose & Gerba (1999) and for Cryptosporidium from the supporting document Risk assessment of Cryptosporidium in drinking water (Annex 1). For a person drinking 1 litre per day (V). that in developing countries, all children above the age of 5 years are immune to rota-viruses because of repeated exposure in the first years of life. This translates to an average of 17% of the population being susceptible to rotavirus illness. In developed countries, rotavirus infection is also common in the first years of life, and the illness is diagnosed mainly in young children, but the percentage of young children as part of the total population is lower. This translates to an average of 6% of the population in developed countries being susceptible. The uncertainty of the risk outcome is the result of the uncertainty and variability of the data collected in the various steps of the risk assessment. Risk assessment models should ideally account for this variability and uncertainty, although here we present only point estimates (see below). It is important to choose the most appropriate point estimate for each of the variables. Theoretical considerations show that risks are directly proportional to the arithmetic mean of the ingested dose. Hence, arithmetic means of variables such as concentration in raw water, removal by treatment and consumption of drinking-water are recommended. This recommendation is different from the usual practice among microbiologists and engineers of converting concentrations and treatment effects to log values and making calculations or specifications on the log scale. Such calculations result in estimates of the geometric mean rather than the arithmetic mean, and these may significantly underestimate risk. Analysing site-specific data may therefore require going back to the raw data (i.e. counts and tested volumes) rather than relying on reported log-transformed values, as these introduce ambiguity. Emergencies such as major storms and floods can lead to substantial deteriorations in source water quality, including large short-term increases in pathogen concentrations. These should not be included in calculations of arithmetic means. Inclusion will lead to higher levels of treatment being applied on a continuous basis, with substantial cost implications. It is more efficient to develop specific plans to deal with the events and emergencies (see section 4.4). Such plans can include enhanced treatment or (if possible) selection of alternative sources of water during an emergency. 7.2.4 Risk-based performance target setting The process outlined above enables estimation of risk on a population level, taking account of raw water quality and impact of control. This can be compared with the reference level of risk (see section 3.2) or a locally developed tolerable risk. The calculations enable quantification of the degree of source protection or treatment that is needed to achieve a specified level of tolerable risk and analysis of the estimated impact of changes in control measures. Performance targets are most frequently applied to treatment performance—that is, to determine the microbial reduction necessary to ensure water safety. A performance target may be applied to a specific system (i.e. formulated in response to local raw water characteristics) or generalized (e.g. formulated in response to raw water quality assumptions based on a certain type of source) (see also the supporting document Water treatment and pathogen control; Annex 1). Figure 7.2 illustrates the targets for treatment performance for a range of pathogens occurring in raw water. For example, 10 microorganisms per litre of raw water will lead to a performance target of 5.89 logs (or 99.999 87% reduction) for Cryptosporidium or of 5.96 logs (99.999 89% reduction) for rotaviruses in high-income regions to achieve 10−6 DALY per person per year (see also Table 7.5 below). The difference in performance targets for rotaviruses in high- and low-income countries (5.96 and 7.96 logs; Figure 7.2) is related to the difference in disease severity caused by this organism. In low-income countries, the child case fatality rate is relatively high, and, as a consequence, the disease burden is higher. Also, a larger proportion of the population in low-income countries is under the age of 5 and at risk for rotavirus infection. The derivation of these performance targets is described in Table 7.5, which provides an example of the data and calculations that would normally be used to construct a risk assessment model for waterborne pathogens. The table presents data Figure 7.2 Performance targets for example bacterial, viral and protozoan pathogens in relation to raw water quality (to achieve 10−6 DALY per person per year) for representatives of the three major groups of pathogens (bacteria, viruses and protozoa) from a range of sources. These example calculations aim at achieving the reference level of risk of 10−6 DALY per person per year, as described in section 3.2. The data in the table illustrate the calculations needed to arrive at a risk estimate and are not guideline values. 7.2.5 Presenting the outcome of performance target development Table 7.5 presents some data from Table 7.4 in a format that is more meaningful to risk managers. The average concentration of pathogens in drinking-water is included for information. It is not a water quality target, nor is it intended to encourage pathogen monitoring in finished water. As an example, a concentration of 1.3 × 10−5 Cryptosporidium per litre (see Table 7.4) corresponds to 1 oocyst per 79 000 litres (see Table 7.5). The performance target (in the row “Treatment effect” in Table 7.4), expressed as a log10 reduction value, is the most important management information in the risk assessment table. It can also be expressed as a per cent reduction. For example, a 5.96 log10 unit reduction for rotaviruses corresponds to a 99.999 89% reduction. 7.2.6 Adapting risk-based performance target setting to local circumstances The reference pathogens illustrated in the previous sections will not be priority pathogens in all regions of the world. Wherever possible, country- or site-specific information should be used in assessments of this type. If no specific data are available, an approximate risk estimate can be based on default values (see Table 7.6 below). Table 7.5 accounts only for changes in water quality derived from treatment and not from source protection measures, which are often important contributors to overall safety, affecting pathogen concentration and/or variability. The risk estimates preTable 7.5 Health-based targets derived from example calculation in Table 7.4 Cryptosporidium Campylobacter Rotavirusa Organisms per litre in raw water 10 100 10 Health outcome target 10−6 DALY per person 10−6 DALY per person 10−6 DALY per person per year per year per year Risk of diarrhoeal illnessb 1 per 1500 per year 1 per 4600 per year 1 per 14 000 per year Drinking‑water quality 1 per 79 000 litres 1 per 9500 litres 1 per 90 000 litres Performance targetc 5.89 log units 5.98 log units 5.96 log units1010 10 a Data from high‑income regions. In low‑income regions, severity is typically higher, but drinking‑water transmission is unlikely to dominate. b For the susceptible population. Performance target is a measure of log reduction of pathogens based on raw water quality. sented in Table 7.4 also assume that there is no degradation of water quality in the distribution network. These may not be realistic assumptions under all circumstances, and it is advisable to take these factors into account wherever possible. Table 7.5 presents point estimates only and does not account for variability and uncertainty. Full risk assessment models would incorporate such factors by representing the input variables by statistical distributions rather than by point estimates. However, such models are currently beyond the means of many countries, and data to define such distributions are scarce. Producing such data may involve considerable efforts in terms of time and resources, but will lead to much improved insight into the actual raw water quality and treatment performance. The necessary degree of treatment also depends on the values assumed for variables that can be taken into account in the risk assessment model. One such variable is drinking-water consumption. Figure 7.3 shows the effect of variation in the consumption of unboiled drinking-water on the performance targets for Cryptosporidium. If the raw water concentration is 1 oocyst per litre, the performance target varies between 4.3 and 5.2 log10 units if consumption values vary between 0.25 and 2 litres per day. Another variable is the fraction of the population that is susceptible. Some outbreak data suggest that in developed countries, a significant proportion of the population above 5 years of age may not be immune to rotavirus illness. Figure 7.4 shows the effect of variation in the susceptible fraction of the population. If the raw water concentration is 10 rotavirus particles per litre, the performance target increases from 5.96 to 7.18 as the susceptible fraction increases from 6% to 100%. 7.2.7 Health outcome targets Health outcome targets that identify disease reductions in a community should be responded to by the control measures set out in water safety plans and associated water quality interventions at community and household levels. These targets would identify expected disease reductions in communities receiving the interventions. The prioritization of water quality interventions should focus on those aspects that are estimated to contribute more than, for example, 5% of the burden of a given 0 1 2 3 4 5 6 7 8 9 0.001 0.01 0.1 1 10 100 1000 Raw water quality (organisms per litre) 1 litre 0.25 litre 2 litres Cryptosporidium Performance target (log10 reduction) 0 1 2 3 4 5 6 7 8 9 0.001 0.01 0.1 1 10 100 1000 Raw water quality (organisms per litre) 1 litre 0.25 litre 2 litres Cryptosporidium Performance target (log10 reduction) Figure 7.3 Performance targets for Cryptosporidium in relation to the daily consumption of unboiled drinking-water (to achieve 10−6 DALY per person per year) Rotavirus, high-income countries 0 1 2 3 4 5 6 7 8 9 0.001 0.01 0.1 1 10 100 1000 Raw water quality (organisms per litre) 6% susceptible 20% susceptible 100% susceptible Performance target (log10 reduction) Figure 7.4 Performance targets for rotaviruses in relation to the fraction of the population that is susceptible to illness (to achieve 10−6 DALY per person per year) disease (e.g. 5% of total diarrhoea). In many parts of the world, the implementation of a water quality intervention that results in an estimated health gain of more than 5% would be considered extremely worthwhile. Directly demonstrating the health gains arising from improving water quality—as assessed, for example, by reduced E. coli counts at the point of consumption—may be possible where disease burden is high and effective interventions are applied and can be a powerful tool to demonstrate a first step in incremental drinking-water safety improvement. Table 7.6 Example occurrence of selected indicators and pathogens in faeces, wastewater and raw water (local data will vary) Number per gram Number per litre in Number per litre in Microbe of faeces untreated wastewater raw water Faecal coliforms (E. coli and 107 (mostly non‑106−1010 100–100 000 Klebsiella) pathogenic) Campylobacter spp. 106 100−106 100–10 000 Vibrio choleraea 106 100−106 100–108 Enteroviruses 106 1−1000 0.01–10 Rotaviruses 109 50–5000 0.01–100 Cryptosporidium 107 1–10 000 0–1000 Giardia intestinalis 107 1–10 000 0–1000 a Vibrio can grow in the aquatic environment. Sources: Feachem et al. (1983); Stelzer (1988); Jones, Betaieb & Telford (1990); Stampi et al. (1992); Koenraad et al. (1994); Gerba et al. (1996); AWWA (1999); Maier, Pepper & Gerba (2000); Metcalf & Eddy, Inc. (2003); Bitton (2005); Lodder & de Roda Husman (2005); Schijven & de Roda Husman (2006); Masini et al. (2007); Rutjes et al. (2009); Lodder et al. (2010) Where a specified quantified disease reduction is identified as a health outcome target, it is advisable to undertake ongoing proactive public health surveillance among representative communities to measure the effectiveness of water quality interventions. 7.3 Occurrence and treatment of pathogens As discussed in section 4.1, system assessment involves determining whether the drinking-water supply chain as a whole can deliver drinking-water quality that meets identified targets. This requires an understanding of the quality of source water and the efficacy of control measures, such as treatment. 7.3.1 Occurrence An understanding of pathogen occurrence in source waters is essential, because it facilitates selection of the highest-quality source for drinking-water supply, determines pathogen concentrations in source waters and provides a basis for establishing treatment requirements to meet health-based targets within a water safety plan. By far the most accurate way of determining pathogen concentrations in specific catchments and other water sources is by analysing pathogen concentrations in water over a period of time, taking care to include consideration of seasonal variation and peak events such as storms. Direct measurement of pathogens and indicator organisms in the specific source waters for which a water safety plan and its target pathogens are being established is recommended wherever possible, because this provides the best estimates of microbial concentrations. However, resource limitations in many settings preclude this. In the absence of measured pathogen concentrations, an alternative interim approach is to make estimations based on available data, such as the results of sanitary surveys combined with indicator testing. In the case of absence of data on the occurrence and distribution of human pathogens in water for the community or area of implementation, concentrations in raw waters can be inferred from observational data on numbers of pathogens per gram of faeces representing direct faecal contamination or from numbers of pathogens per litre of untreated wastewater (Table 7.6). Data from sanitary surveys can be used to estimate the impact of raw or treated wastewater discharged into source waters. In treated wastewater, the concentrations of pathogens may be reduced 10- to 100-fold or more, depending on the efficiency of the treatment process. The concentrations of pathogens in raw waters can be estimated from concentrations of pathogens in wastewater and the fraction of wastewater present in source waters. In addition, some indicative concentrations of pathogens in source waters are given that were measured at specific locations, but these concentrations may differ widely between locations. From Table 7.6, it may be clear that faecal indicator bacteria, such as E. coli, are always present at high concentrations in wastewater. Everybody sheds E. coli; nevertheless concentrations vary widely. Only infected persons shed pathogens; therefore, the concentrations of pathogens in wastewater vary even more. Such variations are due to shedding patterns, but they also depend on other factors, such as the size of the population discharging into wastewater and dilution with other types of wastewater, such as industrial wastewater. Conventional wastewater treatment commonly reduces microbial concentrations by one or two orders of magnitude before the wastewater is discharged into surface waters. At other locations, raw wastewater may be discharged directly, or discharges may occur occasionally during combined sewer overflows. Discharged wastewater is diluted in receiving surface waters, leading to reduced pathogen numbers, with the dilution factor being very location specific. Pathogen inactivation, die-off or partitioning to sediments may also play a role in pathogen reduction. These factors differ with the surface water body and climate. This variability suggests that concentrations of faecal indicators and pathogens vary even more in surface water than in wastewater. Because of differences in survival, the ratio of pathogen to E. coli at the point of discharge will not be the same as farther downstream. A comparison of data on E. coli with pathogen concentrations in surface waters indicates that, overall, there is a positive relationship between the presence of pathogens in surface water and E. coli concentration, but that pathogen concentrations may vary widely from low to high at any E. coli concentration. Even the absence of E. coli is not a guarantee for the absence of pathogens or for pathogen concentrations to be below those of significance for public health. The estimates based on field data in Table 7.6 provide a useful guide to the concentrations of enteric pathogens in a variety of sources affected by faecal contamination. However, there are a number of limitations and sources of uncertainty in these data, including the following: • Although data on pathogens and E. coli were derived from different regions in the world, they are by far mostly from high-income countries. • There are concerns about the sensitivity and robustness of analytical techniques, particularly for viruses and protozoa, largely associated with the recoveries achieved by techniques used to process and concentrate large sample volumes typically used in testing for these organisms. • Numbers of pathogens were derived using a variety of methods, including culture-based methods using media or cells, molecular-based tests (such as polymerase chain reaction) and microscopy, and should be interpreted with care. • The lack of knowledge about the infectivity of the pathogens for humans has implications in risk assessment and should be addressed. 7.3.2 Treatment Understanding the efficacy of control measures includes validation (see sections 2.2 and 4.1.7). Validation is important both in ensuring that treatment will achieve the desired goals (performance targets) and in assessing areas in which efficacy may be improved (e.g. by comparing performance achieved with that shown to be achievable through well-run processes). Water treatment could be applied in a drinking-water treatment plant (central treatment) to piped systems or in the home or at the point of use in settings other than piped supplies. Central treatment Waters of very high quality, such as groundwater from confined aquifers, may rely on protection of the source water and the distribution system as the principal control measures for provision of safe water. More typically, water treatment is required to remove or destroy pathogenic microorganisms. In many cases (e.g. poor quality surface water), multiple treatment stages are required, including, for example, coagulation, flocculation, sedimentation, filtration and disinfection. Table 7.7 provides a summary of treatment processes that are commonly used individually or in combination to achieve microbial reductions (see also Annex 5). The minimum and maximum removals are indicated as log10 reduction values and may occur under failing and optimal treatment conditions, respectively. The microbial reductions presented in Table 7.7 are for broad groups or categories of microbes: bacteria, viruses and protozoa. This is because it is generally the case that treatment efficacy for microbial reduction differs among these microbial groups as a result of the inherently different properties of the microbes (e.g. size, nature of protective outer layers, physicochemical surface properties). Within these microbial groups, differences in treatment process efficiencies are smaller among the specific species, types or strains of microbes. Such differences do occur, however, and the table presents conservative estimates of microbial reductions based on the more resistant or persistent pathogenic members of that microbial group. Where differences in removal by treatment between specific members of a microbial group are great, the results for the individual microbes are presented separately in the table. Treatment efficacy for microbial reduction can also differ when aggregating different treatment processes. Applying multiple barriers in treatment, for example in drinking-water treatment plants, may strengthen performance, as failure of one process does not result in failure of the entire treatment. However, both positive and negative interactions can occur between multiple treatment steps, and how these interactions affect the overall water quality and water treatment performance is not yet completely understood. In positive interactions, the inactivation of a contaminant is higher when two steps are occurring together than when each of the steps occurs separately—as happens, for example, when coagulation and sedimentation are operating under optimal conditions, and there is an increase in performance of rapid sand filters. In contrast, negative interactions can occur when failure in the first step of the treatment process could lead to a failure of the next process—for example, if coagulation fails to remove organic material, this could lead to a reduced efficacy of subsequent disinfection and a potential increase in DBPs. An overall assessment of the drinking-water treatment performance, as part of the implementation of the WSP, will assist in understanding the efficacy of the multiple treatment processes to ensure the safety of the drinking-water supply. Further information about these water treatment processes, their operations and their performance for pathogen reduction in piped water supplies is provided in more detail in the supporting document Water treatment and pathogen control (Annex 1). Table 7.7 Reductions of bacteria, viruses and protozoa achieved by water treatment technologies at drinking-water treatment plants for large communities Enteric Minimum Maximum pathogen removal removal Treatment process group (LRV) (LRV) Notes Pretreatment Roughing filters Bacteria 0.2 2.3 Depends on filter medium, coagulant Storage reservoirs Bacteria 0.7 2.2 Residence time > 40 days Protozoa 1.4 2.3 Residence time 160 days Bank filtration Viruses > 2.1 8.3 Depends on travel distance, soil type, pumping rate, pH, ionic strength Bacteria 2 > 6 Protozoa > 1 > 2 Coagulation, flocculation and sedimentation Conventional Viruses 0.1 3.4 Depends on coagulation conditions clarification Bacteria 0.2 2 Protozoa 1 2 High‑rate clarification Protozoa > 2 2.8 Depends on use of appropriate blanket polymer Dissolved air flotation Protozoa 0.6 2.6 Depends on coagulant dose Lime softening Viruses 2 4 Depends on pH and settling time Bacteria 1 4 Protozoa 0 2 Filtration Granular high‑rate Viruses 0 3.5 Depends on filter media and filtration coagulation pretreatment; filtered Bacteria 0.2 4.4 water turbidity of ≤ 0.3 NTU in 95% of Protozoa 0.4 3.3 samples (and none to exceed 1 NTU) associated with 1–2 log reduction of viruses and 3 log reduction of Cryptosporidiuma Table 7.7 (continued) Enteric Minimum Maximum pathogen removal removal Treatment process group (LRV) (LRV) Notes Slow sand filtration Viruses Bacteria Protozoa 0.25 2 0.3 4 6 > 5 Depends on presence of schmutzdecke, grain size, flow rate, operating conditions (mainly temperature, pH); filtered water turbidity of ≤ 1NTU in 95% of samples (and none to exceed 5 NTU) associated with 1–2 log reduction of viruses and 2.5–3 log reduction of Cryptosporidiuma Precoat filtration Viruses 1 1.7 If filter cake is present Bacteria 0.2 2.3 Depends on chemical pretreatment Protozoa 3 6.7 Depends on media grade and filtration rate Membrane filtration: microfiltration, ultrafiltration, nanofiltration, reverse osmosis Viruses Bacteria Protozoa < 1 1 2.3 > 6.5 > 7 > 7 Varies with membrane pore size (microfilters, ultrafilters, nanofilters and reverse osmosis filters), integrity of filter medium and filter seals, and resistance to chemical and biological (“grow‑through”) degradation; maximum reductions associated with filtered water turbidity of < 0.1 NTUa Primary disinfectionb,c Chlorine Viruses 2 (Ct 2–30 min·mg/l;990–10 °C; pH 7–9) Bacteria 2 (Ct 0.04–0.0899min·mg/l; 5 °C; pH 6‑7) Protozoa 2 (Ct 25–24599min·mg/l; 0–25 °C; pH 7–8; mainly Giardia) Chlorine dioxide Viruses 2 (Ct99 2–30 min·mg/l; 0–10 °C; pH 7–9) Bacteria 2 (Ct 0.02–0.399min·mg/l; 15–25 °C; pH 6.5–7) Protozoa 2 (Ct 100 min·mg/l)99Free chlorine × contact time predicts efficacy; not effective against Cryptosporidium oocysts. Turbidity and chlorine‑demanding solutes inhibit this process; hence, turbidity should be kept below 1 NTU to support effective disinfection. Where this is not practical, turbidities should be kept below 5 NTU with higher chlorine doses or contact times.a In addition to initial disinfection, the benefits of maintaining free chlorine residuals throughout distribution systems at or above 0.2 mg/l should be considered Table 7.7 (continued) Enteric Minimum Maximum pathogen removal removal Treatment process group (LRV) (LRV) Notes Ozone Viruses 2 (Ct 0.006–0.299min·mg/l) Bacteria 2 (Ct 0.02 min·mg/l)99Protozoa 2 (Ct 0.5–4099min·mg/l) UV Viruses 4 (7–186 mJ/cm2) Bacteria 4 (0.65–230 mJ/cm2) Protozoa 4 (< 1–60 mJ/cm2) Viruses generally more resistant than bacteria Depends on temperature; Cryptosporidium varies widely Effectiveness of disinfection depends on delivered fluence (dose), which varies with intensity, exposure time and UV wavelength. Excessive turbidity and certain dissolved species inhibit this process; hence, turbidity should be kept below 1 NTU to support effective disinfection. Where this is not practical, turbidities should be kept below 5 NTU with higher fluencesa Ct, product of disinfectant concentration and contact time; LRV, log10 reduction value a See Turbidity: Information for regulators and operators of water supplies (Annex 1) b Chemical disinfection: Ct values are given that achieve 2 LRV. UV irradiation: UV dose range is given that achieves 4 LRV. Sources: Chevrefils et al. (2006); Dullemont et al. (2006); Hijnen, Beerendonk & Medema (2006); see also the supporting document Water treatment and pathogen control (Annex 1). Household treatment Household water treatment technologies are any of a range of devices or methods employed for the purposes of treating water in the home or at the point of use in other settings. These are also known as point-of-use or point-of-entry water treatment technologies (Cotruvo & Sobsey, 2006; Nath, Bloomfield & Jones, 2006; see also the supporting document Managing water in the home, Annex 1). Household water treatment technologies comprise a range of options that enable individuals and communities to treat collected water or contaminated piped water to remove or inactivate microbial pathogens. Many of these methods are coupled with safe storage of the treated water to preclude or minimize contamination after household treatment (Wright, Gundry & Conroy, 2003). Household water treatment and safe storage have been shown to significantly improve water quality and reduce waterborne infectious disease risks (Fewtrell & Col-ford, 2004; Clasen et al., 2006). Household water treatment approaches have the potential to have rapid and significant positive health impacts in situations where piped water systems are not possible and where people rely on source water that may be contaminated or where stored water becomes contaminated because of unhygienic handling during transport or in the home. Household water treatment can also be used to overcome the widespread problem of microbially unsafe piped water supplies. Similar small technologies can also be used by travellers in areas where the drinking-water quality is uncertain (see also section 6.12). Not all household water treatment technologies are highly effective in reducing all classes of waterborne pathogens (bacteria, viruses, protozoa and helminths). For example, chlorine is ineffective for inactivating oocysts of the waterborne protozoan Cryptosporidium, whereas some filtration methods, such as ceramic and cloth or fibre filters, are ineffective in removing enteric viruses. Therefore, careful consideration of the health-based target microbes to control in a drinking-water source is needed when choosing among these technologies. Definitions and descriptions of the various household water treatment technologies for microbial contamination follow: • Chemical disinfection: Chemical disinfection of drinking-water includes any chlorine-based technology, such as chlorine dioxide, as well as ozone, some other oxidants and some strong acids and bases. Except for ozone, proper dosing of chemical disinfectants is intended to maintain a residual concentration in the water to provide some protection from post-treatment contamination during storage. Disinfection of household drinking-water in developing countries is done primarily with free chlorine, either in liquid form as hypochlorous acid (commercial household bleach or more dilute sodium hypochlorite solution between 0.5% and 1% hypochlorite marketed for household water treatment use) or in dry form as calcium hypochlorite or sodium dichloroisocyanurate. This is because these forms of free chlorine are convenient, relatively safe to handle, inexpensive and easy to dose. However, sodium trichloroisocyanurate and chlorine dioxide are also used in some household water treatment technologies. Proper dosing of chlorine for household water treatment is critical in order to provide enough free chlorine to maintain a residual during storage and use. Recommendations are to dose with free chlorine at about 2 mg/l to clear water (< 10 nephelometric turbidity units [NTU]) and twice that (4 mg/l) to turbid water (> 10 NTU). Although these free chlorine doses may lead to chlorine residuals that exceed the recommended chlorine residual for water that is centrally treated at the point of delivery, 0.2–0.5 mg/l, these doses are considered suitable for household water treatment to maintain a free chlorine residual of 0.2 mg/l in stored household water treated by chlorination. Further information on point-of-use chlorination can be found in the document Preventing travellers’ diarrhoea: How to make drinking water safe (WHO, 2005). Disinfection of drinking-water with iodine, which is also a strong oxidant, is generally not recommended for extended use unless the residual concentrations are controlled, because of concerns about adverse effects of excess intake on the thyroid gland; however, this issue is being re-examined, because dietary iodine deficiency is a serious health problem in many parts of the world (see also section 6.12 and Table 6.1). As for central treatment, ozone for household water treatment must be generated on site, typically by corona discharge or electrolytically, both of which require electricity. As a result, ozone is not recommended for household water treatment because of the need for a reliable source of electricity to generate it, its complexity of generation and proper dosing in a small application, and its relatively high cost. Strong acids or bases are not recommended as chemical disinfectants for drinking-water, as they are hazardous chemicals that can alter the pH of the water to dangerously low or high levels. However, as an emergency or short-term intervention, the juices of some citrus fruits, such as limes and lemons, can be added to water to inactivate Vibrio cholerae, if enough is added to sufficiently lower the pH of the water (probably to pH less than 4.5). • Membrane, porous ceramic or composite filters: These are filters with defined pore sizes and include carbon block filters, porous ceramics containing colloidal silver, reactive membranes, polymeric membranes and fibre/cloth filters. They rely on physical straining through a single porous surface or multiple surfaces having structured pores to physically remove and retain microbes by size exclusion. Some of these filters may also employ chemical antimicrobial or bacteriostatic surfaces or chemical modifications to cause microbes to become adsorbed to filter media surfaces, to be inactivated or at least to not multiply. Cloth filters, such as those of sari cloth, have been recommended for reducing Vibrio cholerae in water. However, these filters reduce only vibrios associated with copepods, other large crustaceans or other large eukaryotes retained by the cloth. These cloths will not retain dispersed vibrios or other bacteria not associated with copepods, other crustaceans, suspended sediment or large eukaryotes, because the pores of the cloth fabric are much larger than the bacteria, allowing them to pass through. Most household filter technologies operate by gravity flow or by water pressure provided from a piped supply. However, some forms of ultrafiltration, nanofiltration and reverse osmosis filtration may require a reliable supply of electricity to operate. • Granular media filters: Granular media filters include those containing sand or diatomaceous earth or others using discrete particles as packed beds or layers of surfaces over or through which water is passed. These filters retain microbes by a combination of physical and chemical processes, including physical straining, sedimentation and adsorption. Some may also employ chemically active antimicrobial or bacteriostatic surfaces or other chemical modifications. Other granular media filters are biologically active because they develop layers of microbes and their associated exopolymers on the surface of or within the granular medium matrix. This biologically active layer, called the schmutzdecke in conventional slow sand filters, retains microbes and often leads to their inactivation and biodegradation. A household-scale filter with a biologically active surface layer that can be dosed intermittently with water has been developed. • Solar disinfection: There are a number of technologies using solar irradiation to disinfect water. Some use solar radiation to inactivate microbes in either dark or opaque containers by relying on heat from sunlight energy. Others, such as the solar water disinfection or SODIS system, use clear plastic containers penetrated by UV radiation from sunlight that rely on the combined action of the UV radiation, oxidative activity associated with dissolved oxygen and heat. Other physical forms of solar radiation exposure systems also employ combinations of these solar radiation effects in other types of containers, such as UV-penetrable plastic bags (e.g. the “solar puddle”) and panels. • UV light technologies using lamps: A number of drinking-water treatment technologies employ UV light radiation from UV lamps to inactivate microbes. For household- or small-scale water treatment, most employ low-pressure mercury arc lamps producing monochromatic UV radiation at a germicidal wavelength of 254 nm. Typically, these technologies allow water in a vessel or in flow-through reactors to be exposed to the UV radiation from the UV lamps at sufficient dose (fluence) to inactivate waterborne pathogens. These may have limited application in developing countries because of the need for a reliable supply of electricity, cost and maintenance requirements. • Thermal (heat) technologies: Thermal technologies are those whose primary mechanism for the destruction of microbes in water is heat produced by burning fuel. These include boiling and heating to pasteurization temperatures (typically > 63 °C for 30 minutes when applied to milk). The recommended procedure for water treatment is to raise the temperature so that a rolling boil is achieved, removing the water from the heat and allowing it to cool naturally, and then protecting it from post-treatment contamination during storage (see the supporting document Boil water; Annex 1). The above-mentioned solar technologies using solar radiation for heat or for a combination of heat and UV radiation from sunlight are distinguished from this category. • Coagulation, precipitation and/or sedimentation: Coagulation or precipitation is any device or method employing a natural or chemical coagulant or precipitant to coagulate or precipitate suspended particles, including microbes, to enhance their sedimentation. Sedimentation is any method for water treatment using the settling of suspended particles, including microbes, to remove them from the water. These methods may be used along with cloth or fibre media for a straining step to remove the floc (the large coagulated or precipitated particles that form in the water). This category includes simple sedimentation (i.e. that achieved without the use of a chemical coagulant). This method often employs a series of three pots or other water storage vessels in series, in which sedimented (settled) water is carefully transferred by decanting daily; by the third vessel, the water has been sequentially settled and stored a total of at least 2 days to reduce microbes. • Combination (multiple-barrier) treatment approaches: These are any of the above technologies used together, either simultaneously or sequentially, for water treatment. These combination treatments include coagulation plus disinfection, media filtration plus disinfection or media filtration plus membrane filtration. Some are commercial single-use chemical products in the form of granules, powders or tablets containing a chemical coagulant, such as an iron or aluminium salt, and a disinfectant, such as chlorine. When added to water, these chemicals coagulate and flocculate impurities to promote rapid and efficient sedimentation and also deliver the chemical disinfectant (e.g. free chlorine) to inactivate microbes. Other combined treatment technologies are physical devices that include two or more stages of treatment, such as media or membrane filters or adsorbents to physically remove microbes and either chemical disinfectants or another physical treatment process (e.g. UV radiation) to kill any remaining microbes not physically removed by filtration or adsorption. Many of these combined household water treatment technologies are commercial products that can be purchased for household or other local use. It is important to choose commercial combination devices based on consideration of the treatment technologies that have been included in the device. It is also desirable to require that they meet specific microbial reduction performance criteria and preferably be certified for such performance by a credible national or international authority, such as government or an independent organization representing the private sector that certifies good practice and documented performance. Estimated reductions of waterborne bacteria, viruses and protozoan parasites by several of the above-mentioned household water treatment technologies are summarized in Table 7.8. These reductions are based on the results of studies reported in the scientific literature. Two categories of effectiveness are reported: baseline removals and maximum removals. Baseline removals are those typically expected in actual field practice when done by relatively unskilled persons who apply the treatment to raw waters of average and varying quality and where there are minimum facilities or supporting instruments to optimize treatment conditions and practices. Maximum removals are those possible when treatment is optimized by skilled operators who are supported with instrumentation and other tools to maintain the highest level of performance in waters of predictable and unchanging quality (e.g. a test water seeded with known concentrations of specific microbes). It should be noted that there are differences in the log10 reduction value performance of certain water treatment processes as specified for household water treatment in Table 7.8 and for central treatment in Table 7.7. These differences in performance by the same treatment technologies are to be expected, because central treatment is often applied to water that is of desirable quality for the treatment process, and treatment is applied by trained operators using properly engineered and operationally controlled processes. In contrast, household water treatment is often applied to waters having a range of water qualities, some of which are suboptimal for best technology performance, and the treatment is often applied without the use of specialized operational controls by people who are relatively untrained and unskilled in treatment operations, compared with people managing central water treatment facilities. Further details on these treatment processes, including the factors that influence their performance and the basis for the log10 reduction value performance levels provided in Table 7.8, can be found in the supporting documents Managing water in the home and Evaluating household water treatment options (Annex 1). The values in Table 7.8 do not account for post-treatment contamination of stored water, which may limit the effectiveness of some technologies where safe storage methods are not practised. The best options for water treatment at the household level will also employ means for safe storage, such as covered, narrow-mouthed vessels with a tap system or spout for dispensing stored water. Validation, surveillance and certification of household water treatment and storage are recommended, just as they are for central water supplies and systems. The entities responsible for these activities for household water treatment systems may differ from those of central supplies. In addition, separate entities may be responsible for validation, independent surveillance and certification. Nevertheless, validation and surveillance as well as certification are critical for effective management of household and other point-of-use and point-of-entry drinking-water supplies and their treatment and storage technologies, just as they are for central systems (see sections 2.3 and 5.2.3). Table 7.8 Reductions of bacteria, viruses and protozoa achieved by household water treatment technologies Enteric Baseline Maximum pathogen removal removal Treatment process group (LRV) (LRV) Notes Chemical disinfection Free chlorine Bacteria 3 6 Free chlorine × contact time predicts disinfection Viruses 3 6 efficacy; not effective against Cryptosporidium oocysts. Turbidity and Protozoa, 3 5 chlorine‑demanding solutes inhibit non‑Cryptosporidium this process; hence, turbidity should be kept below 1 NTU to support effective disinfection. Where this is not Cryptosporidium 0 1 practical, the aim should be to keep turbidities below 5 NTU, although disinfection should still be practiced if 5 NTU cannot be achieved. At turbidities of more than 1 NTU, higher chlorine doses or contact times will be requireda Membrane, porous ceramic or composite filtration Porous ceramic and carbon block filtration Membrane filtration (microfiltration, ultrafiltration, nanofiltration, reverse osmosis) Fibre and fabric filtration (e.g. sari cloth filtration) Bacteria Viruses Protozoa Bacteria Viruses Protozoa Bacteria Viruses Protozoa 2 1 4 2 MF; 3 UF, NF or RO 0 MF; 3 UF, NF or RO 2 MF; 3 UF, NF or RO 1 0 0 6 4 6 4 MF; 6 UF, NF or RO 4 MF; 6 UF, NF or RO 6 MF; 6 UF, NF or RO 2 0 1 Varies with pore size, flow rate, filter medium and inclusion of augmentation with silver or other chemical agents Varies with membrane pore size, integrity of filter medium and filter seals, and resistance to chemical and biological (“grow‑through”) degradation; maximum reductions associated with filtered water turbidity of < 0.1 NTUa Particle or plankton association increases removal of microbes, notably copepod‑associated guinea worm (Dracunculus medinensis) and plankton‑associated Vibrio cholerae; larger protozoa (> 20 μm) may be removed; ineffective for viruses, dispersed bacteria and small protozoa (e.g. Giardia intestinalis, 8–12 μm, and Cryptosporidium 4–6 μm) Table 7.8 (continued) Enteric Baseline Maximum pathogen removal removal Treatment process group (LRV) (LRV) Notes Granular media filtration Rapid granular, Bacteria diatomaceous earth, Viruses biomass and fossil fuel–based (granular Protozoa and powdered activated carbon, wood and charcoal ash, burnt rice hulls, etc.) filters 1 1 1 4+ 4+ 4+ Varies considerably with media size and properties, flow rate and operating conditions; some options are more practical than others for use in developing countries Household‑level intermittently operated slow sand filtration Bacteria Viruses 1 0.5 3 2 Varies with filter maturity, operating conditions, flow rate, grain size and filter bed contact time Protozoa 2 4 Solar disinfection Solar disinfection (solar Bacteria 3 5+ Varies depending on oxygenation, UV radiation + thermal effects) Viruses 2 4+ sunlight intensity, exposure time, temperature, turbidity and size of Protozoa 2 4+ water vessel (depth of water) UV light technologies using lamps UV irradiation Bacteria 3 Viruses 2 Protozoa 3 5+ Effectiveness of disinfection depends 5+ on delivered fluence (dose), which varies with intensity, exposure 5+ time and UV wavelength. Excessive turbidity and certain dissolved species inhibit this process; hence, turbidity should be kept below 1 NTU to support effective disinfection. Where this is not practical, turbidities should be kept below 5 NTU with higher fluencesa Thermal (heat) technologies Thermal (e.g. boiling) Bacteria Viruses Protozoa 6 6 6 9+ 9+ 9+ Values are based on vegetative cells; spores are more resistant to thermal inactivation than are vegetative cells; treatment to reduce spores by boiling must ensure sufficient temperature and time Sedimentation Simple sedimentation Bacteria Viruses Protozoa 0 0 0 0.5 0.5 1 Effective due to settling of particle‑associated and large (sedimentable) microbes; varies with storage time and particulates in the water Table 7.8 (continued) Enteric Baseline Maximum pathogen removal removal Treatment process group (LRV) (LRV) Notes Combination treatment approaches Flocculation plus Bacteria disinfection systems Viruses (e.g. commercial powder sachets or tablets) Protozoa 7 4.5 3 9 6 5 Some removal of Cryptosporidium possible by coagulation LRV, log10 reduction value; MF, microfilter; NF, nanofilter; RO, reverse osmosis; UF, ultrafilter a See Turbidity: Information for regulators and operators of water supplies (Annex 1). Non-piped water treatment technologies manufactured by or obtained from commercial or other external sources should be certified to meet performance or effectiveness requirements or guidelines, preferably by an independent, accredited certification body. If the treatment technologies are locally made and managed by the household itself, efforts to document effective construction and use and to monitor performance during use are recommended and encouraged. 7.4 Microbial monitoring Microbial monitoring can be undertaken for a range of purposes, including: • validation (see also section 4.1.7);• operational monitoring (see also sections 2.2.2 and 4.2);• verification (see also sections 2.4.1 and 4.3);• surveillance (see chapter 5);• source water monitoring for identifying performance targets (see sections 7.2 and 7.3.1);• collecting data for QMRA (see also section 7.2.3 and the supporting document Quantitative microbial risk assessment: application to water safety management, Annex 1). Owing to issues relating to complexity, sensitivity of detection, cost and timeliness of obtaining results, testing for specific pathogens is generally limited to assessing raw water quality as a basis for identifying performance targets and validation, where monitoring is used to determine whether a treatment or other process is effective in removing target organisms. Very occasionally, pathogen testing may be performed to verify that a specific treatment or process has been effective. However, microbial testing included in verification, operational and surveillance monitoring is usually limited to testing for indicator organisms. Different methods can be employed for the detection of bacteria, viruses, protozoan parasites and helminths in water. The use of some methods, such as microscopy, relies on detection of the whole particle or organism. Other methods, such as molecular amplification using polymerase chain reaction (PCR), target the genomic material, deoxyribonucleic acid (DNA) or ribonucleic acid (RNA). Still other methods, such as immunological detection methods (e.g. enzyme-linked immunosorbent assay [ELISA]), target proteins. Culture-based methods, such as broth cultures or agar-based bacterial media and cell cultures for viruses and phages, detect organisms by infection or growth. Culture in broth or on solid media is largely applied to determine the number of viable bacteria in water. The best known examples are culture-based methods for indicators such as E. coli. Viruses can be detected by several methods. Using cell culture, the number of infectious viruses in water can be determined. Alternatively, viral genomes can be detected by use of PCR. Protozoan parasites are often detected by immunomagnetic separation in combination with immunofluorescence microscopy. PCR can also be applied. Helminths are generally detected using microscopy. In source investigation associated with waterborne infectious disease outbreaks, microbial hazards are generally typed by use of PCR, which can be followed by sequencing analysis to improve the precision of identification. One innovative approach is metagenome analysis (i.e. sequencing nucleic acid obtained directly from environmental samples). This can detect a multitude of microbial hazards in a water sample. It is important to recognize that the different methods measure different properties of microorganisms. Culture-based methods detect living organisms, whereas microscopy, detection of nucleic acid and immunological assays measure the physical presence of microorganisms or components of them, and do not necessarily determine if what is detected is alive or infectious. This creates greater uncertainty regarding the significance of the human health risk compared with detection by culture-based methods. When using non-culture methods that do not measure in units indicative of culturability or infectivity, assumptions are often made about the fraction of pathogens or components detected that represent viable and infectious organisms. The concept of using organisms such as E. coli as indicators of faecal pollution is a well-established practice in the assessment of drinking-water quality. The criteria determined for such faecal indicators are that they should not be pathogens themselves and they should: • be universally present in faeces of humans and animals in large numbers; • not multiply in natural waters; • persist in water in a similar manner to faecal pathogens; • be present in higher numbers than faecal pathogens; • respond to treatment processes in a similar fashion to faecal pathogens; • be readily detected by simple, inexpensive culture methods. These criteria reflect an assumption that the same organism could be used as an indicator of both faecal pollution and treatment/process efficacy. However, it has become clear that one indicator cannot fulfil these two roles and that a range of organisms should be considered for different purposes (Table 7.9). For example, heterotrophic bacteria can be used as operational indicators of disinfection effectiveness and distribution system cleanliness; Clostridium perfringens and coliphage can be used to validate the effectiveness of treatment systems. Escherichia coli has traditionally been used to monitor drinking-water quality, and it remains an important parameter in monitoring undertaken as part of verificaTable 7.9 Use of indicator organisms in monitoring Type of monitoring Verification and Microorganism(s) Validation of process Operational surveillance E. coli (or thermotolerant coliforms) Total coliforms Heterotrophic plate counts Clostridium perfringensa Coliphages Bacteroides fragilis phages Enteric viruses Not applicable Not applicable Indicator for effectiveness of disinfection of bacteria Indicator for effectiveness of disinfection and physical removal processes for viruses and protozoa Indicator for effectiveness of disinfection and physical removal processes for viruses Not applicable Faecal indicator Indicator for cleanliness and Not applicable integrity of distribution systems Indicator for effectiveness of Not applicable disinfection processes and cleanliness and integrity of distribution systems Not applicable Not applicableb Not applicable Not applicableb a Use of Clostridium perfringens for validation will depend on the treatment process being assessed. b Could be used for verification where source waters are known to be contaminated with enteric viruses and protozoa or where such contamination is suspected as a result of impacts of human faecal waste. tion or surveillance. Thermotolerant coliforms can be used as an alternative to the test for E. coli in many circumstances. Water intended for human consumption should contain no faecal indicator organisms. In the majority of cases, monitoring for E. coli or thermotolerant coliforms provides a high degree of assurance because of their large numbers in polluted waters. However, increased attention has focused on the shortcomings of traditional indicators, such as E. coli, as indicator organisms for enteric viruses and protozoa. Viruses and protozoa more resistant to conventional environmental conditions or treatment technologies, including filtration and disinfection, may be present in treated drinking-water in the absence of E. coli. Retrospective studies of waterborne disease outbreaks have shown that complete reliance on assumptions surrounding the absence or presence of E. coli may not ensure safety. Under certain circumstances, it may be desirable to include more resistant microorganisms, such as bacteriophages and/or bacterial spores, as indicators of persistent microbial hazards. Their inclusion in monitoring programmes, including control and surveillance programmes, should be evaluated in relation to local circumstances and scientific understanding. Such circumstances could include the use of source water known to be contaminated with enteric viruses and parasites or where such contamination is suspected as a result of the impacts of human and livestock waste. Further discussion on indicator organisms is contained in the supporting document Assessing microbial safety of drinking water (Annex 1). Table 7.10 Guideline values for verification of microbial qualitya (see also Table 5.2) Organisms Guideline value All water directly intended for drinking E. coli or thermotolerant coliform bacteriab,c Must not be detectable in any 100 ml sample Treated water entering the distribution system E. coli or thermotolerant coliform bacteriab Must not be detectable in any 100 ml sample Treated water in the distribution system E. coli or thermotolerant coliform bacteriab Must not be detectable in any 100 ml sample a Immediate investigative action must be taken if E. coli are detected. b Although E. coli is the more precise indicator of faecal pollution, the count of thermotolerant coliform bacteria is an acceptable alternative. If necessary, proper confirmatory tests must be carried out. Total coliform bacteria are not acceptable as an indicator of the sanitary quality of water supplies, particularly in tropical areas, where many bacteria of no sanitary significance occur in almost all untreated supplies. c It is recognized that in the great majority of rural water supplies, especially in developing countries, faecal contamination is widespread. Especially under these conditions, medium‑term targets for the progressive improvement of water supplies should be set. Table 7.10 presents guideline values for verification of the microbial quality of drinking-water. Individual values should not be used directly from the table. The guideline values should be used and interpreted in conjunction with the information contained in these Guidelines and other supporting documentation. A consequence of variable susceptibility to pathogens is that exposure to drinking-water of a particular quality may lead to different health effects in different populations. For derivation of national standards, it is necessary to define reference populations or, in some cases, to focus on specific vulnerable subpopulations. National or local authorities may wish to apply specific characteristics of their populations in deriving national standards. 7.5 Methods of detection of faecal indicator organisms Analysis for faecal indicator organisms provides a sensitive, although not the most rapid, indication of pollution of drinking-water supplies. Because the growth medium and the conditions of incubation, as well as the nature and age of the water sample, can influence the species isolated and the count, microbiological examinations may have variable accuracy. This means that the standardization of methods and of laboratory procedures is of great importance if criteria for the microbial quality of water are to be uniform in different laboratories and internationally. International standard methods should be evaluated under local circumstances before being adopted. Established standard methods are available, such as those of the International Organization of Standardization (ISO) (Table 7.11) or methods of equivalent efficacy and reliability. It is desirable that established standard methods be used for routine examinations. Whatever method is chosen for detection of E. coli or thermotolerant coliforms, the importance of “resuscitating” or recovering environmentally damaged or disinfectant-damaged strains must be considered. Table 7.11 International Organization for Standardization (ISO) standards for detection and enumeration of faecal indicator organisms in water ISO standard Title (water quality) 6461‑1:1986 Detection and enumeration of the spores of sulfite‑reducing anaerobes (clostridia)— Part 1: Method by enrichment in a liquid medium 6461‑2:1986 Detection and enumeration of the spores of sulfite‑reducing anaerobes (clostridia)— Part 2: Method by membrane filtration 7704:1985 Evaluation of membrane filters used for microbiological analyses 9308‑1:2000 Detection and enumeration of Escherichia coli and coliform bacteria—Part 1: Membrane filtration method 9308‑2:1990 Detection and enumeration of coliform organisms, thermotolerant coliform organisms and presumptive Escherichia coli—Part 2: Multiple tube (most probable number) method 9308‑3:1998 Detection and enumeration of Escherichia coli and coliform bacteria—Part 3: Miniaturized method (most probable number) for the detection and enumeration of E. coli in surface and waste water 10705‑1:1995 Detection and enumeration of bacteriophages—Part 1: Enumeration of F‑specific RNA bacteriophages 10705‑2:2000 Detection and enumeration of bacteriophages—Part 2: Enumeration of somatic coliphages 10705‑3:2003 Detection and enumeration of bacteriophages—Part 3: Validation of methods for concentration of bacteriophages from water 10705‑4:2001 Detection and enumeration of bacteriophages—Part 4: Enumeration of bacteriophages infecting Bacteroides fragilis 7.6 Identifying local actions in response to microbial water quality problems and emergencies During an emergency in which there is evidence of faecal contamination of the drinking-water supply, it may be necessary either to modify the treatment of existing sources or to temporarily use alternative sources of drinking-water. It may be necessary to increase disinfection at source, following treatment or during distribution. If microbial quality cannot be maintained, it may be necessary to advise consumers to boil the water during the emergency (see section 7.6.1). Initiating superchlorination and undertaking immediate corrective measures may be preferable where the speed of response is sufficient to prevent significant quantities of contaminated water from reaching consumers. During outbreaks of potentially waterborne disease or when faecal contamination of a drinking-water supply is detected, the concentration of free chlorine should be increased to greater than 0.5 mg/l throughout the system as a minimum immediate response. It is most important that decisions are taken in consultation with public health authorities and, where appropriate, civil authorities (see also sections 4.4.3, 6.8 and 8.7). 7.6.1 Boil water advisories Boil water advisories share many features with water avoidance advisories used in the event of serious chemical contamination (see section 8.7). Water suppliers in conjunction with public health authorities should develop protocols for boil water orders. Protocols should be prepared prior to the occurrence of incidents and incorporated within management plans. Decisions to issue advisories are often made within a short period of time, and developing responses during an event can complicate decision-making, compromise communication and undermine public confidence. In addition to the information discussed in section 4.4.3, the protocols should deal with: • criteria for issuing and rescinding advisories; • information to be provided to the general public and specific groups; • activities affected by the advisory. Protocols should identify mechanisms for the communication of boil water advisories. The mechanisms may vary, depending on the nature of the supply and the size of the community affected, and could include: • media releases through television, radio and newspapers; • telephone, e-mail and fax contact of specific facilities, community groups and local authorities; • posting of notices in conspicuous locations; • personal delivery; • mail delivery. The methods chosen should provide a reasonable surety that all of those affected by the advisory, including residents, workers and travellers, are notified as soon as possible. Boil water advisories should indicate that the water can be made safe by bringing it to a rolling boil. After boiling, the water should be allowed to cool down on its own without the addition of ice. This procedure is effective at all altitudes and with turbid water. The types of event that should lead to consideration of boil water advisories include: • substantial deterioration in source water quality; • major failures associated with treatment processes or the integrity of distribution systems; • inadequate disinfection; • detection of pathogens or faecal indicator organisms in drinking-water; • epidemiological evidence suggesting that drinking-water is responsible for an outbreak of illness. Boil water advisories are a serious measure that can have substantial adverse consequences. Advice to boil water can have negative public health consequences through scalding and increased anxiety, even after the advice is rescinded. In addition, not all consumers will follow the advice issued, even at the outset; if boil water advisories are issued frequently or are left in place for long periods, compliance will decrease. Hence, advisories should be issued only after careful consideration of all available information by the public health authority and the incident response team and conclusion that there is an ongoing risk to public health that outweighs any risk from the advice to boil water. For example, where microbial contamination is detected in samples of drinking-water, factors that should be considered in evaluating the need for an advisory include: • reliability and accuracy of results; • vulnerability of source water to contamination; • evidence of deterioration in source water quality; • source water monitoring results; • results from operational monitoring of treatment and disinfection processes; • disinfectant residuals; • physical integrity of the distribution system. The available information should be reviewed to determine the likely source of the contamination and the likelihood of recurrence or persistence. When issued, a boil water advisory should be clear and easily understood by recipients, or it may be ignored. Advisories should normally include a description of the problem, potential health risks and symptoms, activities that are affected, investigative actions and corrective measures that have been initiated, as well as the expected time to resolve the problem. If the advisory is related to an outbreak of illness, specific information should be provided on the nature of the outbreak, the illness and the public health response. Boil water advisories should identify both affected and unaffected uses of drinking-water supplies. Generally, the advisory will indicate that unboiled water should not be used for drinking, preparing cold drinks, making ice, preparing or washing food or brushing teeth. Unless heavily contaminated, unboiled water will generally be safe for bathing (providing swallowing of water is avoided) and washing clothes. A boil water advisory could include specific advice for vulnerable subpopulations, such as pregnant women and others who might be immunocompromised. Specific advice should also be provided to facilities such as dental clinics, dialysis centres, doctors’ offices, hospitals and other health-care facilities, child-care facilities, schools, food suppliers and manufacturers, hotels, restaurants and operators of public swimming pools and spas. Provision of alternative supplies of drinking-water, such as bottled water or bulk water, should be considered when temporary boil water advisories are in place. The protocols should identify sources of alternative supplies and mechanisms for delivery. Protocols should include criteria for rescinding boil water advisories. Depending on the reason for issuing the advisory, the criteria could include one or more of the following: • evidence that source water quality has returned to normal; • correction of failures associated with treatment processes or distribution systems; • correction of faults in disinfection processes and restoration of normal disinfectant residuals; • where detection of microbial contamination in drinking-water initiated the advisory, evidence that this contamination has been removed or inactivated; • evidence that sufficient mains flushing or water displacement has removed potentially contaminated water and biofilms; • epidemiological evidence indicating that an outbreak has concluded. When boil water advisories are rescinded, information should be provided through similar channels and to the same groups that received the original advice. In addition, operators/managers or occupants of large buildings and buildings with storage tanks should be advised of the need to ensure that storages and extensive internal distribution systems are thoroughly flushed before normal uses are restored. 7.6.2 Actions following an incident It is important that any incident be properly investigated and remedial action instigated to prevent its recurrence. The water safety plan will require revision to take into account the experience gained, and the findings may also be of importance in informing actions regarding other water supplies to prevent a similar event from occurring elsewhere. Where appropriate, epidemiological investigations by the health authority will also help to inform actions for the future. 8 Chemical aspects MMost chemicals arising in drinking-water are of health concern only after extended exposure of years, rather than months. The principal exception is nitrate. Typically, changes in water quality occur progressively, except for those substances that are discharged or leach intermittently to flowing surface waters or groundwater supplies from, for example, contaminated landfill sites. In some cases, there are groups of chemicals that arise from related sources—for example, disinfection byproducts (DBPs)—and it may not be necessary to set standards for all of the DBPs for which there are guideline values. If chlorination is practised, the trihalomethanes (THMs) and haloacetic acids (HAAs) will be the main DBPs. If bromide is present, brominated as well as chlorinated DBPs will be produced. Maintaining THM and HAA concentrations below the guideline values by controlling precursor compounds will provide adequate control over other chlorination by-products. Several of the inorganic elements for which guideline values have been established are recognized to be essential elements in human nutrition. No attempt has been made here at this time to define a minimum desirable concentration of such substances in drinking-water, although the issue of nutritional essentiality is considered during the guideline development process. 167 Fact sheets for individual chemical contaminants are provided in chapter 12. For those contaminants for which a guideline value or health-based value has been established (see section 8.2), the fact sheets include a brief toxicological overview of the chemical, the basis for guideline derivation, treatment performance and analytical limit of detection. More detailed chemical reviews are available (https://www.who.int/ teams/environment-climate-change-and-health/water-sanitation-and-health/chemical-hazards-in-drinking-water). 8.1 Chemical hazards in drinking-water A few chemical contaminants have been shown to cause adverse health effects in humans as a consequence of prolonged exposure through drinking-water. The lists of chemicals addressed in these Guide‑However, this is only a very small lines do not imply that all of these chemicals proportion of the chemicals that may will always be present or that other chemicals reach drinking-water from various not addressed will be absent. sources. The substances considered here have been assessed for possible health effects, and guideline values have been established only on the basis of health concerns. Additional consideration of the potential effects of chemical contaminants on the acceptability (i.e. taste, odour and appearance) of drinking-water to consumers is included in chapter 10. Some substances of health concern have effects on the acceptability of drinking-water that would normally lead to rejection of the water at concentrations significantly lower than those of health concern. For such substances, no formal guideline value is usually proposed, but a health-based value (see section 8.2) may be needed, for instance, in order to assist in judging the response required when problems are encountered and in some cases to provide reassurance to health authorities and consumers with regard to possible health risks. Regulators are required to establish health-based targets that must be met through water safety plans. In the case of chemical contaminants, these are normally based on the guideline value, which is, in turn, based on health-related end-points. In this case, the guideline value and the local water quality target are similar, but not necessarily identical, because the latter value may need to be adjusted to take into account local sociocultural, economic and environmental/geological circumstances, as indicated in section 2.6. Guideline values provide a benchmark for the development of local water quality targets for chemicals (usually a national standard expressing a maximum allowable concentration). Guideline values may not directly reflect the target of 10−6 disability-adjusted life year (DALY), as these are frequently derived based on evidence indicating a no-adverse effect or negligible risk level. Some guideline values are based on extrapolation of the risk of cancer from exposures at which this can be measured to low exposures where measurement is currently not possible. In section 2.6, it is stated that “In developing national drinking-water standards based on these Guidelines, it will be necessary to take account of a variety of environmental, social, cultural, economic, dietary and other conditions affecting potential exposure. This may lead to national standards that differ appreciably from these Guidelines.” This is particularly applicable to chemical contaminants, for which there is a long list, and setting standards for, or including, all of them in It is important that chemical contaminants be primonitoring programmes is nei-oritized so that the most important in the country or ther feasible nor desirable. local region are considered for inclusion in national The probability that any par-standards and monitoring programmes. ticular chemical may occur in significant concentrations in any particular setting must be assessed on a case-by-case basis. The presence of certain chemicals may already be known within a particular country, but others may be more difficult to assess. In most countries, whether developing or industrialized, water sector professionals are likely to be aware of a number of chemicals that are present in significant concentrations in some drinking-water supplies. A body of local knowledge that has been built up by practical experience over a period of time is invaluable. Hence, the presence of a limited number of chemical contaminants in drinking-water is usually already known in many countries and in many local systems. Significant problems, even crises, can occur, however, when chemicals posing high health risk are widespread but their presence is unknown, because their long-term health effect is caused by chronic exposure as opposed to acute exposure. Such has been the case of arsenic in groundwater in Bangladesh and West Bengal, India, for example. For many contaminants, there will be exposure from sources other than drinking-water, and this may need to be taken into account when setting, and considering the need for, standards. It may also be important when considering the need for monitoring. In some cases, drinking-water will be a minor source of exposure, and controlling levels in water will have little impact on overall exposure. In other cases, controlling a contaminant in water may be the most cost-effective way of reducing exposure. Drinking-water monitoring strategies should therefore not be considered in isolation from other potential routes of exposure to chemicals in the environment. The scientific basis for each of the guideline values is summarized in chapter 12. This information is important in helping to adapt guideline values to suit national requirements or for assessing the health significance of a contaminant that is of a higher concentration than the guideline value. Chemical contaminants in drinking-water may be categorized in various ways; however, the most appropriate is to consider the primary source of the contaminant— that is, to group chemicals according to where control may be effectively exercised. This aids in the development of approaches that are designed to prevent or minimize contamination, rather than those that rely primarily on the measurement of contaminant levels in final waters. In general, approaches to the management of chemical hazards in drinking-water vary between those where the source water is a significant contributor (with control effected, for example, through source water selection, pollution control, treatment or blending) and those from materials and chemicals used in the production and distribution of drinking-water (controlled by process optimization or product specification). In these Guidelines, chemicals are therefore divided into five major source groups, as shown in Table 8.1. Table 8.1 Categorization of source of chemical constituents Source of chemical constituents Examples of sources Naturally occurring Rocks, soils and the effects of the geological setting and climate; eutrophic water bodies (also influenced by sewage inputs and agricultural runoff ) Industrial sources and human dwellings Mining (extractive industries) and manufacturing and processing industries, sewage (including a number of contaminants of emerging concern), solid wastes, urban runoff, fuel leakages Agricultural activities Manures, fertilizers, intensive animal practices and pesticides Water treatment or materials in contact Coagulants, DBPs, piping materials with drinking‑water Pesticides used in water for public Larvicides used in the control of insect vectors of disease health Categories may not always be clear-cut. The group of naturally occurring contaminants, for example, includes many inorganic chemicals that are found in drinking-water as a consequence of release from rocks and soils by rainfall, some of which may become problematical where there is environmental disturbance, such as in mining areas. 8.2 Derivation of chemical guideline values and health-based values In order for a particular chemical constituent to be evaluated to determine whether a guideline value or health-based value should be derived, one of the following criteria must be satisfied: • There is credible evidence of occurrence of the chemical in drinking-water, combined with evidence of actual or potential toxicity. • The chemical is of significant international concern. • The chemical is being considered for inclusion or is included in the WHO Pesticide Evaluation Scheme (WHOPES), which coordinates the testing and evaluation of pesticides for public health, including those applied directly to drinking-water for control of insect vectors of disease. Guideline values are derived for many chemical constituents of drinking-water. A guideline value normally represents the concentration of a constituent that does not result in any significant risk to health over a lifetime of consumption. A number of provisional guideline values have been established at concentrations that are reasonably achievable through practical treatment approaches or in analytical laboratories; in these cases, the guideline value is above the concentration that would normally represent the calculated health-based value. Guideline values are also designated as provisional when there is a high degree of uncertainty in the toxicological and health data (see also section 8.2.5). For some chemicals, no formal guideline value is proposed, on the grounds that occurrence is only at concentrations well below those that would be of concern for health. Establishing a formal guideline value for such substances could encourage some Member States to incorporate the value into their national standards when this is neither necessary nor appropriate. However, to provide guidance for Member States should the chemical be found in drinking-water or in source water in the hazard identification phase of developing a WSP, a health-based value has been determined. In addition, health-based values for acute exposures are now being developed for a small number of substances that may be implicated in emergency situations as a result of a spill, usually to surface water sources. The derivation of these acute health-based values is explained in section 8.7.5. Where data are inadequate to establish a formal guideline value, a provisional reference value may be proposed. These values are derived from the limited data and are too uncertain to be used for developing regulations or standards. Nevertheless these “bounding values” may be useful to guide actions by Member States in the event of need. An explanation of these different values and considerations for their application are outlined in Table 8.2. There are two principal sources of information on health effects resulting from exposure to chemicals that can be used in deriving guideline values. The first and preferred source is studies on human populations. However, the availability of such studies for most substances is limited, owing to the ethical barriers to conducting human toxicological studies and the lack of quantitative information on the concentration to which people have been exposed or on simultaneous exposure to other agents. However, for a few substances, such studies are the primary basis on which guideline values are developed. The second and most frequently used source of information is toxicological studies using laboratory animals. The limitations of toxicological studies include the relatively small number of experimental animals used and the relatively high doses administered, which create uncertainty as to the relevance of particular findings to human health. This uncertainty stems from the need to extrapolate the results from experimental animals to humans and to the low doses to which human populations are usually exposed. In most cases, the study used to derive the guideline value is supported by a range of other studies, including human data, and these are also considered in carrying out a health risk assessment. In order to derive a guideline value to protect human health, it is necessary to select the most suitable study or studies. Data from well-conducted studies, where a clear dose–response relationship has been demonstrated, are preferred. Expert judgement, applied against criteria described in section 8.2.4, is exercised in the selection of the most appropriate studies from the range of information available. Safety or uncertainty factors using standard risk assessment principles are included to provide conservative guideline values that are considered to be protective. 8.2.1 Approaches taken Two approaches to the derivation of guideline values are used: one for “threshold chemicals” and the other for “non-threshold chemicals” (mostly genotoxic carcinogens). Table 8.2 Use and types of chemical values Type of value Comments Application Guideline value Represents the concentration of a constituent that does not result in any significant risk to health. This is for a lifetime of consumption unless indicated. Health‑based Represents the concentration of value a constituent that does not result in any significant risk to health over a lifetime of consumption unless indicated. Provisional As above, but derived with reference value limited data for a very limited number of substances. For adaptation into national drinking‑water standards if warranted by local circumstances including data on occurrence. Where a provisional guideline value is presented along with the health‑based value, the provisional guideline value is recommended for adaptation into national drinking‑water standards because the provisional guideline value takes into account practical issues associated with monitoring, management or competing risks. See Table 8.4 for more information on provisional guideline values. Health‑based values may also be presented without a provisional guideline value. These values can be useful when there is a local concern, such as to inform temporary advisories or limits during a spill situation or to communicate health risks in the case of contaminants that can cause aesthetic issues at lower levels than the health‑based value. Allows for an interim risk assessment, as above, but with greater uncertainty and therefore not to be used for developing regulations or standards. It is generally considered that the initiating event in the process of genotoxic chemical carcinogenesis is the induction of a mutation in the genetic material (deoxyribonucleic acid [DNA]) of somatic cells (i.e. cells other than ova or sperm) and that there is a theoretical risk at any level of exposure (i.e. no threshold). In contrast, there are carcinogens that are capable of producing tumours in experimental animals or humans without exerting a genotoxic activity, but acting through an indirect mechanism. It is generally believed that a demonstrable threshold dose exists for nongenotoxic carcinogens. In deriving guideline values for carcinogens, consideration is given to the potential mechanisms by which the substance may cause cancer, in order to decide whether a threshold or non-threshold approach should be used (see sections 8.2.2 and 8.2.3). The evaluation of the potential carcinogenicity of chemical substances is usually based on long-term laboratory animal studies. Sometimes data are available on carcinogenicity in humans, mostly from occupational exposure. On the basis of the available evidence, the International Agency for Research on Cancer (IARC) categorizes chemical substances with respect to their potential carcinogenic risk into the following groups: Group 1: the agent is carcinogenic to humans Group 2A: the agent is probably carcinogenic to humans Group 2B: the agent is possibly carcinogenic to humans Group 3: the agent is not classifiable as to its carcinogenicity to humans Group 4: the agent is probably not carcinogenic to humans According to IARC, these classifications represent a first step in carcinogenic risk assessment, which leads to a second step of quantitative risk assessment where possible. In establishing guideline values for drinking-water, the IARC evaluation of carcinogenic compounds, where available, is taken into consideration. 8.2.2 Threshold chemicals For most kinds of toxicity, it is believed that there is a dose below which no adverse effect will occur. For chemicals that give rise to such toxic effects, a tolerable daily intake (TDI) should be derived as follows, using the most sensitive end-point in the most relevant study, preferably involving administration in drinking-water: NOAEL or LOAEL or BMDL TDI = UF and/or CSAF where: NOAEL = no-observed-adverse-effect level LOAEL = lowest-observed-adverse-effect level BMDL = lower confidence limit on the benchmark dose UF = uncertainty factor CSAF = chemical-specific adjustment factor The guideline value (GV) is then derived from the TDI as follows: TDI × bw × P GV = C where: bw P C = = = body weight (see below) fraction of the TDI allocated to drinking-water daily drinking-water consumption (see below) Tolerable daily intake The TDI is an estimate of the amount of a substance in food and drinking-water, expressed on a body weight basis (milligram or microgram per kilogram of body weight), that can be ingested over a lifetime without appreciable health risk, and with a margin of safety. Acceptable daily intakes (ADIs) are established for food additives and pesticide residues that occur in food for necessary technological purposes or plant protection reasons. For chemical contaminants, which usually have no intended function in drinking-water, the term “tolerable daily intake” is more appropriate than “acceptable daily intake”, as it signifies permissibility rather than acceptability. Over many years, the Joint Food and Agriculture Organization of the United Nations (FAO)/World Health Organization (WHO) Expert Committee on Food Additives (JECFA) and the Joint FAO/WHO Meeting on Pesticide Residues (JMPR) have developed certain principles in the derivation of ADIs (FAO/WHO, 2009). These principles have been adopted, where appropriate, in the derivation of TDIs used in developing guideline values for drinking-water quality. As TDIs are regarded as representing a tolerable intake for a lifetime, they are not so precise that they cannot be exceeded for short periods of time. Short-term exposure to levels exceeding the TDI is not a cause for concern, provided the individual’s intake averaged over longer periods of time does not appreciably exceed the level set. The large uncertainty factors generally involved in establishing a TDI (see below) serve to provide assurance that exposure exceeding the TDI for short periods is unlikely to have any deleterious effects upon health. However, consideration should be given to any potential acute effects that may occur if the TDI is substantially exceeded for short periods of time. No‑observed‑adverse‑effect level and lowest‑observed‑adverse‑effect level The NOAEL is defined as the highest dose or concentration of a chemical in a single study, found by experiment or observation, that causes no detectable adverse health effect. Wherever possible, the NOAEL is based on long-term studies, preferably of ingestion in drinking-water. However, NOAELs obtained from short-term studies and studies using other sources of exposure (e.g. food, air) may also be used. If a NOAEL is not available, a LOAEL may be used, which is the lowest observed dose or concentration of a substance at which there is a detectable adverse health effect. When a LOAEL is used instead of a NOAEL, an additional uncertainty factor is normally applied (see below). Benchmark dose Increasingly, the preferred approaches for the derivation of TDIs/ADIs for threshold effects include the benchmark dose (BMD) or the lower confidence limit on the benchmark dose (BMDL) (IPCS, 1994). When appropriate data for mathematical modelling of dose–response relationships are available, BMDLs are used as alternatives to NOAELs in the calculation of health-based guideline values. In such a case, use of the BMDL could eliminate the need for application of an additional uncertainty factor to the LOAEL. The BMDL is the lower confidence limit of the dose that produces a small increase (e.g. 5% or 10%) in the level of adverse effects. The BMDL is derived on a quantitative basis using data from the entire dose–response curve for the critical effect rather than from a single dose at the NOAEL or LOAEL and accounts for the statistical power and quality of the data (IPCS, 2009). Uncertainty factors The application of uncertainty or safety factors has been traditionally and successfully used in the derivation of ADIs and TDIs for food additives, pesticides and environmental contaminants. The derivation of these factors requires expert judgement and careful consideration of the available scientific evidence. Table 8.3 Source of uncertainty in derivation of guideline values Source of uncertainty Uncertainty factor Interspecies variation (extrapolating from experimental animals to humans) 1–10 Intraspecies variation (accounting for individual variations within humans) 1–10 Adequacy of studies or database 1–10 Nature and severity of effect 1–10 In the derivation of guideline values, uncertainty factors are applied to the NOAEL, LOAEL or BMD/BMDL for the response considered to be the most biologically significant. In relation to exposure of the general population, the NOAEL or BMD/BMDL for the critical effect in experimental animals is normally divided by an uncertainty factor of 100. This comprises two 10-fold factors, one for interspecies differences and one for interindividual variability in humans (Table 8.3). Extra uncertainty factors may be incorporated to allow for database deficiencies and for the severity or irreversibility of effects. Factors lower than 10 are used, for example, for interspecies variation when humans are known to be less sensitive than the experimental animal species studied. Inadequate studies or databases include those where a LOAEL is used instead of a NOAEL and studies considered to be shorter in duration than desirable. Situations in which the nature or severity of effect might warrant an additional uncertainty factor include studies in which the end-point is malformation of a fetus or in which the end-point determining the NOAEL is directly related to possible carcinogenicity. In the latter case, an additional uncertainty factor is usually applied for carcinogenic compounds for which the guideline value is derived using a TDI approach rather than a theoretical risk extrapolation approach. For substances for which the uncertainty factors are equal to or greater than 1000, guideline values are designated as provisional in order to emphasize the higher level of uncertainty inherent in these values. A high uncertainty factor indicates that the guideline value may be considerably lower than the concentration at which health effects would actually occur in a real human population. Guideline values with high uncertainty are more likely to be modified as new information becomes available. The selection and application of uncertainty factors are important in the derivation of guideline values for chemicals, as they can make a considerable difference in the values set. For contaminants for which there is sufficient confidence in the database, the guideline value is derived using a small uncertainty factor. For most contaminants, however, there is greater scientific uncertainty, and a relatively large uncertainty factor is used. The use of uncertainty factors enables the particular attributes of the chemical and the data available to be considered in the derivation of guideline values. Use of chemical‑specific adjustment factors instead of uncertainty factors Approaches to the derivation of TDIs are increasingly being based on understanding of a chemical’s mode of action in order to reduce reliance on default assumptions. This approach provides a departure from the use of default uncertainty factors (such as a simple 10 for interspecies variation and 10 for intraspecies variation) and relies on the use of quantitative toxicokinetic and toxicodynamic data to derive CSAFs for use in interspecies and intraspecies extrapolations (IPCS, 2005; Bhat et al., 2017). Previously, CSAFs were called “data-derived uncertainty factors”. The part of the CSAF approach that is at present best developed is the use of physiologically based pharmacokinetic models to replace the default values for extrapolation between species and between differing routes of exposure (e.g. inhalation to oral). Relative source allocation Drinking-water is usually not the only source of human exposure to the chemicals for which guideline values have been derived. In many cases, the exposure to or intake of chemical contaminants from drinking-water is much lower than that from other sources, such as food, air and consumer products. Some consideration of the proportion of the ADI or TDI that may be attributed to different sources is therefore needed in developing guideline values and risk management strategies. This approach ensures that total daily intake from all sources (including drinking-water containing concentrations of the chemical at or near the guideline value) does not exceed the ADI or TDI. Wherever possible or in an ideal situation, derivation of guideline values uses data on the proportion of total daily intake normally ingested in drinking-water (based on mean levels in food, drinking-water, consumer products, soil and air), or data on intakes estimated on the basis of physical and chemical properties of the substances of concern. As the primary sources of exposure to chemicals are generally food (e.g. pesticide residues) and water, it is important to quantify, whenever possible, the exposures from both sources. To inform this process, it is desirable to collect as much high-quality data as possible on food intake in different parts of the world as possible. The data collected can then be used to estimate the proportion of the intake that comes from food and the proportion that comes from drinking-water. However, for most contaminants, data from the various exposure sources, most notably food and drinking-water, are available only from developed countries. In the absence of adequate exposure data or where documented evidence is available regarding widespread presence in one or more of the other media (i.e. air, food, soil or consumer products), the normal allocation of the total daily intake to drinking-water is 20% (floor value), which reflects a reasonable level of exposure based on broad experience, while still being protective (Krishnan & Carrier, 2013). This value reflects a change from the previous allocation of 10%, which was found to be excessively conservative. As chemicals are progressively reassessed, overall exposure will be reconsidered, and a change in the default allocation factor from 10% to 20% will be made, if appropriate. Therefore, not all older guideline values reflect this change. In some circumstances, there is clear evidence that water is the main (and possibly only) source of exposure, such as for some of the DBPs; the allocation in such cases may be as high as 80% (ceiling value), which still allows for some exposure from other sources (Krishnan & Carrier, 2013). Where chemical and context-specific allocation factors can be developed using exposure data or models, the allocation factor applied should still be bounded by the floor and ceiling values (i.e. 20–80%). For pesticides, even when available food exposure data suggest that exposure via this route is minimal, the default allocation factor of 20% is used to account for the fact that available food exposure data do not generally include information from developing countries, where exposure via this route may be higher. A detailed explanation of the reasoning behind the choice of allocation factor is an essential component of the evaluation. This assists Member States in making appropriate decisions about incorporating or adapting guideline values into national standards where local circumstances need to be taken into account. It also provides assistance in making decisions regarding potential risks when a guideline value is exceeded. As a general principle, efforts should be made to keep contaminant concentrations as low as possible and not allow increases up to the guideline value. Although the values chosen are, in most cases, sufficient to account for additional routes of intake (i.e. inhalation and dermal absorption) of contaminants in water, under certain circumstances (e.g. limited ventilation), authorities may wish to take inhalation and dermal exposure into account in adapting the guideline values to local conditions (see section 8.2.9). Some elements are essential for human nutrition. In developing guideline values and in considering allocation factors, it is necessary to take into account the recommended minimum daily intake and exposures from food and to ensure that the allocation does not result in an apparent conflict with essentiality. Default assumptions There is variation in both the volume of water consumed daily and the body weight of consumers. It is therefore necessary to apply some assumptions in order to determine a guideline value. The default assumption for consumption by an adult is 2 litres of water per day, whereas the default assumption for body weight is 60 kg. In some cases, the guideline value is based on children, where they are considered to be particularly vulnerable to a particular substance. In this event, a default intake of 1 litre is assumed for a body weight of 10 kg; where the most vulnerable group is considered to be bottle-fed infants, an intake of 0.75 litre is assumed for a body weight of 5 kg. Significant figures The calculated ADI or TDI is used to derive the guideline value, which is usually rounded to one significant figure. In calculating the guideline value, the unrounded ADI or TDI value should be used. The guideline value is generally rounded to one significant figure to reflect the uncertainty in, for example, experimental animal toxicity data, exposure assumptions made and the uncertainty factors selected. In a few cases, rounding to two significant figures is appropriate because the practical impact of rounding depends on the units; for example, rounding from 1.5 to 2.0 μg/l has less influence on treatment requirements than rounding from 1.5 to 2.0 mg/l. These are considered on a case-by-case basis. The general rounding rule for mid-way values (x.5) is to round up, in line with common convention. Examples for rounding to one significant figure are as follows: 1.25 becomes 1, 0.73 becomes 0.7 and 1.5 becomes 2. 8.2.3 Non-threshold chemicals In the case of compounds considered to be genotoxic carcinogens, guideline values are normally determined using a mathematical model. Although several models exist, the linearized multistage model is generally adopted. Other models are considered more appropriate in certain cases. These models compute an estimate of risk at a particular level of exposure, along with upper and lower bounds of confidence on the calculation, which may include zero at the lower bound. Guideline values are conservatively presented as the concentrations in drinking-water associated with an estimated upper-bound excess lifetime cancer risk of 10−5 (or one additional case of cancer per 100 000 of the population ingesting drinking-water containing the substance at the guideline value for 70 years). This value does not equate to the number of cases of cancer that will be caused by exposure to the substance at this level. It is the maximum potential risk, taking into account large uncertainties. It is highly probable that the actual level of risk is less than this, even approaching zero, but risks at low levels of exposure cannot be experimentally verified. The recognition that the cancer risk may approach zero or be indistinguishable from zero stems from the uncertainties associated with mechanisms of carcinogenesis, including the role of the chemical in the cancer process and the possibility of detoxification and repair mechanisms. Member States may consider that a different level of hypothetical risk is more appropriate to their circumstances, and values relating to risks of 10−4 or 10−6 additional cancer cases over a lifetime of exposure may be determined by respectively multiplying or dividing the guideline value by 10. The mathematical models used for deriving guideline values for non-threshold chemicals cannot be verified experimentally, and they do not usually take into account a number of biologically important considerations, such as pharmacokinetics, pre-systemic and metabolic detoxification, DNA repair or protection by the immune system. They also assume the validity of a linear extrapolation of very high dose exposures in test animals to very low dose exposures in humans. As a consequence, the models used are conservative (i.e. err on the side of caution). The guideline values derived using these models should be interpreted differently from TDI-derived values because of the lack of precision of the models. Moderate short-term exposure to levels exceeding the guideline value for non-threshold chemicals does not significantly affect the risk. 8.2.4 Data quality The following factors were taken into account in assessing the quality and reliability of available information: • Oral studies are preferred (in particular, drinking-water studies), using the pure substance with appropriate dosing regime and a good quality clinical biochemistry and histopathology. • The database should be sufficiently broad that all potential toxicological endpoints of concern have been identified. • The quality of the studies is such that they are considered reliable; for example, there has been adequate consideration of confounding factors in epidemiological studies. • There is reasonable consistency between studies; the end-point and study used to derive a guideline value do not contradict the overall weight of evidence. • For inorganic substances, there is some consideration of speciation in drinking-water. • There is appropriate consideration of multimedia exposure in the case of epidemiological studies. In the development of guideline values, existing international approaches are carefully considered. In particular, previous risk assessments developed by the International Programme on Chemical Safety (IPCS) in Environmental Health Criteria monographs and Concise International Chemical Assessment Documents, IARC, JMPR and JECFA are reviewed. These assessments are relied upon except where new information justifies a reassessment, but the quality of new data is critically evaluated before it is used in any risk assessment. Where international reviews are not available, other sources of data are used in the derivation of guideline values, including published reports from peer-reviewed open literature, national reviews recognized to be of high quality, information submitted by governments and other interested parties and, to a limited extent, unpublished proprietary data (primarily for the evaluation of pesticides). 8.2.5 Provisional guideline values The use and designation of provisional guideline values are outlined in Table 8.4. For non-threshold substances, in cases in which the concentration associated with an upper-bound excess lifetime cancer risk of 10−5 is not feasible as a result of inadequate analytical or treatment technology, a provisional guideline value (designated A or T, respectively) is recommended at a practicable level. 8.2.6 Chemicals with effects on acceptability Some substances of health concern have effects on the taste, odour or appearance of drinking-water that would normally lead to rejection of water at concentrations significantly lower than those of concern for health. Such substances are not normally Table 8.4 Use and designation of provisional guideline values Situations where a provisional guideline applies Designation Significant scientific uncertainties regarding derivation of health‑based guideline value P Calculated health‑based value is below the achievable analytical quantification level A (Guideline value is set at the achievable quantification level) Calculated health‑based value is below the level that can be achieved through practical treatment methods T (Guideline value is set at the practical treatment level) Calculated health‑based value may be exceeded as a result of disinfection procedures D (Guideline value is set considering possible health effects and the need to maintain adequate disinfection. Adequate disinfection of drinking-water remains paramount) appropriate for routine monitoring. However, guideline values have been established for some substances that may cause taste or odour in drinking-water at concentrations much lower than the guideline values because there is such a wide range in the ability of consumers to detect them by taste or odour. For such substances, a fact sheet and health-based guideline value (see chapter 12) are presented in the usual way. In the fact sheet, the relationship between concentrations relevant to health and those relevant to the acceptability of the drinking-water is explained. In tables of guideline values, the health-based guideline values are designated with a “C”. For other substances, health-based guideline values may be needed, for instance, in order to assist in judging the response that is required when problems are encountered and in some cases to provide reassurance to health authorities and consumers with regard to possible health risks. 8.2.7 Chemicals not included in the Guidelines Additional information on many chemicals not included in these Guidelines is available from several credible sources, including WHO Environmental Health Criteria monographs and Concise International Chemical Assessment Documents (https:// www.who.int/teams/environment-climate-change-and-health/chemical-safety-andhealth/health-impacts), chemical risk assessment reports from JMPR, JECFA and IARC and published documents from a number of national sources, such as the United States Environmental Protection Agency. Although these information sources may not have been reviewed for these Guidelines, they have been peer reviewed and provide readily accessible information on the toxicology of many additional chemicals. They can help drinking-water suppliers and health officials decide upon the significance (if any) of a detected chemical and on the response that might be appropriate. 8.2.8 Mixtures Chemical contaminants of drinking-water supplies are present with numerous other inorganic and organic constituents. The guideline values are calculated separately for individual substances, without specific consideration of the potential for interaction of each substance with other compounds present. Synergistic interactions between substances are usually selective and very limited, especially at the very low levels usually encountered in drinking-water. The large margin of uncertainty incorporated in the majority of the guideline values is considered to be sufficient to account for potential interactions. In addition, the majority of contaminants will not be continuously present at concentrations at or near their guideline value. For many chemical contaminants, mechanisms of toxicity are different; consequently, there is no reason to assume that interactions occur. There may, however, be occasions when a number of contaminants with similar toxicological mechanisms are present at levels near their respective guideline values. In such cases, decisions concerning appropriate action should be made, taking into consideration local circumstances. Unless there is evidence to the contrary, it is appropriate to assume that the toxic effects of these compounds are additive. A pragmatic approach in considering the additive approach is to base any regulation or guidance on the basis that the sum (hazard index) of the concentration of each component divided by its standard or guideline value (hazard quotient) should not exceed 1. This approach is recommended in the Guidelines to be applied, for example, to nitrate/nitrite and the trihalomethanes. Where chemicals are present that act on the same target organ with a dissimilar toxicity mechanism, an additive approach is also usually assumed and a modified hazard index approach is considered appropriate. No examples for application of this approach are currently included in the Guidelines. Further information on chemical mixtures is provided in the supporting document Chemical mixtures in source water and drinking-water (Annex 1). 8.2.9 Adapting guideline values to local circumstances In order to account for the variations in exposure from different sources in different parts of the world, default values, generally between 20% and 80%, are used to make an allocation of the TDI to drinking-water in setting guideline values for many chemicals. Where relevant exposure data are available, authorities are encouraged to develop context-specific guideline values that are tailored to local circumstances and conditions. For example, in areas where the intake of a particular contaminant in drinking-water is known to be much greater than that from other sources (e.g. air and food), it may be appropriate to allocate a greater proportion of the TDI to drinking-water to derive a guideline value more suited to the local conditions. Daily water intake can vary significantly in different parts of the world, seasonally and particularly where consumers are involved in manual labour in hot climates. Local adjustments to the daily water consumption value may be needed in setting local standards, as in the case of fluoride, for example. For most other substances, the drinking-water intake range is very small (perhaps a factor of 2–4) compared with the much larger range in the toxicological uncertainty factors; hence, no such adjustment is necessary. Volatile substances in water may be released to the atmosphere in showering and through a range of other household activities. Under such circumstances, inhalation may become a significant route of exposure. Some substances may also be absorbed through the skin during bathing, but this is not usually a major source of uptake. For those substances that are particularly volatile, such as chloroform, the correction factor would be approximately equivalent to a doubling of exposure, which is small in relation to the uncertainties inherent in the derivation of guideline values. However, in some parts of the world, houses have a very low rate of ventilation, and authorities may wish to take inhalation exposure into account in adapting the guideline values to local conditions, although other uncertainty factors used in the quantitative assessments may render this unnecessary. Where such exposure is shown to be important for a particular substance (i.e. high volatility, low ventilation rates and high rates of showering/bathing), it may be appropriate to adjust the guideline value accordingly. Further considerations on adapting chemical guideline values is provided in the supporting document Developing drinking-water quality regulations and standards (Annex 1). 8.3 Analytical achievability As noted above, guideline values are not set at concentrations of substances that cannot reasonably be measured. In such circumstances, provisional guideline values are set at the reasonable analytical limits. Guidance provided in this section and in Annex 4 is intended to assist readers to select appropriate analytical methods for specific circumstances. In carrying out hazard identification and risk assessment and for verification and auditing of the water safety plan for chemical contaminants, it is usually necessary to carry out some analysis. It is important that appropriate facilities are available to ensure that suitable methods are used in carrying out chemical analysis. Various collections of “standard” or “recommended” methods for water analysis are published by a number of national and international agencies. It is often thought that adequate analytical accuracy can be achieved provided that all laboratories use the same standard method. Experience shows that this is not always the case, as a variety of factors may affect the accuracy of the results. Examples include reagent purity, apparatus type and performance, degree of modification of the method in a particular laboratory and the skill and care of the analyst. These factors are likely to vary both between laboratories and over time in an individual laboratory. Moreover, the precision and accuracy that can be achieved with a particular method frequently depend upon the adequacy of sampling and nature of the sample (“matrix”). While it is not essential to use standard methods, it is important that the methods used are properly validated and their precision and accuracy determined before significant decisions are made based on the results. In the case of “nonspecific” variables such as taste, odour, colour and turbidity, the result is method specific, and this needs to be considered when using the data to make comparisons. A number of considerations are important in selecting methods: • The overriding consideration is that the method chosen is demonstrated to have the required accuracy. Other factors, such as speed and convenience, should be considered only in selecting among methods that meet this primary criterion. • Of primary importance is the expertise and diligence of the laboratories performing the analyses. They must utilize auditable quality control and quality assurance procedures for their results to be credible. External certification is highly desirable. • There are a number of markedly different procedures for measuring and reporting the errors to which all methods are subject. This complicates and prejudices the effectiveness of method selection, and suggestions for standardizing such procedures have been made. It is therefore desirable that details of all analytical methods are published together with performance characteristics that can be interpreted unambiguously. • If the analytical results from one laboratory are to be compared with those from others or with a numerical standard, it is obviously preferable for them not to have any associated systematic error. In practice, this is not possible, but each laboratory should select methods whose systematic errors have been thoroughly evaluated and shown to be acceptably small. Table 8.5 Ranking of complexity of analytical methods for inorganic chemicals Ranking Example of analytical methods 1 Volumetric method, colorimetric method 2 Electrode method 3 Ion chromatography 4 High‑performance liquid chromatography 5 Flame atomic absorption spectrometry 6 Electrothermal atomic absorption spectrometry 7 Inductively coupled plasma atomic emission spectrometry 8 Inductively coupled plasma mass spectrometry Table 8.6 Ranking of complexity of analytical methods for organic chemicals Ranking Example of analytical methods 1 High‑performance liquid chromatography 2 Gas chromatography 3 Gas chromatography–mass spectrometry 4 Headspace gas chromatography–mass spectrometry 5 Purge‑and‑trap gas chromatography Purge‑and‑trap gas chromatography–mass spectrometry A qualitative ranking of analytical methods based on their degree of technical complexity is given in Table 8.5 for inorganic chemicals and in Table 8.6 for organic chemicals. These groups of chemicals are separated, as the analytical methods used differ greatly. The higher the ranking, the more complex the process in terms of equipment or operation. In general, higher rankings are also associated with higher total costs. Analytical achievabilities, based on detection limits, of the inorganic and organic chemicals for which guideline values have been established are given in Annex 4, by source category. Many kinds of field test kits are available to measure the concentrations of various chemicals in water. These are generally used for compliance examinations as well as for operational monitoring of drinking-water quality. Although the field test kits have the advantage of being simple to use in non-laboratory environments and are often available at relatively low prices, their analytical accuracy is generally less than that of the methods shown in Tables 8.5 and 8.6. However, when properly used, they provide valuable tools for rapidly assessing numerous contaminants in a non-formal laboratory setting at low cost compared with commercial laboratory tests. It is therefore necessary to check the validity of the field test kit before applying it. A brief description of the analytical methods listed in Tables 8.5 and 8.6 is provided in Annex 4. 8.4 Treatment As noted above, where a health-based guideline value cannot be achieved by reasonably practicable treatment, then the guideline value is designated as provisional and set at the concentration that can be reasonably achieved through treatment. Collection, treatment, storage and distribution of drinking-water involve deliberate additions of numerous chemicals to improve the safety and quality of the finished drinking-water for consumers (direct additives). In addition, water is in constant contact with pipes, valves, taps and tank surfaces, all of which have the potential to impart additional chemicals to the water (indirect additives). The chemicals used in water treatment or from materials in contact with drinking-water are discussed in more detail in section 8.5.4. 8.4.1 Treatment performance Treatment performance varies according to local conditions and circumstances. The ability to achieve a guideline value within a drinking-water supply depends on a number of factors, including: • the concentration of the chemical in the raw water; • control measures employed throughout the drinking-water system; • nature of the raw water (groundwater or surface water, presence of natural organic matter and inorganic solutes and other components, such as turbidity); • treatment processes already installed. If a guideline value cannot be met with the existing system, then additional treatment may need to be considered, or water might need to be obtained from alternative sources. The cost of achieving a guideline value will depend on the complexity of any additional treatment or other control measures required. It is not possible to provide general quantitative information on the cost of achieving individual guideline values. Treatment costs (capital and operating) will depend not only on the factors identified above, but also on issues such as plant throughput; local costs for labour, civil and mechanical works, chemicals and electricity; life expectancy of the plant; and so on. Guideline values may be progressively achieved in the long term through less capital-intensive non-treatment options, such as through agreements with land users to reduce application of chemicals (fertilizers, pesticides, etc.) A qualitative ranking of treatment processes based on their degree of technical complexity is given in Table 8.7. The higher the ranking, the more complex the process in terms of plant or operation. In general, higher rankings are also associated with higher costs. Annex 5 summarizes the treatment processes that are capable of removing chemical contaminants of health significance. The tables in Annex 5 include only those chemicals, by source category, for which some treatment data are available and for which guideline values have been established. The tables in Annex 5 are provided to help inform decisions regarding the ability of existing treatment to meet guidelines and what additional treatment might need Table 8.7 Ranking of technical complexity and cost of water treatment processes Ranking Examples of treatment processes 1 Simple chlorination Plain filtration (rapid sand, slow sand) 2 Prechlorination plus filtration Aeration 3 Chemical coagulation Process optimization for control of DBPs 4 Granular activated carbon treatment Ion exchange 5 Ozonation 6 Advanced oxidation processes Membrane treatment to be installed. They have been compiled on the basis of published literature, which includes mainly laboratory experiments, some pilot plant investigations and relatively few full-scale studies of water treatment processes. Consequently: • Many of the treatments outlined are designed for larger treatment plants and may not necessarily be appropriate for smaller treatment plants or individual-type treatment. In these cases, the choice of technology must be made on a caseby-case basis. • The information is probably “best case”, as the data would have been obtained under laboratory conditions or with a carefully controlled plant for the purposes of experimentation. • Actual process performance will depend on the concentration of the chemical in the raw water and on general raw water quality. For example, chlorination and removal of organic chemicals and pesticides using activated carbon or ozonation will be impaired if there is a high concentration of natural organic matter. • For many contaminants, potentially several different processes could be appropriate, and the choice between processes should be made on the basis of technical complexity and cost, taking into account local circumstances. For example, membrane processes can remove a broad spectrum of chemicals, but simpler and cheaper alternatives are effective for the removal of most chemicals. • It is normal practice to use a series of unit processes (e.g. coagulation, sedimentation, filtration, chlorination) to achieve desired water quality objectives. Each of these may contribute to the removal of chemicals. It may be technically and economically advantageous to use a combination of processes (e.g. ozonation plus granular activated carbon or membranes) to remove particular chemicals. • The effectiveness of potential processes should be assessed using laboratory or pilot plant tests on the actual raw water concerned. These tests should be of sufficient duration to identify potential seasonal or other temporal variations in contaminant concentrations and process performance. • These treatment technology characterizations are estimates and are not comprehensive, but are intended to provide some indications of the types of technologies that have shown greater or lesser capabilities for removing the indicated chemicals from drinking-water. A brief description of the various treatment processes referred to in Table 8.7 is included in Annex 5. 8.4.2 Process control measures for disinfection by-products All chemical disinfectants produce inorganic or organic DBPs that may be of concern. The principal DBPs formed during chlorination are THMs, HAAs, haloketones and haloacetonitriles, as a result of chlorination of naturally occurring organic precursors such as humic substances. Monochloramine produces lower THM concentrations than chlorine but produces other DBPs, including cyanogen chloride. Chlorine and ozone oxidize bromide to produce hypohalous In attempting to control DBP concentrations, it is of acids, which react with precurparamount importance that the efficiency of dissors to form brominated THMs. infection is not compromised and that a suitable A range of other DBPs, including residual level of disinfectant is maintained throughout the distribution system. aldehydes and carboxylic acids, may also be formed. Of particular concern is bromate, formed by the oxidation of bromide. Bromate may also be present in some sources of hypochlorite, but usually at concentrations that will give rise to levels in final water that are below the guideline value. The main by-products from the use of chlorine dioxide are chlorite ion, which is an inevitable decomposition product, and chlorate ion. Chlorate is also produced in hypochlorate as it ages. The basic strategies that can be adopted for reducing the concentrations of DBPs are: • changing the process conditions (including removal of precursor compounds prior to application); • using a different chemical disinfectant with a lower propensity to produce byproducts with the source water; • using non-chemical disinfection; • removing DBPs prior to distribution. Changes to process conditions The formation of THMs during chlorination can be reduced by removing precursors prior to contact with chlorine—for example, by installing or enhancing coagulation (this may involve using higher coagulant doses or lower coagulation pH values than are applied conventionally). DBP formation can also be reduced by lowering the applied chlorine dose; if this is done, it must be ensured that disinfection is still effective. The pH value during chlorination affects the distribution of chlorinated byproducts. Reducing the pH lowers the THM concentration, but at the expense of increased formation of HAAs. Conversely, increasing the pH reduces HAA production but leads to increased THM formation. The formation of bromate during ozonation depends on several factors, including concentrations of bromide and ozone and the pH. It is not practicable to remove bromide from raw water, and it is difficult to remove bromate once formed, although granular activated carbon filtration has been reported to be effective under certain circumstances. Bromate formation can be minimized by using lower ozone dose, shorter contact time and a lower residual ozone concentration. Operating at lower pH (e.g. pH 6.5) followed by raising the pH after ozonation also reduces bromate formation, and addition of ammonia can also be effective. Addition of hydrogen peroxide can either increase or decrease bromate formation, depending on the point at which it is applied and local treatment conditions. Changing disinfectants It may be feasible to change disinfectant in order to achieve guideline values for DBPs. The extent to which this is possible will be dependent on the raw water quality and installed treatment (e.g. for precursor removal). It may be effective to change from chlorine to monochloramine to provide a secondary disinfectant residual within distribution, in order to reduce THM formation and subsequent development within the distribution system. Although monochloramine provides a more stable residual within distribution, it is a less powerful disinfectant and should not be used as a primary disinfectant. Chlorine dioxide can be considered as a potential alternative to both chlorine and ozone disinfection, although it does not provide a residual effect, as chlorine would. The main concerns with chlorine dioxide are with the residual concentrations of chlorine dioxide and the by-products chlorite and chlorate. These can be addressed by controlling the dose of chlorine dioxide at the treatment plant. Non‑chemical disinfection Ultraviolet (UV) irradiation or membrane processes can be considered as alternatives to chemical disinfection. UV is particularly effective at inactivating Cryptosporidium, which is extremely resistant to chlorination. Neither of these provides any residual disinfection, and it may be considered appropriate to add a small dose of a persistent disinfectant such as chlorine or monochloramine to act as a preservative during distribution. Removing DBPs prior to distribution It is technically feasible to remove DBPs prior to distribution; however, this is the least attractive option for controlling DBP concentrations. Strategies for DBP control include source control, precursor removal, use of alternative disinfectants and removal of DBPs by technologies such as air stripping, activated carbon, UV light and advanced oxidation. These processes would need to be followed by a further disinfection step to guard against microbial contamination and to ensure a residual concentration of disinfectant within distribution. 8.4.3 Treatment for corrosion control Corrosion is the partial dissolution of the materials constituting the treatment and supply systems, tanks, pipes, valves and pumps. In certain circumstances, all water can be corrosive. Corrosion may lead to structural failure, leaks, loss of capacity and deterioration of chemical and microbial water quality. The internal corrosion of pipes and fittings can have a direct impact on the concentration of water constituents, including lead and copper. Corrosion control is therefore an important aspect of the management of a drinking-water system for safety. Corrosion control involves many parameters, including the concentrations of calcium, bicarbonate, carbonate and dissolved oxygen, as well as pH. The detailed requirements differ depending on water quality and the materials used in the distribution system. The pH controls the solubility and rate of reaction of most of the metal species involved in corrosion reactions. It is particularly important in relation to the formation of a protective film at the metal surface. For some metals, alkalinity (carbonate and bicarbonate) and calcium (hardness) also affect corrosion rates. Characterizing corrosivity Most of the indices that have been developed to characterize the corrosion potential of waters are based on the assumption that water with a tendency to deposit a calcium carbonate scale on metal surfaces will be less corrosive. The Langelier index is the difference between the actual pH of a water and its “saturation pH”, this being the pH at which a water of the same alkalinity and calcium hardness would be at equilibrium with solid calcium carbonate. Waters with a positive Langelier index are capable of depositing calcium carbonate scale from solution. There is no corrosion index that applies to all materials, and corrosion indices, particularly those related to calcium carbonate saturation, have given mixed results. The parameters related to calcium carbonate saturation status are, strictly speaking, indicators of the tendency to deposit or dissolve calcium carbonate (calcite) scale, not indicators of the “corrosivity” of a water. For example, there are many waters with a negative Langelier index that are non-corrosive and many with a positive Langelier index that are corrosive. Nevertheless, there are many documented instances of the use of saturation indices for corrosion control based on the concept of laying down a protective “eggshell” scale of calcite in iron pipes. In general, waters with high pH, calcium and alkalinity are less corrosive, and this tends to be correlated with a positive Langelier index. However, these calcium carbonate precipitation indices are not necessarily considered to be good corrosion predictors for copper systems. The ratio of the chloride and sulfate concentrations to the bicarbonate concentration (Larson ratio) has been shown to be helpful in assessing the corrosiveness of water to cast iron and steel. A similar approach has been used in studying zinc dissolution from brass fittings—the Turner diagram. Water treatment for corrosion control To control corrosion in water distribution networks, the methods most commonly applied are adjusting pH, increasing the alkalinity or hardness or adding corrosion inhibitors, such as polyphosphates, silicates and orthophosphates. The quality and maximum dose to be used should be in line with specifications for such water treatment chemicals. Although pH adjustment is an important approach, its possible impact on other aspects of water supply technology, including disinfection, must always be taken into account. It is not always possible to achieve the desired values for all parameters. For example, the pH of hard waters cannot be increased too much, or softening will occur. The application of lime and carbon dioxide to soft waters can be used to increase both the calcium concentration and the alkalinity to at least 40 mg/l as calcium carbonate. More detailed information on the corrosion of various metals commonly used in water treatment and distribution systems can be found in Annex 5. 8.4.4 Household treatment The chemicals of greatest health concern in some natural waters are usually excess natural fluoride, nitrate/nitrite and arsenic. Some commercial water treatment technologies are available for small applications for the removal of chemical contaminants. For example, anion exchange using activated alumina or iron-containing products will effectively reduce excess fluoride concentrations. Bone char has also been used to reduce fluoride concentrations. Arsenic is also removed by anion exchange processes similar to those employed for fluoride. Nitrates and nitrates, which are frequently present due to sewage contamination or agricultural runoff, are best managed by protecting the source water from contamination. They are difficult to remove, although disinfection will oxidize nitrite, the more toxic form, to nitrate. In addition, disinfection will sanitize the water and reduce the risk of gastrointestinal infection, which is a risk factor for methaemoglobinaemia caused by excess nitrate/nitrite exposure of infants up to approximately 3–6 months of age. Cation exchange water softening is widely used in homes to remove excess hardness due to high calcium or magnesium, and it can also remove metals including iron and radium. Synthetic and natural organic chemicals can be removed by granular activated carbon or carbon block technologies. The treatment systems must be well managed and replaced regularly, because their effectiveness is eventually lost, depending upon the types of contaminating chemicals and their concentrations in the water. Reverse osmosis technologies have general applicability for removal of most organic and inorganic chemicals; however, there is some selectivity, and also there is a significant amount of water wastage when low-pressure units are used in small-volume applications. 8.5 Guideline values for individual chemicals, by source category 8.5.1 Naturally occurring chemicals There are a number of sources of naturally occurring chemicals in drinking-water. All natural water contains a range of inorganic and organic chemicals. Inorganic chemicals derive from the rocks and soil through which water percolates or over which it flows. Organic chemicals derive from the breakdown of plant material or from algae and other microorganisms that grow in the water or on sediments. Most of the naturally occurring chemicals for which guideline values have been derived or that have been considered for guideline value derivation are inorganic. Toxins produced by cyanobacteria (blue-green algae; see section 11.5)—that is, “cyanotoxins”—are the only organic substances for which guideline values have been derived. Cyanobacteria occur widely in lakes, reservoirs, ponds and slow-flowing rivers. They produce a wide range of compounds, some of which show activity in in vitro bioassays. Four key groups—anatoxin-a variants, cylindrospermopsins, microcystins and saxitoxins (see also the corresponding chapter 12 fact sheets)—account for much, but not all, of the toxicity to humans (and other vertebrates) and are frequently reported. Whereas anatoxins, microcystins and saxitoxins are primarily found within cells, major fractions of cylindrospermopsins are released into water. The toxicity of different structural variants within a cyanotoxin group varies widely, and further cyanotoxins may yet be recognized. Mass developments of cyanobacteria (“blooms”) also cause a high organic load that challenges treatment. Therefore, preventing blooms is the preferred control option. The approach to dealing with naturally occurring chemicals will vary according to the nature of the chemical and the source. For inorganic contaminants that arise from rocks and sediments, it is important to screen possible water sources to determine whether the source is suitable for use or whether it will be necessary to treat the water to remove the contaminants of concern along with microbial contaminants. In some cases, where a number of sources may be available, dilution or blending of the water containing high levels of a contaminant with a water containing much lower levels may achieve the desired result. A number of the most important chemical contaminants (i.e. those that have been shown to cause adverse health effects as a consequence of exposure through drinking-water) fall into the category of naturally occurring chemicals. Some naturally occurring chemicals have other primary sources and are therefore discussed in other sections of this chapter. Guideline values have not been established for the naturally occurring chemicals listed in Table 8.8 for the reasons indicated in the table. However health-based values have been developed for a number of these chemicals in order to provide guidance to Member States when there is a reason for local concern, although, as noted in the table, in many circumstances the acceptability of drinking-water is the overriding concern (for further information on guideline values and health-based values, see section 8.2). Fact sheets are included in chapter 12. Guideline values have been established for the naturally occurring chemicals listed in Table 8.9, which meet the criteria for inclusion. Fact sheets are included for each in chapter 12. 8.5.2 Chemicals from industrial sources and human dwellings Chemicals from industrial sources can reach drinking-water directly from discharges or indirectly from diffuse sources arising from the use and disposal of materials and products containing the chemicals. In some cases, inappropriate handling and disposal may lead to contamination (e.g. degreasing agents that are allowed to reach groundwater). Some of these chemicals, particularly inorganic substances, may also Table 8.8 Naturally occurring chemicals for which guideline values have not been established Reason for not establishing a guideline Chemical value Remarks Inorganic Bromide Occurs in drinking‑water at concentrations well below those of health concern Chloride Not of health concern at levels found in drinking‑water Hardness Not of health concern at levels found in drinking‑water Hydrogen sulfide Not of health concern at levels found in drinking‑water Iron Not of health concern at levels causing acceptability problems in drinking‑water Molybdenum Occurs in drinking‑water at concentrations well below those of health concern pH Not of health concern at levels found in drinking‑water Potassium Occurs in drinking‑water at concentrations well below those of health concern Sodium Not of health concern at levels found in drinking‑water Sulfate Not of health concern at levels found in drinking‑water Total dissolved Not of health concern at levels found in solids drinking‑water May affect acceptability of drinking‑water (see chapter 10) May affect acceptability of drinking‑water (see chapter 10) May affect acceptability of drinking‑water (see chapter 10) May affect acceptability of drinking‑water (see chapter 10) An important operational water quality parameter May affect acceptability of drinking‑water (see chapter 10) May affect acceptability of drinking‑water (see chapter 10) May affect acceptability of drinking‑water (see chapter 10) Organic Anatoxins Available data inadequate to permit derivation of health‑based guideline value. However, a reference valuea of 0.3 mg/l has been proposed. The reference value is for total anatoxins (sum of all congeners, free plus cell‑bound), for short‑term exposureb a A provisional reference value may be useful to guide actions by Member States when there is reason for local concern. However, the value is too uncertain to be used for developing regulations or standards. See section 8.2 for more information. b Also applicable for acute exposure. See the chemical fact sheet in chapter 12 for considerations for bottle‑fed infants. be encountered as a consequence of natural contamination, but this may also be a byproduct of industrial activity, such as mining, that changes drainage patterns. Many of these chemicals are used in small industrial units within human settlements, and, particularly where such units are found in groups of similar enterprises, they may be a significant source of pollution. Petroleum oils are widely used in human settlements, and improper handling or disposal can lead to significant pollution of surface water and groundwater. Where plastic pipes are used, the smaller aromatic molecules in petroleum oils can sometimes penetrate the pipes where they are surrounded by earth soaked in the oil, with subsequent pollution of the local water supply. Table 8.9 Guideline values for naturally occurring chemicals that are of health significance in drinking-water Guideline value Chemical μg/l mg/l Remarks Inorganic Arsenic 10 (A, T) 0.01 (A, T) Barium 1300 1.3 Boron 2400 2.4 Chromium 50 0.05 For total chromium Fluoride 1500 1.5 Volume of water consumed and intake from other sources should be considered when setting national standards Manganese 80 (P) 0.08 (P) For total manganese. Aesthetic as well as health aspects should be considered when setting national standards Selenium 40 (P) 0.04 (P) Uranium 30 (P) 0.03 (P) Only chemical, not radiological, aspects of uranium addressed Organic Cylindrospermopsins 0.7 (P) 0.0007 (P) 3 (P) 0.003 (P) For short‑term exposurea Both values are for total cylindrospermopsins (sum of all congeners, free plus cell‑bound) Microcystins 1 (P) 0.001 (P) 12 (P) 0.012 (P) For short‑term exposurea Both values are for total microcystins (sum of all congeners, free plus cell‑bound) Saxitoxins 3 0.003 For acute exposure For total saxitoxins (sum of all congeners, free plus cell‑bound) A, provisional guideline value because calculated guideline value is below the achievable quantification level; P, provisional guideline value because of uncertainties in the health database; T, provisional guideline value because calculated guideline value is below the level that can be achieved through practical treatment methods, source protection, etc. a See the respective chemical fact sheet in chapter 12 for considerations for bottle‑fed infants. A number of chemicals can reach water as a consequence of disposal of general household chemicals; in particular, a number of heavy metals may be found in domestic wastewater. Where wastewater is treated, these will usually partition out into the sludge. Some chemicals that are widely used both in industry and in materials used in a domestic setting are found widely in the environment (e.g. di(2-ethylhexyl)phthalate), and these may be found in water sources, although usually at low concentrations. Some chemicals that reach drinking-water from industrial sources or human settlements have other primary sources and are therefore discussed in other sections of this chapter. Where latrines and septic tanks are poorly sited, these can lead to contamination of drinking-water sources with nitrate (see sections 8.5.3). Table 8.10 Chemicals from industrial sources and human dwellings for which guideline values have not been established Chemical Reason for not establishing a guideline value Beryllium Rarely found in drinking‑water at concentrations of health concern Cyanide Occurs in drinking‑water at concentrations well below those of health concern, except in emergency situations following a spill to a water source 1,3‑Dichlorobenzene Available data inadequate to permit derivation of health‑based guideline value 1,1‑Dichloroethane Available data inadequate to permit derivation of health‑based guideline value 1,1‑Dichloroethene Occurs in drinking‑water at concentrations well below those of health concern Di(2‑ethylhexyl)adipate Occurs in drinking‑water at concentrations well below those of health concern Hexachlorobenzene Occurs in drinking‑water at concentrations well below those of health concern Methyl tertiary‑butyl ether Any guideline that would be derived would be significantly higher than concentrations at which methyl tertiary‑butyl ether would be detected by odour Monochlorobenzene Occurs in drinking‑water at concentrations well below those of health concern, and health‑based value would far exceed lowest reported taste and odour threshold Nitrobenzene Rarely found in drinking‑water at concentrations of health concern Organotins TBT, TPT, DBT, DOTa Occurs in drinking‑water at concentrations well below those of health concern Other organotinsb Available data inadequate to permit derivation of health‑based guideline value Petroleum products Taste and odour will in most cases be detectable at concentrations below those of health concern, particularly with short‑term exposure Trichlorobenzenes (total) Occur in drinking‑water at concentrations well below those of health concern, and health‑based value would exceed lowest reported odour threshold 1,1,1‑Trichloroethane Occurs in drinking‑water at concentrations well below those of health concern a Tributyltin, triphenyltin, dibutyltin and di‑n‑octyltin, respectively. b Also excluding monomethyltin, dimethyltin and dimethyltin dichloride, which are covered in Table 8.16. Identification of the potential for contamination by chemicals from industrial activities and human dwellings requires assessment of activities in the catchment and of the risk that particular contaminants may reach water sources. The primary approach to addressing these contaminants is prevention of contamination by encouraging good practices. However, if contamination has occurred, then it may be necessary to consider the introduction of treatment. Guideline values have not been established for the chemicals listed in Table 8.10 for the reasons indicated in the table. However, health-based values have been developed for a number of these chemicals in order to provide guidance to Member Table 8.11 Guideline values for chemicals from industrial sources and human dwellings that are of health significance in drinking-water Guideline value Chemicals μg/l mg/l Remarks Inorganic Cadmium 3 0.003 Mercury 6 0.006 For inorganic mercury Organic Benzene 10a 0.01a Carbon tetrachloride 4 0.004 1,2‑Dichlorobenzene 1000 (C) 1 (C) 1,4‑Dichlorobenzene 300 (C) 0.3 (C) 1,2‑Dichloroethane 30a 0.03a 1,2‑Dichloroethene 50 0.05 Dichloromethane 20 0.02 Di(2‑ethylhexyl)phthalate 8 0.008 1,4‑Dioxane 50a 0.05a Derived using TDI approach as well as linear multistage modelling Edetic acid 600 0.6 Applies to the free acid Ethylbenzene 300 (C) 0.3 (C) Hexachlorobutadiene 0.6 0.0006 Nitrilotriacetic acid 200 0.2 Pentachlorophenol 9a (P) 0.009a (P) Styrene 20 (C) 0.02 (C) Tetrachloroethene 100 0.1 Toluene 700 (C) 0.7 (C) Trichloroethene 8 0.008 Xylenes 500 (C) 0.5 (C) C, concentrations of the substance at or below the health‑based guideline value may affect the appearance, taste or odour of the water, leading to consumer complaints; P, provisional guideline value because of uncertainties in the health database a For non‑threshold substances, the guideline value is the concentration in drinking‑water associated with an upper‑bound excess lifetime cancer risk of 10−5 (one additional case of cancer per 100 000 of the population ingesting drinking‑water containing the substance at the guideline value for 70 years). Concentrations associated with estimated upper‑bound excess lifetime cancer risks of 10−4 and 10−6 can be calculated by multiplying and dividing, respectively, the guideline value by 10. States when there is a reason for local concern (for further information on guideline values and health-based values, see section 8.2). Fact sheets for each are included in chapter 12. Guideline values have been established for the chemicals listed in Table 8.11, which meet all of the criteria for inclusion. Fact sheets for each are included in chapter 12. 8.5.3 Chemicals from agricultural activities Chemicals are used in agriculture on crops and in animal husbandry. Nitrate may be present as a consequence of tillage when there is no growth to take up nitrate released from decomposing plants, from the application of excess inorganic or organic fertilizer and in slurry from animal production. Most chemicals that may arise from agriculture are pesticides, although their presence will depend on many factors, and not all pesticides are used in all circumstances or climates. Contamination can result from application and subsequent movement following rainfall or from inappropriate disposal methods. Some pesticides are also used in non-agricultural circumstances, such as the control of weeds on roads and railway lines. These pesticides are also included in this section. Guideline values have not been established for the chemicals listed in Table 8.12, as a review of the literature on occurrence or credibility of occurrence in drinking-water has shown evidence that the chemicals do not occur in drinking-water. Guideline values have not been established for the chemicals listed in Table 8.13 for the reasons indicated in the table. However, health-based values and, in some cases, acute health-based values have been developed for a number of these pesticides in order to provide guidance to Member States when there is a reason for local concern such as an emergency or spill situation (for further information on guideline values and health-based values, see section 8.2). Fact sheets for each are included in chapter 12. Guideline values have been established for the chemicals listed in Table 8.14, which meet the criteria for inclusion (see section 8.2). Fact sheets for each are included in chapter 12. Guideline values and health-based values are protective against health effects resulting from lifetime exposure. Small exceedances for short periods would not normally constitute a health emergency. In the event of a spill, a higher allocation of the ADI to drinking-water could be justified. Alternatively, in cases where acute health-based values have been derived, normally based on JMPR evaluations, these may provide useful guidance (for further information, see section 8.7.5). Routine monitoring of pesticides is generally not considered necessary. Member States should consider local usage and potential situations such as spills in deciding whether and where to monitor. In the event that monitoring results show levels above the guideline value or health-based value on a regular basis, it is advisable that a plan be developed and implemented to address the situation. As a general principle, efforts should be made to keep the concentration of pesticides in water as low as possible, and to not allow concentrations to increase up to the guideline value or health-based value. 8.5.4 Chemicals used in water treatment or from materials in contact with drinking-water Chemicals used in water treatment and chemicals arising from materials in contact with water may give rise to contaminants in the final water. Table 8.12 Chemicals from agricultural activities excluded from guideline value derivation Chemical Reason for exclusion Amitraz Degrades rapidly in the environment and is not expected to occur at measurable concentrations in drinking‑water supplies Chlorobenzilate Unlikely to occur in drinking‑water Chlorothalonil Unlikely to occur in drinking‑water Cypermethrin Unlikely to occur in drinking‑water Deltamethrin Unlikely to occur in drinking‑water Diazinon Unlikely to occur in drinking‑water Dinoseb Unlikely to occur in drinking‑water Ethylene thiourea Unlikely to occur in drinking‑water Fenamiphos Unlikely to occur in drinking‑water Formothion Unlikely to occur in drinking‑water Hexachlorocyclohexanes Unlikely to occur in drinking‑water (mixed isomers) MCPBa Unlikely to occur in drinking‑water Methamidophos Unlikely to occur in drinking‑water Methomyl Unlikely to occur in drinking‑water Mirex Unlikely to occur in drinking‑water Monocrotophos Has been withdrawn from use in many countries and is unlikely to occur in drinking‑water Oxamyl Unlikely to occur in drinking‑water Phorate Unlikely to occur in drinking‑water Propoxur Unlikely to occur in drinking‑water Pyridate Not persistent and only rarely found in drinking‑water Pyriproxyfen Unlikely to occur in drinking‑waterb Quintozene Unlikely to occur in drinking‑water Toxaphene Unlikely to occur in drinking‑water Triazophos Unlikely to occur in drinking‑water Trichlorfon Unlikely to occur in drinking‑water a 4‑(4‑chloro‑o‑tolyloxy)butyric acid. b The use of pyriproxyfen as a larvicide for public health purposes is discussed further in section 8.6. Some substances are deliberately added to water in the course of treatment (direct additives), some of which may be inadvertently retained in the finished water (e.g. salts, coagulant polymer residues or monomers). Chloramine and chlorine disinfectant residuals, for example, are deliberate additives, and their presence confers a benefit. Others, such as DBPs, are generated during chemical interactions between disinfectant chemicals and substances normally in water (Table 8.15). Chlorination by-products and other DBPs may also occur in swimming pools, from which exposure by inhalation and skin absorption will be of greater importance (WHO, 2006). Table 8.13 Chemicals from agricultural activities for which guideline values have not been established Chemical Reason for not establishing a guideline value Ammonia Occurs in drinking‑water at concentrations well below those of health concern Bentazone Occurs in drinking‑water or drinking‑water sources at concentrations well below those of health concern Carbaryl Occurs in drinking‑water at concentrations well below those of health concern 1,3‑Dichloropropane Available data inadequate to permit derivation of health‑based guideline value Dichlorvos Occurs in drinking‑water or drinking‑water sources at concentrations well below those of health concern Dicofol Unlikely to occur in drinking‑water or drinking‑water sourcesa Diquat Occurs in drinking‑water or drinking‑water sources at concentrations well below those of health concern Endosulfan Occurs in drinking‑water at concentrations well below those of health concern Fenitrothion Occurs in drinking‑water at concentrations well below those of health concern Glyphosate and AMPAb Occur in drinking‑water at concentrations well below those of health concern Heptachlor and heptachlor epoxide Occur in drinking‑water at concentrations well below those of health concern Malathion Occurs in drinking‑water at concentrations well below those of health concern MCPAc Occurs in drinking‑water or drinking‑water sources at concentrations well below those of health concern Methyl parathion Occurs in drinking‑water at concentrations well below those of health concern Parathion Occurs in drinking‑water at concentrations well below those of health concern 2‑Phenylphenol and its sodium salt Occurs in drinking‑water at concentrations well below those of health concern Propanil Readily transformed into metabolites that are more toxic; a guideline value for the parent compound is considered inappropriate, and there are inadequate data to enable the derivation of guideline values for the metabolites a Although dicofol does not fulfil one of the three criteria for evaluation in the Guidelines, a background document has been prepared, and a health‑based value has been established, in response to a request from Member States for guidance. b Aminomethylphosphonic acid. (2‑Methyl‑4‑chlorophenoxy)acetic acid. Table 8.14 Guideline values for chemicals from agricultural activities that are of health significance in drinking-water Guideline value Chemical μg/l mg/l Remarks Non-pesticides Nitrate (as NO−)3 50 000 50 Based on short‑term effects, but protective for long‑term effects Nitrite (as NO−)2 3 000 3 Based on short‑term effects, but protective for long‑term effects Pesticides used in agriculture Alachlor 20a 0.02a Aldicarb 10 0.01 Applies to aldicarb sulfoxide and aldicarb sulfone Aldrin and dieldrin 0.03 0.000 03 For combined aldrin plus dieldrin Atrazine and its chloro‑s‑100 0.1 triazine metabolites Carbofuran 7 0.007 Chlordane 0.2 0.000 2 Chlorotoluron 30 0.03 Chlorpyrifos 30 0.03 Cyanazine 0.6 0.000 6 2,4‑Db 30 0.03 Applies to free acid 2,4‑DBc 90 0.09 1,2‑Dibromo‑3‑chloropropane 1a 0.001a 1,2‑Dibromoethane 0.4a (P) 0.000 4a (P) 1,2‑Dichloropropane 40 (P) 0.04 (P) 1,3‑Dichloropropene 20a 0.02a Dichlorprop 100 0.1 Dimethoate 6 0.006 Endrin 0.6 0.000 6 Fenoprop 9 0.009 Hydroxyatrazine 200 0.2 Atrazine metabolite Isoproturon 9 0.009 Lindane 2 0.002 Mecoprop 10 0.01 Methoxychlor 20 0.02 Metolachlor 10 0.01 Table 8.14 (continued) Guideline value Chemical μg/l mg/l Remarks Molinate 6 0.006 Pendimethalin 20 0.02 Simazine 2 0.002 2,4,5‑Td 9 0.009 Terbuthylazine 7 0.007 Trifluralin 20 0.02 P, provisional guideline value because of uncertainties in the health database a For substances that are considered to be carcinogenic, the guideline value is the concentration in drinking‑water associated with an upper‑bound excess lifetime cancer risk of 10−5 (one additional cancer per 100 000 of the population ingesting drinking‑water containing the substance at the guideline value for 70 years). Concentrations associated with estimated upper‑bound excess lifetime cancer risks of 10−4 and 10−6 can be calculated by multiplying and dividing, respectively, the guideline value by 10. b 2,4‑Dichlorophenoxyacetic acid. 2,4‑Dichlorophenoxybutyric acid. d 2,4,5‑Trichlorophenoxyacetic acid. Other chemicals, such as lead or copper from pipes or brass taps and chemicals leaching from coatings, may be taken up from contact with surfaces during treatment or distribution (indirect or unintentional additives). Some chemicals used in water treatment (e.g. aluminium) or in materials in contact with drinking-water (e.g. styrene) have other principal sources and are therefore discussed in detail in other sections of this chapter. Many of these additives, both direct and indirect or unintentional, are components of processes for producing safe drinking-water. The approach to monitoring and management is preferably through control of the material or chemical. It is important to optimize treatment processes and to ensure that such processes remain optimized in order to control residuals of chemicals used in treatment and to control the formation of DBPs. Inadvertent contamination caused by poor quality materials is best controlled by applying specifications governing the composition of the products themselves rather than by setting limits on the quality of finished water, whereas contamination due to the inappropriate use of additives can be addressed by guidance on use. Similarly, regulations on the quality of pipe can avoid possible contamination of water by leachable materials. Control of contamination from in situ applied coatings requires suitable codes of practice on their application in addition to controls on the composition of materials. Numerous national and third-party evaluation and approval systems for additives and materials for contact with drinking-water exist throughout the world; however, many countries do not have or operate such systems. Governments and other organizations should consider establishing or adapting additive management systems and setting product quality standards and guidance on use that would apply to determining acceptable water contact products. Ideally, harmonized standards between countries or reciprocal recognition would reduce costs and increase access to such standards (see also section 1.2.9). Table 8.15 Disinfection by-products present in disinfected waters (based on IPCS, 2000) Significant Significant Significant non-Disinfectant organohalogen products inorganic products halogenated products Chlorine/ hypochlorous acid (hypochlorite) Chlorine dioxide Chloramine Ozone Sodium dichloroisocyanurate THMs, HAAs, haloacetonitriles, chloral hydrate, chloropicrin, chlorophenols, N‑chloramines, halofuranones, bromohydrins Haloacetonitriles, cyanogen chloride, organic chloramines, chloramino acids, chloral hydrate, haloketones Bromoform, monobromoacetic acid, dibromoacetic acid, dibromoacetone, cyanogen bromide As for chlorine/ hypochlorous acid (hypochlorite) Chlorate (mostly from hypochlorite use) Reduced primarily to chlorite, chlorate and chloride in drinking‑water, and to chlorite and chloride upon ingestion; the provisional guideline values for chlorite and chlorate are protective for potential toxicity from chlorine dioxide Nitrate, nitrite, chlorate, hydrazine Chlorate, iodate, bromate, hydrogen peroxide, hypobromous acid, epoxides, ozonates Aldehydes, cyanoalkanoic acids, alkanoic acids, benzene, carboxylic acids, N‑nitrosodimethylamine Unknown Aldehydes, ketones, N‑nitrosodimethylamine Aldehydes, ketoacids, ketones, carboxylic acids Cyanuric acid Guideline values have not been established for the chemicals listed in Table 8.16 for the reasons indicated in the table. However, health-based values have been developed for a number of these chemicals in order to provide guidance to Member States when there is a reason for local concern (for further information on guideline values and health-based values, see section 8.2). Fact sheets for each are included in chapter 12. Guideline values have been established for the chemicals listed in Table 8.17, which meet the criteria for inclusion. Fact sheets for each are included in chapter 12. Indicator substances for monitoring chlorination by‑products Although guideline values have been established for a number of chlorination by-products, data from drinking-water supplies indicate that THMs and HAAs are adequate as indicators of the majority of chlorination by-products. The most appropriate means of controlling chlorination by-products is to remove the organic precursors, which are largely of natural origin. Measurement of THMs and, if appropriate, Table 8.16 Chemicals used in water treatment or materials in contact with drinking-water for which guideline values have not been established Chemical Reason for not establishing a guideline value Disinfectants Chlorine dioxide Reduced primarily to chlorite, chlorate and chloride in drinking‑water, and to chlorite and chloride upon ingestion; the provisional guideline values for chlorite and chlorate are protective for potential toxicity from chlorine dioxide Dichloramine Available data inadequate to permit derivation of health‑based guideline value Iodine Available data inadequate to permit derivation of health‑based guideline value. Additionally, occurrence in drinking‑water is usually low. Although higher levels of exposure may occur when iodine is used as a drinking‑water disinfectant at the point of use, extended periods of exposure to iodine through water disinfection are unlikely. Silver Available data inadequate to permit derivation of health‑based guideline value and usually occurs in drinking‑water at concentrations well below those of health concern. However, a reference valuea of 0.1 mg/l has been proposed in cases of local concern. Trichloramine Available data inadequate to permit derivation of health‑based guideline value Disinfection by-products Bromochloroacetate Available data inadequate to permit derivation of health‑based guideline value Bromochloroacetonitrile Available data inadequate to permit derivation of health‑based guideline value Chloral hydrate Occurs in drinking‑water at concentrations well below those of health concern Chloroacetones Available data inadequate to permit derivation of health‑based guideline values for any of the chloroacetones 2‑Chlorophenol Available data inadequate to permit derivation of health‑based guideline value Chloropicrin Available data inadequate to permit derivation of health‑based guideline value Cyanogen chloride Occurs in drinking‑water at concentrations well below those of health concern Dibromoacetate Available data inadequate to permit derivation of health‑based guideline value 2,4‑Dichlorophenol Available data inadequate to permit derivation of health‑based guideline value Formaldehyde Occurs in drinking‑water at concentrations well below those of health concern Monobromoacetate Available data inadequate to permit derivation of health‑based guideline value MXb Occurs in drinking‑water at concentrations well below those of health concern Trichloroacetonitrile Available data inadequate to permit derivation of health‑based guideline value Table 8.16 (continued) Chemical Reason for not establishing a guideline value Contaminants from treatment chemicals Aluminium The health‑based value exceeds practicable levels based on optimization of the coagulation process in drinking‑water plants using aluminium‑based coagulants: 0.1 mg/l or less in large water treatment facilities and 0.2 mg/l or less in small facilities Contaminants from pipes and fittings Asbestos No consistent evidence that ingested asbestos is hazardous to health and available data inadequate to permit derivation of health‑based guideline value Fluoranthenec Occurs in drinking‑water at concentrations well below those of health concern Inorganic tin Occurs in drinking‑water at concentrations well below those of health concern Organotins MMT, DMT, DMTCd Unnecessary since their use as stabilizers in polyvinyl chloride and chlorinated polyvinyl chloride are normally controlled by product specification Zinc Not of health concern at levels found in drinking‑watere a A provisional reference value may be useful to guide actions by Member States in the event of need. However, the value is too uncertain to be used for developing regulations or standards. See section 8.2 for more information. b 3‑Chloro‑4‑dichloromethyl‑5‑hydroxy‑2(5H)‑furanone. See fact sheet on polynuclear aromatic hydrocarbons. d Monomethyltin, dimethyltin and dimethyltin dichloride, respectively. Other organotins are covered in Table 8.10. e May affect acceptability of drinking‑water (see chapter 10). Table 8.17 Guideline values for chemicals used in water treatment or materials in contact with drinking-water that are of health significance in drinking-water Guideline valuea Chemical μg/l mg/l Remarks Disinfectants Chlorine 5 000 (C) 5 (C) For free chlorine. For effective disinfection, there should be a residual concentration of free chlorine of ≥ 0.5 mg/l after at least 30 min contact time at pH < 8.0. A chlorine residual should be maintained throughout the distribution system. At the point of delivery, the minimum residual concentration of free chlorine should be 0.2 mg/l. Monochloramine 3 000 3 Sodium dichloroisocyanurate 50 000 40 000 50 40 As sodium dichloroisocyanurate As cyanuric acid Table 8.17 (continued) Guideline valuea Chemical μg/l mg/l Remarks Disinfection by-products Bromate 10a (A, T) 0.01a (A, T) Bromodichloromethane 60a 0.06a Bromoform 100 0.1 Chlorate 700 (D) 0.7 (D) Chlorite 700 (D) 0.7 (D) Chloroform 300 0.3 Dibromoacetonitrile 70 0.07 Dibromochloromethane 100 0.1 Dichloroacetate 50a (D) 0.05a (D) Dichloroacetonitrile 20 (P) 0.02 (P) Monochloroacetate 20 0.02 N‑Nitrosodimethylamine 0.1 0.0001 Trichloroacetate 200 0.2 2,4,6‑Trichlorophenol 200a (C) 0.2a (C) Trihalomethanes The sum of the ratio of the concentration of each to its respective guideline value should not exceed 1 Contaminants from treatment chemicals Acrylamide 0.5a 0.0005a Epichlorohydrin 0.4 (P) 0.0004 (P) Contaminants from pipes and fittings Antimony 20 0.02 Benzo[a]pyrene 0.7a 0.0007a Copper 2000 2 Staining of laundry and sanitary ware may occur below guideline value Lead 10 (A, T) 0.01 (A, T) Nickel 70 0.07 Based on long‑term effects, but protective for short‑term effects Vinyl chloride 0.3a 0.0003a A, provisional guideline value because calculated guideline value is below the achievable quantification level; C. concentrations of the substance at or below the health‑based guideline value may affect the appearance, taste or odour of the water, leading to consumer complaints; D, provisional guideline value because disinfection is likely to result in the guideline value being exceeded; P, provisional guideline value because of uncertainties in the health database; T, provisional guideline value because calculated guideline value is below the level that can be achieved through practical treatment methods, source control, etc. a For substances that are considered to be carcinogenic, the guideline value is the concentration in drinking‑water associated with an upper‑bound excess lifetime cancer risk of 10−5 (one additional case of cancer per 100 000 of the population ingesting drinking‑water containing the substance at the guideline value for 70 years). Concentrations associated with estimated upper‑bound excess lifetime cancer risks of 10−4 and 10−6 can be calculated by multiplying and dividing, respectively, the guideline value by 10. HAAs (e.g. where water is chlorinated at a low pH) can be used to optimize treatment efficiency and to establish the boundaries of other operational parameters that can be used to monitor treatment performance. In these circumstances, monitoring frequencies of other chlorination by-products can be reduced. Although total organohalogen does not correlate well with either THMs or HAAs, it is a measure of total chlorination by-products and may be another potential indicator for operational purposes. In all circumstances, disinfection efficiency should not be compromised in trying to meet guidelines for DBPs, including chlorination by-products, or in trying to reduce concentrations of these substances. Contaminants from storage and generation of hypochlorite solutions Sodium hypochlorite solutions slowly decompose—more rapidly at warmer temperatures—to produce chlorate and chlorite ions. As the solution ages and the available chlorine concentration decreases, it is necessary to dose more product to achieve the desired residual chlorine concentration, with a consequent increase in the amounts of chlorate and chlorite added to the treated water. The decomposition of solid calcium hypochlorite is much slower, and consequently contamination is less likely to be significant. However, if calcium hypochlorite solutions are prepared and stored before use, then decomposition to form chlorate and chlorite would also occur. Sodium hypochlorite is manufactured by electrolysing sodium chloride dissolved in water, which would naturally also contain small concentrations of sodium bromide. This results in the presence of bromate in the sodium hypochlorite solution and will contribute bromate to the treated water. The quality and acceptability of sodium hypochlorite will partly be a function of the concentration of the bromate residue. Industrial-grade product may not be acceptable for drinking-water applications. The sodium bromide naturally present in sodium chloride will also be oxidized to form bromate in systems using on-site electrochemical generation of hypochlorite. Contaminants from use of ozone and chlorine dioxide The use of ozone can lead to elevated bromate concentrations through oxidation of bromide present in the water. As a general rule, the higher the bromide concentration in the water, the more bromate that is produced. Chlorine dioxide solutions can contain chlorate as a result of reactions that compete with the desired reaction for generation of chlorine dioxide. Chlorite ion is an inevitable decomposition product from the use of chlorine dioxide; typically, 60–70% of the applied dose is converted to chlorite in the treated water. 8.5.5 Chemicals of emerging concern Pharmaceuticals Pharmaceuticals can be introduced into water sources in sewage by excretion from individuals using these chemicals, from uncontrolled drug disposal (e.g. discarding drugs into toilets) and from agricultural runoff from livestock manure. They have become chemicals of emerging concern to the public because of their potential to reach drinking-water. The specific types of pharmaceuticals and their metabolites in water sources can differ between countries or regions depending on social, cultural, technological and agricultural factors. Urban and rural areas may exhibit important differences in the occurrence and concentrations of these chemicals as a result of different usage patterns. The local physical and chemical characteristics of source waters can also affect the occurrence levels of pharmaceuticals by influencing their natural degradation. Most occurrence data in drinking-water and source water have resulted from targeted investigations, rather than from systematic monitoring. Advancements in the sensitivity and accuracy of detection technologies and methodologies have led to increasing detection of trace amounts of pharmaceuticals, ranging from concentrations in the nanogram per litre to low microgram per litre range (although largely less than 0.1 μg/l) in drinking-water, surface water and groundwater. Higher concentrations of these contaminants are found in wastewater treatment effluents or wastewater discharges from poorly controlled manufacturing facilities. The concentrations of pharmaceuticals found in drinking-water are typically orders of magnitude less than the lowest therapeutic doses. Therefore, exposure to individual compounds in drinking-water is unlikely to have appreciable adverse impacts on human health. Formal guideline values are therefore not proposed in these Guidelines. Routine monitoring for pharmaceuticals in drinking-water and additional or specialized drinking-water treatment to reduce the concentrations of pharmaceuticals in drinking-water are not considered necessary. However, where local circumstances indicate a potential for elevated concentrations of pharmaceuticals in drinking-water, investigative monitoring and surveys of impacted water sources can be undertaken to assess possible exposure. If undertaken, these surveys should be quality assured and should target pharmaceuticals that are of local significance—i.e. those that are commonly prescribed and used or manufactured locally. Based on the risk assessment, screening values can be developed to assess the potential risks from exposure through drinking-water, and possible control measures could be considered within the context of water safety plans. Practical difficulties with implementing monitoring programmes include lack of standardized sampling and analysis protocols, high costs and limited availability of technologies needed to detect the diverse range of pharmaceuticals that may be present. Effective treatment of pharmaceuticals depends on the physicochemical properties of the specific compounds. Typically, conventional treatment processes are less effective than advanced treatment processes for the removal of many organic compounds, particularly those that are more water soluble. Preventive measures, such as rational drug use and education of prescribers and the public to reduce disposal and discharges to the environment, will likely reduce human exposure. Further information is available in Pharmaceuticals in drinking-water (see Annex 1). 8.6 Pesticides used in water for public health purposes The control of insect vectors of disease (e.g. dengue fever) is vital in many countries, and there are occasions when vectors, particularly mosquitoes, breed in containers used for the storage and collection of drinking-water. Although actions should be Table 8.18 Pesticides used for public health purposes for which guideline values have not been derived Pesticide Reason for not establishing a guideline value Bacillus thuringiensis Not considered appropriate to set guideline values for pesticides used for israelensis (Bti) vector control in drinking‑water Diflubenzuron Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water Methoprene Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water Novaluron Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water Permethrin Not recommended for direct addition to drinking‑water as part of WHO’s policy to exclude the use of any pyrethroids for larviciding of mosquito vectors of human disease Pirimiphos‑methyl Not recommended for use for vector control in drinking‑water Pyriproxyfen Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water Spinosad Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water Temephos Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water taken to prevent access of vectors to or breeding of vectors in these containers, this is not always possible or may not always be fully effective, and use of mosquito larvicides may be indicated in certain settings. WHOPES carries out evaluations of pesticides for public health uses. There are currently seven larvicidal compounds (diflubenzuron, methoprene, novaluron, pirimiphos-methyl, pyriproxyfen, spinosad and temephos) and a bacterial larvicide (Bacillus thuringiensis israelensis) that have been evaluated and listed by WHOPES for the control of container-breeding mosquitoes. While it is not appropriate to set guideline values for pesticides used for vector control, it is valuable to provide information regarding their safety in use. Formulations of pesticides used for vector control in drinking-water should strictly follow the label recommendations and should only be those approved for such use by national authorities, taking into consideration the ingredients and formulants used in making the final product. In evaluating vector control pesticides for the Guidelines, an assessment is made of the potential exposure compared with the ADI. However, exceeding the ADI does not necessarily mean that this will result in adverse health effects. The diseases spread by vectors are significant causes of morbidity and mortality. It is therefore important to achieve an appropriate balance between the intake of the pesticide from drinking-water and the control of disease-carrying insects. It is stressed that every effort should be made to keep overall exposure and the concentration of any larvicide no greater than that recommended by WHOPES and as low as possible commensurate with efficacy. Member States should consider the use of larvicides within the context of their broad vector control strategy. The use of larvicides should only be part of a compreTable 8.19 Guideline values for pesticides that were previously used for public health purposes and are of health significance in drinking-water Guideline value Pesticides previously used for public health purposes μg/l mg/l DDT and metabolites 1 0.001 hensive management plan for household water storage and domestic waste management that does not rely exclusively on larviciding by insecticides, but also includes other environmental management measures and social behaviour change. Nevertheless, it would be valuable to obtain actual data on exposure to these substances under field conditions in order to carry out a more refined assessment of margins of exposure. In addition to the use of larvicides approved for drinking-water application to control disease vector insects, other control measures should also be considered. For example, the stocking of fish of appropriate varieties (e.g. larvae-eating mosquito-fish and predatory copepods) in water bodies may adequately control infestations and breeding of mosquitoes in those bodies. Other mosquito breeding areas where water collects should be managed by draining, especially after rainfall. Those pesticides used for public health purposes for which guideline values have not been derived are listed in Table 8.18. Dichlorodiphenyltrichlorethane (DDT) has been used for public health purposes in the past. It is being reintroduced (but not for water applications) in some areas to control malaria-carrying mosquitoes. Its guideline value is shown in Table 8.19. A summary of the product formulations and dosage rates, with corresponding exposures, is provided in Table 8.20. Fact sheets for all larvicides considered in the Guidelines are included in chapter 12. 8.7 Identifying local actions in response to chemical water quality problems and emergencies It is difficult to give comprehensive guidance concerning emergencies in which chemicals cause massive contamination of the drinking-water supply, caused either by accident or by deliberate action. Most of the guideline values recommended in these Guidelines (see section 8.5 and Annex 3) relate to a level of exposure that is regarded as tolerable throughout life. Acute toxic effects are considered for a limited number of chemicals. The length of time for which exposure to a chemical far in excess of the guideline value would have adverse effects on health will depend upon factors that vary from contaminant to contaminant. In an emergency situation, the public health authorities should be consulted about appropriate action. The exceedance of a guideline value may not result in a significant or increased risk to health. Therefore, deviations above the guideline values in either the short or long term may not necessarily mean that the water is unsuitable for consumption. The amount by which, and the period for which, any guideline value can be exceeded without affecting public health depends upon the specific substance involved, and acceptability judgements need to be made by qualified health officials. However, exceedance should be a signal: Table 8.20 WHO-recommended compounds and formulations for control of mosquito larvae in container habitatsa Insecticide Formulation Dosage (mg/l)b ADI (mg/kg bw) Exposure (mg/kg bw)c Use in drinking-water Bacillus thuringiensis israelensis (Bti)d WG 1–5 — Adult: 0.17 Child: 0.5 Infant: 0.75 Can be used at recommended doses Diflubenzuron DT, GR, WP 0.02–0.25 0–0.02 Adult: 0.008 Child: 0.025e Infant: 0.0375e Can be used at recommended doses Methoprene EC 1 0–0.09 Adult: 0.033 Child: 0.1e Infant: 0.15e Can be used at recommended doses Novaluron EC 0.01–0.05 0–0.01 Adult: 0.0017 Can be used at Child: 0.005 Infant: 0.0075 recommended doses Pirimiphos‑methyl EC 1 0–0.03 Adult: 0.033 Child: 0.1e Infant: 0.15e Not recommended for direct application to drinking‑water Pyriproxyfen GR 0.01 0–0.1 Adult: 0.000 33 Child: 0.001 Infant: 0.0015 Can be used at recommended doses Spinosad DT, GR, SC 0.1–0.5f 0–0.02 Adult: 0.0017 Child: 0.0052 Infant: 0.0078 Can be used at recommended doses Temephos EC, GR 1 0.023g Adult: 0.033 Child: 0.1e Infant: 0.15e Can be used at recommended doses bw, body weight; DT, tablet for direct application; EC, emulsifiable concentrate; GR, granule; SC, suspension concentration; WG, water dispersible granule; WP, wettable powder a WHO recommendations on the use of pesticides in public health are valid only if linked to WHO specifications for their quality control. WHO specifications for public health pesticides are available at http://who.int/whopes/quality/en. Label instructions must always be followed when using insecticides. b Active ingredient for control of container‑breeding mosquitoes. Exposure at the maximum dosage in drinking‑water for (a) a 60 kg adult drinking 2 litres of water per day, (b) a 10 kg child drinking 1 litre of water per day and (c) a 5 kg bottle‑fed infant drinking 0.75 litre of water per day. d Bti itself is not considered to pose a hazard to humans through drinking‑water. e Consideration should be given to using alternative sources of water for small children and bottle‑fed infants for a period after application, where this is practical. However, exceeding the ADI will not necessarily result in adverse effects. f The maximum concentration actually achieved with the slow‑release formulation of spinosad was approximately 52 μg/l. g This is a TDI rather than an ADI, as JMPR considered that the database was insufficiently robust to serve as the basis for establishing an ADI for temephos. For the purposes of these Guidelines, a TDI has been calculated from the lowest oral NOAEL in the critical study identified by JMPR. Source: Adapted from WHO/TDR (2009) • as a minimum, to investigate the cause with a view to taking remedial action as necessary; • to consult the authority responsible for public health for advice on suitable action, taking into account the intake of the substance from sources other than drinking-water, the toxicity of the substance, the likelihood and nature of any adverse effects and the practicality of remedial measures. If a guideline value is to be exceeded by a significant amount or for more than a few days, it may be necessary to act rapidly so as to ensure that health protective action is taken and to inform consumers of the situation so that they can act appropriately. The primary aim with regard to chemical contaminants when a guideline value is exceeded or in an emergency is to prevent exposure of the population to toxic concentrations of pollutants. However, in applying the Guidelines under such circumstances, an important consideration is that, unless there are appropriate alternative supplies of drinking-water available, maintenance of adequate quantities of water is a high priority. In the case of an incident in which chemical contaminants are spilt into a source water and enter a drinking-water supply or enter a supply through treatment or during distribution, the primary aim is to minimize the risk of adverse effects without unnecessarily disrupting the use of the water supply. This section of the Guidelines can be used to assist evaluation of the risks associated with a particular situation and—especially if a guideline value exists or an authoritative risk assessment is available from an alternative source—support appropriate decision-making on short- and medium-term actions. The approaches proposed provide a basis for discussion between various authorities and for judging the urgency of taking further action. Normally, a specific review of the situation will be required and should call on suitable expertise. It is important to take local circumstances into account, including the availability of alternative water supplies and exposure to the contaminant from other sources, such as food. It is also important to consider what water treatment is applied or available and whether this will reduce the concentration of the substance. Where the nature of contamination is unknown, expert opinion should be sought as quickly as possible to identify the contaminants, to determine what actions can be taken to prevent the contaminants from entering the supply and to minimize the exposure of the population and so minimize any potential for adverse effects. A water safety plan should include planning for response to both predictable events and undefined “emergencies”. Such planning facilitates rapid and appropriate response to events when they occur (see section 4.4). Consideration of emergency planning and planning for response to incidents in which a guideline value is exceeded, covering both microbial and chemical contaminants, is discussed in section 4.4. Broader discussion of actions in emergency situations can be found in section 6.8 and, for microbial contamination, section 7.6. 8.7.1 Trigger for action Triggers for action may include: • detection of a spill by, or reporting of a spill to, the drinking-water supplier; • an alarm raised by the observation of items, such as chemical drums, adjacent to a vulnerable part of the drinking-water supply; • the detection of a substance in the water; • a sudden change to water treatment; • consumer complaints (e.g. an unusual odour, taste or discoloration). 8.7.2 Investigating the situation Each incident is unique, and it is therefore important to determine associated facts, including what the contaminant is; what the likely concentration is, and by how much the guideline value has been exceeded, if at all; and the potential duration of the incident. These are important in determining the actions to be taken. 8.7.3 Talking to the right people In any emergency, it is important that there be good communication between the various authorities, particularly the water supplier and health authorities. It will usually be the health authorities that make the final decisions, but knowledge of the water supply and the nature of the supply is vital in making the most appropriate decisions. In addition, timely and clear communication with consumers is a vital part of successfully handling drinking-water problems and emergencies. Liaison with key authorities is discussed in section 4.4. It is particularly important to inform the public health authority of any exceedance or likely exceedance of a guideline value or other conditions likely to affect human health and to ensure that the public health authority is involved in decision-making. In the event of actions that require all consumers to be informed or where the provision of temporary supplies of drinking-water is appropriate, civil authorities should also be involved. Planning for these actions is an important part of the development of water safety plans. Involving the public health authorities at an early stage enables them to obtain specialist information and to make the appropriate staff available. 8.7.4 Informing the public Consumers may be aware of a potential problem with the safety of their drinking-water because of media coverage, their own senses or informal networks. Lack of confidence in the drinking-water or the authorities may drive consumers to alternative, potentially less safe sources. Not only do consumers have a right to information on the safety of their drinking-water, but they have an important role to play in assisting the authorities in an incident by their own actions and by carrying out the necessary measures at the household level. Trust and goodwill from consumers are extremely important in both the short and long term. The health authorities should be involved whenever a decision to inform the public of health-based concerns or advice to adopt health protection measures such as boiling of water may be required. Such guidance needs to be both timely and clear. 8.7.5 Evaluating the significance to public health and individuals In assessing the significance of an exceedance of a guideline value, account should be taken of: • information underpinning the guideline value derivation; • local exposure to the substance of concern through other routes (e.g. food); • any sensitive subpopulations; • locally relevant protective measures to prevent the chemical from entering the source water or supply in the case of a spill. Information underpinning guideline value derivation The derivation of guideline values for chemical contaminants is described in section 8.2. Most guideline values are derived by calculating a TDI or using an existing TDI or ADI. A proportion of the TDI or ADI is then allocated to drinking-water to make allowance for exposure from other sources, particularly food. This allocation is often 20%, but it may be as low as 1% or as high as 80%. In many circumstances, a review of likely local sources of exposure may identify that sources other than drinking-water are less significant than assumed and that a larger proportion of total exposure can be safely allocated to drinking-water. The fact sheets in chapter 12 and background documents on all chemicals addressed in these Guidelines (https://www.who.int/ teams/environment-climate-change-and-health/water-sanitation-and-health/chemical-hazards-in-drinking-water) provide further information on likely sources of the chemicals concerned, including their allocation factors. When rapid decision-making is required for such chemicals, it is possible to allow 100% of the TDI to come from drinking-water for a short period (e.g. a few days) while undertaking a more substantive review. In the event that there is significant exposure from other sources or exposure is likely to be for more than a few days, then it is possible to allocate more than the allocation used in the guideline value derivation, but no more than 100%. In some cases, the guideline value is derived from epidemiological or clinical studies in humans. In most cases (e.g. benzene, barium), these relate to long-term exposure, and short-term exposure to concentrations higher than the guideline value are unlikely to be of significant concern; however, it is important to seek expert advice. In other cases of guideline values derived from epidemiological studies, the associated health effects are acute in nature. For example: • The guideline value for nitrate is 50 mg/L, (as nitrate ion), to be protective of the health of the most sensitive subpopulation, bottle-fed infants. This guideline value is based on the absence of adverse health effects (methaemoglobinaemia and thyroid effects) at concentrations below 50 mg/L in epidemiological studies. Although the guideline value is based on short-term effects, it is protective for long-term effects and in other population groups, such as older children and adults. Methaemoglobinaemia is complicated by the presence of microbial contamination and subsequent gastrointestinal infection, which can increase the risk for this group significantly. Authorities should therefore be all the more vigilant that water to be used for bottle-fed infants is microbiologically safe when nitrate is present at concentrations near or above the guideline value. It is also particularly important to ensure that these infants are not currently exhibiting symptoms of gastrointestinal infection (diarrhoea). In addition, because excessive boiling of water to ensure microbiological safety can concentrate levels of nitrate in the water, care should be taken to ensure that water is heated only until the water reaches a rolling boil. In extreme situations, alternative sources of water (e.g. bottled water) can be used. • The guideline value for copper is also based on short-term exposure but is intended to protect against direct gastric irritation, which is a concentration-dependent phenomenon. The guideline value may be exceeded, but there will be an increasing risk of consumers suffering from gastrointestinal irritation as the concentration increases above the guideline value. The occurrence of such irritation can be assessed in exposed populations. In some cases, the guideline value is derived from a cancer risk estimate derived from studies in laboratory animals. In these cases, short-term (a few months to a year) exposure to concentrations up to 10 times the guideline value would result in only a small increase in estimated risk of cancer. Because the estimate of risk varies over a wide range, there may be no, or a very small, increase in risk. In such a circumstance, accepting a 10-fold increase in the guideline value for a short period would have no discernible impact on the risk over a lifetime. However, care would be needed to determine whether other toxicological end-points more relevant for short-term exposure, such as neurotoxicity, would become significant. Health-based values for short-term exposures are now being developed for a small number of substances that are used in significant quantities and are frequently implicated in an emergency as a consequences of spills, usually to surface water sources. The methodology used in the derivation of these health-based values is described below. Health‑based values for use in emergencies Health-based values for acute and short-term exposures (called acute and short-term health-based values) can be derived for any chemicals that are used in significant quantities and are involved in an emergency, such as a spill into surface water sources. JMPR has provided guidance on the setting of acute reference doses (ARfDs) for pesticides (Solecki et al., 2005). These ARfDs can be used as a basis for deriving acute health-based values for pesticides in drinking-water, and the general guidance can also be applied to derive ARfDs for other chemicals. The JMPR ARfD is usually established to cover the whole population, and must be adequate to protect the embryo or fetus from possible in utero effects. An ARfD based on developmental (embryo/fetal) effects, which applies to women of childbearing age only, may be conservative and not relevant to other population subgroups.1 The ARfD can be defined as the amount of a chemical, normally expressed on a body weight basis, that can be ingested in a period of 24 hours or less without appreciable health risk to the consumer. Most of the scientific concepts applicable to the setting of ADIs or TDIs for chronic exposure apply equally to the setting of ARfDs. 1 ARfDs established for pesticides by JMPR may be found at http://apps.who.int/pesticide-residues-jmprdatabase. The toxicological end-points most relevant for a single or 1-day exposure should be selected. For ARfDs for pesticides, possible relevant end-points include haematotoxicity (including methaemoglobin formation), immunotoxicity, acute neurotoxicity, liver and kidney toxicity (observed in single-dose studies or early in repeated-dose studies), endocrine effects and developmental effects. The most relevant or adequate study in which these end-points have been determined (in the most sensitive species or most vulnerable subgroup) is selected, and NOAELs are established. The most relevant end-point providing the lowest NOAEL is then used in the derivation of the ARfD. Uncertainty factors are used to extrapolate from experimental animal data to the average human and to allow for variation in sensitivity within the human population. An ARfD derived in such a manner can then be used to establish an acute health-based value by allocating 100% of the ARfD to drinking-water, as follows: ARfD × bw × P Acute health-based value = C where: bw = body weight (60 kg for adult, 10 kg for children, 5 kg for infants) P = fraction of the ARfD allocated to drinking-water (100%) C = daily drinking-water consumption (2 L for adults, 1 L for children, 0.75 L for bottle-fed infants) However, available data sets do not allow the accurate evaluation of the acute toxicity for a number of compounds of interest. If appropriate single-dose or short-term data are lacking, an end-point from a repeated-dose toxicity study can be used. This is likely to be a more conservative approach, and this should be clearly stated in the health-based value derivation. When a substance has been spilt into a drinking-water source, contamination may be present for a period longer than 24 hours, but is not usually present for longer than a few days. Under these circumstances, the use of data from repeated-dose toxicity studies is appropriate to derive a short-term health-based value (using the approach outlined in sections 8.2.2). As the period of exposure used in these studies will often be much longer than a few days, this, too, is likely to be a conservative approach. Where there is a need for a rapid response, and suitable data are not available to establish an ARfD but a guideline value or health-based value is available for the chemical of concern, a pragmatic approach would be to allocate a higher proportion of the ADI or TDI to drinking-water. As the ADI or TDI is intended to be protective of lifetime exposure, small exceedances of the ADI or TDI for short periods will not be of significant concern for health. In these circumstances, it would be reasonable to allow 100% of the ADI or TDI to come from drinking-water for a short period. Assessing locally relevant sources of the substance of concern through other routes of exposure The most useful sources of information regarding local exposure to substances through food and, to a lesser extent, air and other environmental routes are usually government departments dealing with food and environmental pollution. Other sources of information may include universities. In the absence of specific data, the Guidelines background documents consider the sources of exposure and give a generic assessment that can be used to make a local evaluation as to the potential use of a chemical and whether this would be likely to enter the food-chain. Further information is available in the supporting document Chemical safety of drinking-water (Annex 1). Sensitive subpopulations In some cases, there may be a specific subpopulation that is at greater risk from a substance than the rest of the population. These usually relate to high exposure relative to body weight (e.g. bottle-fed infants) or a particular sensitivity (e.g. fetal haemoglobin and nitrate/nitrite). However, some genetic subpopulations may show greater sensitivity to particular toxicity (e.g. glucose-6-phosphate dehydrogenase–deficient groups and oxidative stress on red blood cells). If the potential exposure from drinking-water in an incident is greater than the ADI or TDI or exposure is likely to be extended beyond a few days, then this would require consideration in conjunction with health authorities. In such circumstances, it may be possible to target action to avoid exposure of the specific group concerned, such as supplying bottled water for bottle-fed infants. Specific mitigation measures affecting risk assessment Such measures relate to actions taken locally or on a household basis that can have an impact on the presence of a particular contaminant. For example, the presence of a substance that is volatile or heat labile will be affected by heating the water for cooking or the preparation of beverages. Where such measures are routinely undertaken by the exposed population, the risk assessment may be modified accordingly. Alternatively, such steps can be used on a household basis to reduce exposure and allow the continued use of the supply without interruption. 8.7.6 Determining appropriate action Determining appropriate action means that various risks will need to be balanced. The interruption of water supply to consumers is a serious step and can lead to risks associated with contamination of drinking-water stored in the household with pathogens and limiting use for purposes of hygiene and health protection. Issuing a “do not drink” notice may allow the use of the supply for hygiene purposes such as showering or bathing, but creates pressure on consumers and authorities to provide a safe alternative for drinking and cooking. In some cases, this option will be expensive and could divert resources from other, more important issues. Appropriate action will always be decided on a case-by-case basis in conjunction with other authorities, including the health protection and civil authorities, who may be required to participate in informing consumers, delivering alternative supplies or supervising the collection of water from bowsers and tankers. Responding to a potential risk to health from a chemical contaminant should not lead to an increase in overall health risk from disruption of supply, microbial contaminants or other chemical contaminants. 8.7.7 Consumer acceptability Even though, in an emergency, supplying water that contains a substance present at higher concentrations than would normally be desirable may not result in an undue risk to health, the water may not be acceptable to consumers. A number of substances that can contaminate drinking-water supplies as a consequence of spills can give rise to severe problems with taste or odour. Under these circumstances, drinking-water may become so unpalatable as to render the water undrinkable or to cause consumers to turn to alternative drinking-water sources that may present a greater risk to health. In addition, water that is clearly contaminated may cause some consumers to feel unwell due to a perception of poor water quality. Consumer acceptability may be the most important factor in determining the advice given to consumers about whether or not the water should be used for drinking or cooking. 8.7.8 Ensuring remedial action, preventing recurrence and updating the water safety plan The recording of an incident, the decisions taken and the reasons for them are essential parts of handling an incident. The water safety plan, as discussed in chapter 4, should be updated in the light of experience. This would include making sure that problem areas identified during an incident are corrected. Where possible, it would also mean that the cause of the incident is dealt with to prevent its recurrence. For example, if the incident has arisen as a consequence of a spill from industry, the source of the spill can be advised as to how to prevent another spill and the information passed on to other similar industrial establishments. 8.7.9 Mixtures A spill may contain more than one contaminant of potential health concern (see section 8.2.8). Under these circumstances, it will be important to determine whether the substances present interact. Where the substances have a similar mechanism or mode of action, it is appropriate to consider them as additive. This may be particularly true of some pesticides, such as atrazine and simazine. In these circumstances, appropriate action must take local circumstances into consideration. Specialist advice should generally be sought. 8.7.10 Water avoidance advisories Water avoidance advisories share many features with boil water advisories (see section 7.6.1), but are less common. Like boil water advisories, they are a serious measure that should be instituted only when there is evidence that an advisory is necessary to reduce a substantial public health risk. In cases where alternative sources of water are recommended, particular consideration should be given to the potential for microbial hazards in those alternative sources. Water avoidance advisories are applied when the parameter of concern is not susceptible to boiling or when risks from dermal contact or inhalation of the contaminant are also significant. Water avoidance advisories may also be issued when an unknown agent or chemical substance is detected in the distribution system. It is important that the water avoidance advisories include the information that boiling is ineffective or insufficient to reduce the risk. As with the case of boil water advisories, water suppliers in conjunction with public health authorities should develop protocols for water avoidance advisories. Protocols should be prepared before any incident occurs and incorporated within water safety plans. Decisions to issue advisories are often made within a short period of time, and developing responses during an event can complicate decision-making, compromise communication and undermine public confidence. In addition to the information discussed in section 4.4.3, the protocols should provide information to the general public and specific groups on the following: • criteria for issuing and rescinding an advisory; • activities impacted by the advisory; • alternative sources of safe water for drinking and other domestic uses. Protocols should identify mechanisms for the communication of water avoidance advisories. The mechanisms may vary, depending on the nature of the supply and the size of the community affected, and could include: • media releases through television, radio and newspapers; • telephone, e-mail and fax contact of specific facilities, community groups and local authorities; • posting of notices in conspicuous locations; • personal delivery; • mail delivery. The methods chosen should provide a reasonable assurance that all of those affected by the advisory, including residents, workers and travellers, are notified as soon as possible. The issuing of a water avoidance advisory may be necessary, for example, following contamination—for example, chemical or radiological—as a result of accidental, natural or malicious origin that leads to: • a significant exceedance of a guideline value, which may pose a threat to health from short-term exposure; • concentrations of a chemical with no guideline value that may pose a threat to health from short-term exposure; • significant odour or taste that has no identified source or that will give rise to significant public anxiety. When issued, water avoidance advisories should provide information on the same issues included in boil water advisories (see section 7.6.1), although recommendations relating to affected uses and users will vary, depending on the nature of the problem. For example, for elevated concentrations of contaminants that are of concern only from a drinking or cooking perspective, the public could be advised to avoid using the water for drinking, food preparation, preparing cold drinks, making ice and hygienic uses, such as tooth brushing. Where the advisory applies to elevated levels of chemicals that can cause skin or eye irritation or gastrointestinal upsets, the public could be advised not to use the water for drinking, cooking, tooth brushing or bathing/showering. Alternatively, specific water avoidance advice might be issued where the contamination might affect subgroups of the population—for example, pregnant women or bottle-fed infants. As for boil water advisories, specific advice may need to be issued for dentists, doctors, hospitals and other health-care facilities, child-care facilities, schools, food suppliers and manufacturers, hotels, restaurants and operators of public swimming pools. Water avoidance advisories do not equate to cessation of supply; water will generally be suitable for flushing toilets and other uses, such as clothes washing. However, suitable alternative supplies of drinking-water, such as bottled water and carted or tankered water, will be required for drinking and other domestic uses. Criteria for rescinding water avoidance advisories will generally be based on evidence that the source of elevated concentrations of hazardous contaminants has been removed, that distribution systems have been appropriately flushed and that the water is safe for drinking and other uses. In buildings, the flushing would extend to storages and internal plumbing systems. 9 Radiological aspects DDrinking-water may contain radioactive substances (“radionuclides”) that could present a risk to human health. These risks are normally small compared with the risks from microorganisms and chemicals that may be present in drinking-water. Except in extreme circumstances, the radiation dose resulting from the ingestion of radionuclides in drinking-water is much lower than that received from other sources of radiation. The objective of this chapter is to provide criteria with which to assess the safety of drinking-water with respect to its radionuclide content and to provide guidance on reducing health risks by taking measures to decrease radionuclide concentrations, and therefore radiation doses, in situations where this is considered necessary. In terms of health risk assessment, the Guidelines do not differentiate between radionuclides that occur naturally and those that arise from human activities. However, in terms of risk management, a differentiation is made because, in principle, human-made radionuclides are often controllable at the point at which they enter the water supply. Naturally occurring radionuclides, in contrast, can potentially enter the water supply at any point, or at several points, prior to consumption. For this reason, naturally occurring radionuclides in drinking-water are often less amenable to control. 219 Naturally occurring radionuclides in drinking-water usually give radiation doses higher than those provided by artificially produced radionuclides and are therefore of greater concern. Radiological risks are best controlled through a preventive risk management approach following the framework for safe drinking-water (see chapter 2) and the water safety plan approach (see chapter 4). When considering what action to take in assessing and managing radiological risks, care should be taken to ensure that scarce resources are not diverted away from other, more important public health concerns. The screening levels and guidance levels for radioactivity presented in these Guidelines are based on the latest recommendations of the International Commission on Radiological Protection (ICRP, 2007). Some drinking-water supplies, in particular those sourced from groundwater, may contain radon, a radioactive gas. Although radon can enter indoor air in buildings through its release from water from taps or during showering, the most significant source of radon in indoor air is the underlying soil and building materials. An evaluation of international research data (UNSCEAR, 2000) has concluded that, on average, 90% of the dose attributable to radon in drinking-water comes from inhalation rather than ingestion. Consequently, the setting of screening levels and guidance levels to limit the dose from ingestion of radon contained in drinking-water is not usually necessary. The screening measurements for gross alpha and gross beta activities will include the contribution from radon progeny, which is the principal source of dose from ingestion of radon present in drinking-water supplies. This is further discussed in section 9.7. For further information on radionuclides in drinking-water, including more detailed guidance on considerations for developing drinking-water standards and actions to take when WHO criteria have been exceeded, see the supporting document Management of radioactivity in drinking-water (Annex 1). 9.1 Sources1 and health effects of radiation exposure Radioactivity from several naturally occurring and human-made sources is present throughout the environment. Some chemical elements present in the environment are naturally radioactive. These are found in varying amounts in soils, water, indoor and outdoor air and even within our bodies, and so exposure to them is inevitable. In addition, Earth is constantly bombarded by high-energy particles originating both from the sun and from outside the solar system. Collectively, these particles are referred to as cosmic radiation. Everybody receives a dose from cosmic radiation, which is influenced by latitude, longitude and height above sea level. The use of radiation in medicine for diagnosis and treatment is the largest human-made source of radiation exposure today. The testing of nuclear weapons, routine discharges from industrial and medical facilities and accidents such as Chernobyl have added human-made radionuclides to our environment. 1 When the term “source” appears in this chapter without any other reference, it is used in the context of “radiation source”. For any other purpose, additional information is provided (e.g. “water source”). Box 9.1 Key terms, quantities and units Becquerel (Bq)—The becquerel is the unit of radioactivity in the International System of Units (abbreviated SI from the French Système international d’unités), corresponding to one radioactive disintegration per second. In the case of drinking‑water, it is usual to talk about the activity concentration, expressed in units of Bq/l. Effective dose—When radiation interacts with body tissues and organs, the radiation dose received is a function of factors such as the type of radiation, the part of the body affected and the exposure pathway. This means that 1 Bq of radioactivity will not always deliver the same radiation dose. A unit called “effective dose” has been developed to take account of the differences between different types of radiation so that their biological impacts can be compared directly. The effective dose is expressed in SI units called sieverts (Sv). The sievert is a very large unit, and it is often more practical to talk in terms of millisieverts (mSv). There are 1000 mSv in 1 Sv. Effective half-life—Radioisotopes have a “physical” half‑life, which is the period of time it takes for one half of the atoms to disintegrate. Physical half‑lives for various radioisotopes can range from a few microseconds to billions of years. When a radioisotope is present in a living organism, it may be excreted. The rate of this elimination is influenced by biological factors and is referred to as the “biological” half‑life. The effective half‑life is the actual rate of halving the radioactivity in a living organism as determined by both the physical and biological half‑lives. Whereas for certain radionuclides, the biological processes are dominant, for others, physical decay is the dominant influence. Radioactivity is described using different quantities and units (see Box 9.1). The United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR, 2008) has estimated that the global average annual dose per person from all sources of radiation in the environment is approximately 3.0 mSv/year. Of this, 80% (2.4 mSv) is due to naturally occurring sources of radiation, 19.6% (almost 0.6 mSv) is due to the use of radiation for medical diagnosis and the remaining 0.4% (around 0.01 mSv) is due to other sources of human-made radiation (see Figure 9.1). There can be large variability in the dose received by individual members of the population, depending on where they live, their dietary preferences and other lifestyle choices. Individual radiation doses can also differ depending on medical treatments and occupational exposures. Annual average doses and typical ranges of individual doses from naturally occurring sources are presented in Table 9.1 (UNSCEAR, 2008). 9.1.1 Radiation exposure through ingestion of drinking-water Water sources can contain radionuclides of natural and artificial origin (i.e. human-made): • Natural radionuclides, including potassium-40, and those of the thorium and uranium decay series, in particular radium-226, radium-228, uranium-234, uranium238, lead-210 and polonium-210, can be found in water in varying amounts, as a result of either natural processes (e.g. absorption from the soil) or technological processes involving naturally occurring radioactive materials (e.g. the mining and processing of mineral sands or phosphate fertilizer production). Figure 9.1 Distribution of average radiation exposure for the world population Table 9.1 Average radiation dose from naturally occurring sources Worldwide average annual Typical annual effective Source effective dose (mSv) dose range (mSv) External exposure Cosmic rays 0.39 0.3–1a Terrestrial radiation (outdoors and indoors) 0.48 0.3–1b Internal exposure Inhalation (mainly radon) 1.26 0.2–10c Ingestion (food and drinking‑water) 0.29 0.2–1d Total 2.4 1–13 a Range from sea level to high ground elevation. b Depending on radionuclide composition of soil and building material. Depending on indoor accumulation of radon gas. d Depending on radionuclide composition of foods and drinking‑water; the drinking‑water contribution to the ingested dose is typically 0.05 mSv. Source: Adapted from UNSCEAR (2008) • Human-made radionuclides may be present in water from several sources, such as — radionuclides discharged from nuclear fuel cycle facilities (e.g. helium-3, chromium-51, cobalt-60); — manufactured radionuclides (produced and used in unsealed form in medicine or industry) as a result of regular or incidental discharges (e.g. iodium-131, technetium-99m); — radionuclides released in the past into the environment, including drinking-water sources (e.g. cesium-137, strontium-90, plutonium-239, plutonium-240). 9.1.2 Radiation-induced health effects through drinking-water Radiation protection is based on the assumption that any exposure to radiation involves some level of risk. For prolonged exposures, as is the case for ingestion of drinkingwater containing radionuclides over extended periods of time, evidence of an increased cancer risk in humans is available at doses above 100 mSv (Brenner et al., 2003; UNSCEAR, 2012). Below this dose, an increased risk is difficult to identify through epidemiological studies, because of the required large sample sizes, although some large studies have shown increased cancer risk at doses lower than 100 mSv (Brenner et al., 2003; Rühm, Laurier & Wakeford, 2022). It is assumed that there is a linear relationship between exposure and risk, with no threshold value below which there is no risk (UNSCEAR, 2010). The WHO strategy to control the presence of radionuclides in drinking-water is based on an “individual dose criterion” (IDC) in terms of an annual effective dose of 0.1 mSv/year. This level of effective dose represents a very low level of risk that is not expected to give rise to any detected adverse health effect (see Box 9.3). 9.2 Rationale for screening levels and guidance levels The current Guidelines are based on the approach proposed by the ICRP in situations of prolonged radiation exposure of the public. According to the ICRP, in planned exposure situations (see Box 9.2), it is prudent to restrict the prolonged component of the individual dose to 0.1 mSv in any given year (ICRP, 2000). It is recog-Screening levels and guidance levels are connized that exposure to radionuclides servative and should not be interpreted as in drinking-water may be a conse-mandatory limits. Exceeding a guidance level quence of a planned exposure situa-should be taken as a trigger for further investigation, but not necessarily as an indication that tion, but is more likely to be from an the drinking‑water is unsafe. existing exposure situation. Rather than adopt a different approach depending on whether or not the radionuclides are naturally occurring or human-made, a pragmatic and conservative approach was adopted, with an IDC of 0.1 mSv from 1 year’s consumption of drinking-water, regardless of the origin of the radionuclides (see Box 9.3). Box 9.2 Radiation exposure situations The ICRP (2007) distinguishes between three types of radiation exposure situations—planned, existing and emergency exposure situations: • A planned exposure situation is a situation that arises from the planned operation of a radiation source or from a planned activity that results in an exposure to a radiation source (e.g. exposure to a radiation source during a medical procedure for diagnosis or treatment). • An existing exposure situation is a situation that already exists when a decision on the need for control has to be taken (e.g. exposure to indoor radon in dwellings). • An emergency exposure situation is a situation that arises as a result of an accident, a malicious act or any other unexpected event. The criteria included in the present Guidelines do not apply during emergency exposure situations (see chapter 6). Exposure to radionuclides due to ingestion of drinking‑water in non‑emergency situations is regarded as an existing exposure situation (IAEA, 2014). Box 9.3 Individual dose criterion (IDC) and health risks The additional risk to health from exposure to an annual dose of 0.1 mSv (i.e. the IDC) associated with the intake of radionuclides from drinking‑water is considered to be low for the following reasons: • Individual doses from natural radioactivity in the environment vary widely. The worldwide average is about 2.4 mSv/year, but, in some parts of the world, average doses can be up to 10 times higher. An IDC of 0.1 mSv/year therefore represents a small addition compared with these values. • The nominal risk coefficient for radiation‑induced cancer incidence is approximately 5.5 × 10−2/Sv (ICRP, 2007). Multiplying this by an IDC of 0.1 mSv/year from drinking‑water gives an estimated annual cancer risk of approximately 5.5 × 10−6. The IDC provides the basis for the development of the operational criteria—that is, the screening levels and guidance levels included in these Guidelines—that can be applied by water suppliers and authorities. The WHO screening levels and guidance levels can be adapted in national standards for radionuclides in drinking‑water considering the national context. Adaptation may particularly be considered where there are elevated levels of naturally occurring radionuclides in groundwater and minimal options for alternative water sources or water treatment. In such situations, the Basic Safety Standards (BSS) reference level should be considered. See section 9.3.3 for more information on the BSS reference level, as well as Management of radioactivity in drinking-water (Annex 1) for further information on considerations for developing national standards. In the second edition of the Guidelines, the IDC of 0.1 mSv/year was based on screening levels for gross alpha activity and gross beta activity of 0.1 Bq/l and 1 Bq/l, respectively. This IDC represents less than 5% of the average annual dose attributable to radiation of natural origin (see section 9.1). Subsequent experience indicated that, in practice, the 0.1 mSv annual dose would usually not be exceeded if the gross alpha activity was equal to or below 0.5 Bq/l. For this reason, in the third edition of the Guidelines, the IDC was based on screening levels of 0.5 Bq/l for gross alpha activity and 1 Bq/l for gross beta activity. This change was carried forward to the current edition of the Guidelines. These values have been derived to cover the most common radionuclides in drinking-water (see section 9.1.1) and their contributions to the radiation dose from the consumption of drinking-water (see section 9.3.1). 9.3 Monitoring and assessment for dissolved radionuclides The recommended assessment methodology for controlling radionuclide health risks from drinking-water is illustrated in Figure 9.2 and summarized in Box 9.4. 9.3.1 Screening of drinking-water supplies The process of identifying individual radionuclides in drinking-water and determining their concentration may be time-consuming and expensive. Because, in most circumstances, the concentrations are low, such detailed analysis is normally not justified for routine monitoring. A more practical approach, particularly if resources are limited, is to use a screening procedure, where the total radioactivity present in the Figure 9.2 Application of screening and guidance levels for radionuclides in drinking-water form of alpha and beta radiation is first determined, without regard to the identity of specific radionuclides. These measurements are suitable as a preliminary screening approach to determine whether further radioisotope-specific analysis is necessary. They can also be used for detecting changes in the radiological characteristics of the drinking-water source as well as for identifying spatial and/or temporal trends in the radionuclide content of drinking-water. Screening levels for drinking-water, below which no further action is required, are 0.5 Bq/l for gross alpha activity and 1 Bq/l for gross beta activity. If neither of these values is exceeded, the IDC of 0.1 mSv/year will usually not be exceeded. The use of these screening levels is recommended, as this maximizes both the reliability and the cost-effectiveness of assessing the radionuclide content of drinking-water. Box 9.4 Recommended assessment methodology The recommended assessment methodology for controlling radionuclide health risks from drinking‑water involves four steps: 1. An IDC1 of 0.1 mSv from 1 year’s consumption of drinking‑water is adopted. 2. Initial screening is undertaken for both gross alpha activity and gross beta activity. If the measured activity concentrations are below the screening levels of 0.5 Bq/l for gross alpha activity and 1 Bq/l for gross beta activity, no further action is required. 3. If either of the screening levels is consistently exceeded, the concentrations of individual radionuclides should be determined and compared with the guidance levels (see Table 9.2). 4. The outcome of this further evaluation may indicate that no action is required or that further evaluation is necessary before a decision can be made on the need for measures to reduce the dose. For further guidance on actions to take when screening levels or guidance levels are exceeded, see Management of radioactivity in drinking-water (Annex 1). However, the IDC of 0.1 mSv/year could be slightly or even significantly exceeded, even if the screening levels are not exceeded, in the uncommon situation where some specific radionuclides are the only significant contributors to the total gross activity concentration. This can happen for certain radionuclides, such as radium–226 and polonium–210 among the alpha emitters, and radium-228 and lead210 among the beta emitters. If the local geology and hydrology indicate that these radionuclides may be present, the individual radionuclides should be measured and compared with the guidance levels. Furthermore, radionuclides emitting low-energy beta radiation, such as tritium, and some gaseous or volatile radionuclides, such as iodine, will not be detected by standard gross beta activity measurements. Routine analysis for these radionuclides is not necessary, but, if there are any reasons for believing that they may be present, radionuclide-specific sampling and measurement techniques should be used.2 Gross beta measurements include a contribution from potassium-40, a beta emitter that occurs naturally in a fixed ratio to stable potassium. Potassium is an essential element for humans and is absorbed mainly from ingested food. If the screening level of 1 Bq/l for gross beta is exceeded, the contribution of potassium-40 to beta activity should be subtracted following a separate determination of total potassium. The beta activity of potassium-40 per unit of weight of stable potassium is 27.9 Bq/g, which is the factor that should be used to calculate the beta activity due to potassium-40. 1 In the European Drinking Water Directive (Euratom, 2013), this parameter is called the indicative dose (ID), and the same value of 0.1 mSv/year is adopted. 2 References for analytical methods and treatment technologies specific to radionuclides are provided in Annex 6. 9.3.2 Strategy for assessing drinking-water if screening levels are exceeded If either of the screening levels are consistently exceeded, then the specific radionuclides should be identified and their individual activity concentrations measured. This will allow the contribution from each radionuclide to the IDC to be calculated. If the following additive formula is satisfied, then no further action is required: where: Ci = the measured activity concentration of radionuclide i, and GL = the guidance level (see Tables 9.2 and A6.1 in Annex 6) of radionuclide i that, at an intake of 2 litres/day1 for 1 year, will result in an effective dose of 0.1 mSv/year. If any of the guidance levels is exceeded, then the sum will exceed unity. The sum may also exceed unity even if none of the individual guidance levels is exceeded. Where the sum exceeds unity for a single sample, the IDC of 0.1 mSv/year would be exceeded only if the exposure to the same measured concentrations were to continue for a full year. In practice, activity concentrations often vary throughout the year. Hence, such a result does not in itself imply that the water is unsuitable for consumption. 9.3.3 Strategy for assessing drinking-water if guidance levels are exceeded An annual dose at the IDC of 0.1 mSv is a small percentage of the average radiation dose received by any individual. Both the screening levels and guidance levels, which are based on the IDC, are highly conservative values that allow national authorities to determine, without further consideration, that the drinking-water is fit for consumption from a radiological viewpoint. National experiences have shown that the vast majority of water supplies comply with these criteria. The screening levels and guidance levels should not be interpreted as a limit above which drinking-water is unsafe for consumption, but they represent useful operational tools to put into practice the optimization principle. Occasionally, the situation may arise where the guidance levels are consistently exceeded for one or a combination of specific radionuclides. National authorities will then need to conduct further investigations and make a decision regarding the need to implement remedial measures or to place some restriction on the continued use of the water supply for drinking purposes. Ideally, the decision should be made in consultation with all the relevant authorities—that is, those involved in drinking-water quality and in radiation protection, and civil authorities that may be required to participate in informing consumers, delivering alternative supplies or supervising the collection of water from bowsers and tankers. 1 In certain situations, where national or regional consumption rates are known, the guidance level could be adjusted to take this into account. Table 9.2 Guidance levels for commona natural and artificial radionuclides Dose Guidance coefficient level b Category Radionuclide (Sv/Bq) (Bq/l) Natural occurring radioactive isotope that starts the uranium decay seriesc Natural occurring radioactive isotopes belonging to the uranium decay series Natural occurring radioactive isotope that starts the thorium decay series Natural occurring radioactive isotopes belonging to the thorium decay series Artificial radionuclides that can be released to the environment as part of the fission products found in reactor emissions or nuclear weapons tests Artificial radionuclide that can be released to the environment as a fission product (see above). It is also used in nuclear medicine procedures and thus can be released into water bodies through sewage effluent. Radioactive isotope of the hydrogen produced artificially as a fission product from nuclear power reactors and nuclear weapons tests. It may be naturally present in the environment in a very small amount. Its presence in a water source suggests potential industrial contamination. Naturally occurring radioactive isotope widely distributed in nature and present in organic compounds and in the human body. Artificial isotope formed in nuclear reactors that also exists in trace quantities in natural uranium ores. Artificial isotope by‑product formed in nuclear reactors. Uranium‑238 Uranium‑234 Thorium‑230 Radium‑226 Lead‑210 Polonium‑210 Thorium‑232 Radium‑228 Thorium‑228 Caesium‑134d Caesium‑137d Strontium‑90d Iodine‑131d,e Tritiume Carbon‑14 Plutonium‑239d Americium‑241d 4.5 × 10−8 10 4.9 × 10−8 1 2.1 × 10−7 1 2.8 × 10−7 1 6.9 × 10−7 0.1 1.2 × 10−6 0.1 2.3 × 10−7 1 6.9 × 10−7 0.1 7.2 × 10−8 1 1.9 × 10−8 10 1.3 × 10−8 10 2.8 × 10−8 10 2.2 × 10−8 10 1.8 × 10−11 10 000 5.8 × 10−10 100 2.5 × 10−7 1 2.0 × 10−7 1 a This list is not exhaustive. In certain circumstances, other radionuclides should be investigated (see Annex 6). b Guidance levels were rounded to the nearest order of magnitude by averaging the log scale values (to 10n if the calculated value was below 3 × 10n and to 10n+1 if the value was 3 × 10n or above). For example, if the calculated value was 2.8 Bq/l (i.e. 2.8 × 100, as is the case for uranium‑234), the guidance level was rounded to 100 (i.e. = 1), whereas, if the calculated value was 3 Bq/l, (i.e. 3 × 100, as is the case for uranium‑238), or above, the guidance level was rounded to 101 (i.e. = 10). There may situations where it is useful to use the unrounded guidance levels to calculate the IDC, particularly considering that the rounded guidance levels are conservative in most cases. See section 1.5.2 in Management of radioactivity in drinking-water (Annex 1) for guidance on when to consider using unrounded guidance levels, as well as a table for the unrounded guidance levels for common radionuclides. Separate guidance levels are provided for individual uranium radioisotopes in terms of radioactivity (i.e. expressed as Bq/l). The provisional guideline value for total content of uranium in drinking‑water is 30 μg/l based on its chemical toxicity, which is predominant compared with its radiological toxicity (see chapter 12). Table 9.2 (continued) d These radionuclides either may not occur in drinking‑water in normal situations or may be found at doses that are too low to be of significance to public health. Therefore, they are of lower priority for investigation following an exceedance of a screening level. e Although iodine and tritium will not be detected by standard gross activity measure¬ments and routine analysis for these radionuclides is not necessary, if there are any reasons for believing that they may be present, radionuclide‑specific sampling and measurement tech¬niques should be used. This is the reason for including them in this table. From a radiological point of view, one of the key considerations is the extent to which the guidance levels are exceeded. The International Basic Safety Standards (BSS) for Radiation Protection and Safety of Radiation Sources (IAEA, 2014) address drinking-water in the chapter on existing exposure situations and contain a requirement that, generally, the annual effective dose received from the consumption of drinking-water should not exceed a value of about 1 mSv.1 This reference level should not be regarded either as an “acceptable” dose or as a dose limit, and all reasonable efforts should be made to minimize the doses received as part of the optimization process. Each situation will be different, and non-radiological factors, such as the costs of remediation and the availability of other drinking-water supplies, will need to be taken into account in reaching a final decision. In general, however, restrictions on the use of a drinking-water supply are not justified from a public health perspective when the dose is at or below the BSS reference level unless other supplies of safe drinking-water in the same area are available. Restrictions on the water supply may be considered at higher levels, but it is important that radiological risks are balanced with other risks, including the risk of not having a water supply. National authorities also need to be aware that radionuclides such as uranium are chemically toxic, and the allowable concentrations in drinking-water may be determined by a radioisotope’s toxicological rather than its radioactive properties (see chapter 12). 9.3.4 Sampling frequency Criteria for monitoring radiological contamination of drinking-water should be developed, taking into account available resources and the potential for radiological risks. It should not detract from the adequate assessment and management of microbial and chemical risks. New water supplies should be sampled to determine their suitability for drinking-water, whereas existing supplies would need monitoring occasionally. If the water supply is adequately characterized and measured concentrations are consistently below screening levels, then sampling frequency should be reduced. However, if sources of potential radionuclide contamination exist nearby or are expected to be changing rapidly with time, then the sampling should be more frequent. Sampling frequency should be maintained, or even increased, if concentrations are approaching the screening levels or if the sum of ratios of the observed concentrations of individual radionuclides to their guidance levels approaches unity (see below). A graded approach to sampling frequency should be developed commensurate with the degree 1 IAEA International Safety Standards Series No. GSR Part 3, IAEA, Vienna (IAEA, 2014). These standards are co-sponsored by eight international organizations, including WHO. of contamination, the source of supply (i.e. surface water or groundwater), the size of the population served, the expected variability of radionuclide concentrations and the availability and results of historical monitoring records. International standards are available relating to the assessment of radiological water quality, including sampling procedures (e.g. preservation and handling of samples) and programmes (Standards Australia & Standards New Zealand, 1998; ISO, 2003, 2006a,b, 2009). 9.4 Guidance levels for radionuclides commonly found in drinking-water Guidance levels established for naturally occurring and human-made radionuclides most commonly detected in drinking-water supplies as well as for human-made radionuclides potentially relevant for prolonged exposure situations resulting from past nuclear emergency situations are presented in Table 9.2. The respective dose coefficients for adults are also presented (IAEA, 1996; ICRP, 1996). The guidance level for each radionuclide in Table 9.2 represents the concentration that, if present in the drinking-water consumed throughout the year, would result in an individual dose of 0.1 mSv. The guidance levels were calculated using dose coefficients for adults. Insufficient evidence was found to introduce separate guidance levels for different age groups. Although infants and children consume a lower mean volume of drinking-water, the age-dependent dose coefficients for children are higher than those for adults, accounting for higher uptake or metabolic rates. In the case of prolonged contamination of the water source, an assessment of doses to infants and children may be considered. The guidance levels apply to routine (“normal”) operational conditions of existing or new drinking-water supplies. They do not apply during an emergency exposure situation involving the release of radionuclides into the environment. However, the guidance levels apply again once the relevant authorities have declared an end to the emergency exposure situation. Additional guidance is provided in section 6.8, the supporting document Management of radioactivity in drinking-water (Annex 1), and in several other publications (IAEA, 2011, 2015, 2018; ICRP, 2020). The guidance levels for radionuclides in drinking-water were calculated using the following equation:  IDC GL = hing × q where: GL = guidance level of radionuclide in drinking-water (Bq/l) IDC = individual dose criterion, equal to 0.1 mSv/year for this calculation hing = dose coefficient for ingestion by adults (mSv/Bq) q = annual ingested volume of drinking-water, assumed to be 730 litres/year (equivalent to the standard World Health Organization drinking-water consumption rate of 2 litres/day) 9.5 Analytical methods 9.5.1 Measuring gross alpha and gross beta activity concentrations To analyse drinking-water for gross alpha and gross beta activities (excluding radon and other volatile radionuclides), the most common approach is to evaporate a known volume of the sample to dryness and measure the activity of the residue. As alpha radiation is easily absorbed within a thin layer of solid material, the reliability and sensitivity of the method for alpha determination may be reduced in samples with high total dissolved solids (TDS) content (ISO, 2017, 2018a). The determination of gross beta activity using the evaporation method includes the contribution from potassium-40. An additional analysis of total potassium is therefore required if the gross beta screening value is exceeded; in this case, the contribution of potassium-40 needs to be subtracted from the gross beta value (see section 9.3.1 and Figure 9.2). The co-precipitation technique (APHA et al., 2017) excludes the contribution due to potassium-40; therefore, determination of total potassium is not necessary. This method is not applicable to assessment of water samples containing certain fission products, such as caesium-137. However, under normal circumstances, concentrations of fission products in drinking-water supplies are extremely low. The use of liquid scintillation counters capable of performing alpha–beta discrimination is increasing and provides an alternative for gross alpha and gross beta determination (ISO, 2018b; WHO, 2018). Where possible, standardized methods should be used to determine concentrations of gross alpha and gross beta activities. Standardized methods for this analysis are listed in Table 9.3. 9.5.2 Measuring specific radionuclides If either of the gross alpha and gross beta screening levels is exceeded, then the specific radionuclides should be identified and their individual activity concentrations measured. References for analytical methods for specific radionuclides are provided in Annex 6. Information on measuring radon concentrations in water is provided in section 9.7.4. 9.6 Remedial measures If the IDC of 0.1 mSv/year is exceeded consistently, then the options available to the regulatory authority to reduce the dose should be examined. Where remedial measures are contemplated, any strategy considered should first be justified (in the sense that it achieves a net benefit). Any decision that alters the radiation exposure situation should do more good than harm. This means that by reducing existing exposure, it will achieve sufficient individual or societal benefit to offset the detriment it causes (ICRP, 2007). Once the remedial action is justified, then protection should be optimized in accordance with the recommendations of ICRP (2007). The principle of optimization of protection implies that the likelihood of incurring exposures, the number of Table 9.3 Methods for the analysis of gross alpha and gross beta activities in drinking-water Method (reference) Preparation technique Application International Organization for Standardization: ISO 9696 for gross alpha counting on thick sources (ISO, 2017) Evaporation Water with TDS less than 0.1 g/l, non‑volatile radionuclides ISO 9697 for gross beta counting on thick sources (ISO, 2018a) Evaporation ISO 10704 for gross alpha and gross beta counting on thin sources (ISO, 2019) Evaporation or co‑precipitation ISO 11704 for gross alpha and gross beta using liquid scintillation counting (ISO, 2018b) None Water with TDS less than 5 g/l and when no correction for colour quench is necessary, non‑volatile radionuclides below 80 °C American Public Health Association 7110C for gross alpha and gross beta (APHA et al., 2017) Co‑precipitation Surface water and groundwater (TDS is not a factor) Note: Detection limits for these different standardized methods depend on a number of factors, mainly the detection efficiency of the equipment used and the counting time; they will also vary between laboratories depending on the exact protocols used. The detection limits for the analytical methods presented in the table can be expected to be in the range 1.4–340 mBq/l for gross alpha and 0–424 mBq/l for gross beta (Jobbágy et al., 2012). people exposed and the magnitude of their individual doses should all be kept as low as reasonably achievable, taking economic and societal factors into account. When source water contains unacceptably high concentrations of radionuclides, control options include use of an alternative supply, controlled blending with another source or additional water treatment. Treatment plants with a combination of coagulation, sedimentation and sand filtration processes may remove up to 100% of the suspended radioactivity present in raw waters. Lime–soda ash softening plants can also remove practically all of the suspended radioactivity, depending on the radionuclide and on the proportion of radioactivity that might be associated with particulates. A comprehensive review of the removal of dissolved radionuclides by water treatment processes has been undertaken (Brown, Hammond & Wilkins, 2008). The results summarized in that report are reproduced in Table 9.4. References for treatment technologies specific to radionuclides are provided in Annex 6. 9.7 Radon 9.7.1 Radon in air and water Uranium, radium and radon are all soluble in water. Radon present in surface waters, such as lakes and rivers, is readily released into outdoor air by agitation as it passes over rocks and soils. Groundwater from wells and boreholes usually contains higher radon concentrations than surface waters. In some extreme circumstances, very high radon concentrations can be found in drinking-water supplies from these sources (see Box 9.5). Table 9.4 Treatment performance for some common radionuclidesa Sand Activated Precipitation Ion Reverse Element Coagulation filtration carbon softening exchange osmosis Strontium xx xx x xxxx xxx xxxx Iodine xx xx xxx x xxx xxxx Caesium xx xx x xx xxx xxxx Radium xx xxx xx xxxx xxxx xxxx Uranium xxxx x xx xxxx xxxx xxxx Plutonium xxxx xx xxx x xxxx xxxx Americium xxxx xx xxx x xxxx xxxx Tritium Not possible to remove a x = 0–10% removal; xx = 10–40% removal; xxx = 40–70% removal; xxxx = > 70% removal. Box 9.5 Radon in drinking-water • Some groundwater supplies may contain elevated concentrations of radon. High radon concentrations are seldom found in surface drinking‑water supplies. • Radon dissolved in drinking‑water can be released into indoor air. Normally, a higher radon dose is received from inhaling the radon and radon progeny compared with their ingestion.• Radon released from drinking‑water is not the only source of radon in indoor air. Where high indoor radon concentrations exist, the underlying soil and building materials, rather than the drinking‑water, are normally the predominant sources. • Straightforward and effective techniques exist to reduce the concentration of radon in drinking‑water supplies. • In deciding whether or not to take steps to reduce the concentration of radon in drinking‑water supplies, it is important to take account of the contribution of other sources of radon to the total radiation dose. Any action should be both justified and optimized and take account of local conditions. Radon is soluble in water, its solubility decreasing rapidly with an increase in temperature. When a tap or shower is turned on, some of the dissolved radon is released into indoor air. This adds to the radon present from other sources and will give rise to a radiation dose when inhaled. An evaluation of international research data (UNSCEAR, 2000) has concluded that, on average, 90% of the dose attributable to radon in drinking-water comes from inhalation rather than ingestion. Therefore, controlling the inhalation pathway rather than the ingestion pathway is the most effective way to control doses from radon in drinking-water. The percentage of radon present in drinking-water that is released into indoor air will depend on local conditions, such as the total consumption of water in the house, the volume of the house and its ventilation rate, and is likely to be highly variable. It has been estimated that a radon concentration of 1000 Bq/l in drinking-water discharged from a tap or shower will, on average, increase the radon concentration by 100 Bq/m3 in indoor air (NAS, 1999; European Commission, 2001; Health Canada, 2009). This contribution is not constant, as it occurs only while the water is being discharged through the tap or shower. Radon in air also comes from other sources, and normally the underlying soil and building materials, rather than the drinking-water, are the predominant sources. 9.7.2 Health risks from radon Epidemiological studies have clearly shown that long-term exposure to high radon concentrations in indoor air increases the risk of lung cancer (WHO, 2009). Radon ingested in drinking-water will give a radiation dose to the lining of the stomach. Scientific studies however, have not shown a definitive link between consumption of drinking-water containing radon and an increased risk of stomach cancer (Ye et al., 1998; Auvinen et al., 2005; WHO, 2009). 9.7.3 Guidance on radon in drinking-water supplies As the dose from radon present in drinking-water is normally received from inhalation rather than ingestion, it is more appropriate to measure the radon concentration in air than in drinking-water. The WHO reference level for radon concentration in indoor air is 100 Bq/m3 in dwellings. If this level cannot be reached under prevailing country-specific conditions, the level should not exceed 300 Bq/m3, corresponding to an annual dose of approximately 10 mSv (WHO, 2009). This recommendation is consistent with the International BSS (IAEA, 2014) and with the most recent recommendations of the ICRP (2009). If a country establishes a national standard for radon in drinking-water, screening levels for radon in water should be set on the basis of the national reference level for radon in air and the distribution of radon in the national housing stock. Where high radon concentrations are identified in indoor air, this is nearly always due to ingress of radon from the soil rather than degassing from the drinking-water supply. Nevertheless, in circumstances where high radon concentrations might be expected in drinking-water, it is prudent to measure for radon and, if high concentrations are identified, consider whether measures to reduce the concentrations are justified. The concentration of radon in groundwater supplies can vary considerably. Consequently, in situations where high radon concentrations have been identified or are suspected, the frequency of gross alpha and gross beta measurements may need to be increased so that the presence of long-lived radon progeny (notably lead-210 and polonium-210), which can be major contributors to dose, can be assessed and monitored on an ongoing basis. 9.7.4 Measuring radon in drinking-water There are difficulties in deriving activity concentrations of radon in drinking-water because of the ease with which radon is released from water during handling. Stirring and transferring water from one container to another will release dissolved radon. Water that has been left to stand will have reduced radon activity, and boiling will also completely release radon from the water into the air. A variety of methods can be used to measure radon in water, including liquid scintillation counting, which is a sensitive and widely used method (WHO, 2009). Samples should ideally be taken at the point of use. This can be complemented by taking measurements at the source to understand the potential for radon to be in drinking-water and to inform decisions on remedial actions. 9.7.5 Decreasing radon concentrations in drinking-water Reasonably simple measures are available to decrease radon concentrations in drinking-water by aeration. High-performance aeration is an effective means for the removal of radon in groundwater supplies and can achieve up to 99.9% removal. However, these methods may create a large source of airborne radon. Adsorption via granular activated carbon, with or without ion exchange, can also achieve high radon removal efficiencies, but is less efficient and requires large amounts of granular activated carbon. Other possibilities are blending the water with water that has very low radon concentrations or storing1 the water in tanks or basins to allow the radioactive decay of radon gas before distribution to consumers. A combination of these methods is frequently adopted. Decreasing radon in drinking-water should be considered in the overall context of radon exposure. Radon exposure from ingested water is small compared with exposure from inhalation. Controlling overall exposure should therefore most likely focus on reducing radon concentrations in the indoor air from radon entering buildings from the ground and/or building materials. 9.8 Risk communication 9.8.1 Reporting results The analytical results for each sample should contain the following information: • sample identification code; • sample collection date and time; • standard analytical methods used or brief description of any non-standard analytical methods used; • identification of the radionuclides or type of radioactivity and total radioactivity determined; • measurement-based concentration or activity value calculated using the appropriate blank for each radionuclide; • estimates of the counting uncertainty; 1 Storing is not effective for radium-226-rich waters because, in such cases, radon is continuously produced by the decay of its parent radium-226; radon is usually in equilibrium with radium-226 throughout the storage period, regardless of its duration. Furthermore, if storing water is used to reduce its radon content, attention should be paid to the accumulation of long-lived radon progeny (i.e. lead-210 and polonium-210). • a minimum detectable concentration for each radionuclide or parameter analysed; • estimate of total projected uncertainty of the reported result, including the contributions from all the parameters within the analytical method (i.e. counting and other random and systematic uncertainties or errors). 9.8.2 Communicating risks Communicating radiation risks clearly and effectively includes identifying target audiences (e.g. public, policy-makers and decision-makers) and tailoring the messages to them (WHO, 2002). Risk has different meaning for different people, but, in general, risk communication requires a description of the likelihood of harm and its severity. Risk communication with the public should utilize plain language. The technical lexicon of radiation protection is not readily understood by non-specialists (Picano, 2008). In some situations, comparisons are helpful to explain radiation risks (e.g. placing possible health risks from ingestion of drinking-water in the context of risk associated with exposure to natural radiation in different parts of the world). It should be clearly explained that guidance levels should not be interpreted as mandatory limits and that exceeding a guidance level may be taken as a trigger for further investigation, but it is not necessarily an indication that the drinking-water is unsafe. The persons in charge of communicating risk should be skilled in interpersonal communication, able to convey empathy, effective listeners and respectful of people’s concerns. They should be knowledgeable about the topic area with which they are dealing and be able to answer basic questions about the current as well as possible future risks. Guidance on radiation risk communication is provided elsewhere (USEPA, 2007; WHO, 2009). 10 Acceptability aspects: Taste, odour and appearance TThe provision of drinking-water that is not only safe but also acceptable in appearance, taste and odour is of high priority. Water that is aesthetically unacceptable will undermine the confidence of consumers, will lead to complaints and, more importantly, could lead to the use of water from sources that are less safe. To a large extent, consumers have no means of judging the safety of their drinking-water themselves, but their attitude towards their drinking-water supply and their drinking-water suppliers will be affected to a considerable extent by the aspects of water quality that they are able to per-The appearance, taste and odour of drinking‑water ceive with their own senses. It should be acceptable to the consumer. Water that is is natural for consumers to re-aesthetically unacceptable can lead to the use of water gard with suspicion water that from sources that are aesthetically more acceptable, but potentially less safe. appears dirty or discoloured or that has an unpleasant taste or smell, even though these characteristics may not in themselves be of direct consequence to health. 237 Some substances of health concern have effects on the taste, odour or appearance of drinking-water that would normally lead to rejection of the water at concentrations significantly lower than those of concern for health. The concentration at which constituents are objectionable to consumers is variable and dependent on Guideline values have not individual and local factors, including the quality of been established for constituents influencing water quality the water to which the community is accustomed that have no direct link to ad‑and a variety of social, environmental and cultural verse health impacts. considerations. Guideline values have not been established for constituents influencing water quality that have no direct link to adverse health impacts. However, guideline values have been established for some substances that may cause taste or odour in drinking-water at much lower concentrations than the guideline value because there is such a wide range in the ability of consumers to detect them by taste or odour. In the summaries in this chapter and the fact sheets in chapter 12, reference is made to levels likely to give rise to complaints from consumers. These are not precise numbers, and tastes or odours may be detectable by consumers at lower or higher levels, depending on individual and local circumstances. It is important to consider whether existing or proposed water treatment and distribution practices can affect the acceptability of drinking-water and to manage change and operations to minimize the risk of problems for acceptability as well as health. For example, chloramination that is not properly managed can lead to the formation of trichloramines, which can cause unacceptable taste and odour. Other problems may be indirect, such as the disturbance of internal pipe deposits and bio-films when the flow is disturbed or changed in distribution systems. It is not normally appropriate to directly regulate or monitor substances of health concern whose effects on the acceptability of water would normally lead to rejection of the water at concentrations significantly lower than those of concern for health; rather, these substances may be addressed through a general requirement that water be acceptable to the majority of consumers. For such substances, a formal guideline value is not usually derived, but a health-based value is derived in order to assist in judging the response that is needed when problems are encountered and in some cases to provide reassurance to health authorities and consumers with regard to possible health risks. In the fact sheets in chapter 12, this is explained, and information on acceptability is described. In the tables of guideline values (see chapter 8 and Annex 3), for those chemicals for which health-based guideline values were derived, the guideline value is designated with a “C”, with a footnote explaining that while the substance is of health significance, water would normally be rejected by consumers at concentrations well below the health-based guideline value. Monitoring of such substances should be undertaken in response to consumer complaints. Taste and odour can originate from natural inorganic and organic chemical contaminants and biological sources or processes (e.g. aquatic microorganisms), from contamination by synthetic chemicals, from corrosion or as a result of problems with water treatment (e.g. chlorination). Taste and odour may also develop during storage and distribution as a result of microbial activity. Taste and odour in drinking-water may be indicative of some form of pollution or of a malfunction during water treatment or distribution. It may therefore be an indication of the presence of potentially harmful substances. The cause should be investigated and the appropriate health authorities should be consulted, particularly if there is a sudden or substantial change. Colour, cloudiness, particulate matter and visible organisms may also be noticed by consumers and may create concerns about the quality and acceptability of a drinking-water supply. 10.1 Biologically derived contaminants There are a number of diverse organisms that often have no public health significance but which are undesirable because they produce taste and odour. As well as affecting the acceptability of the water, they indicate that water treatment and/or the state of maintenance and repair of the distribution system are insufficient. Actinomycetes and fungi Actinomycetes and fungi can be abundant in surface water sources, including reservoirs, and they can also grow on unsuitable materials in the water supply distribution systems, such as rubber. They can produce geosmin, 2-methyl isoborneol and other substances, resulting in objectionable tastes and odours in the drinking-water. Cyanobacteria and algae High amounts of phytoplankton (i.e. cyanobacteria and eukaryotic algae) in reservoirs and in surface waters used as drinking-water sources may impede coagulation and filtration, causing discoloration and turbidity of water after filtration. Some of these organisms can also produce geosmin, 2-methyl isoborneol and other chemicals, which have taste thresholds in drinking-water of a few nanograms per litre. Some other cyanobacterial products—cyanotoxins—are of direct health significance (see section 8.5.1), but their production does not seem to be linked to the production of taste and odour chemicals. Invertebrate animal life1 Invertebrate animals are naturally present in many water resources used as sources for the supply of drinking-water and often infest shallow, open wells. Small numbers of invertebrates may also pass through water treatment works where the barriers to particulate matter are not completely effective and colonize filters or the distribution system. Their motility may enable them and their larvae to penetrate filters at the treatment works and vents on storage reservoirs. The types of invertebrates concerned can be considered, for control purposes, as belonging to two groups. First, there are free-swimming organisms in the water itself or on water surfaces, such as the crustaceans Gammarus pulex (freshwater shrimp), Crangonyx pseudogracilis, Cyclops spp. and Chydorus sphaericus. Second, there are 1 The section was drawn largely from chapter 6 of the supporting document Safe piped water (Annex 1). other invertebrates that either move along surfaces or are anchored to them (e.g. water louse [Asellus aquaticus], snails, zebra mussel [Dreissena polymorpha], other bivalve molluscs and the bryozoan Plumatella sp.) or inhabit slimes (e.g. Nais spp., nematodes and the larvae of chironomids). In warm weather, slow sand filters can sometimes discharge the larvae of gnats (Chironomus and Culex spp.) into the water. In certain circumstances, these can reproduce parthenogenetically (i.e. asexual reproduction), which can exacerbate the problem in service reservoirs and distribution. Many of these invertebrates can survive, deriving food from bacteria, algae and protozoa in the water or present on slimes on pipe and tank surfaces. Few water distribution systems are completely free of animals at all times. However, the density and composition of invertebrate populations vary widely, from heavy infestations, including readily visible species that are objectionable to consumers, to sparse occurrences of microscopic species. The presence of invertebrates has largely been regarded by piped drinking-water suppliers in temperate regions as an acceptability problem, either directly or through their association with discoloured water. Large invertebrate populations also indicate high levels of organic material that may give rise to other water quality issues, such as microbial growth. In tropical and subtropical countries, in contrast, there are species of aquatic invertebrates that act as secondary hosts for parasites. For example, the small crustacean Cyclops is the intermediate host of the guinea worm (Dracunculus medinensis) (see sections 7.1.1 and 11.4). However, there is no evidence that guinea worm transmission occurs from piped drinking-water supplies. The presence of invertebrates in drinking-water, especially if visible, raises consumer concern about the quality of the drinking-water supply and should be controlled. Penetration of waterworks and mains is more likely to be a problem when high-rate filtration processes are used, but problems can arise even at well-run treatment works. Regular cleaning of water mains by flushing and/or swabbing will usually control infestation. Treatment of invertebrate infestations in piped distribution systems is discussed in detail in chapter 6 of the supporting document Safe piped water (Annex 1). Iron bacteria In waters containing ferrous and manganous salts, oxidation by iron bacteria (or by exposure to air) may cause rust-coloured deposits on the walls of tanks, pipes and channels and carry-over of deposits into the water. 10.2 Chemically derived contaminants Aluminium Naturally occurring aluminium as well as aluminium salts used as coagulants in drinking-water treatment are the primary sources of aluminium in drinking-water. The presence of aluminium at concentrations in excess of 0.1–0.2 mg/l often leads to consumer complaints as a result of deposition of aluminium hydroxide floc and the exacerbation of discoloration of water by iron. It is therefore important to optimize treatment processes in order to minimize any residual aluminium entering the distribution system. Under good operating conditions, aluminium concentrations of less than 0.1 mg/l are achievable in many circumstances. Available evidence does not support the derivation of a health-based guideline value for aluminium in drinking-water (see sections 8.5.4 and 12.1). Ammonia The threshold odour concentration of ammonia at alkaline pH is approximately 1.5 mg/l, and a taste threshold of 35 mg/l has been proposed for the ammonium cation. Ammonia is not of direct relevance to health at these levels, and no health-based guideline value has been proposed (see sections 8.5.3 and 12.1). However, ammonia does react with chlorine to reduce free chlorine and to form chloramines. Chloramines Chloramines, such as monochloramine, dichloramine and trichloramine (nitrogen trichloride), are generated from the reaction of chlorine with ammonia. Among chloramines, monochloramine is the only useful chlorine disinfectant, and chloramination systems are operated to minimize the formation of dichloramine and trichloramine. Higher chloramines, particularly trichloramine, are likely to give rise to taste and odour complaints, except at very low concentrations. For monochloramine, no odour or taste was detected at concentrations between 0.5 and 1.5 mg/l. However, slight organoleptic effects within this range and odour and taste thresholds of 0.65 and 0.48 mg/l have been reported. For dichloramine, the organoleptic effects between 0.1 and 0.5 mg/l were found to be “slight” and “acceptable”. Odour and taste thresholds of 0.15 and 0.13 mg/l were reported, respectively. An odour threshold of 0.02 mg/l has been reported for trichloramine, and it has been described as “geranium”. A guideline value for monochloramine has been established (see sections 8.5.4 and 12.1). Chloride High concentrations of chloride give a salty taste to water and beverages. Taste thresholds for the chloride anion depend on the associated cation and are in the range of 200–300 mg/l for sodium, potassium and calcium chloride. Concentrations in excess of 250 mg/l are increasingly likely to be detected by taste, but some consumers may become accustomed to low levels of chloride-induced taste. No health-based guideline value is proposed for chloride in drinking-water (see sections 8.5.1 and 12.1). Chlorine Most individuals are able to taste or smell chlorine in drinking-water at concentrations well below 5 mg/l, and some at levels as low as 0.3 mg/l. The taste threshold for chlorine is below the health-based guideline value of 5 mg/l (see sections 8.5.4 and 12.1). Chlorobenzenes Taste and odour thresholds of 10–20 μg/l and odour thresholds ranging from 40 to 120 μg/l have been reported for monochlorobenzene. A health-based guideline value has not been derived for monochlorobenzene (see sections 8.5.2 and 12.1), although the health-based value that could be derived far exceeds the lowest reported taste and odour threshold in water. Odour thresholds of 2–10 and 0.3–30 μg/l have been reported for 1,2- and 1,4dichlorobenzene, respectively. Taste thresholds of 1 and 6 μg/l have been reported for 1,2- and 1,4-dichlorobenzene, respectively. The health-based guideline values of 1 mg/l derived for 1,2-dichlorobenze and of 0.3 mg/l for 1,4-dichlorobenzene (see sections 8.5.2 and 12.1) far exceed the lowest reported taste and odour thresholds for these compounds. Odour thresholds of 10, 5–30 and 50 μg/l have been reported for 1,2,3-, 1,2,4- and 1,3,5-trichlorobenzene, respectively. A taste and odour threshold concentration of 30 μg/l has been reported for 1,2,4-trichlorobenzene. A health-based guideline value was not derived for trichlorobenzenes, although the health-based value that could be derived (see sections 8.5.2 and 12.1) exceeds the lowest reported odour threshold in water of 5 μg/l. Chlorophenols Chlorophenols generally have very low taste and odour thresholds. The taste thresholds in water for 2-chlorophenol, 2,4-dichlorophenol and 2,4,6-trichlorophenol are 0.1, 0.3 and 2 μg/l, respectively. Odour thresholds are 10, 40 and 300 μg/l, respectively. If water containing 2,4,6-trichlorophenol is free from taste, it is unlikely to present a significant risk to health (see sections 8.5.4 and 12.1). Microorganisms in distribution systems may sometimes methylate chlorophenols to produce chlorinated anisoles, for which the odour threshold is considerably lower. Colour Drinking-water should ideally have no visible colour. Colour in drinking-water is usually due to the presence of coloured organic matter (primarily humic and fulvic acids) associated with the humus fraction of soil. Colour is also strongly influenced by the presence of iron and other metals, either as natural impurities or as corrosion products. It may also result from the contamination of the water source with industrial effluents and may be the first indication of a hazardous situation. The source of colour in a drinking-water supply should be investigated, particularly if a substantial change has taken place. Most people can detect colour above 15 true colour units (TCU) in a glass of water. Levels of colour below 15 TCU are often acceptable to consumers. High colour from natural organic carbon (e.g. humics) could also indicate a high propensity to produce by-products from disinfection processes. No health-based guideline value is proposed for colour in drinking-water. Copper Copper in a drinking-water supply usually arises from the corrosive action of water leaching copper from copper pipes in buildings. High levels of dissolved oxygen have been shown to accelerate copper corrosion in some cases. Concentrations can vary significantly with the period of time the water has been standing in contact with the pipes; for example, first-draw water would be expected to have a higher copper concentration than a fully flushed sample. High concentrations can interfere with the intended domestic uses of the water. Staining of sanitary ware and laundry may occur at copper concentrations above 1 mg/l. At levels above 5 mg/l, copper also imparts a colour and an undesirable bitter taste to water. Although copper can give rise to taste, it should be acceptable at the health-based guideline value of 2 mg/l (see sections 8.5.4, 12.1 and A5.3 in Annex 5). Dissolved oxygen The dissolved oxygen content of water is influenced by the source, raw water temperature, treatment and chemical or biological processes taking place in the distribution system. Depletion of dissolved oxygen in water supplies can encourage the microbial reduction of nitrate to nitrite and sulfate to sulfide. It can also cause an increase in the concentration of ferrous iron in solution, with subsequent discoloration at the tap when the water is aerated. No health-based guideline value is recommended. However, very high levels of dissolved oxygen may exacerbate corrosion of metal pipes. Ethylbenzene Ethylbenzene has an aromatic odour; the reported odour threshold in water ranges from 2 to 130 μg/l. The lowest reported odour threshold is 100-fold lower than the health-based guideline value of 0.3 mg/l (see sections 8.5.2 and 12.1). The taste threshold ranges from 72 to 200 μg/l. Hardness Hardness caused by calcium and magnesium is usually indicated by precipitation of soap scum and the need for excess use of soap to achieve cleaning. Consumers are likely to notice changes in hardness. Public acceptability of the degree of hardness of water may vary considerably from one community to another. The taste threshold for the calcium ion is in the range of 100–300 mg/l, depending on the associated anion, and the taste threshold for magnesium is probably lower than that for calcium. In some instances, consumers tolerate water hardness in excess of 500 mg/l. Depending on the interaction of other factors, such as pH and alkalinity, water with a hardness above approximately 200 mg/l may cause scale deposition in the treatment works, distribution system and pipework and tanks within buildings. It will also result in high soap consumption and subsequent “scum” formation. On heating, hard waters form deposits of calcium carbonate scale. Soft water, but not necessarily cation exchange softened water, with a hardness of less than 100 mg/l, may, in contrast, have a low buffering capacity and so be more corrosive for water pipes. No health-based guideline value is proposed for hardness in drinking-water (see the supporting document Calcium and magnesium in drinking-water; Annex 1). Hydrogen sulfide The taste and odour thresholds of hydrogen sulfide in water are estimated to be between 0.05 and 0.1 mg/l. The “rotten eggs” odour of hydrogen sulfide is particularly noticeable in some groundwaters and in stagnant drinking-water in the distribution system, as a result of oxygen depletion and the subsequent reduction of sulfate by bacterial activity. Sulfide is oxidized rapidly to sulfate in well-aerated or chlorinated water, and hydrogen sulfide levels in oxygenated water supplies are normally very low. The presence of hydrogen sulfide in drinking-water can be easily detected by the consumer and requires immediate corrective action. It is unlikely that a person could consume a harmful dose of hydrogen sulfide from drinking-water; hence, a health-based guideline value has not been derived for this compound (see sections 8.5.1 and 12.1). Iron Anaerobic groundwater may contain ferrous iron at concentrations up to several milligrams per litre without discoloration or turbidity in the water when directly pumped from a well. On exposure to the atmosphere, however, the ferrous iron oxidizes to ferric iron, giving an objectionable reddish-brown colour to the water. Iron also promotes the growth of “iron bacteria”, which derive their energy from the oxidation of ferrous iron to ferric iron and in the process deposit a slimy coating on the piping. At levels above 0.3 mg/l, iron stains laundry and plumbing fixtures. There is usually no noticeable taste at iron concentrations below 0.3 mg/l, although turbidity and colour may develop. No health-based guideline value is proposed for iron (see sections 8.5.4 and 12.1). Manganese At levels as low as 0.02 mg/l, manganese as insoluble manganese oxides in water supplies may cause discoloured water and staining of laundry and plumbing fixtures. The presence of manganese in drinking-water, like iron, may lead to accumulation of deposits in the distribution system. Even at a concentration of 0.02 mg/l, manganese may deposit in pipes as manganese oxides, which may slough off as a black precipitate. In contrast, soluble manganese(II) is colourless and is visually undetectable at concentrations as high as 506 mg/l, the maximum tested concentration. The health-based guideline value of 0.08 mg/l for manganese (see section 12.1) is higher than the acceptability threshold of insoluble manganese. However, since dissolved manganese can also be released at the point of use or collection, the presence of discoloured water cannot be used reliably to assess if manganese is present. Therefore, aesthetic as well as health aspects should be considered when setting regulations and standards for drinking-water quality. Petroleum oils Petroleum oils can give rise to the presence of a number of low molecular weight hydrocarbons that have low odour thresholds in drinking-water. Benzene, toluene, ethylbenzene and xylenes (BTEX) are considered individually in this section, as healthbased guideline values have been derived for these chemicals. However, a number of other hydrocarbons, particularly alkylbenzenes such as trimethylbenzene, may give rise to a very unpleasant “diesel-like” odour at concentrations of a few micrograms per litre. There is experience indicating that the taste threshold of a mixture of low molecular weight aromatic hydrocarbons is lower than the threshold of individual substances. Diesel is a particularly rich source of such substances. pH and corrosion Although pH usually has no direct impact on consumers, it is one of the most important operational water quality parameters. Careful attention to pH control is necessary at all stages of water treatment to ensure satisfactory water clarification and disinfection (see the supporting document Safe piped water; Annex 1). For effective disinfection with chlorine, the pH should preferably be less than 8; however, low-er-pH water (approximately pH 7 or less) is more likely to be corrosive. The pH of the water entering the distribution system must be controlled to minimize the corrosion of water mains and pipes in household water systems. Alkalinity and calcium management also contribute to the stability of water and control its aggressiveness to pipes and appliances. Failure to minimize corrosion can result in the contamination of drinking-water and in adverse effects on its taste and appearance. The optimum pH required will vary in different supplies according to the composition of the water and the nature of the construction materials used in the distribution system, but it is usually in the range 6.5–8.5 (see section 8.4.3). Extreme values of pH can result from accidental spills, treatment breakdowns and insufficiently cured cement mortar pipe linings or cement mortar linings applied when the alkalinity of the water is low. No health-based guideline value has been proposed for pH (see section 12.1). Sodium The taste threshold concentration of sodium in water depends on the associated anion and the temperature of the solution. At room temperature, the average taste threshold for sodium is about 200 mg/l. No health-based guideline value has been derived (see sections 8.5.1 and 12.1), as the contribution from drinking-water to daily intake is small. Styrene Styrene has a sweet/sickly odour, and reported odour thresholds for styrene in water range from 0.004 to 2.6 mg/l, depending on temperature. Styrene may therefore be detected in water at concentrations below its health-based guideline value of 0.02 mg/l (see sections 8.5.2 and 12.1). Sulfate The presence of sulfate in drinking-water can cause noticeable taste, and very high levels might cause a laxative effect in unaccustomed consumers. Taste impairment varies with the nature of the associated cation; taste thresholds have been found to range from 250 mg/l for sodium sulfate to 1000 mg/l for calcium sulfate. It is generally considered that taste impairment is minimal at levels below 250 mg/l. No health-based guideline value has been derived for sulfate (see sections 8.5.1 and 12.1). Synthetic detergents In many countries, persistent types of anionic detergent have been replaced by others that are more easily biodegraded, and hence the levels found in water sources have decreased substantially. The concentration of detergents in drinking-water should not be allowed to reach levels giving rise to either foaming or taste problems. The presence of any detergent may indicate contamination of source water with sewage or ingress of detergent solution into the distribution system, as a result of back-flow, for example. Toluene Toluene has a sweet, pungent, benzene-like odour. The reported taste threshold ranges from 0.04 to 0.12 mg/l. The reported odour threshold for toluene in water ranges from 0.024 to 0.17 mg/l. Toluene may therefore affect the acceptability of water at concentrations below its health-based guideline value of 0.7 mg/l (see sections 8.5.2 and 12.1). Total dissolved solids The palatability of water with a total dissolved solids (TDS) level of less than about 600 mg/l is generally considered to be good; drinking-water becomes significantly and increasingly unpalatable at TDS levels greater than about 1000 mg/l. The presence of high levels of TDS may also be objectionable to consumers, owing to excessive scaling in water pipes, heaters, boilers and household appliances. No health-based guideline value for TDS has been proposed (see sections 8.5.1 and 12.1). Turbidity Turbidity, typically expressed as nephelometric turbidity units (NTU), describes the cloudiness of water caused by suspended particles (e.g. clay and silts), chemical precipitates (e.g. manganese and iron), organic particles (e.g. plant debris) and organisms. Turbidity can be caused by poor source water quality, poor treatment and, within distribution systems, disturbance of sediments and biofilms or the ingress of dirty water through main breaks and other faults. At high levels, turbidity can lead to staining of materials, fittings and clothes exposed during washing, in addition to interfering with the effectiveness of treatment processes (see Tables 7.7 and 7.8 in chapter 7). Increasing turbidity reduces the clarity of water to transmitted light. Below 4 NTU, turbidity can be detected only using instruments, but at 4 NTU and above, a milky-white, muddy, red-brown or black suspension can be visible. Large municipal supplies should consistently produce water with no visible turbidity (and should be able to achieve 0.5 NTU before disinfection at all times and average 0.2 NTU or less). However, small supplies, particularly those where resources are limited, may not be able to achieve such levels. Visible turbidity reduces the acceptability of drinking-water. Although most particles that contribute to turbidity have no health significance (even though they may indicate the presence of hazardous chemical and microbial contaminants), many consumers associate turbidity with safety and consider turbid water as being unsafe to drink. This response is exacerbated when consumers have been used to receiving high-quality filtered water. If consumers lose confidence in a drinking-water supply, they may drink less water or use lower turbidity alternatives that may not be safe. Any complaints about unexpected turbidity should always be investigated because they could reflect significant faults or breaches in distribution systems. Further information is available in Turbidity: information for regulators and operators of water supplies (see Annex 1). Xylenes Xylene concentrations in the range of 0.3 mg/l produce a detectable taste and odour. The odour threshold for xylene isomers in water has been reported to range from 0.02 to 1.8 mg/l. The lowest odour threshold is well below the health-based guideline value of 0.5 mg/l for xylene (see sections 8.5.2 and 12.1). Zinc Zinc imparts an undesirable astringent taste to water at a taste threshold concentration of about 4 mg/l (as zinc sulfate). Water containing zinc at concentrations in excess of 3–5 mg/l may appear opalescent and develop a greasy film on boiling. Although drinking-water seldom contains zinc at concentrations above 0.1 mg/l, levels in tap water can be considerably higher because of the zinc used in older galvanized plumbing materials; this may also be an indicator of elevated cadmium from such older material. No health-based guideline value has been proposed for zinc in drinking-water (see sections 8.5.4 and 12.1). 10.3 Treatment of taste, odour and appearance problems In many cases, aesthetic problems will be prevented by optimizing conventional treatment processes such as coagulation, sedimentation and chlorination. If specific treatment is deemed necessary, aeration, granular or powdered activated carbon and ozonation are generally effective in removing organic chemicals and some inorganic chemicals, such as hydrogen sulfide, that cause tastes and odours (see Annex 5). Tastes and odours caused by disinfectants are best controlled through careful operation of the disinfection process and pretreatment to remove precursors. Manganese can be removed by several treatment methods, including oxidation followed by filtration (see section 12.1, and Table A5.1 in Annex 5 for more information). Techniques for removing hydrogen sulfide include aeration, granular activated carbon, filtration and oxidation. Ammonia can be removed by biological nitrification. Precipitation softening or cation exchange can reduce hardness. Other taste-and odour-causing inorganic chemicals (e.g. chloride and sulfate) are generally not amenable to treatment (see the supporting document Chemical safety of drinking-water; Annex 1). 10.4 Temperature Cool water is generally more palatable than warm water, and temperature will have an impact on the acceptability of a number of other inorganic constituents and chemical contaminants that may affect taste. High water temperature enhances the growth of microorganisms and may increase problems related to taste, odour, colour and corrosion. 11 Microbial fact sheets FFact sheets are provided on potential waterborne pathogens as well as on indicator microorganisms. The waterborne microorganisms potentially causing illness include: • bacteria, viruses, protozoa and helminths identified in Table 7.1 and Figure 7.1;• potentially emerging pathogens, including Helicobacter pylori, Tsukamurella, Isospora belli and microsporidia, for which waterborne transmission is plausible but unconfirmed; • hazardous cyanobacteria. The human health effects caused by waterborne transmission vary in severity from mild gastroenteritis to severe and sometimes fatal diarrhoea, dysentery, hepatitis and typhoid fever. Contaminated water can be the source of large outbreaks of disease, including cholera, dysentery and cryptosporidiosis; for the majority of waterborne pathogens, however, there are other important sources of infection, such as person-toperson contact and food. Most waterborne pathogens are introduced into drinking-water supplies in human or animal faeces, do not grow in water and initiate infection in the gastrointestinal tract following ingestion. However, Legionella, atypical mycobacteria, Burkholderia 249 pseudomallei, Acanthamoeba spp. and Naegleria fowleri are environmental organisms that can grow in water and soil. Besides ingestion, other routes of transmission can include inhalation, leading to infections of the respiratory tract (e.g. Legionella, atypical mycobacteria), and contact, leading to infections at sites as diverse as the skin and brain (e.g. Naegleria fowleri, Burkholderia pseudomallei). Of all the waterborne pathogens, the helminth Dracunculus medinensis is unique in that it is the only pathogen that is solely transmitted through drinking-water. The fact sheets on potential pathogens include information on human health effects, sources and occurrence, routes of transmission and the significance of drinking-water as a source of infection. The fact sheets on microorganisms that can be used as indicators of the effectiveness of control measures or of the potential presence of pathogenic microorganisms provide information on indicator value, source and occurrence, application and significance of detection. 11.1 Bacterial pathogens Most bacterial pathogens potentially transmitted by water infect the gastrointestinal tract and are excreted in the faeces of infected humans and animals. However, there are also some waterborne bacterial pathogens, such as Legionella, Burkholderia pseudomallei and atypical mycobacteria, that can grow in water and soil. The routes of transmission of these bacteria include inhalation and contact (bathing), with infections occurring in the respiratory tract, in skin lesions or in the brain. Acinetobacter General description Acinetobacter spp. are Gram-negative, oxidase-negative, non-motile coccobacilli (short plump rods). Owing to difficulties in naming individual species and biovars, the term Acinetobacter calcoaceticus baumannii complex is used in some classification schemes to cover all subgroups of this species, such as A. baumannii, A. iwoffii and A. junii. Human health effects Acinetobacter spp. are usually commensal organisms, but they occasionally cause infections, predominantly in susceptible patients in hospitals. They are opportunistic pathogens that may cause urinary tract infections, pneumonia, bacteraemia, secondary meningitis and wound infections. These diseases are predisposed by factors such as malignancy, burns, major surgery and weakened immune systems, such as in neonates and elderly individuals. The emergence and rapid spread of multidrug-resistant A. calcoaceticus baumannii complex, causing nosocomial infections, are of concern in health-care facilities. Source and occurrence Acinetobacter spp. are ubiquitous inhabitants of soil, water and sewage environments. Acinetobacter has been isolated from 97% of natural surface water samples in numbers of up to 100/ml. The organisms have been found to represent 1.0–5.5% of the hetero-trophic plate count (HPC) flora in drinking-water samples and have been isolated from 5–92% of distribution water samples. In a survey of untreated groundwater supplies in the United States of America (USA), Acinetobacter spp. were detected in 38% of the groundwater supplies at an arithmetic mean density of 8/100 ml. The study also revealed that slime production, a virulence factor for A. calcoaceticus, was not significantly different between well water isolates and clinical strains, suggesting some degree of pathogenic potential for strains isolated from groundwater. Acinetobacter spp. are part of the natural microbial flora of the skin and occasionally the respiratory tract of healthy individuals. Routes of exposure Environmental sources within hospitals and person-to-person transmission are the likely sources for most outbreaks of hospital infections. Infection is most commonly associated with contact with wounds and burns or inhalation by susceptible individuals. In patients with Acinetobacter bacteraemia, intravenous catheters have also been identified as a source of infection. Outbreaks of infection have been associated with water baths and room humidifiers. Ingestion is not a usual source of infection. Significance in drinking‑water Although Acinetobacter spp. are often detected in treated drinking-water supplies, an association between the presence of Acinetobacter spp. in drinking-water and clinical disease has not been confirmed. There is no evidence of gastrointestinal infection through ingestion of Acinetobacter spp. in drinking-water among the general population. However, transmission of non-gastrointestinal infections by drinking-water may be possible in susceptible individuals, particularly in settings such as health-care facilities and hospitals. As discussed in chapter 6, specific water safety plans should be developed for buildings, including hospitals and other health-care facilities. These plans need to take account of particular sensitivities of occupants. Acinetobacter spp. are sensitive to disinfectants such as chlorine, and numbers will be low in the presence of a disinfectant residual. Control measures that can limit growth of the bacteria in distribution systems include treatment to optimize organic carbon removal, restriction of the residence time of water in distribution systems and maintenance of disinfectant residuals. Acinetobacter spp. are detected by HPC, which can be used together with parameters such as disinfectant residuals to indicate conditions that could support growth of these organisms. However, Escherichia coli (or, alternatively, thermotolerant coliforms) cannot be used as an indicator for the presence/absence of Acinetobacter spp. Selected bibliography Bartram J et al., eds (2003) Heterotrophic plate counts and drinking-water safety: The significance of HPCs for water quality and human health. London, IWA Publishing (WHO Emerging Issues in Water and Infectious Disease Series). Bergogne-Berezin E, Towner KJ (1996) Acinetobacter as nosocomial pathogens: Microbiological, clinical and epidemiological features. Clinical Microbiology Reviews, 9:148–165. Bifulco JM, Shirey JJ, Bissonnette GK (1989) Detection of Acinetobacter spp. in rural drinking water supplies. Applied and Environmental Microbiology, 55:2214–2219. Jellison TK, McKinnon PS, Rybak MJ (2001) Epidemiology, resistance and outcomes of Acinetobacter baumannii bacteremia treated with imipenem-cilastatin or ampicillinsulbactam. Pharmacotherapy, 21:142–148. Rusin PA et al. (1997) Risk assessment of opportunistic bacterial pathogens in drinking-water. Reviews of Environmental Contamination and Toxicology, 152:57–83. Aeromonas General description Aeromonas spp. are Gram-negative, non-spore-forming, facultative anaerobic bacilli belonging to the family Vibrionaceae. They bear many similarities to the Enterobacteriaceae. The genus is divided into two groups. The group of psychrophilic non-motile aeromonads consists of only one species, A. salmonicida, an obligate fish pathogen that is not considered further here. The group of mesophilic motile (single polar flagellum) aeromonads is considered of potential human health significance and consists of the species A. hydrophila, A. caviae, A. veronii subsp. sobria, A. jandaei, A. veronii subsp. veronii and A. schubertii. The bacteria are normal inhabitants of fresh water and occur in water, soil and many foods, particularly meat and milk. Human health effects Aeromonas spp. can cause infections in humans, including septicaemia, particularly in immunocompromised patients, wound infections and respiratory tract infections. There have been some claims that Aeromonas spp. can cause gastrointestinal illness, but epidemiological evidence is not consistent. Despite marked toxin production by Aeromonas spp. in vitro, diarrhoea has not yet been introduced in test animals or human volunteers. Source and occurrence Aeromonas spp. occur in water, soil and food, particularly meat, fish and milk. Aeromonas spp. are generally readily found in most fresh waters, and they have been detected in many treated drinking-water supplies, mainly as a result of regrowth in distribution systems. The factors that affect the occurrence of Aeromonas spp. in water distribution systems are not fully understood, but organic content, temperature, the residence time of water in the distribution network and the presence of residual chlorine have been shown to influence population sizes. Routes of exposure Wound infections have been associated with contaminated soil and water-related activities, such as swimming, diving, boating and fishing. Septicaemia can follow from such wound infections. In immunocompromised individuals, septicaemia may arise from aeromonads present in their own gastrointestinal tract. Significance in drinking‑water Despite frequent isolation of Aeromonas spp. from drinking-water, the body of evidence does not provide significant support for waterborne transmission. Aeromonads typically found in drinking-water do not belong to the same deoxyribonucleic acid (DNA) homology groups as those associated with cases of gastroenteritis. The presence of Aeromonas spp. in drinking-water supplies is generally considered a nuisance. Entry of aeromonads into distribution systems can be minimized by adequate disinfection. Control measures that can limit growth of the bacteria in distribution systems include treatment to optimize organic carbon removal, restriction of the residence time of water in distribution systems and maintenance of disinfectant residuals. Aeromonas spp. are detected by HPC, which can be used together with parameters such as disinfectant residuals to indicate conditions that could support growth of these organisms. However, E. coli (or, alternatively, thermotolerant coliforms) cannot be used as an indicator for the presence/absence of Aeromonas spp. Selected bibliography Bartram J et al., eds (2003) Heterotrophic plate counts and drinking-water safety: The significance of HPCs for water quality and human health. London, IWA Publishing (WHO Emerging Issues in Water and Infectious Disease Series). Borchardt MA, Stemper ME, Standridge JH (2003) Aeromonas isolates from human diarrheic stool and groundwater compared by pulsed-field gel electrophoresis. Emerging Infectious Diseases, 9:224–228. WHO (2002) Aeromonas. In: Guidelines for drinking-water quality, 2nd ed. Addendum: Microbiological agents in drinking water. Geneva, World Health Organization. Burkholderia pseudomallei General description Burkholderia pseudomallei is a Gram-negative bacillus commonly found in soil and muddy water, predominantly in tropical regions such as northern Australia and southeast Asia. The organism is acid tolerant and survives in water for prolonged periods in the absence of nutrients. Human health effects Burkholderia pseudomallei can cause the disease melioidosis, which is endemic in northern Australia and other tropical regions. The most common clinical manifestation is pneumonia, which may be fatal. In some of these areas, melioidosis is the most common cause of community-acquired pneumonia. Cases appear throughout the year but peak during the rainy season. Many patients present with milder forms of pneumonia, which respond well to appropriate antibiotics, but some may present with a severe septicaemic pneumonia. Other symptoms include skin abscesses or ulcers, abscesses in internal organs and unusual neurological illnesses, such as brainstem encephalitis and acute paraplegia. Although melioidosis can occur in healthy children and adults, it occurs mainly in people whose defence mechanisms against infection are impaired by underlying conditions or poor general health associated with poor nutrition or living conditions. Source and occurrence The organism occurs predominantly in tropical regions, typically in soil or surface-accumulated muddy water, from where it may reach raw water sources and also drinking-water supplies. The number of organisms in drinking-water that would constitute a significant risk of infection is not known. Routes of exposure Most infections appear to be through contact of skin cuts or abrasions with contaminated water. In south-east Asia, rice paddies represent a significant source of infection. Infection may also occur via other routes, particularly through inhalation or ingestion. The relative importance of these routes of infection is not known. Significance in drinking‑water In two Australian outbreaks of melioidosis, indistinguishable isolates of B. pseudomallei were cultured from cases and the drinking-water supply. The detection of the organisms in one drinking-water supply followed replacement of water pipes and chlorination failure, whereas the second supply was unchlorinated. Within a water safety plan, control measures that should provide effective protection against this organism include application of established treatment and disinfection processes for drinking-water coupled with protection of the distribution system from contamination, including during repairs and maintenance. HPC and disinfectant residual as measures of water treatment effectiveness and application of appropriate mains repair procedures could be used to indicate protection against B. pseudomallei. Because of the environmental occurrence of B. pseudomallei, E. coli (or, alternatively, thermotolerant coliforms) is not a suitable indicator for the presence/absence of this organism. Selected bibliography Ainsworth R, ed. (2004) Safe piped water: Managing microbial water quality in piped distribution systems. IWA Publishing, London, for the World Health Organization, Geneva. Currie BJ (2000) The epidemiology of melioidosis in Australia and Papua New Guinea. Acta Tropica, 74:121–127. Currie BJ et al. (2001) A cluster of melioidosis cases from an endemic region is clonal and is linked to the water supply using molecular typing of Burkholderia pseudomallei isolates. American Journal of Tropical Medicine and Hygiene, 65:177–179. Inglis TJJ et al. (2000) Outbreak strain of Burkholderia pseudomallei traced to water treatment plant. Emerging Infectious Diseases, 6:56–59. Campylobacter General description Campylobacter spp. are microaerophilic (require decreased oxygen) and capnophilic (require increased carbon dioxide), Gram-negative, curved spiral rods with a single unsheathed polar flagellum. Campylobacter spp. are one of the most important causes of acute gastroenteritis worldwide. Campylobacter jejuni is the most frequently isolated species from patients with acute diarrhoeal disease, whereas C. coli, C. laridis and C. fetus have also been isolated in a small proportion of cases. Two closely related genera, Helicobacter and Archobacter, include species previously classified as Campylobacter spp. Human health effects An important feature of C. jejuni is relatively high infectivity compared with other bacterial pathogens. As few as 1000 organisms can cause infection. Most symptomatic infections occur in infancy and early childhood. The incubation period is usually 2–4 days. Clinical symptoms of C. jejuni infection are characterized by abdominal pain, diarrhoea (with or without blood or faecal leukocytes), vomiting, chills and fever. The infection is self-limited and resolves in 3–7 days. Relapses may occur in 5–10% of untreated patients. Other clinical manifestations of C. jejuni infections in humans include reactive arthritis and meningitis. Several reports have associated C. jejuni infection with Guillain-Barré syndrome, an acute demyelinating disease of the peripheral nerves. Source and occurrence Campylobacter spp. occur in a variety of environments. Wild and domestic animals, especially poultry, wild birds and cattle, are important reservoirs. Pets and other animals may also be reservoirs. Food, including meat and unpasteurized milk, are important sources of Campylobacter infections. Water is also a significant source. The occurrence of the organisms in surface waters has proved to be strongly dependent on rainfall, water temperature and the presence of waterfowl. Routes of exposure Most Campylobacter infections are reported as sporadic in nature, with food considered a common source of infection. Transmission to humans typically occurs by the consumption of animal products. Meat, particularly poultry products, and unpasteurized milk are important sources of infection. Contaminated drinking-water supplies have been identified as a source of outbreaks. The number of cases in these outbreaks ranged from a few to several thousand, with sources including unchlorinated or inadequately chlorinated surface water supplies and faecal contamination of water storage reservoirs by wild birds. Significance in drinking‑water Contaminated drinking-water supplies have been identified as a significant source of outbreaks of campylobacteriosis. The detection of waterborne outbreaks and cases appears to be increasing. Waterborne transmission has been confirmed by the isolation of the same strains from patients and drinking-water they had consumed. Within a water safety plan, control measures that can be applied to manage potential risk from Campylobacter spp. include protection of raw water supplies from waste from humans and animals, adequate treatment and protection of water during distribution. Storages of treated and disinfected water should be protected from bird faeces. Campylobacter spp. are faecally borne pathogens and are not particularly resistant to disinfection. Hence, E. coli (or thermotolerant coliforms) is an appropriate indicator for the presence/absence of Campylobacter spp. in drinking-water supplies. Selected bibliography Frost JA (2001) Current epidemiological issues in human campylobacteriosis. Journal of Applied Microbiology, 90:85S–95S. Koenraad PMFJ, Rombouts FM, Notermans SHW (1997) Epidemiological aspects of thermophilic Campylobacter in water-related environments: A review. Water Environment Research, 69:52–63. Kuroki S et al. (1991) Guillain-Barré syndrome associated with Campylobacter infection. Pediatric Infectious Diseases Journal, 10:149–151. Enterobacter sakazakii General description Enterobacter sakazakii is a motile, Gram-negative, non-spore-forming, rod-shaped bacterium that has been found in infant formulas as a contaminant. Enterobacter species are biochemically similar to Klebsiella; unlike Klebsiella, however, Enterobacter is ornithine positive. Enterobacter sakazakii has been found to be more resistant to osmotic and dry stress than other members of the Enterobacteriaceae family. Human health effects Enterobacter sakazakii has been associated with sporadic cases or small outbreaks of sepsis, meningitis, cerebritis and necrotizing enterocolitis. Most of the infections are seen in low-birth-weight infants (i.e. less than 2 kg) or infants born prematurely (i.e. less than 37 weeks of gestation). Mortality has been reported to be as high as 50% but has decreased to less than 20% in recent years. Source and occurrence The reservoir for E. sakazakii is unknown. Various environmental samples (surface water, soil, mud, bird faeces) have tested negative. Enterobacter sakazakii has been identified in the guts of certain flies. The organism has been frequently identified in factories that produce milk powder and other food substances and in households. Commercially produced non-sterile powdered infant formula has often been implicated as the source of the bacteria during outbreaks. In a study of 141 powdered infant formulas, 20 were found to be culture-positive for E. sakazakii, even though the formulas complied with Codex microbial requirements for coliforms (< 3 colony-forming units per gram). The bacteria have been found in samples from newly opened sealed cans. Although sources of the bacteria other than infant formula have not been identified, environmental sources probably exist. Routes of exposure Disease caused by E. sakazakii in infants has been associated with the consumption of commercially prepared non-sterile infant formula. Contamination has been linked back to either the infant formula itself or formula preparation equipment (e.g. blenders). Many of the outbreaks have occurred without identified hygienic lapses during formula preparation. The organism has not been found in drinking-water sources used to prepare the formula. There is no evidence for person-to-person or more general environmental transmission. Significance in drinking‑water There is no evidence that these bacteria are transmitted through drinking-water, although it is plausible that the organism could be present in water of poor quality. Enterobacter sakazakii is sensitive to disinfectants, and its presence can be prevented by adequate treatment. Selected bibliography Block C et al. (2002) Cluster of neonatal infections in Jerusalem due to unusual biochemical variant of Enterobacter sakazakii. European Journal of Microbiology and Infectious Disease, 21:613–616. Breeuwer P et al. (2003) Desiccation and heat tolerance of Enterobacter sakazakii. Journal of Applied Microbiology, 95:967–973. Hamilton JV, Lehane MJ, Braig HR (2003) Isolation of Enterobacter sakazakii from midgut of Stomoxys calcitrans [letter to the editor]. Emerging Infectious Diseases, 9(10):1355–1356. Kandhai CM et al. (2004) Occurrence of Enterobacter sakazakii in food production environments and households [research letters]. Lancet, 363:39–40. WHO/FAO (2004) Enterobacter sakazakii and other microorganisms in powdered infant formula, meeting report. Geneva, World Health Organization and Food and Agriculture Organization of the United Nations (Microbiological Risk Assessment Series 6). Escherichia coli pathogenic strains General description Escherichia coli is present in large numbers in the normal intestinal flora of humans and animals, where it generally causes no harm. However, in other parts of the body, E. coli can cause serious disease, such as urinary tract infections, bacteraemia and meningitis. A limited number of enteropathogenic strains can cause acute diarrhoea. Several classes of enteropathogenic E. coli have been identified on the basis of different virulence factors, including enterohaemorrhagic E. coli (EHEC), enterotoxigenic E. coli (ETEC), enteropathogenic E. coli (EPEC), enteroinvasive E. coli (EIEC), enteroaggregative E. coli (EAEC) and diffusely adherent E. coli (DAEC). More is known about the first four classes named; the pathogenicity and prevalence of EAEC and DAEC strains are less well established. Human health effects EHEC serotypes, such as E. coli O157:H7 and E. coli O111, cause diarrhoea that ranges from mild and non-bloody to highly bloody, which is indistinguishable from haemorrhagic colitis. Between 2% and 7% of cases can develop the potentially fatal haemolytic uraemic syndrome, which is characterized by acute renal failure and haemolytic anaemia. Children under 5 years of age are at most risk of developing haemolytic uraemic syndrome. The infectivity of EHEC strains is substantially higher than that of the other strains. As few as 100 EHEC organisms can cause infection. ETEC produces heat-labile or heat-stable E. coli enterotoxin, or both toxins simultaneously, and is an important cause of diarrhoea in developing countries, especially in young children. Symptoms of ETEC infection include mild watery diarrhoea, abdominal cramps, nausea and headache. Infection with EPEC has been associated with severe, chronic, non-bloody diarrhoea, vomiting and fever in infants. EPEC infections are rare in developed countries, but occur commonly in developing countries, with infants presenting with malnutrition, weight loss and growth retardation. EIEC causes watery and occasionally bloody diarrhoea where strains invade colon cells by a pathogenic mechanism similar to that of Shigella. Source and occurrence Enteropathogenic E. coli are enteric organisms, and humans are the major reservoir, particularly of EPEC, ETEC and EIEC strains. Livestock, such as cattle and sheep and, to a lesser extent, goats, pigs and chickens, are a major source of EHEC strains. The latter have also been associated with raw vegetables, such as bean sprouts. The pathogens have been detected in a variety of water environments. Routes of exposure Infection is associated with person-to-person transmission, contact with animals, food and consumption of contaminated water. Person-to-person transmissions are particularly prevalent in communities where there is close contact between individuals, such as nursing homes and day-care centres. Significance in drinking‑water Waterborne transmission of pathogenic E. coli has been well documented for recreational waters and contaminated drinking-water. A well-publicized waterborne outbreak of illness caused by E. coli O157:H7 (and Campylobacter jejuni) occurred in the farming community of Walkerton in Ontario, Canada. The outbreak took place in May 2000 and led to 7 deaths and more than 2300 illnesses. The drinking-water supply was contaminated by rainwater runoff containing cattle excreta. Within a water safety plan, control measures that can be applied to manage potential risk from enteropathogenic E. coli include protection of raw water supplies from human and animal waste, adequate treatment and protection of water during distribution. There is no indication that the response of enteropathogenic strains of E. coli to water treatment and disinfection procedures differs from that of other E. coli. Hence, conventional testing for E. coli (or, alternatively, thermotolerant coliform bacteria) provides an appropriate indicator for the enteropathogenic serotypes in drinking-water. This applies even though standard tests will generally not detect EHEC strains. Selected bibliography Nataro JP, Kaper JB (1998) Diarrheagenic Escherichia coli. Clinical Microbiology Reviews, 11:142–201. O’Connor DR (2002) Report of the Walkerton Inquiry: The events of May 2000 and related issues. Part 1: A summary. Toronto, Ontario, Ontario Ministry of the Attorney General, Queen’s Printer for Ontario. Helicobacter pylori General description Helicobacter pylori, originally classified as Campylobacter pylori, is a Gram-negative, microaerophilic, spiral-shaped, motile bacterium. There are at least 14 species of Helicobacter, but only H. pylori has been identified as a human pathogen. Human health effects Helicobacter pylori is found in the stomach; although most infections are asymptomatic, the organism is associated with chronic gastritis, which may lead to complications such as peptic and duodenal ulcer disease and gastric cancer. Whether the organism is truly the cause of these conditions remains unclear. The majority of H. pylori infections are initiated in childhood and without treatment are chronic. The infections are more prevalent in developing countries and are associated with overcrowded living conditions. Interfamilial clustering is common. Source and occurrence Humans appear to be the primary host of H. pylori. Other hosts may include domestic cats. There is evidence that H. pylori is sensitive to bile salts, which would reduce the likelihood of faecal excretion, although it has been isolated from faeces of young children. Helicobacter pylori has been detected in water. Although H. pylori is unlikely to grow in the environment, it has been found to survive for 3 weeks in biofilms and up to 20–30 days in surface waters. In a study conducted in the USA, H. pylori was found in the majority of surface water and shallow groundwater samples. The presence of H. pylori was not correlated with the presence of E. coli. Possible contamination of the environment can be through children with diarrhoea or through vomiting by children as well as adults. Routes of exposure Person-to-person contact within families has been identified as the most likely source of infection through oral–oral transmission. Helicobacter pylori can survive well in mucus or vomit. However, it is difficult to detect in mouth or faecal samples. Faecal– oral transmission is also considered possible. Significance in drinking‑water Consumption of contaminated drinking-water has been suggested as a potential source of infection, but further investigation is required to establish any link with waterborne transmission. Humans are the principal source of H. pylori, and the organism is sensitive to oxidizing disinfectants. Hence, control measures that can be applied to protect drinking-water supplies from H. pylori include preventing contamination by human waste and adequate disinfection. Escherichia coli (or, alternatively, thermotolerant coliforms) is not a reliable indicator for the presence/absence of this organism. Selected bibliography Dunn BE, Cohen H, Blaser MJ (1997) Helicobacter pylori. Clinical Microbiology Reviews, 10: 720–741. Hegarty JP, Dowd MT, Baker KH (1999) Occurrence of Helicobacter pylori in surface water in the United States. Journal of Applied Microbiology, 87:697–701. Hulten K et al. (1996) Helicobacter pylori in drinking-water in Peru. Gastroenterology, 110: 1031–1035. Mazari-Hiriart M, López-Vidal Y, Calva JJ (2001) Helicobacter pylori in water systems for human use in Mexico City. Water Science and Technology, 43:93–98. Klebsiella General description Klebsiella spp. are Gram-negative, non-motile bacilli that belong to the family Enterobacteriaceae. The genus Klebsiella consists of a number of species, including K. pneumoniae, K. oxytoca, K. planticola and K. terrigena. The outermost layer of Klebsiella spp. consists of a large polysaccharide capsule that distinguishes the organisms from other members of the family. Approximately 60–80% of all Klebsiella spp. isolated from faeces and clinical specimens are K. pneumoniae and are positive in the thermotolerant coliform test. Klebsiella oxytoca has also been identified as a pathogen. Human health effects Klebsiella spp. have been identified as colonizing hospital patients, where spread is associated with the frequent handling of patients (e.g. in intensive-care units). Patients at highest risk are those with impaired immune systems, such as the elderly or very young, patients with burns or excessive wounds, those undergoing immunosuppressive therapy or those with human immunodeficiency virus (HIV)/acquired immunodeficiency syndrome (AIDS) infection. Colonization may lead to invasive infections. On rare occasions, Klebsiella spp., notably K. pneumoniae and K. oxytoca, may cause serious infections, such as destructive pneumonia. Source and occurrence Klebsiella spp. are natural inhabitants of many water environments, and they may multiply to high numbers in waters rich in nutrients, such as pulp mill wastes, textile finishing plants and sugar-cane processing operations. In drinking-water distribution systems, they are known to colonize washers in taps. The organisms can grow in water distribution systems. Klebsiella spp. are also excreted in the faeces of many healthy humans and animals, and they are readily detected in sewage-polluted water. Routes of exposure Klebsiella can cause nosocomial infections, and contaminated water and aerosols may be a potential source of the organisms in hospital environments and other health-care facilities. Significance in drinking‑water Klebsiella spp. are not considered to represent a source of gastrointestinal illness in the general population through ingestion of drinking-water. Klebsiella spp. detected in drinking-water are generally biofilm organisms and are unlikely to represent a health risk. The organisms are reasonably sensitive to disinfectants, and entry into distribution systems can be prevented by adequate treatment. Growth within distribution systems can be minimized by strategies that are designed to minimize bio-film growth, including treatment to optimize organic carbon removal, restriction of the residence time of water in distribution systems and maintenance of disinfectant residuals. Klebsiella is a coliform and can be detected by traditional tests for total coliforms. Selected bibliography Ainsworth R, ed. (2004) Safe piped water: Managing microbial water quality in piped distribution systems. IWA Publishing, London, for the World Health Organization, Geneva. Bartram J et al., eds (2003) Heterotrophic plate counts and drinking-water safety: The significance of HPCs for water quality and human health. London, IWA Publishing (WHO Emerging Issues in Water and Infectious Disease Series). Legionella General description The genus Legionella, a member of the family Legionellaceae, has at least 50 species comprising 70 distinct serogroups. Legionellae are Gram-negative, rod-shaped, nonspore-forming bacteria that require L-cysteine for growth and primary isolation. Legionella spp. are heterotrophic bacteria found in a wide range of water environments and can proliferate at temperatures above 25 °C. Human health effects Although all Legionella spp. are considered potentially pathogenic for humans, L. pneumophila is the major waterborne pathogen responsible for legionellosis, of which two clinical forms are known: Legionnaires’ disease and Pontiac fever. The former is a pneumonic illness with an incubation period of 3–6 days. Host factors influence the likelihood of illness: males are more frequently affected than females, and most cases occur in the 40- to 70-year age group. Risk factors include smoking, alcohol abuse, cancer, diabetes, chronic respiratory or kidney disease and immunosuppression, as in transplant recipients. Pontiac fever is a milder, self-limiting disease with a high attack rate and an onset (5 hours to 3 days) and symptoms similar to those of influenza: fever, headache, nausea, vomiting, aching muscles and coughing. Studies of seroprevalence of antibodies indicate that many infections are asymptomatic. Source and occurrence Legionella spp. are members of the natural flora of many freshwater environments, such as rivers, streams and impoundments, where they occur in relatively low numbers. However, they thrive in certain human-made water environments, such as water cooling devices (cooling towers and evaporative condensers) associated with air-conditioning systems, hot water distribution systems and spas, which provide suitable temperatures (25–50 °C) and conditions for their multiplication. Devices that support multiplication of Legionella have been associated with outbreaks of Legionnaires’ disease. Legionella survive and grow in biofilms and sediments and are more easily detected from swab samples than from flowing water. Legionellae can be ingested by trophozoites of certain amoebae such as Acanthamoeba, Hartmanella and Naegleria, which play an important role in their persistence in water environments. Routes of exposure The most common route of infection is the inhalation of aerosols containing the bacteria. Such aerosols can be generated by contaminated cooling towers, warm water showers, humidifiers and spas. Aspiration has also been identified as a route of infection in some cases associated with contaminated water, food and ice. There is no evidence of person-to-person transmission. Significance in drinking‑water Legionella spp. are common waterborne organisms, and devices such as cooling towers, hot water systems and spas that utilize mains water have been associated with outbreaks of infection. Owing to the prevalence of Legionella, the potential for ingress into drinking-water systems should be considered as a possibility, and control measures should be employed to reduce the likelihood of survival and multiplication. Disinfection strategies designed to minimize biofilm growth and temperature control can minimize the potential risk from Legionella spp. The organisms are sensitive to disinfection. Monochloramine has been shown to be particularly effective, probably due to its stability and greater effectiveness against biofilms. Water temperature is an important element of control strategies. Wherever possible, water temperatures should be kept outside the range of 25–50 °C and preferably 20–50 °C to prevent the growth of the organism. In hot water systems, temperatures leaving heaters should be above 60 °C, and temperatures above 50°C should be maintained throughout associated pipework. However, maintaining temperatures of hot water above 50 °C may represent a scalding risk in young children, the elderly and other vulnerable groups. Where temperatures in hot or cold water distribution systems cannot be maintained outside the range of 25–50 °C, greater attention to disinfection and strategies aimed at limiting development of biofilms are required. Accumulation of sludge, scale, rust, algae or slime deposits in water distribution systems supports the growth of Legionella spp., as does stagnant water. Systems that are kept clean and flowing are less likely to support excess growth of Legionella spp. Care should also be taken to select plumbing materials that do not support microbial growth and the development of biofilms. Legionella spp. represent a particular concern in devices such as cooling towers and hot water systems in large buildings. As discussed in chapter 6, specific water safety plans incorporating control measures for Legionella spp. should be developed for these buildings. Legionella are not detected by HPC techniques, and E. coli (or, alternatively, thermotolerant coliforms) is not a suitable indicator for the presence/ absence of this organism. Selected bibliography Bartram J et al., eds (2007) Legionella and the prevention of legionellosis. Geneva, World Health Organization. Codony F et al. (2002) Factors promoting colonization by legionellae in residential water distribution systems: An environmental case–control survey. European Journal of Clinical Microbiology and Infectious Diseases, 21:717–721. Emmerson AM (2001) Emerging waterborne infections in health-care settings. Emerging Infectious Diseases, 7:272–276. Rusin PA et al. (1997) Risk assessment of opportunistic bacterial pathogens in drinking-water. Reviews of Environmental Contamination and Toxicology, 152:57–83. Leptospira General description Leptospires are aerobic spirochetes that are typically 0.1 μm in diameter and 5–25 μm in length. There are two genera: Leptospira, which includes the pathogenic L. interrogans, and Leptonoma. Leptospira interrogans causes the important zoonotic and widespread disease leptospirosis. Pathogenic leptospires are maintained in host animals but, depending on conditions, can survive for days to weeks in water. More than 200 pathogenic serovars have been identified, and these have been divided into 25 serogroups based on serologic relatedness. Human health effects Leptospirosis occurs globally, affecting people living in temperate and tropical climates in both rural and urban areas. The severity of illness and the types of symptoms vary widely. Infections are often subclinical or so mild that medical attention is not sought. Symptoms include fever, headache, muscle pain, chills, redness in the eyes, abdominal pain, jaundice, haemorrhages in skin and mucous membranes (including pulmonary bleeding), vomiting, diarrhoea and rash. Pulmonary bleeding has been recognized as a dangerous and often fatal result of leptospirosis, but the way it develops after infection remains unclear. Long-lasting sequelae have been identified, including depression, headaches, fatigue and joint pains. Weil disease, characterized by jaundice, renal failure, haemorrhage and myocarditis, has been used as an alternative term for leptospirosis, but it represents a subset of the manifestations. Estimates of case fatalities vary from less than 5% to 30%, but the figures are not considered reliable owing to uncertainties over case prevalence. Fatality rates are influenced by timeliness of treatment interventions. The number of cases is not well documented as a result of lack of awareness and adequate methods of diagnosis. It has been estimated that there are about 0.1–1 cases per 100 000 persons per year in temperate climates and up to 10–100 cases per 100 000 persons per year in tropical climates. Source and occurrence Pathogenic Leptospira interrogans are maintained in the renal tubules of many animal hosts. This can take the form of chronic asymptomatic infections, with excretion persisting for very long periods and even for life. Rats, especially the brown rat (Rattus norvegicus), serve as a reservoir for Leptospira interrogans serovars Icterohaemorrhagiae and Copenhageni. Cattle are the most important reservoir for serovar Hardjo, and field mice (Microtus arvalis) and muskrats (Ondatra zibethicus) are the most important reservoirs for serovar Grippotyphosa. Recent research has shown that the house mouse (Crocidura russula) may be a reservoir for serovar Mozdok (type 3). Water contaminated with urine and tissues of infected animals is an established source of pathogenic leptospires. Leptospires have a relatively low resistance to adverse environmental conditions (e.g. low pH, desiccation, direct sunlight); in the right circumstances (neutral pH, moderate temperatures), however, they can survive for months in water. Routes of exposure Leptospira interrogans can enter the body through cuts and abrasions or via the mucous membranes of the mouth, nose and eyes. It is not transmitted by the faecal– oral route. Leptospirosis is associated with a broad range of occupational activities predominantly associated with direct contact with dead or living animals, but also indirectly via urine-contaminated environments, especially surface water, plants and mud. Ingestion of contaminated food and water or inhalation of aerosols may occasionally cause infection. Direct person-to-person transmission is rarely observed. Sexual contact, transplacental transmission and mothers’ milk are potential routes of exposure. Transmission via urine of infected patients could represent a risk to those who provide medical attention. There is an increasing trend of outbreaks associated with recreational exposure to water contaminated with urine from infected animals. Outbreaks have also been associated with natural disasters involving flooding. Significance in drinking‑water Waterborne leptospirosis is normally caused by contact with contaminated surface water. Leptospires are sensitive to disinfectants; within a water safety plan, control measures that should provide effective protection against this organism include application of standard disinfection processes for drinking-water together with protection of distribution systems from contamination associated with flooding events. Because leptospires are excreted in urine and persist in favourable environments, E. coli (or, alternatively, thermotolerant coliforms) is not a suitable indicator for the presence/ absence of this organism. Selected bibliography Bharti AR et al. (2003) Leptospirosis: A zoonotic disease of global importance. Lancet Infectious Diseases, 3:757–771. Pond K (2005) Water recreation and disease. Plausibility of associated infections: Acute effects, sequelae and mortality. IWA Publishing, London, for the World Health Organization. WHO (2003) Human leptospirosis: Guidance for diagnosis, surveillance and control. Geneva, World Health Organization. Mycobacterium General description The tuberculous or “typical” species of Mycobacterium, such as M. tuberculosis, M. bovis, M. africanum and M. leprae, have only human or animal reservoirs and are not transmitted by water. In contrast, the non-tuberculous or “atypical” species of Mycobacterium are natural inhabitants of a variety of water environments. These aerobic, rod-shaped and acid-fast bacteria grow slowly in suitable water environments and on culture media. Typical examples include the species M. gordonae, M. kansasii, M. marinum, M. scrofulaceum, M. xenopi, M. intracellulare and M. avium and the more rapid growers M. chelonae and M. fortuitum. The term M. avium complex has been used to describe a group of pathogenic species including M. avium and M. intracellulare. However, other atypical mycobacteria are also pathogenic. A distinct feature of all Mycobacterium spp. is a cell wall with high lipid content, which is used in identification of the organisms using acid-fast staining. Human health effects Atypical Mycobacterium spp. can cause a range of diseases involving the skeleton, lymph nodes, skin and soft tissues, as well as the respiratory, gastrointestinal and genitourinary tracts. Manifestations include pulmonary disease, Buruli ulcer, osteomyelitis and septic arthritis in people with no known predisposing factors. These bacteria are a major cause of disseminated infections in immunocompromised patients and are a common cause of death in HIV-positive persons. Source and occurrence Atypical Mycobacterium spp. multiply in a variety of suitable water environments, notably biofilms. One of the most commonly occurring species is M. gordonae. Other species have also been isolated from water, including M. avium, M. intracellulare, M. kansasii, M. fortuitum and M. chelonae. High numbers of atypical Mycobacterium spp. may occur in distribution systems after events that dislodge biofilms, such as flushing or flow reversals. They are relatively resistant to treatment and disinfection and have been detected in well-operated and well-maintained drinking-water supplies with HPC less than 500/ml and total chlorine residuals of up to 2.8 mg/l. The growth of these organisms in biofilms reduces the effectiveness of disinfection. In one survey, the organisms were detected in 54% of ice and 35% of public drinking-water samples. Routes of exposure Principal routes of infection appear to be inhalation, contact and ingestion of contaminated water. Infections by various species have been associated with their presence in drinking-water supplies. In 1968, an endemic of M. kansasii infections was associated with the presence of the organisms in the drinking-water supply, and the spread of the organisms was associated with aerosols from showerheads. In Rotterdam, the Netherlands, an investigation into the frequent isolation of M. kansasii from clinical specimens revealed the presence of the same strains, confirmed by phage type and weak nitrase activity, in tap water. An increase in numbers of infections by the M. avium complex in Massachusetts, USA, has also been attributed to their incidence in drinking-water. In all these cases, there is only circumstantial evidence of a causal relationship between the occurrence of the bacteria in drinking-water and human disease. Infections have been linked to contaminated water in spas. Significance in drinking‑water Detections of atypical mycobacteria in drinking-water and the identified routes of transmission suggest that drinking-water supplies are a plausible source of infection. There are limited data on the effectiveness of control measures that could be applied to reduce the potential risk from these organisms. One study showed that a water treatment plant could achieve a 99% reduction in numbers of mycobacteria from raw water. Atypical mycobacteria are relatively resistant to disinfection. Persistent residual disinfectant should reduce numbers of mycobacteria in the water column but is unlikely to be effective against organisms present in biofilms. Control measures that are designed to minimize biofilm growth, including treatment to optimize organic carbon removal, restriction of the residence time of water in distribution systems and maintenance of disinfectant residuals, could result in less growth of these organisms. Mycobacteria are not detected by HPC techniques, and E. coli (or, alternatively, thermotolerant coliforms) is not a suitable indicator for the presence/absence of this organism. Selected bibliography Bartram J et al., eds (2003) Heterotrophic plate counts and drinking-water safety: The significance of HPCs for water quality and human health. London, IWA Publishing (WHO Emerging Issues in Water and Infectious Disease Series). Bartram J et al., eds (2004) Pathogenic mycobacteria in water: A guide to public health consequences, monitoring and management. Geneva, World Health Organization. Covert TC et al. (1999) Occurrence of nontuberculous mycobacteria in environmental samples. Applied and Environmental Microbiology, 65:2492–2496. Falkinham JO, Norton CD, LeChevallier MW (2001) Factors influencing numbers of Mycobacterium avium, Mycobacterium intracellulare and other mycobacteria in drinking water distribution systems. Applied and Environmental Microbiology, 66:1225–1231. Grabow WOK (1996) Waterborne diseases: Update on water quality assessment and control. Water SA, 22:193–202. Rusin PA et al. (1997) Risk assessment of opportunistic bacterial pathogens in drinking-water. Reviews of Environmental Contamination and Toxicology, 152:57–83. Singh N, Yu VL (1994) Potable water and Mycobacterium avium complex in HIV patients: Is prevention possible? Lancet, 343:1110–1111. Von Reyn CF et al. (1994) Persistent colonization of potable water as a source of Mycobacterium avium infection in AIDS. Lancet, 343:1137–1141. Pseudomonas aeruginosa General description Pseudomonas aeruginosa is a member of the family Pseudomonadaceae and is a polarly flagellated, aerobic, Gram-negative rod. When grown in suitable media, it produces the non-fluorescent bluish pigment pyocyanin. Many strains also produce the fluorescent green pigment pyoverdin. Pseudomonas aeruginosa, like other fluorescent pseudomonads, produces catalase, oxidase and ammonia from arginine and can grow on citrate as the sole source of carbon. Human health effects Pseudomonas aeruginosa can cause a range of infections but rarely causes serious illness in healthy individuals without some predisposing factor. It predominantly colonizes damaged sites such as burn and surgical wounds, the respiratory tract of people with underlying disease and physically damaged eyes. From these sites, it may invade the body, causing destructive lesions or septicaemia and meningitis. Cystic fibrosis and immunocompromised patients are prone to colonization with P. aeruginosa, which may lead to serious progressive pulmonary infections. Water-related folliculitis and ear infections are associated with warm, moist environments such as swimming pools and spas. Many strains are resistant to a range of antimicrobial agents, which can increase the significance of the organism in hospital settings. Source and occurrence Pseudomonas aeruginosa is a common environmental organism and can be found in faeces, soil, water and sewage. It can multiply in water environments and also on the surface of suitable organic materials in contact with water. Pseudomonas aeruginosa is a recognized cause of hospital-acquired infections with potentially serious complications. It has been isolated from a range of moist environments such as sinks, water baths, hot water systems, showers and spa pools. Routes of exposure The main route of infection is by exposure of susceptible tissue, notably wounds and mucous membranes, to contaminated water or contamination of surgical instruments. Cleaning of contact lenses with contaminated water can cause a form of keratitis. Ingestion of drinking-water is not an important source of infection. Significance in drinking‑water Although P. aeruginosa can be significant in certain settings such as health-care facilities, there is no evidence that normal uses of drinking-water supplies are a source of infection in the general population. However, the presence of high numbers of P. aeruginosa in potable water, notably in packaged water, can be associated with complaints about taste, odour and turbidity. Pseudomonas aeruginosa is sensitive to disinfection, and entry into distribution systems can be minimized by adequate disinfection. Control measures that are designed to minimize biofilm growth, including treatment to optimize organic carbon removal, restriction of the residence time of water in distribution systems and maintenance of disinfectant residuals, should reduce the growth of these organisms. Pseudomonas aeruginosa is detected by HPC, which can be used together with parameters such as disinfectant residuals to indicate conditions that could support growth of these organisms. However, as P. aeruginosa is a common environmental organism, E. coli (or, alternatively, thermotolerant coliforms) cannot be used for this purpose. Selected bibliography Bartram J et al., eds (2003) Heterotrophic plate counts and drinking-water safety: The significance of HPCs for water quality and human health. London, IWA Publishing (WHO Emerging Issues in Water and Infectious Disease Series). De Victorica J, Galván M (2001) Pseudomonas aeruginosa as an indicator of health risk in water for human consumption. Water Science and Technology, 43:49–52. Hardalo C, Edberg SC (1997) Pseudomonas aeruginosa: Assessment of risk from drinking-water. Critical Reviews in Microbiology, 23:47–75. Salmonella General description Salmonella spp. belong to the family Enterobacteriaceae. They are motile, Gram-negative bacilli that do not ferment lactose, but most produce hydrogen sulfide or gas from carbohydrate fermentation. Originally, they were grouped into more than 2000 species (serotypes) according to their somatic (O) and flagellar (H) antigens (Kauffmann-White classification). There has been much debate about nomenclature and taxonomy of Salmonella, but it is now considered that there are actually two species (Salmonella enterica and Salmonella bongori). Other previously named species, including S. Typhi and S. Paratyphi, are considered to be serovars. Human health effects Salmonella infections typically cause four clinical manifestations: gastroenteritis (ranging from mild to fulminant diarrhoea, nausea and vomiting), bacteraemia or septicaemia (high spiking fever with positive blood cultures), typhoid fever/enteric fever (sustained fever with or without diarrhoea) and a carrier state in persons with previous infections. With regard to enteric illness, Salmonella spp. can be divided into two fairly distinct groups: the typhoidal species/serovars (S. Typhi and S. Paratyphi) and the remaining non-typhoidal species/serovars. Symptoms of non-typhoidal gastroenteritis appear from 6 to 72 hours after ingestion of contaminated food or water. Diarrhoea lasts 3–5 days and is accompanied by fever and abdominal pain. Usually the disease is self-limiting. The incubation period for typhoid fever can be 1–14 days but is usually 3–5 days. Typhoid fever is a more severe illness and can be fatal. Although typhoid is uncommon in areas with good sanitary systems, it is still prevalent elsewhere, and there are many millions of cases each year. Source and occurrence Salmonella spp. are widely distributed in the environment, but some species or serovars show host specificity. Notably, S. Typhi and generally S. Paratyphi are restricted to humans, although livestock can occasionally be a source of S. Paratyphi. A large number of serovars, including S. Typhimurium and S. Enteritidis, infect humans and also a wide range of animals, including poultry, cows, pigs, sheep, birds and even reptiles. The pathogens typically gain entry into water systems through faecal contamination from sewage discharges, livestock and wild animals. Contamination has been detected in a wide variety of foods and milk. Routes of exposure Salmonella is spread by the faecal–oral route. Infections with non-typhoidal serovars are primarily associated with person-to-person contact, the consumption of a variety of contaminated foods and exposure to animals. Infection by typhoid species is associated with the consumption of contaminated water or food, with direct person-toperson spread being uncommon. Significance in drinking‑water Waterborne typhoid fever outbreaks have devastating public health implications. However, despite their widespread occurrence, non-typhoidal Salmonella spp. rarely cause drinking-water-borne outbreaks. Transmission, most commonly involving S. Typhimurium, has been associated with the consumption of contaminated groundwater and surface water supplies. In an outbreak of illness associated with a communal rainwater supply, bird faeces were implicated as a source of contamination. Salmonella spp. are relatively sensitive to disinfection. Within a water safety plan, control measures that can be applied to manage risk include protection of raw water supplies from human and animal waste, adequate treatment and protection of water during distribution. Escherichia coli (or, alternatively, thermotolerant coliforms) is a generally reliable indicator for Salmonella spp. in drinking-water supplies. Selected bibliography Angulo FJ et al. (1997) A community waterborne outbreak of salmonellosis and the effectiveness of a boil water order. American Journal of Public Health, 87:580–584. Escartin EF et al. (2002) Potential Salmonella transmission from ornamental fountains. Journal of Environmental Health, 65:9–12. Koplan JP et al. (1978) Contaminated roof-collected rainwater as a possible cause of an outbreak of salmonellosis. Journal of Hygiene, 81:303–309. Tindall BJ et al. (2005) Nomenclature and taxonomy of the genus Salmonella. International Journal of Systematic and Evolutionary Microbiology, 5:521–524. Shigella General description Shigella spp. are Gram-negative, non-spore-forming, non-motile, rod-like members of the family Enterobacteriaceae, which grow in the presence or absence of oxygen. Members of the genus have a complex antigenic pattern, and classification is based on their somatic O antigens, many of which are shared with other enteric bacilli, including E. coli. There are four species: S. dysenteriae, S. flexneri, S. boydii and S. sonnei. Human health effects Shigella spp. can cause serious intestinal diseases, including bacillary dysentery. Over 2 million infections occur each year, resulting in about 600 000 deaths, predominantly in developing countries. Most cases of Shigella infection occur in children under 10 years of age. The incubation period for shigellosis is usually 24–72 hours. Ingestion of as few as 10–100 organisms may lead to infection, which is substantially less than the infective dose of most other enteric bacteria. Abdominal cramps, fever and watery diarrhoea occur early in the disease. All species can produce severe disease, but illness due to S. sonnei is usually relatively mild and self-limiting. In the case of S. dysenteriae, clinical manifestations may proceed to an ulceration process, with bloody diarrhoea and high concentrations of neutrophils in the stool. The production of Shiga toxin by the pathogen plays an important role in this outcome. Shigella spp. seem to be better adapted to cause human disease than most other enteric bacterial pathogens. Source and occurrence Humans and other higher primates appear to be the only natural hosts for the shigellae. The bacteria remain localized in the intestinal epithelial cells of their hosts. Epidemics of shigellosis occur in crowded communities and where hygiene is poor. Many cases of shigellosis are associated with day-care centres, prisons and psychiatric institutions. Military field groups and travellers to areas with poor sanitation are also prone to infection. Routes of exposure Shigella spp. are enteric pathogens predominantly transmitted by the faecal–oral route through person-to-person contact, contaminated food and water. Flies have also been identified as a transmission vector from contaminated faecal waste. Significance in drinking‑water A number of large waterborne outbreaks of shigellosis have been recorded. As the organisms are not particularly stable in water environments, their presence in drinking-water indicates recent human faecal pollution. Available data on prevalence in water supplies may be an underestimate, because detection techniques generally used can have a relatively low sensitivity and reliability. The control of Shigella spp. in drinking-water supplies is of special public health importance in view of the severity of the disease caused. Shigella spp. are relatively sensitive to disinfection. Within a water safety plan, control measures that can be applied to manage potential risk include protection of raw water supplies from human waste, adequate treatment and protection of water during distribution. Escherichia coli (or, alternatively, thermotolerant coliforms) is a generally reliable indicator for Shigella spp. in drinking-water supplies. Selected bibliography Alamanos Y et al. (2000) A community waterborne outbreak of gastro-enteritis attributed to Shigella sonnei. Epidemiology and Infection, 125:499–503. Pegram GC, Rollins N, Espay Q (1998) Estimating the cost of diarrhoea and epidemic dysentery in Kwa-Zulu-Natal and South Africa. Water SA, 24:11–20. Staphylococcus aureus General description Staphylococcus aureus is an aerobic or anaerobic, non-motile, non-spore-forming, catalase- and coagulase-positive, Gram-positive coccus, usually arranged in grapelike irregular clusters. The genus Staphylococcus contains at least 15 different species. Apart from S. aureus, the species S. epidermidis and S. saprophyticus are also associated with disease in humans. Human health effects Although Staphylococcus aureus is a common member of the human microflora, it can produce disease through two different mechanisms. One is based on the ability of the organisms to multiply and spread widely in tissues, and the other is based on the ability of the organisms to produce extracellular enzymes and toxins. Infections based on the multiplication of the organisms are a significant problem in hospitals and other health-care facilities. Multiplication in tissues can result in manifestations such as boils, skin sepsis, post-operative wound infections, enteric infections, septicaemia, endocarditis, osteomyelitis and pneumonia. The onset of clinical symptoms for these infections is relatively long, usually several days. Gastrointestinal disease (enterocolitis or food poisoning) is caused by a heat-stable staphylococcal enterotoxin and characterized by projectile vomiting, diarrhoea, fever, abdominal cramps, electrolyte imbalance and loss of fluids. Onset of disease in this case has a characteristic short incubation period of 1–8 hours. The same applies to the toxic shock syndrome caused by toxic shock syndrome toxin-1. Source and occurrence Staphylococcus aureus is relatively widespread in the environment but is found mainly on the skin and mucous membranes of animals. The organism is a member of the normal microbial flora of the human skin and is found in the nasopharynx of 20–30% of adults at any one time. Staphylococci are occasionally detected in the gastrointestinal tract and can be detected in sewage. Staphylococcus aureus can be released by human contact into water environments such as swimming pools, spa pools and other recreational waters. It has also been detected in drinking-water supplies. Routes of exposure Hand contact is by far the most common route of transmission. Inadequate hygiene can lead to contamination of food. Foods such as ham, poultry and potato and egg salads kept at room or higher temperature offer an ideal environment for the multiplication of S. aureus and the release of toxins. The consumption of foods containing S. aureus toxins can lead to enterotoxin food poisoning within a few hours. Significance in drinking‑water Although S. aureus can occur in drinking-water supplies, there is no evidence of transmission through the consumption of such water. Although staphylococci are slightly more resistant to chlorine residuals than E. coli, their presence in water is readily controlled by conventional treatment and disinfection processes. As faecal material is not their usual source, E. coli (or, alternatively, thermotolerant coliforms) is not a suitable indicator for S. aureus in drinking-water supplies. Selected bibliography Antai SP (1987) Incidence of Staphylococcus aureus, coliforms and antibiotic-resistant strains of Escherichia coli in rural water supplies in Port Harcourt. Journal of Applied Bacteriology, 62:371–375. LeChevallier MW, Seidler RJ (1980) Staphylococcus aureus in rural drinking-water. Applied and Environmental Microbiology, 39:739–742. Tsukamurella General description The genus Tsukamurella belongs to the family Nocardiaceae. Tsukamurella spp. are Gram-positive, weakly or variably acid-fast, non-motile, obligate aerobic, irregular rod-shaped bacteria. They are actinomycetes related to Rhodococcus, Nocardia and Mycobacterium. The genus was created in 1988 to accommodate a group of chemically unique organisms characterized by a series of very long chain (68–76 carbons), highly unsaturated mycolic acids, meso-diaminopimelic acid and arabinogalactan, common to the genus Corynebacterium. The type species is T. paurometabola, and the following additional species were proposed in the 1990s: T. wratislaviensis, T. inchonensis, T. pulmonis, T. tyrosinosolvens and T. strandjordae. Human health effects Tsukamurella spp. cause disease mainly in immunocompromised individuals. Infections with these microorganisms have been associated with chronic lung diseases, immune suppression (leukaemia, tumours, HIV/AIDS infection) and post-operative wound infections. Tsukamurella were reported in four cases of catheter-related bacteraemia and in individual cases including chronic lung infection, necrotizing tenosynovitis with subcutaneous abscesses, cutaneous and bone infections, meningitis and peritonitis. Source and occurrence Tsukamurella spp. exist primarily as environmental saprophytes in soil, water and foam (thick stable scum on aeration vessels and sedimentation tanks) of activated sludge. Tsukamurella are represented in HPC populations in drinking-water. Routes of exposure Tsukamurella spp. appear to be transmitted through devices such as catheters or lesions. The original source of the contaminating organisms is unknown. Significance in drinking‑water Tsukamurella organisms have been detected in drinking-water supplies, but the significance is unclear. There is no evidence of a link between organisms in water and illness. As Tsukamurella is an environmental organism, E. coli (or, alternatively, thermotolerant coliforms) is not a suitable indicator for this organism. Selected bibliography Bartram J et al., eds (2003) Heterotrophic plate counts and drinking-water safety: The significance of HPCs for water quality and human health. London, IWA Publishing (WHO Emerging Issues in Water and Infectious Disease Series). Kattar MM et al. (2001) Tsukamurella strandjordae sp. nov., a proposed new species causing sepsis. Journal of Clinical Microbiology, 39:1467–1476. Larkin JA et al. (1999) Infection of a knee prosthesis with Tsukamurella species. Southern Medical Journal, 92:831–832. Vibrio General description Vibrio spp. are small, curved (comma-shaped), Gram-negative bacteria with a single polar flagellum. Species are typed according to their O antigens. There are a number of pathogenic species, including V. cholerae, V. parahaemolyticus and V. vulnificus. Vibrio cholerae is the only pathogenic species of significance from freshwater environments. Although a number of serotypes can cause diarrhoea, only O1 and O139 currently cause the classical cholera symptoms in which a proportion of cases suffer fulminating and severe watery diarrhoea. The O1 serovar has been further divided into “classical” and “El Tor” biotypes. The latter is distinguished by features such as the ability to produce a dialysable heat-labile haemolysin, active against sheep and goat red blood cells. The classical biotype is considered responsible for the first six cholera pandemics, whereas the El Tor biotype is responsible for the seventh pandemic that commenced in 1961. Strains of V. cholerae O1 and O139 that cause cholera produce an enterotoxin (cholera toxin) that alters the ionic fluxes across the intestinal mucosa, resulting in substantial loss of water and electrolytes in liquid stools. Other factors associated with infection are an adhesion factor and an attachment pilus. Not all strains of serotypes O1 or O139 possess the virulence factors, and they are rarely possessed by non-O1/ O139 strains. Human health effects Cholera outbreaks continue to occur in many areas of the developing world. Symptoms are caused by heat-labile cholera enterotoxin carried by toxigenic strains of V. cholerae O1/O139. A large percentage of infected persons do not develop illness; about 60% of the classical and 75% of the El Tor group infections are asymptomatic. Symptomatic illness ranges from mild or moderate to severe disease. The initial symptoms of cholera are an increase in peristalses followed by loose, watery and mucus-flecked “rice-water” stools that may cause a patient to lose as much as 10–15 litres of liquid per day. Decreasing gastric acidity by administration of sodium bicarbonate reduces the infective dose of V. cholerae O1 from more than 108 to about 104 organisms. Case fatality rates vary according to facilities and preparedness. As many as 60% of untreated patients may die as a result of severe dehydration and loss of electrolytes, but well-established diarrhoeal disease control programmes can reduce fatalities to less than 1%. Non-toxigenic strains of V. cholerae can cause self-limiting gastroenteritis, wound infections and bacteraemia. Source and occurrence Non-toxigenic V. cholerae is widely distributed in water environments, but toxigenic strains are not distributed as widely. Humans are an established source of toxigenic V. cholerae; in the presence of disease, the organism can be detected in sewage. Although V. cholerae O1 can be isolated from water in areas without disease, the strains are not generally toxigenic. Toxigenic V. cholerae has also been found in association with live copepods as well as other aquatic organisms, including molluscs, crustaceans, plants, algae and cyanobacteria. Numbers associated with these aquatic organisms are often higher than in the water. Non-toxigenic V. cholerae has been isolated from birds and herbivores in areas far away from marine and coastal waters. The prevalence of V. cholerae decreases as water temperatures fall below 20 °C. Routes of exposure Cholera is typically transmitted by the faecal–oral route, and the infection is predominantly contracted by the ingestion of faecally contaminated water and food. The high numbers required to cause infection make person-to-person contact an unlikely route of transmission. Significance in drinking‑water Contamination of water due to poor sanitation is largely responsible for transmission, but this does not fully explain the seasonality of recurrence, and factors other than poor sanitation must play a role. The presence of the pathogenic V. cholerae O1 and O139 serotypes in drinking-water supplies is of major public health importance and can have serious health and economic implications in the affected communities. Vibrio cholerae is highly sensitive to disinfection processes. Within a water safety plan, control measures that can be applied to manage potential risk from toxigenic V. cholerae include protection of raw water supplies from human waste, adequate treatment and protection of water during distribution. Vibrio cholerae O1 and non-O1 have been detected in the absence of E. coli, and this organism (or, alternatively, thermotolerant coliforms) is not a reliable indicator for V. cholerae in drinking-water. Selected bibliography Kaper JB, Morris JG, Levine MM (1995) Cholera. Clinical Microbiology Reviews, 8:48–86. Ogg JE, Ryder RA, Smith HL (1989) Isolation of Vibrio cholerae from aquatic birds in Colorado and Utah. Applied and Environmental Microbiology, 55:95–99. Rhodes JB, Schweitzer D, Ogg JE (1985) Isolation of non-O1 Vibrio cholerae associated with enteric disease of herbivores in western Colorado. Journal of Clinical Microbiology, 22:572– 575. WHO (2002) Vibrio cholerae. In: Guidelines for drinking-water quality, 2nd ed. Addendum: Microbiological agents in drinking water. Geneva, World Health Organization, pp. 119–142. Yersinia General description The genus Yersinia is classified in the family Enterobacteriaceae and comprises seven species. The species Y. pestis, Y. pseudotuberculosis and certain serotypes of Y. enterocolitica are pathogens for humans. Yersinia pestis is the cause of bubonic plague through contact with rodents and their fleas. Yersinia spp. are Gram-negative rods that are motile at 25 °C but not at 37 °C. Human health effects Yersinia enterocolitica penetrates cells of the intestinal mucosa, causing ulcerations of the terminal ilium. Yersiniosis generally presents as an acute gastroenteritis with diarrhoea, fever and abdominal pain. Other clinical manifestations include greatly enlarged painful lymph nodes referred to as “buboes”. The disease seems to be more acute in children than in adults. Source and occurrence Domestic and wild animals are the principal reservoir for Yersinia spp.; pigs are the major reservoir of pathogenic Y. enterocolitica, whereas rodents and small animals are the major reservoir of Y. pseudotuberculosis. Pathogenic Y. enterocolitica has been detected in sewage and polluted surface waters. However, Y. enterocolitica strains detected in drinking-water are more commonly non-pathogenic strains of probable environmental origin. At least some species and strains of Yersinia seem to be able to replicate in water environments if at least trace amounts of organic nitrogen are present, even at temperatures as low as 4 °C. Routes of exposure Yersinia spp. are transmitted by the faecal–oral route, with the major source of infection considered to be foods, particularly meat and meat products, milk and dairy products. Ingestion of contaminated water is also a potential source of infection. Direct transmission from person to person and from animals to humans is also known to occur. Significance in drinking‑water Although most Yersinia spp. detected in water are probably non-pathogenic, circumstantial evidence has been presented to support transmission of Y. enterocolitica and Y. pseudotuberculosis to humans from untreated drinking-water. The most likely source of pathogenic Yersinia spp. is human or animal waste. The organisms are sensitive to disinfection processes. Within a water safety plan, control measures that can be used to minimize the presence of pathogenic Yersinia spp. in drinking-water supplies include protection of raw water supplies from human and animal waste, adequate disinfection and protection of water during distribution. Owing to the long survival and/ or growth of some strains of Yersinia spp. in water, E. coli (or, alternatively, thermotolerant coliforms) is not a suitable indicator for the presence/absence of these organisms in drinking-water. Selected bibliography Aleksic S, Bockemuhl J (1988) Serological and biochemical characteristics of 416 Yersinia strains from well water and drinking water plants in the Federal Republic of Germany: Lack of evidence that these strains are of public health significance. Zentralblatt für Bakteriologie, Mikrobiologie und Hygiene B, 185:527–533. Inoue M et al. (1988) Three outbreaks of Yersinia pseudotuberculosis infection. Zentralblatt für Bakteriologie, Mikrobiologie und Hygiene B, 186:504–511. Ostroff SM et al. (1994) Sources of sporadic Yersinia enterocolitica infections in Norway: A prospective case control study. Epidemiology and Infection, 112:133–141. Waage AS et al. (1999) Detection of low numbers of pathogenic Yersinia enterocolitica in environmental water and sewage samples by nested polymerase chain reaction. Journal of Applied Microbiology, 87:814–821. 11.2 Viral pathogens Viruses associated with waterborne transmission are predominantly those that can infect the gastrointestinal tract and are excreted in the faeces of infected humans (enteric viruses). With the exception of hepatitis E virus, humans are considered to be the only source of human infectious species. Enteric viruses typically cause acute disease with a short incubation period. Water may also play a role in the transmission of other viruses with different modes of action. As a group, viruses can cause a wide variety of infections and symptoms involving different routes of transmission, routes and sites of infection and routes of excretion. The combination of these routes and sites of infection can vary and will not always follow expected patterns. For example, viruses that are considered to primarily cause respiratory infections and symptoms are usually transmitted by person-to-person spread of respiratory droplets. However, some of these respiratory viruses may be discharged in faeces, leading to potential contamination of water and subsequent transmission through aerosols and droplets. Another example is viruses excreted in urine, such as polyomaviruses, which could contaminate and then be potentially transmitted by water, with possible long-term health effects, such as cancer, that are not readily associated epidemiologically with waterborne transmission. Adenoviruses General description The family Adenoviridae is classified into the two genera Mastadenovirus (mammal hosts) and Aviadenovirus (avian hosts). Adenoviruses are widespread in nature, infecting birds, mammals and amphibians. To date, 51 antigenic types of human adenoviruses have been described. Human adenoviruses have been classified into six groups (A–F) on the basis of their physical, chemical and biological properties. Adenoviruses consist of a double-stranded DNA genome in a non-enveloped icosahedral capsid with a diameter of about 80 nm and unique fibres. The subgroups A–E grow readily in cell culture, but serotypes 40 and 41 are fastidious and do not grow well. Identification of serotypes 40 and 41 in environmental samples is generally based on polymerase chain reaction (PCR) techniques with or without initial cell culture amplification. Human health effects Human adenoviruses cause a wide range of infections with a spectrum of clinical manifestations. These include infections of the gastrointestinal tract (gastroenteritis), the respiratory tract (acute respiratory diseases, pneumonia, pharyngoconjunctival fever), the urinary tract (cervicitis, urethritis, haemorrhagic cystitis) and the eyes (epidemic keratoconjunctivitis, also known as “shipyard eye”; pharyngoconjunctival fever, also known as “swimming pool conjunctivitis”). Different serotypes are associated with specific illnesses; for example, types 40 and 41 are the main causes of enteric illness. Adenoviruses are an important source of childhood gastroenteritis. In general, infants and children are most susceptible to adenovirus infections, and many infections are asymptomatic. High attack rates in outbreaks imply that infecting doses are low. Source and occurrence Adenoviruses are excreted in large numbers in human faeces and are known to occur in sewage, raw water sources and treated drinking-water supplies worldwide. Although the subgroup of enteric adenoviruses (mainly types 40 and 41) is a major cause of gastroenteritis worldwide, notably in developing communities, little is known about the prevalence of these enteric adenoviruses in water sources. The limited availability of information on enteric adenoviruses is largely due to the fact that they are not detectable by conventional cell culture isolation. Routes of exposure Owing to the diverse epidemiology of the wide spectrum of human adenoviruses, exposure and infection are possible by a variety of routes. Person-to-person contact plays a major role in the transmission of illness; depending on the nature of the illness, this can include faecal–oral, oral–oral and hand–eye contact transmission, as well as indirect transfer through contaminated surfaces or shared utensils. There have been numerous outbreaks associated with hospitals, military establishments, childcare centres and schools. Symptoms recorded in most outbreaks were acute respiratory disease, keratoconjunctivitis and conjunctivitis. Outbreaks of gastroenteritis have also been reported. The consumption of contaminated food or water may be an important source of enteric illness, although there is no substantial evidence supporting this route of transmission. Eye infections may be contracted by the exposure of eyes to contaminated water, the sharing of towels at swimming pools or the sharing of goggles, as in the case of “shipyard eye”. Confirmed outbreaks of adenovirus infections associated with water have been limited to pharyngitis or conjunctivitis, with exposure arising from use of swimming pools. Significance in drinking‑water Human adenoviruses have been shown to occur in substantial numbers in raw water sources and treated drinking-water supplies. In one study, the incidence of human adenoviruses in such waters was exceeded only by the group of enteroviruses among viruses detectable by PCR-based techniques. In view of their prevalence as an enteric pathogen and detection in water, contaminated drinking-water represents a likely but unconfirmed source of human adenovirus infections. Human adenoviruses are also considered important because they are exceptionally resistant to some water treatment and disinfection processes, notably ultraviolet (UV) light irradiation. Human adenoviruses have been detected in drinking-water supplies that met accepted specifications for treatment, disinfection and conventional indicator organisms. Within a water safety plan, control measures to reduce potential risk from human adenoviruses should focus on prevention of source water contamination by human waste, followed by adequate treatment and disinfection. The effectiveness of treatment processes used to remove human adenoviruses will require validation. Drinking-water supplies should also be protected from contamination during distribution. Because of the high resistance of the viruses to disinfection, E. coli (or, alternatively, thermotolerant coliforms) is not a reliable indicator of the presence/absence of human adenoviruses in drinking-water supplies. Selected bibliography Chapron CD et al. (2000) Detection of astroviruses, enteroviruses and adenoviruses types 40 and 41 in surface waters collected and evaluated by the information collection rule and integrated cell culture–nested PCR procedure. Applied and Environmental Microbiology, 66:2520–2525. D’Angelo LJ et al. (1979) Pharyngoconjunctival fever caused by adenovirus type 4: Report of a swimming pool–related outbreak with recovery of virus from pool water. Journal of Infectious Diseases, 140:42–47. Grabow WOK, Taylor MB, de Villiers JC (2001) New methods for the detection of viruses: Call for review of drinking water quality guidelines. Water Science and Technology, 43:1–8. Puig M et al. (1994) Detection of adenoviruses and enteroviruses in polluted water by nested PCR amplification. Applied and Environmental Microbiology, 60:2963–2970. Astroviruses General description Human and animal strains of astroviruses are single-stranded ribonucleic acid (RNA) viruses classified in the family Astroviridae. Astroviruses consist of a single-stranded RNA genome in a non-enveloped icosahedral capsid with a diameter of about 28 nm. In a proportion of the particles, a distinct surface star-shaped structure can be seen by electron microscopy. Eight different serotypes of human astroviruses have been described. The most commonly identified is human astrovirus serotype 1. Human astroviruses can be detected in environmental samples using PCR techniques with or without initial cell culture amplification. Human health effects Human astroviruses cause gastroenteritis, predominantly diarrhoea, mainly in children under 5 years of age, although it has also been reported in adults. Seroprevalence studies showed that more than 80% of children between 5 and 10 years of age have antibodies against human astroviruses. Occasional outbreaks in schools, nurseries and families have been reported. The illness is self-limiting, is of short duration and has a peak incidence in the winter. Human astroviruses are the cause of only a small proportion of reported gastroenteritis infections. However, the number of infections may be underestimated, as the illness is usually mild, and many cases will go unreported. Source and occurrence Infected individuals generally excrete large numbers of human astroviruses in faeces; hence, the viruses will be present in sewage. Human astroviruses have been detected in water sources and in drinking-water supplies. Routes of exposure Human astroviruses are transmitted by the faecal–oral route. Person-to-person spread is considered the most common route of transmission, and clusters of cases are seen in child-care centres, paediatric wards, families, homes for the elderly and military establishments. Ingestion of contaminated food or water could also be important. Significance in drinking‑water The presence of human astroviruses in treated drinking-water supplies has been confirmed. As the viruses are typically transmitted by the faecal–oral route, transmission by drinking-water seems likely, but has not been confirmed. Human astroviruses have been detected in drinking-water supplies that met accepted specifications for treatment, disinfection and conventional indicator organisms. Within a water safety plan, control measures to reduce potential risk from human astroviruses should focus on prevention of source water contamination by human waste, followed by adequate treatment and disinfection. The effectiveness of treatment processes used to remove human astroviruses will require validation. Drinking-water supplies should also be protected from contamination during distribution. Owing to the higher resistance of the viruses to disinfection, E. coli (or, alternatively, thermotolerant coliforms) is not a reliable indicator of the presence/absence of human astroviruses in drinking-water supplies. Selected bibliography Grabow WOK, Taylor MB, de Villiers JC (2001) New methods for the detection of viruses: Call for review of drinking water quality guidelines. Water Science and Technology, 43:1–8. Nadan S et al. (2003) Molecular characterization of astroviruses by reverse transcriptase PCR and sequence analysis: Comparison of clinical and environmental isolates from South Africa. Applied and Environmental Microbiology, 69:747–753. Pintó RM et al. (2001) Astrovirus detection in wastewater. Water Science and Technology, 43: 73–77. Caliciviruses General description The family Caliciviridae consists of four genera of single-stranded RNA viruses with a non-enveloped capsid (diameter 35–40 nm), which generally displays a typical surface morphology resembling cup-like structures. Human caliciviruses include the genera Norovirus (Norwalk-like viruses) and Sapovirus (Sapporo-like viruses). Sapovirus spp. demonstrate the typical calicivirus morphology and are called classical caliciviruses. Noroviruses generally fail to reveal the typical morphology and were in the past referred to as small round-structured viruses. The remaining two genera of the family contain viruses that infect animals other than humans. Human caliciviruses cannot be propagated in available cell culture systems. The viruses were originally discovered by electron microscopy. Some Norovirus spp. can be detected by enzyme-linked immunosorbent assay using antibodies raised against baculovirus-expressed Norovirus capsid proteins. Several reverse transcriptase PCR procedures have been described for the detection of human caliciviruses. Human health effects Human caliciviruses are a major cause of acute viral gastroenteritis in all age groups. Symptoms include nausea, vomiting and abdominal cramps. Usually about 40% of infected individuals present with diarrhoea; some have fever, chills, headache and muscular pain. As some cases present with vomiting only and no diarrhoea, the condition is also known as “winter vomiting disease”. Infections by human caliciviruses induce a short-lived immunity. The symptoms are usually relatively mild and rarely last for more than 3 days. High attack rates in outbreaks indicate that the infecting dose is low. Source and occurrence Human caliciviruses are excreted in faeces of infected individuals and will therefore be present in domestic wastewaters as well as faecally contaminated food and water, including drinking-water supplies. Routes of exposure The epidemiology of the disease indicates that person-to-person contact and the inhalation of contaminated aerosols and dust particles, as well as airborne particles of vomitus, are the most common routes of transmission. Drinking-water and a wide variety of foods contaminated with human faeces have been confirmed as major sources of exposure. Numerous outbreaks have been associated with contaminated drinking-water, ice, water on cruise ships and recreational waters. Shellfish harvested from sewage-contaminated waters have also been identified as a source of outbreaks. Significance in drinking‑water Many human calicivirus outbreaks have been epidemiologically linked to contaminated drinking-water supplies. Within a water safety plan, control measures to reduce potential risk from human caliciviruses should focus on prevention of source water contamination by human waste, followed by adequate treatment and disinfection. The effectiveness of treatment processes used to remove human caliciviruses will require validation. Drinking-water supplies should also be protected from contamination during distribution. Owing to the higher resistance of the viruses to disinfection, E. coli (or, alternatively, thermotolerant coliforms) is not a reliable indicator of the presence/ absence of human caliciviruses in drinking-water supplies. Selected bibliography Berke T et al. (1997) Phylogenetic analysis of the Caliciviridae. Journal of Medical Virology, 52:419–424. Jiang X et al. (1999) Design and evaluation of a primer pair that detects both Norwalk- and Sapporo-like caliciviruses by RT-PCR. Journal of Virological Methods, 83:145–154. Mauer AM, Sturchler DA (2000) A waterborne outbreak of small round-structured virus, Campylobacter and Shigella co-infections in La Neuveville, Switzerland, 1998. Epidemiology and Infection, 125:325–332. Monroe SS, Ando T, Glass R (2000) Introduction: Human enteric caliciviruses—An emerging pathogen whose time has come. Journal of Infectious Diseases, 181(Suppl. 2):S249–251. Enteroviruses General description The genus Enterovirus is a member of the family Picornaviridae. This genus consists of 69 serotypes (species) that infect humans: poliovirus types 1–3, coxsackievirus types A1–A24, coxsackievirus types B1–B6, echovirus types 1–33 and the numbered enterovirus types EV68–EV73. Members of the genus are collectively referred to as enteroviruses. Other species of the genus infect animals other than humans—for instance, the bovine group of enteroviruses. Enteroviruses are among the smallest known viruses and consist of a single-stranded RNA genome in a non-enveloped icosahedral capsid with a diameter of 20–30 nm. Some members of the genus, notably poliovirus, coxsackievirus B, echovirus and enterovirus, are readily isolated by cytopathogenic effect in cell cultures. Human health effects Enteroviruses are one of the most common causes of human infections. They have been estimated to cause about 30 million infections in the USA each year. The spectrum of diseases caused by enteroviruses is broad and ranges from a mild febrile illness to myocarditis, meningoencephalitis, poliomyelitis, herpangina, hand-footand-mouth disease and neonatal multi-organ failure. The persistence of the viruses in chronic conditions such as polymyositis, dilated cardiomyopathy and chronic fatigue syndrome has been described. Most infections, particularly in children, are asymptomatic, but still lead to the excretion of large numbers of the viruses, which may cause clinical disease in other individuals. Source and occurrence Enteroviruses are excreted in the faeces of infected individuals. Among the types of viruses detectable by conventional cell culture isolation, enteroviruses are generally the most numerous in sewage, water resources and treated drinking-water supplies. The viruses are also readily detected in many foods. Routes of exposure Person-to-person contact and inhalation of airborne viruses or viruses in respiratory droplets are considered to be the predominant routes of transmission of enteroviruses in communities. Transmission from drinking-water could also be important, but this has not yet been confirmed. Waterborne transmission of enteroviruses (coxsackievirus A16 and B5) has been epidemiologically confirmed for only two outbreaks, and these were associated with children bathing in lake water in the 1970s. Significance in drinking‑water Enteroviruses have been shown to occur in substantial numbers in raw water sources and treated drinking-water supplies. In view of their prevalence, drinking-water represents a likely, although unconfirmed, source of enterovirus infection. The limited knowledge on the role of waterborne transmission could be related to a number of factors, including the wide range of clinical symptoms, frequent asymptomatic infection, the diversity of serotypes and the dominance of person-to-person spread. Enteroviruses have been detected in drinking-water supplies that met accepted specifications for treatment, disinfection and conventional indicator organisms. Within a water safety plan, control measures to reduce potential risk from enteroviruses should focus on prevention of source water contamination by human waste, followed by adequate treatment and disinfection. The effectiveness of treatment processes used to remove enteroviruses will require validation. Drinking-water supplies should also be protected from contamination during distribution. Owing to the higher resistance of the viruses to disinfection, E. coli (or, alternatively, thermotolerant coliforms) is not a reliable indicator of the presence/absence of enteroviruses in drinking-water supplies. Selected bibliography Grabow WOK, Taylor MB, de Villiers JC (2001) New methods for the detection of viruses: Call for review of drinking water quality guidelines. Water Science and Technology, 43:1–8. Hawley HB et al. (1973) Coxsackie B epidemic at a boys’ summer camp. Journal of the American Medical Association, 226:33–36. Hepatitis A virus General description Hepatitis A virus (HAV) is the only species of the genus Hepatovirus in the family Picornaviridae. The virus shares basic structural and morphological features with other members of the family, as described for enteroviruses. Human and simian HAVs are genotypically distinguishable. HAV cannot be readily detected or cultivated in conventional cell culture systems, and identification in environmental samples is based on the use of PCR techniques. Human health effects HAV is highly infectious, and the infecting dose is considered to be low. The virus causes the disease hepatitis A, commonly known as “infectious hepatitis”. Like other members of the group enteric viruses, HAV enters the gastrointestinal tract by ingestion, where it infects epithelial cells. From here, the virus enters the bloodstream and reaches the liver, where it may cause severe damage to liver cells. In as many as 90% of cases, particularly in children, there is little, if any, liver damage, and the infection passes without clinical symptoms and elicits lifelong immunity. In general, the severity of illness increases with age. The damage to liver cells results in the release of liver-specific enzymes such as aspartate aminotransferase, which are detectable in the bloodstream and used as a diagnostic tool. The damage also results in the failure of the liver to remove bilirubin from the bloodstream; the accumulation of bilirubin causes the typical symptoms of jaundice and dark urine. After a relatively long incubation period of 28–30 days on average, there is a characteristic sudden onset of illness, including symptoms such as fever, malaise, nausea, anorexia, abdominal discomfort and eventually jaundice. Although mortality is generally less than 1%, repair of the liver damage is a slow process that may keep patients incapacitated for 6 weeks or longer. This has substantial burden of disease implications. Mortality is higher in those over 50 years of age. Source and occurrence HAV occurs worldwide, but the prevalence of clinical disease has typical geographically based characteristics. HAV is excreted in faecal material of infected people, and there is strong epidemiological evidence that faecally contaminated food and water are common sources of the virus. In areas with poor sanitation, children are often infected at a very early age and become immune for life without clinical symptoms of disease. In areas with good sanitation, infection tends to occur later in life. Routes of exposure Person-to-person spread is probably the most common route of transmission, but contaminated food and water are important sources of infection. There is stronger epidemiological evidence for waterborne transmission of HAV than for any other virus. Foodborne outbreaks are also relatively common, with sources of infection including infected food handlers, shellfish harvested from contaminated water and contaminated produce. Travel of people from areas with good sanitation to those with poor sanitation provides a high risk of infection. Infection can also be spread in association with injecting and non-injecting drug use. Significance in drinking‑water The transmission of HAV by drinking-water supplies is well established, and the presence of HAV in drinking-water constitutes a substantial health risk. Within a water safety plan, control measures to reduce potential risk from HAV should focus on prevention of source water contamination by human waste, followed by adequate treatment and disinfection. The effectiveness of treatment processes used to remove HAV will require validation. Drinking-water supplies should also be protected from contamination during distribution. Owing to the higher resistance of the viruses to disinfection, E. coli (or, alternatively, thermotolerant coliforms) is not a reliable indicator of the presence/absence of HAV in drinking-water supplies. Selected bibliography Cuthbert JA (2001) Hepatitis A: Old and new. Clinical Microbiology Reviews, 14:38–58. WHO (2002) Enteric hepatitis viruses. In: Guidelines for drinking-water quality, 2nd ed. Addendum: Microbiological agents in drinking water. Geneva, World Health Organization, pp. 18–39. Hepatitis E virus General description Hepatitis E virus (HEV) consists of a single-stranded RNA genome in a non-enveloped icosahedral capsid with a diameter of 27–34 nm. HEV shares properties with a number of viruses, and classification is a challenge. At one stage, HEV was classified as a member of the family Caliciviridae, but most recently it has been placed in a separate family called hepatitis E–like viruses. There are indications of antigenic variation, and possibly even differences in serotypes of the virus, whereas human HAV consists of only one clearly defined serotype. HEV cannot be readily detected or cultivated in conventional cell culture systems, and identification in environmental samples is based on the use of PCR techniques. Human health effects HEV causes hepatitis that is in many respects similar to that caused by HAV. However, the incubation period tends to be longer (average 40 days), and infections typically have a mortality rate of up to 25% in pregnant women. In endemic regions, first infections are typically seen in young adults rather than young children. Despite evidence of antigenic variation, single infection appears to provide lifelong immunity to HEV. Global prevalence has a characteristic geographic distribution. HEV is endemic and causes clinical diseases in certain developing parts of the world, such as India, Nepal, central Asia, Mexico and parts of Africa. In many of these areas, HEV is the most important cause of viral hepatitis. Although seroprevalence can be high, clinical cases and outbreaks are rare in certain parts of the world, such as Japan, South Africa, the United Kingdom, North and South America, Australasia and central Europe. The reason for the lack of clinical cases in the presence of the virus is unknown. Source and occurrence HEV is excreted in faeces of infected people, and the virus has been detected in raw and treated sewage. Contaminated water has been associated with very large outbreaks. HEV is distinctive, in that it is the only enteric virus with a meaningful animal reservoir, including domestic animals, particularly pigs, as well as cattle, goats and even rodents. Routes of exposure Secondary transmission of HEV from cases to contacts and particularly nursing staff has been reported, but appears to be much less common than for HAV. The lower level of person-to-person spread suggests that faecally polluted water could play a much more important role in the spread of HEV than of HAV. Waterborne outbreaks involving thousands of cases are on record. These include one outbreak in 1954 with approximately 40 000 cases in Delhi, India; one with more than 100 000 cases in 1986–1988 in the Xinjiang Uighar region of China; and one in 1991 with some 79 000 cases in Kanpur, India. Animal reservoirs may also serve as a route of exposure, but the extent to which humans contract HEV infection from animals remains to be elucidated. Significance in drinking‑water The role of contaminated water as a source of HEV has been confirmed, and the presence of the virus in drinking-water constitutes a major health risk. There is no laboratory information on the resistance of the virus to disinfection processes, but data on waterborne outbreaks suggest that HEV may be as resistant as other enteric viruses. Within a water safety plan, control measures to reduce potential risk from HEV should focus on prevention of source water contamination by human and animal waste, followed by adequate treatment and disinfection. The effectiveness of treatment processes used to remove HEV will require validation. Drinking-water supplies should also be protected from contamination during distribution. Owing to the likelihood that the virus has a higher resistance to disinfection, E. coli (or, alternatively, thermotolerant coliforms) is not a reliable indicator of the presence/absence of HEV in drinking-water supplies. Selected bibliography Pina S et al. (1998) Characterization of a strain of infectious hepatitis E virus isolated from sewage in an area where hepatitis E is not endemic. Applied and Environmental Microbiology, 64:4485–4488. Van der Poel WHM et al. (2001) Hepatitis E virus sequence in swine related to sequences in humans, the Netherlands. Emerging Infectious Diseases, 7:970–976. WHO (2002) Enteric hepatitis viruses. In: Guidelines for drinking-water quality, 2nd ed. Addendum: Microbiological agents in drinking water. Geneva, World Health Organization, pp. 18–39. Rotaviruses and orthoreoviruses General description Members of the genus Rotavirus consist of a segmented double-stranded RNA genome in a non-enveloped icosahedral capsid with a diameter of 50–65 nm. This capsid is surrounded by a double-layered shell, giving the virus the appearance of a wheel—hence the name rotavirus. The diameter of the entire virus is about 80 nm. Rotavirus and Orthoreovirus are the two genera of the family Reoviridae typically associated with human infection. Orthoreoviruses are readily isolated by cytopathogenic effect on cell cultures. The genus Rotavirus is serologically divided into seven groups, A–G, each of which consists of a number of subgroups; some of these subgroups specifically infect humans, whereas others infect a wide spectrum of animals. Groups A–C are found in humans, with group A being the most important human pathogens. Wild-type strains of rotavirus group A are not readily grown in cell culture, but there are a number of PCR-based detection methods available for testing environmental samples. Human health effects Human rotaviruses are the most important single cause of infant death in the world. Typically, 50–60% of cases of acute gastroenteritis of hospitalized children throughout the world are caused by human rotaviruses. The viruses infect cells in the villi of the small intestine, with disruption of sodium and glucose transport. Acute infection has an abrupt onset of severe watery diarrhoea with fever, abdominal pain and vomiting; dehydration and metabolic acidosis may develop, and the outcome may be fatal if the infection is not appropriately treated. The burden of disease of rotavirus infections is extremely high. Members of the genus Orthoreovirus infect many humans, but they are typical “orphan viruses” and not associated with any meaningful disease. Source and occurrence Human rotaviruses are excreted by patients in numbers up to 1011 per gram of faeces for periods of about 8 days. This implies that domestic sewage and any environments polluted with the human faeces are likely to contain large numbers of human rota-viruses. The viruses have been detected in sewage, rivers, lakes and treated drinking-water. Orthoreoviruses generally occur in wastewater in substantial numbers. Routes of exposure Human rotaviruses are transmitted by the faecal–oral route. Person-to-person transmission and the inhalation of airborne human rotaviruses or aerosols containing the viruses would appear to play a much more important role than ingestion of contaminated food or water. This is confirmed by the spread of infections in children’s wards in hospitals, which takes place much faster than can be accounted for by the ingestion of food or water contaminated by the faeces of infected patients. The role of contaminated water in transmission is lower than expected, given the prevalence of human rotavirus infections and presence in contaminated water. However, occasional waterborne and foodborne outbreaks have been described. Two large outbreaks in China in 1982–1983 were linked to contaminated water supplies. Significance in drinking‑water Although ingestion of drinking-water is not the most common route of transmission, the presence of human rotaviruses in drinking-water constitutes a public health risk. There is some evidence that the rotaviruses are more resistant to disinfection than other enteric viruses. Within a water safety plan, control measures to reduce potential risk from human rotaviruses should focus on prevention of source water contamination by human waste, followed by adequate treatment and disinfection. The effectiveness of treatment processes used to remove human rotaviruses will require validation. Drinking-water supplies should also be protected from contamination during distribution. Owing to a higher resistance of the viruses to disinfection, E. coli (or, alternatively, thermotolerant coliforms) is not a reliable indicator of the presence/absence of human rotaviruses in drinking-water supplies. Selected bibliography Baggi F, Peduzzi R (2000) Genotyping of rotaviruses in environmental water and stool samples in southern Switzerland by nucleotide sequence analysis of 189 base pairs at the 5′ end of the VP7 gene. Journal of Clinical Microbiology, 38:3681–3685. Gerba CP et al. (1996) Waterborne rotavirus: A risk assessment. Water Research, 30:2929–2940. Hopkins RS et al. (1984) A community waterborne gastroenteritis outbreak: Evidence for rotavirus as the agent. American Journal of Public Health, 74:263–265. Hung T et al. (1984) Waterborne outbreak of rotavirus diarrhoea in adults in China caused by a novel rotavirus. Lancet, i:1139–1142. Sattar SA, Raphael RA, Springthorpe VS (1984) Rotavirus survival in conventionally treated drinking water. Canadian Journal of Microbiology, 30:653–656. 11.3 Protozoan pathogens Protozoa and helminths are among the most common causes of infection and disease in humans and animals. The diseases have a major public health and socioeconomic impact. Water plays an important role in the transmission of some of these pathogens. The control of waterborne transmission presents real challenges, because most of the pathogens produce cysts, oocysts or eggs that are extremely resistant to processes generally used for the disinfection of water and in some cases can be difficult to remove by filtration processes. Some of these organisms cause “emerging diseases”. In the last 30 years, the most notable example of an emerging disease caused by a protozoan pathogen is cryptosporidiosis. Other examples are diseases caused by microsporidia and Cyclospora. As evidence for waterborne transmission of “emerging diseases” has been reported relatively recently, some questions about their epidemiology and behaviour in water treatment and disinfection processes remain to be elucidated. It would appear that the role of water in the transmission of this group of pathogens may increase substantially in importance and complexity as human and animal populations grow and the demands for potable drinking-water escalate. Further information on emerging diseases is provided in Emerging issues in water and infectious disease (WHO, 2003) and associated texts. Acanthamoeba General description Acanthamoeba spp. are free-living amoebae (10–50 μm in diameter) common in aquatic environments and one of the prominent protozoa in soil. The genus contains some 20 species, of which A. castellanii, A. polyphaga and A. culbertsoni are known to be human pathogens. However, the taxonomy of the genus may change substantially when evolving molecular biological knowledge is taken into consideration. Acanthamoeba has a feeding, replicative trophozoite, which, under unfavourable conditions, such as an anaerobic environment, will develop into a dormant cyst that can withstand extremes of temperature (−20 to 56 °C), disinfection and desiccation. Human health effects Acanthamoeba culbertsoni causes granulomatous amoebic encephalitis, whereas A. castellanii and A. polyphaga are associated with acanthamoebic keratitis and acanthamoebic uveitis. Granulomatous amoebic encephalitis is a multifocal, haemorrhagic and necrotizing encephalitis that is generally seen only in debilitated or immunodeficient persons. It is a rare, but usually fatal, disease. Early symptoms include drowsiness, personality changes, intense headaches, stiff neck, nausea, vomiting, sporadic low fevers, focal neurological changes, hemiparesis and seizures. This is followed by an altered mental status, diplopia, paresis, lethargy, cerebellar ataxia and coma. Death follows within a week to a year after the appearance of the first symptoms, usually as a result of bronchopneumonia. Associated disorders of granulomatous amoebic encephalitis include skin ulcers, liver disease, pneumonitis, renal failure and pharyngitis. Acanthamoebic keratitis is a painful infection of the cornea and can occur in healthy individuals, especially among contact lens wearers. It is a rare disease that may lead to impaired vision, permanent blindness and loss of the eye. The prevalence of antibodies to Acanthamoeba and the detection of the organism in the upper airways of healthy persons suggest that infection may be common with few apparent symptoms in the vast majority of cases. Source and occurrence The wide distribution of Acanthamoeba in the natural environment makes soil, airborne dust and water all potential sources. Acanthamoeba can be found in many types of aquatic environments, including surface water, tap water, swimming pools and contact lens solutions. Depending on the species, Acanthamoeba can grow over a wide temperature range in water, with the optimum temperature for pathogenic species being 30 °C. Trophozoites can exist and replicate in water while feeding on bacteria, yeasts and other organisms. Routes of exposure Acanthamoebic keratitis has been associated with contact lenses being washed with contaminated home-made saline solutions or contamination of the contact lens containers. Although the source of the contaminating organisms has not been established, tap water is one possibility. Warnings have been issued by a number of health agencies that only sterile water should be used to prepare wash solutions for contact lenses. The mode of transmission of granulomatous amoebic encephalitis has not been established, but water is not considered to be a source of infection. The more likely routes of transmission are via the blood from other sites of colonization, such as skin lesions or lungs. Significance in drinking‑water Cases of acanthamoebic keratitis have been associated with drinking-water due to use of tap water in preparing solutions for washing contact lenses. Cleaning of contact lenses is not considered to be a normal use for tap water, and a higher-quality water may be required. Compared with Cryptosporidium oocysts and Giardia cysts, Acanthamoeba cysts are relatively large and amenable to removal from raw water by filtration. Reducing the presence of biofilm organisms is likely to reduce food sources and growth of the organism in distribution systems, but the cyst is highly resistant to disinfection. However, as normal uses of drinking-water lack significance as a source of infection, setting a health-based target for Acanthamoeba spp. is not warranted. Selected bibliography Marshall MM et al. (1997) Waterborne protozoan pathogens. Clinical Microbiology Reviews, 10:67–85. Yagita K, Endo T, De Jonckheere JF (1999) Clustering of Acanthamoeba isolates from human eye infections by means of mitochondrial DNA digestion patterns. Parasitology Research, 85:284–289. Balantidium coli General description Balantidium coli is a unicellular protozoan parasite with a length up to 200 μm, making it the largest of the human intestinal protozoa. The trophozoites are oval in shape and covered with cilia for motility. The cysts are 60–70 μm in length and resistant to unfavourable environmental conditions, such as pH and temperature extremes. Balantidium coli belongs to the largest protozoan group, the ciliates, with about 7200 species, of which only B. coli is known to infect humans. Human health effects Infections in humans are relatively rare, and most are asymptomatic. The trophozoites invade the mucosa and submucosa of the large intestine and destroy the host cells when multiplying. The multiplying parasites form nests and small abscesses that break down into oval, irregular ulcers. Clinical symptoms may include dysentery similar to amoebiasis, colitis, diarrhoea, nausea, vomiting, headache and anorexia. The infections are generally self-limiting, with complete recovery. Source and occurrence Humans seem to be the most important host of B. coli, and the organism can be detected in domestic sewage. Animal reservoirs, particularly swine, also contribute to the prevalence of the cysts in the environment. The cysts have been detected in water sources, but the prevalence in tap water is unknown. Routes of exposure Transmission of B. coli is by the faecal–oral route, from person to person, from contact with infected swine or by consumption of contaminated water or food. One waterborne outbreak of balantidiasis has been reported. This outbreak occurred in 1971 when a drinking-water supply was contaminated with stormwater runoff containing swine faeces after a typhoon. Significance in drinking‑water Although water does not appear to play an important role in the spread of this organism, one waterborne outbreak is on record. Balantidium coli is large and amenable to removal by filtration, but cysts are highly resistant to disinfection. Within a water safety plan, control measures to reduce potential risk from B. coli should focus on prevention of source water contamination by human and swine waste, followed by adequate treatment. Owing to resistance to disinfection, E. coli (or, alternatively, thermotolerant coliforms) is not a reliable indicator for the presence/absence of B. coli in drinking-water supplies. Selected bibliography Garcia LS (1999) Flagellates and ciliates. Clinics in Laboratory Medicine, 19:621–638. Walzer PD et al. (1973) Balantidiasis outbreak in Truk. American Journal of Tropical Medicine and Hygiene, 22:33–41. Blastocystis General description Blastocystis is a common anaerobic intestinal parasite that was first described in the early 1900s. Despite this long history, there are large gaps in knowledge about the organism, and the issue of pathogenicity remains a subject of some debate. Blastocystis spp. have been detected in a range of animal hosts, with isolates from humans identified as Blastocystis hominis. However, molecular studies suggest that there is considerable antigenic and genetic heterogeneity within B. hominis and Blastocystis spp. Blastocystis hominis lives in the colon and has several morphological forms, including a faecal cyst that is believed to be the infective form. Human health effects Blastocystis hominis is probably the most common protozoan detected in human faecal samples worldwide. Infection occurs in both immunocompetent and immunocompromised individuals. Reported prevalence ranges from 2% to 50%, with the highest rates reported for developing countries with poor environmental hygiene. Infection appears to be more common in adults than in children. However, one study showed that peak infection occurs at 10 years of age and then later in life. Pathogenicity of B. hominis is controversial because of the nonspecific symptoms and prevalence of asymptomatic infections. Some case–control studies of individuals with and without symptoms show no difference in the prevalence of B. hominis. Symptoms attributed to B. hominis include watery or loose stools, diarrhoea, abdominal pain, anal itching, weight loss and excess gas. Duration of infection is not well known; some infections can last for weeks, months or years. In some patients, the symptoms resolve, even though Blastocystis can still be detected in stools. It has been suggested that B. hominis may be a commensal organism that becomes pathogenic when the host is immunosuppressed, is malnourished or has other infections. Source and occurrence The source of human infectious Blastocystis is uncertain. Blastocystis occurs in many animals, including insects, reptiles, birds and mammals. Some evidence suggests that Blastocystis may not be host specific and that animal-to-human transmission is possible. A recent survey in Malaysia showed that animal handlers and abattoir workers were at greater risk of infection than a control group of high-rise city dwellers. Blastocystis is excreted as a cyst, which could be environmentally persistent, but there are no data on its survival in the environment. Blastocystis has been identified in sewage samples. Routes of exposure The routes of transmission have not been established, but the faecal–oral route is considered to be the main mode of transmission. Studies of transmission between mice indicate infection after oral inoculation of faecal cysts. Water and foodborne transmission have been suggested, but not confirmed. Significance in drinking‑water The role of drinking-water as a source of Blastocystis infections has not been established. However, an investigation in Thailand provided evidence of waterborne transmission, and identification in sewage samples suggests potential for this to occur. Within a water safety plan, control measures focused on prevention of source water contamination by human and animal waste should reduce potential risks. There is little information on the removal and/or inactivation of Blastocystis by water and wastewater treatment processes. The morphology of Blastocystis varies over a broad range, and size estimates vary. Faecal cysts can be as small as 3–10 μm in diameter, and these are likely to be removed by conventional granular media-based filtration methods in a similar manner to Cryptosporidium oocysts that are 4–6 μm in diameter. It has been reported that Blastocystis cysts are relatively resistant to chlorine. Because of this resistance, E. coli (or, alternatively, thermotolerant coliforms) should not be relied upon as an indicator of the presence/absence of Blastocystis in drinking-water sources. Selected bibliography Leelayoova S et al. (2004) Evidence of waterborne transmission of Blastocystis hominis. American Journal of Tropical Medicine and Hygiene, 70:658–662. Rajah SH et al. (1999) Blastocystis in animal handlers. Parasitology Research, 85:1032–1033. Stenzel DJ, Boreham PFL (1996) Blastocystis hominis revisited. Clinical Microbiology Reviews, 9(4):563–584. Suresh K, Smith HV, Tan TC (2005) Viable Blastocystis cysts in Scottish and Malaysian sewage samples. Applied and Environmental Microbiology, 71:5619–5620. Tan KSW, Singh M, Yap EH (2002) Recent advances in Blastocystis hominis research: Hot spots in terra incognita. International Journal of Parasitology, 32:789–804. Cryptosporidium General description Cryptosporidium is an obligate, intracellular, coccidian parasite with a complex life cycle including sexual and asexual replication. Thick-walled oocysts with a diameter of 4–6 μm are shed in faeces. The genus Cryptosporidium has about 13 species, with human infections predominantly caused by C. hominis and the cattle genotype of C. parvum. Other Cryptosporidium species have been reported to cause infrequent infections. Cryptosporidium was discovered to infect humans in 1976, and waterborne transmission was confirmed for the first time in 1984. Human health effects Cryptosporidium generally causes self-limiting diarrhoea, sometimes including nausea, vomiting and fever, which usually resolves within a week in normally healthy people, but can last for a month or more. Severity of cryptosporidiosis varies according to age and immune status, and infections in severely immunocompromised people can be life-threatening. The impact of cryptosporidiosis outbreaks is relatively high due to the large numbers of people that may be involved and the associated socioeconomic implications. The total cost of illness associated with the 1993 outbreak in Milwaukee, USA, has been estimated at US$ 96.2 million. Source and occurrence A large range of animals are reservoirs of C. hominis/parvum, but humans and livestock, particularly young animals, are the most significant source of human infectious organisms. Calves can excrete 1010 oocysts per day. Concentrations of oocysts as high as 14 000 per litre for raw sewage and 5800 per litre for surface water have been reported. Oocysts can survive for weeks to months in fresh water. Cryptosporidium oocysts have been detected in many drinking-water supplies. However, in most cases, there is little information about whether human infectious species were present. The currently available standard analytical techniques provide an indirect measure of viability and no indication of human infectivity. Oocysts also occur in recreational waters. Routes of exposure Cryptosporidium is transmitted by the faecal–oral route. The major route of infection is person-to-person contact. Other sources of infection include the consumption of contaminated food and water and direct contact with infected farm animals and possibly domestic pets. Contaminated drinking-water, recreational water and, to a lesser extent, food have been associated with outbreaks. In 1993, Cryptosporidium caused the largest waterborne outbreak of disease on record, when more than 400 000 people were infected by the drinking-water supply of Milwaukee, USA. The infectivity of Cryptosporidium oocysts is relatively high. Studies on healthy human volunteers revealed that ingestion of fewer than 10 oocysts can lead to infection. Significance in drinking‑water The role of drinking-water in the transmission of Cryptosporidium, including in large outbreaks, is well established. Attention to these organisms is therefore important. The oocysts are extremely resistant to oxidizing disinfectants such as chlorine, but investigations based on assays for infectivity have shown that UV light irradiation inactivates oocysts. Within a water safety plan, control measures to reduce potential risk from Cryptosporidium should focus on prevention of source water contamination by human and livestock waste, adequate treatment and protection of water during distribution. Because of their relatively small size, the oocysts represent a challenge for removal by conventional granular media–based filtration processes. Acceptable removal requires well-designed and well-operated systems. Membrane filtration processes that provide a direct physical barrier may represent a viable alternative for the effective removal of Cryptosporidium oocysts. Owing to the exceptional resistance of the oocysts to disinfectants, E. coli (or, alternatively, thermotolerant coliforms) cannot be relied upon as an indicator for the presence/absence of Cryptosporidium oocysts in drinking-water supplies. Selected bibliography Corso PS et al. (2003) Cost of illness in the 1993 waterborne Cryptosporidium outbreak, Milwaukee, Wisconsin. Emerging Infectious Diseases, 9:426–431. Haas CN et al. (1996) Risk assessment of Cryptosporidium parvum oocysts in drinking water. Journal of the American Water Works Association, 88:131–136. Leav BA, Mackay M, Ward HD (2003) Cryptosporidium species: New insight and old challenges. Clinical Infectious Diseases, 36:903–908. Linden KG, Shin G, Sobsey MD (2001) Comparative effectiveness of UV wavelengths for the inactivation of Cryptosporidium parvum oocysts in water. Water Science and Technology, 43:171–174. Medema G et al. (2009) Risk assessment of Cryptosporidium in drinking water. Geneva, World Health Organization (WHO/HSE/WSH/09.04). Okhuysen PC et al. (1999) Virulence of three distinct Cryptosporidium parvum isolates for healthy adults. Journal of Infectious Diseases, 180:1275–1281. Cyclospora cayetanensis General description Cyclospora cayetanensis is a single-celled, obligate, intracellular, coccidian protozoan parasite, which belongs to the family Eimeriidae. It produces thick-walled oocysts of 8–10 μm in diameter that are excreted in the faeces of infected individuals. Cyclospora cayetanensis is considered an emerging waterborne pathogen. Human health effects Sporozoites are released from the oocysts when ingested and penetrate epithelial cells in the small intestine of susceptible individuals. Clinical symptoms of cyclosporiasis include watery diarrhoea, abdominal cramping, weight loss, anorexia, myalgia and occasionally vomiting and/or fever. Relapsing illness often occurs. Source and occurrence Humans are the only host identified for this parasite. The unsporulated oocysts pass into the external environment with faeces and undergo sporulation, which is complete in 7–12 days, depending on environmental conditions. Only the sporulated oocysts are infectious. Owing to the lack of a quantification technique, there is limited information on the prevalence of Cyclospora in water environments. However, Cyclospora has been detected in sewage and water sources. Routes of exposure Cyclospora cayetanensis is transmitted by the faecal–oral route. Person-to-person transmission is virtually impossible, because the oocysts must sporulate outside the host to become infectious. The primary routes of exposure are contaminated water and food. The initial source of organisms in foodborne outbreaks has generally not been established, but consumption of food crops irrigated with contaminated water has been implicated in several cases. Drinking-water has also been implicated as a cause of outbreaks. The first report was among staff of a hospital in Chicago, USA, in 1990. The infections were associated with drinking tap water that had possibly been contaminated with stagnant water from a rooftop storage reservoir. Another outbreak was reported from Nepal, where drinking-water consisting of a mixture of river and municipal water was associated with infections in 12 of 14 soldiers. Significance in drinking‑water Transmission of the pathogens by drinking-water has been confirmed. The oocysts are resistant to disinfection and are not inactivated by chlorination practices generally applied in the production of drinking-water. Within a water safety plan, control measures that can be applied to manage potential risk from Cyclospora include prevention of source water contamination by human waste, followed by adequate treatment and protection of water during distribution. Owing to the resistance of the cysts to disinfectants, E. coli (or, alternatively, thermotolerant coliforms) cannot be relied upon as an indicator of the presence/absence of Cyclospora in drinking-water supplies. Selected bibliography Curry A, Smith HV (1998) Emerging pathogens: Isospora, Cyclospora and microsporidia. Parasitology, 117:S143–159. Dowd SE et al. (2003) Confirmed detection of Cyclospora cayetanensis, Encephalitozoon intestinalis and Cryptosporidium parvum in water used for drinking. Journal of Water and Health, 1:117–123. Goodgame R (2003) Emerging causes of traveller’s diarrhea: Cryptosporidium, Cyclospora, Isospora and microsporidia. Current Infectious Disease Reports, 5:66–73. Herwaldt BL (2000) Cyclospora cayetanensis: A review, focusing on the outbreaks of cyclosporiasis in the 1990s. Clinical Infectious Diseases, 31:1040–1057. Rabold JG et al. (1994) Cyclospora outbreak associated with chlorinated drinking-water [letter]. Lancet, 344:1360–1361. WHO (2002) Protozoan parasites (Cryptosporidium, Giardia, Cyclospora). In: Guidelines for drinking-water quality, 2nd ed. Addendum: Microbiological agents in drinking water. Geneva, World Health Organization, pp. 70–118. Entamoeba histolytica General description Entamoeba histolytica is the most prevalent intestinal protozoan pathogen worldwide and belongs to the superclass Rhizopoda in the subphylum Sarcodina. Entamoeba has a feeding, replicative trophozoite (diameter 10–60 μm), which, under unfavourable conditions, will develop into a dormant cyst (diameter 10–20 μm). Infection is contracted by the ingestion of cysts. Recent studies with RNA and DNA probes demonstrated genetic differences between pathogenic and non-pathogenic E. histolytica; the latter has been separated and reclassified as E. dispar. Human health effects About 85–95% of human infections with E. histolytica are asymptomatic. Acute intestinal amoebiasis has an incubation period of 1–14 weeks. Clinical disease results from the penetration of the epithelial cells in the gastrointestinal tract by the amoebic trophozoites. Approximately 10% of infected individuals present with dysentery or colitis. Symptoms of amoebic dysentery include diarrhoea with cramping, lower abdominal pain, low-grade fever and the presence of blood and mucus in the stool. The ulcers produced by the invasion of the trophozoites may deepen into the classic flask-shaped ulcers of amoebic colitis. Entamoeba histolytica may invade other parts of the body, such as the liver, lungs and brain, sometimes with fatal outcome. Source and occurrence Humans are the reservoir of infection, and there would not appear to be other meaningful animal reservoirs of E. histolytica. In the acute phase of infection, patients excrete only trophozoites that are not infectious. Chronic cases and asymptomatic carriers who excrete cysts are more important sources of infection and can discharge up to 1.5 × 107 cysts daily. Entamoeba histolytica can be present in sewage and contaminated water. Cysts may remain viable in suitable aquatic environments for several months at low temperature. The potential for waterborne transmission is greater in the tropics, where the carrier rate sometimes exceeds 50%, compared with more temperate regions, where the prevalence in the general population may be less than 10%. Routes of exposure Person-to-person contact and contamination of food by infected food handlers appear to be the most significant means of transmission, although contaminated water also plays a substantial role. Ingestion of faecally contaminated water and consumption of food crops irrigated with contaminated water can both lead to transmission of amoebiasis. Sexual transmission, particularly among male homosexuals, has also been documented. Significance in drinking‑water The transmission of E. histolytica by contaminated drinking-water has been confirmed. The cysts are relatively resistant to disinfection and may not be inactivated by chlorination practices generally applied in the production of drinking-water. Within a water safety plan, control measures that can be applied to manage potential risk from E. histolytica include prevention of source water contamination by human waste, followed by adequate treatment and protection of water during distribution. Owing to the resistance of the cysts to disinfectants, E. coli (or, alternatively, thermotolerant coliforms) cannot be relied upon as an indicator of the presence/absence of E. histolytica in drinking-water supplies. Selected bibliography Marshall MM et al. (1997) Waterborne protozoan pathogens. Clinical Microbiology Reviews, 10:67–85. Giardia intestinalis General description Giardia spp. are flagellated protozoa that parasitize the gastrointestinal tract of humans and certain other animals. The genus Giardia consists of a number of species, but human infection (giardiasis) is usually assigned to G. intestinalis, also known as G. lamblia or G. duodenalis. Giardia has a relatively simple life cycle consisting of a flagellate trophozoite that multiplies in the gastrointestinal tract and an infective thick-walled cyst that is shed intermittently but in large numbers in faeces. The trophozoites are bilaterally symmetrical and ellipsoidal in shape. The cysts are ovoid in shape and 8–12 μm in diameter. Human health effects Giardia has been known as a human parasite for 200 years. After ingestion and excystation of cysts, the trophozoites attach to surfaces of the gastrointestinal tract. Infections in both children and adults may be asymptomatic. In day-care centres, as many as 20% of children may carry Giardia and excrete cysts without clinical symptoms. The symptoms of giardiasis may result from damage caused by the trophozoites, although the mechanisms by which Giardia causes diarrhoea and intestinal malabsorption remain controversial. Symptoms generally include diarrhoea and abdominal cramps; in severe cases, however, malabsorption deficiencies in the small intestine may be present, mostly among young children. Giardiasis is self-limiting in most cases, but it may be chronic in some patients, lasting more than 1 year, even in otherwise healthy people. Studies on human volunteers revealed that fewer than 10 cysts constitute a meaningful risk of infection. Source and occurrence Giardia can multiply in a wide range of animal species and humans, which excrete cysts into the environment. Numbers of cysts as high as 88 000 per litre in raw sewage and 240 per litre in surface water resources have been reported. These cysts are robust and can survive for weeks to months in fresh water. The presence of cysts in raw water sources and drinking-water supplies has been confirmed. However, there is no information on whether human infectious species were present. The currently available standard analytical techniques provide an indirect measure of viability and no indication of human infectivity. Cysts also occur in recreational waters and contaminated food. Routes of exposure By far the most common route of transmission of Giardia is person-to-person contact, particularly between children. Contaminated drinking-water, recreational water and, to a lesser extent, food have been associated with outbreaks. Animals have been implicated as a source of human infectious G. intestinalis, but further investigations are required to determine their role. Significance in drinking‑water Waterborne outbreaks of giardiasis have been associated with drinking-water supplies for over 30 years; at one stage, Giardia was the most commonly identified cause of waterborne outbreaks in the USA. Giardia cysts are more resistant than enteric bacteria to oxidative disinfectants such as chlorine, but they are not as resistant as Cryptosporidium oocysts. The time required for 90% inactivation at a free chlorine residual of 1 mg/l is about 25–30 minutes. Within a water safety plan, control measures that can be applied to manage potential risk from Giardia include prevention of source water contamination by human and animal waste, followed by adequate treatment and disinfection and protection of water during distribution. Owing to the resistance of the cysts to disinfectants, E. coli (or, alternatively, thermotolerant coliforms) cannot be relied upon as an indicator of the presence/absence of Giardia in drinking-water supplies. Selected bibliography LeChevallier MW, Norton WD, Lee RG (1991) Occurrence of Giardia and Cryptosporidium species in surface water supplies. Applied and Environmental Microbiology, 57:2610–2616. Ong C et al. (1996) Studies of Giardia spp. and Cryptosporidium spp. in two adjacent watersheds. Applied and Environmental Microbiology, 62:2798–2805. Rimhanen-Finne R et al. (2002) An IC-PCR method for detection of Cryptosporidium and Giardia in natural surface waters in Finland. Journal of Microbiological Methods, 50:299–303. Slifko TR, Smith HV, Rose JB (2000) Emerging parasite zoonoses associated with water and food. International Journal for Parasitology, 30:1379–1393. Stuart JM et al. (2003) Risk factors for sporadic giardiasis: A case–control study in southwestern England. Emerging Infectious Diseases, 9:229–233. WHO (2002) Protozoan parasites (Cryptosporidium, Giardia, Cyclospora). In: Guidelines for drinking-water quality, 2nd ed. Addendum: Microbiological agents in drinking water. Geneva, World Health Organization, pp. 70–118. Isospora belli General description Isospora is a coccidian, single-celled, obligate parasite related to Cryptosporidium and Cyclospora. There are many species of Isospora that infect animals, but only I. belli is known to infect humans, the only known host for this species. Isospora belli is one of the few coccidia that undergo sexual reproduction in the human intestine. Sporulated oocysts are ingested, and, after complete asexual and sexual life cycles in the mucosal epithelium of the upper small intestine, unsporulated oocysts are released in faeces. Human health effects Illness caused by I. belli is similar to that caused by Cryptosporidium and Giardia. About 1 week after ingestion of viable oocysts, a low-grade fever, lassitude and malaise may appear, followed soon by mild diarrhoea and vague abdominal pain. The infection is usually self-limited after 1–2 weeks, but occasionally diarrhoea, weight loss and fever may last for 6 weeks to 6 months. Symptomatic isosporiasis is more common in children than in adults. Infection is often associated with immunocompromised patients, in whom symptoms are more severe and likely to be recurrent or chronic, leading to malabsorption and weight loss. Infections are usually sporadic and most common in the tropics and subtropics, although they also occur elsewhere, including industrialized countries. They have been reported from Central and South America, Africa and south-east Asia. Source and occurrence Unsporulated oocysts are excreted in the faeces of infected individuals. The oocysts sporulate within 1–2 days in the environment to produce the potentially infectious form of the organism. Few data are available on numbers of oocysts in sewage and raw and treated water sources. This is largely because sensitive and reliable techniques for the quantitative enumeration of oocysts in water environments are not available. Little is known about the survival of oocysts in water and related environments. Routes of exposure Poor sanitation and faecally contaminated food and water are the most likely sources of infection, but waterborne transmission has not been confirmed. The oocysts are less likely than Cryptosporidium oocysts or Giardia cysts to be transmitted directly from person to person, because freshly shed I. belli oocysts require 1–2 days in the environment to sporulate before they are capable of infecting humans. Significance in drinking‑water The characteristics of I. belli suggest that illness could be transmitted by contaminated drinking-water supplies, but this has not been confirmed. No information is available on the effectiveness of water treatment processes for removal of I. belli, but it is likely that the organism is relatively resistant to disinfectants. It is considerably larger than Cryptosporidium and should be easier to remove by filtration. Within a water safety plan, control measures that can be applied to manage potential risk from I. belli include prevention of source water contamination by human waste, followed by adequate treatment and disinfection and protection of water during distribution. Owing to the likely resistance of the oocysts to disinfectants, E. coli (or, alternatively, thermotolerant coliforms) cannot be relied upon as an indicator of the presence/absence of I. belli in drinking-water supplies. Selected bibliography Ballal M et al. (1999) Cryptosporidium and Isospora belli diarrhoea in immunocompromised hosts. Indian Journal of Cancer, 36:38–42. Bialek R et al. (2002) Comparison of autofluorescence and iodine staining for detection of Isospora belli in feces. American Journal of Tropical Medicine and Hygiene, 67:304–305. Curry A, Smith HV (1998) Emerging pathogens: Isospora, Cyclospora and microsporidia. Parasitology, 117:S143–159. Goodgame R (2003) Emerging causes of traveller’s diarrhea: Cryptosporidium, Cyclospora, Isospora and microsporidia. Current Infectious Disease Reports, 5:66–73. Microsporidia General description Microsporidia are eukaryotic obligate intracellular parasites belonging to the phylum Microspora. Although microsporidia were initially considered to be protozoa, the scientific classification is uncertain, with recent studies indicating that they could be classified as fungi. More than 100 microsporidial genera and almost 1000 species have been identified. Infections occur in every major animal group, including vertebrates and invertebrates. A number of genera have been implicated in human infections, including Enterocytozoon, Encephalitozoon (including Septata), Nosema, Pleistophora, Vittaforma and Trachipleistophora, as well as a collective group of unclassified microsporidia referred to as microsporidium. Microsporidia are among the smallest eukaryotes. They produce unicellular spores with a diameter of 1.0–4.5 μm and a characteristic coiled polar filament for injecting the sporoplasm into a host cell to initiate infection. Within an infected cell, a complex process of multiplication takes place, and new spores are produced and released in faeces, urine, respiratory secretions or other body fluids, depending on the type of species and the site of infection. Human health effects Microsporidia are emerging human pathogens identified predominantly in persons with AIDS, but their ability to cause disease in immunologically normal hosts has been recognized. Reported human infections are globally dispersed and have been documented in persons from all continents. The most common clinical manifestation in AIDS patients is a severe enteritis involving chronic diarrhoea, dehydration and weight loss. Prolonged illness for up to 48 months has been reported. Infections in the general population are less pronounced. Enterocytozoon infection generally appears to be limited to intestinal enterocytes and biliary epithelium. Encephalitozoon spp. infect a variety of cells, including epithelial and endothelial cells, fibroblasts, kidney tubule cells, macrophages and possibly other cell types. Unusual complications include keratoconjunctivitis, myositis and hepatitis. Source and occurrence The sources of microsporidia infecting humans are uncertain. Spores are likely to be excreted in faeces and are also excreted in urine and respiratory secretions. Owing to the lack of a quantification technique, there is limited information on the prevalence of microsporidia spores in water environments. However, microsporidia have been detected in sewage and water sources. Indications are that their numbers in raw sewage may be similar to those of Cryptosporidium and Giardia, and they may survive in certain water environments for many months. Certain animals, notably swine, may serve as a host for human infectious species. Routes of exposure Little is known about transmission of microsporidia. Person-to-person contact and ingestion of spores in water or food contaminated with human faeces or urine are probably important routes of exposure. A waterborne outbreak of microsporidiosis has been reported involving about 200 cases in Lyon, France, during the summer of 1995. However, the source of the organism and faecal contamination of the drinking-water supply were not demonstrated. Transmission by the inhalation of airborne spores or aerosols containing spores seems possible. The role of animals in transmission to humans remains unclear. Epidemiological and experimental studies in mammals suggest that Encephalitozoon spp. can be transmitted transplacentally from mother to offspring. No information is available on the infectivity of the spores. However, in view of the infectivity of spores of closely related species, the infectivity of microsporidia may be high. Significance in drinking‑water Waterborne transmission has been reported, and infection arising from contaminated drinking-water is plausible but unconfirmed. Little is known about the response of microsporidia to water treatment processes. One study has suggested that the spores may be susceptible to chlorine. The small size of the organism is likely to make them difficult to remove by filtration processes. Within a water safety plan, control measures that can be applied to manage potential risk from microsporidia include prevention of source water contamination by human and animal waste, followed by adequate treatment and disinfection and protection of water during distribution. Owing to the lack of information on sensitivity of infectious species of microsporidia to disinfection, the reliability of E. coli (or, alternatively, thermotolerant coliforms) as an indicator for the presence/absence of these organisms from drinking-water supplies is unknown. Selected bibliography Coote L et al. (2000) Waterborne outbreak of intestinal microsporidiosis in persons with and without human immunodeficiency virus infection. Journal of Infectious Diseases, 180:2003–2008. Dowd SE et al. (2003) Confirmed detection of Cyclospora cayetanensis, Encephalitozoon intestinalis and Cryptosporidium parvum in water used for drinking. Journal of Water and Health, 1:117–123. Goodgame R (2003) Emerging causes of traveller’s diarrhea: Cryptosporidium, Cyclospora, Isospora and microsporidia. Current Infectious Disease Reports, 5:66–73. Joynson DHM (1999) Emerging parasitic infections in man. The Infectious Disease Review, 1:131–134. Slifko TR, Smith HV, Rose JB (2000) Emerging parasite zoonoses associated with water and food. International Journal for Parasitology, 30:1379–1393. Naegleria fowleri General description Naegleria are free-living amoeboflagellates distributed widely in the environment. There are several species of Naegleria, of which N. fowleri is the primary infectious species. Naegleria spp. exist as a trophozoite, a flagellate and a cyst stage. The trophozoite (10–20 μm) moves by eruptive pseudopod formation, feeding on bacteria, and reproduces by binary fission. The trophozoite can transform into a flagellate stage with two anterior flagella. The flagellate does not divide but reverts to the trophozoite stage. Under adverse conditions, the trophozoite transforms into a circular cyst (7–15 μm), which is resistant to unfavourable conditions. Human health effects Naegleria fowleri causes primary amoebic meningoencephalitis in healthy individuals. The amoeba enters the brain by penetrating the olfactory mucosa and cribiform plate. The disease is acute, and patients often die within 5–10 days and before the infectious agent can be diagnosed. Treatment is difficult. Although the infection is rare, new cases are reported every year. Source and occurrence Naegleria fowleri is thermophilic and grows well at temperatures up to 45 °C. It occurs naturally in fresh water of suitable temperature, and prevalence is only indirectly related to human activity, inasmuch as such activity may modify temperature or promote bacterial (food source) production. The pathogen has been reported from many countries, usually associated with thermally polluted water environments such as geothermal water or heated swimming pools. However, the organism has been detected in drinking-water supplies, particularly where water temperature can exceed 25–30 °C. Water is the only known source of infection. The first cases of amoebic meningitis were diagnosed in 1965 in Australia and Florida. Since that time, about 100 cases of primary amoebic meningoencephalitis have been reported throughout the world. Routes of exposure Infection with N. fowleri is almost exclusively contracted by exposure of the nasal passages to contaminated water. Infection is predominantly associated with recreational use of water, including swimming pools and spas, as well as surface waters naturally heated by the sun, industrial cooling waters and geothermal springs. In a limited number of cases, a link to recreational water exposure is lacking. The occurrence of primary amoebic meningoencephalitis is highest during hot summer months, when many people engage in water recreation and when the temperature of water is conducive to growth of the organism. Consumption of contaminated water or food and person-to-person spread have not been reported as routes of transmission. Significance in drinking‑water Naegleria fowleri has been detected in drinking-water supplies. Although unproven, a direct or indirect role of drinking-water-derived organisms—for example, through use of drinking-water in swimming pools—is possible. Any water supply that seasonally exceeds 30 °C or that continually exceeds 25 °C can potentially support the growth of N. fowleri. In such cases, a periodic prospective study would be valuable. Free chlorine or monochloramine residuals in excess of 0.5 mg/l have been shown to control N. fowleri, providing the disinfectant persists through the water distribution system. In addition to maintaining persistent disinfectant residuals, other control measures aimed at limiting the presence of biofilm organisms will reduce food sources and hence growth of the organism in distribution systems. Owing to the environmental nature of this amoeba, E. coli (or, alternatively, thermotolerant coliforms) cannot be relied upon as an indicator for the presence/absence of N. fowleri in drinking-water supplies. Selected bibliography Behets J et al. (2003) Detection of Naegleria spp. and Naegleria fowleri: a comparison of flagellation tests, ELISA and PCR. Water Science and Technology, 47:117–122. Cabanes P-A et al. (2001) Assessing the risk of primary amoebic meningoencephalitis from swimming in the presence of environmental Naegleria fowleri. Applied and Environmental Microbiology, 67:2927–2931. Dorsch MM, Cameron AS, Robinson BS (1983) The epidemiology and control of primary amoebic meningoencephalitis with particular reference to South Australia. Transactions of the Royal Society of Tropical Medicine and Hygiene, 77:372–377. Martinez AJ, Visvesvara GS (1997) Free-living amphizoic and opportunistic amebas. Brain Pathology, 7:583–598. Parija SC, Jayakeerthee SR (1999) Naegleria fowleri: a free living amoeba of emerging medical importance. Communicable Diseases, 31:153–159. Toxoplasma gondii General description Toxoplasma gondii is a coccidian parasite, and the cat is the definitive host. Felid animals harbour the parasite in the intestinal tract, where sexual reproduction takes place. The actively multiplying asexual form in the human host is an obligate, intracellular parasite (diameter 3–6 μm) called a tachyzoite. A chronic phase of the disease develops as the tachyzoites transform into slowly replicating bradyzoites, which eventually become cysts in the host tissue. In the natural cycle, mice and rats containing infective cysts are eaten by cats, which host the sexual stage of the parasite. The cyst wall is digested, and bradyzoites penetrate epithelial cells of the small intestine. Several generations of intracellular multiplication lead to the development of microgametes and macrogametes. Fertilization of the latter leads to the development of oocysts that are excreted in faeces as early as 5 days after a cat has ingested the cysts. Oocysts require 1–5 days to sporulate in the environment. Sporulated oocysts and tissue-borne cysts can both cause infections in susceptible hosts. Human health effects Toxoplasmosis is usually asymptomatic in humans. In a small percentage of cases, flu-like symptoms, lymphadenopathy and hepatosplenomegaly present 5–23 days after the ingestion of cysts or oocysts. Dormant cysts, formed in organ tissue after primary infection, can be reactivated when the immune system becomes suppressed, producing disseminated disease involving the central nervous system and lungs and leading to severe neurological disorders or pneumonia. When these infection sites are involved, the disease can be fatal in immunocompromised patients. Congenital toxoplasmosis is mostly asymptomatic, but can produce chorioretinitis, cerebral calcifications, hydrocephalus, severe thrombocytopenia and convulsions. Primary infection during early pregnancy can lead to spontaneous abortion, stillbirth or fetal abnormality. Source and occurrence Toxoplasmosis is found worldwide. Estimates indicate that in many parts of the world, 15–30% of lamb and pork meat is infected with cysts. The prevalence of oocyst-shedding cats may be 1%. By the third decade of life, about 50% of the European population is infected, and in France this proportion is close to 80%. Toxoplasma gondii oocysts may occur in water sources and supplies contaminated with the faeces of infected cats. Owing to a lack of practical methods for the detection of T. gondii oocysts, there is little information on the prevalence of the oocysts in raw and treated water supplies. Details on the survival and behaviour of the oocysts in water environments are also not available. However, qualitative evidence of the presence of oocysts in faecally polluted water has been reported, and results suggest that T. gondii oocysts may be as resistant to unfavourable conditions in water environments as the oocysts of related parasites. Routes of exposure Both T. gondii oocysts that sporulate after excretion by cats and tissue-borne cysts are potentially infectious. Humans can become infected by ingestion of oocysts excreted by cats by direct contact or through contact with contaminated soil or water. Two outbreaks of toxoplasmosis have been associated with consumption of contaminated water. In Panama, creek water contaminated by oocysts from jungle cats was identified as the most likely source of infection, whereas in 1995, an outbreak in Canada was associated with a drinking-water reservoir being contaminated by excreta from domestic or wild cats. A study in Brazil during 1997–1999 identified the consumption of unfiltered drinking-water as a risk factor for T. gondii seropositivity. More commonly, humans contract toxoplasmosis through the consumption of undercooked or raw meat and meat products containing T. gondii cysts. Transplacental infection also occurs. Significance in drinking‑water Contaminated drinking-water has been identified as a source of toxoplasmosis outbreaks. Little is known about the response of T. gondii to water treatment processes. The oocysts are larger than Cryptosporidium oocysts and should be amenable to removal by filtration. Within a water safety plan, control measures to manage potential risk from T. gondii should be focused on prevention of source water contamination by wild and domesticated cats. If necessary, the organisms can be removed by filtration. Owing to the lack of information on sensitivity of T. gondii to disinfection, the reliability of E. coli (or, alternatively, thermotolerant coliforms) as an indicator for the presence/absence of these organisms in drinking-water supplies is unknown. Selected bibliography Aramini JJ et al. (1999) Potential contamination of drinking water with Toxoplasma gondii oocysts. Epidemiology and Infection, 122:305–315. Bahia-Oliveira LMG et al. (2003) Highly endemic, waterborne toxoplasmosis in North Rio de Janeiro State, Brazil. Emerging Infectious Diseases, 9:55–62. Bowie WR et al. (1997) Outbreak of toxoplasmosis associated with municipal drinking water. The BC Toxoplasma Investigation Team. Lancet, 350:173–177. Kourenti C et al. (2003) Development and application of different methods for the detection of Toxoplasma gondii in water. Applied and Environmental Microbiology, 69:102–106. 11.4 Helminth pathogens The word “helminth” comes from the Greek word meaning “worm” and refers to all types of worms, both free-living and parasitic. The major parasitic worms are classified primarily in the phylum Nematoda (roundworms) and the phylum Platyhelminthes (flatworms including trematodes and cestodes). Helminth parasites infect a large number of people and animals worldwide. For most helminths, drinking-water is not a significant route of transmission. There are two exceptions: Dracunculus medinensis (guinea worm) and Fasciola spp. (F. hepatica and F. gigantica) (liver flukes). Dracunculiasis and fascioliasis both require intermediate hosts to complete their life cycles but are transmitted through drinking-water by different mechanisms. Other helminthiases can be transmitted through water contact (schistosomiasis) or are associated with the use of untreated wastewater in agriculture (ascariasis, trichuriasis, hookworm infections and strongyloidiasis) but are not usually transmitted through drinking-water. Dracunculus medinensis Dracunculus medinensis, commonly known as “guinea worm”, belongs to the phylum Nematoda and is the only nematode associated with significant transmission by drinking-water. The eradication of guinea worm infection from the world by 1995 was a target of the International Drinking Water Supply and Sanitation Decade (1981–1990), and the World Health Assembly formally committed itself to this goal in 1991. The Dracunculus Eradication Programme has achieved a massive reduction in the number of cases. There were an estimated 3.3 million cases in 1986, 625 000 cases in 1990, fewer than 60 000 cases in 2002 and only 3190 cases in 2009, with the majority occurring in Sudan. Dracunculiasis is restricted to a central belt of countries in sub-Saharan Africa. General description The D. medinensis worms inhabit the cutaneous and subcutaneous tissues of infected individuals, the female reaching a length of up to 700 mm, and the male, 25 mm. When the female is ready to discharge larvae (embryos), its anterior end emerges from a blister or ulcer, usually on the foot or lower limb, and releases large numbers of rhabditiform larvae when the affected part of the body is immersed in water. The larvae can move about in water for approximately 3 days and during that time can be ingested by many species of Cyclops (cyclopoid Copepoda, Crustacea). The larvae penetrate into the haemocoelom, moult twice and are infective to a new host in about 2 weeks. If the Cyclops (0.5–2.0 mm) are swallowed in drinking-water, the larvae are released in the stomach, penetrate the intestinal and peritoneal walls and inhabit the subcutaneous tissues. Human health effects The onset of symptoms occurs just prior to the local eruption of the worm. The early manifestations of urticaria, erythema, dyspnoea, vomiting, pruritus and giddiness are of an allergic nature. In about 50% of cases, the whole worm is extruded in a few weeks; the lesion then heals rapidly, and disability is of limited duration. In the remaining cases, however, complications ensue, and the track of the worm becomes secondarily infected, leading to a severe inflammatory reaction that may result in abscess formation with disabling pain that lasts for months. Mortality is extremely rare, but permanent disability can result from contractures of tendons and chronic arthritis. The economic impact can be substantial. One study reported an 11% annual reduction in rice production from an area of eastern Nigeria, at a cost of US$ 20 million. Source and occurrence Infection with guinea worm is geographically limited to a central belt of countries in sub-Saharan Africa. Drinking-water containing infected Cyclops is the only source of infection with Dracunculus. The disease typically occurs in rural areas where piped water supplies are not available. Transmission tends to be highly seasonal, depending on changes in water sources. For instance, transmission is highest in the early rainy season in a dry savannah zone of Mali with under 800 mm annual rainfall but in the dry season in the humid savannah area of southern Nigeria with over 1300 mm annual rainfall. The eradication strategy combines a variety of interventions, including integrated surveillance systems, intensified case containment measures, provision of safe water and health education. Routes of exposure The only route of exposure is the consumption of drinking-water containing Cyclops spp. carrying infectious Dracunculus larvae. Significance in drinking‑water Dracunculus medinensis is the only human parasite that may be eradicated in the near future by the provision of safe drinking-water. Infection can be prevented by a number of relatively simple control measures. These include intervention strategies to prevent the release of D. medinensis larvae from female worms in infected patients into water and control of Cyclops spp. in water resources by means of fish. Prevention can also be achieved through the provision of boreholes and safe wells. Wells and springs should be surrounded by cement curbings, and bathing and washing in these waters should be avoided. Other control measures include filtration of water carrying infectious Dracunculus larvae through a fine mesh cloth to remove Cyclops spp. or inactivation of Cyclops spp. in drinking-water by treatment with chlorine. Selected bibliography Cairncross S, Muller R, Zagaria N (2002) Dracunculiasis (guinea worm disease) and the eradication initiative. Clinical Microbiology Reviews, 15:223–246. Hopkins DR, Ruiz-Tiben E (1991) Strategies for dracunculiasis eradication. Bulletin of the World Health Organization, 69:533–540. Fasciola spp. Fascioliasis is caused by two trematode species of the genus Fasciola: F. hepatica, present in Europe, Africa, Asia, the Americas and Oceania, and F. gigantica, mainly distributed in Africa and Asia. Human fascioliasis was considered a secondary zoonotic disease until the mid-1990s. In most regions, fascioliasis is a foodborne disease. However, the discovery of floating metacercariae in hyperendemic regions (including the Andean Altiplano region in South America) indicates that drinking-water may be a significant transmission route for fascioliasis in certain locations. General description The life cycle of F. hepatica and F. gigantica takes about 14–23 weeks and requires two hosts. The life cycle comprises four phases. In the first phase, the definitive host ingests metacercariae. The metacercariae excyst in the intestinal tract and then migrate to the liver and bile ducts. After 3–4 months, the flukes attain sexual maturity and produce eggs, which are excreted into the bile and intestine. Adult flukes can live for 9–14 years in the host. In the second phase, the eggs are excreted by the human or animal. Once in fresh water, a miracidium develops inside. In the third phase, miracidia penetrate a snail host and develop into cercaria, which are released into the water. In the fourth and final phase, cercariae swim for a short period of time until they reach a suitable attachment site (aquatic plants), where they encyst to form metacercariae, which become infective within 24 hours. Some metacercariae do not attach to plants but remain floating in the water. Human health effects The parasites inhabit the large biliary passages and the gall-bladder. Disease symptoms are different for the acute and chronic phases of the infection. The invasive or acute phase may last from 2 to 4 months and is characterized by symptoms such as dyspepsia, nausea and vomiting, abdominal pain and a high fever (up to 40 °C). Anaemia and allergic responses (e.g. pruritis, urticaria) may also occur. In children, the acute infection can be accompanied by severe symptoms and sometimes causes death. The obstructive or chronic phase (after months to years of infection) may be characterized by painful liver enlargement and in some cases obstructive jaundice, chest pains, loss of weight and cholelithiasis. The most important pathogenic sequelae are hepatic lesions and fibrosis and chronic inflammation of the bile ducts. Immature flukes may deviate during migration, enter other organs and cause ectopic fascioliasis in a range of subcutaneous tissues. Fascioliasis can be treated with triclabendazole. Source and occurrence Human cases have been increasing in 51 countries on five continents. Estimates of the numbers of humans with fascioliasis range from 2.4 to 17 million people or even higher, depending on unquantified prevalence in many African and Asian countries. Analysis of the geographical distribution of human cases shows that the correlation between human fascioliasis and fascioliasis in animals occurs only at a basic level. High prevalences in humans are not necessarily related to areas where fascioliasis is a great veterinary problem. Major health problems associated with fascioliasis occur in Andean countries (the Plurinational State of Bolivia, Peru, Chile, Ecuador), the Caribbean (Cuba), northern Africa (Egypt), the Near East (the Islamic Republic of Iran and neighbouring countries) and western Europe (Portugal, France and Spain). Routes of exposure Humans can contract fascioliasis when they ingest infective metacercariae by eating raw aquatic plants (and, in some cases, terrestrial plants, such as lettuce, irrigated with contaminated water), drinking contaminated water, using utensils washed in contaminated water or eating raw liver infected with immature flukes. Significance in drinking‑water Water is often cited as a human infection source. In the Bolivian Altiplano, 13% of metacercariae isolates are floating. Untreated drinking-water in hyperendemic regions often contains floating metacercariae; for example, a small stream crossing in the Altiplano region of the Plurinational State of Bolivia contained up to 7 metacercariae per 500 ml. The importance of fascioliasis transmission through water is supported by indirect evidence. There are significant positive associations between liver fluke infection and infection by other waterborne protozoans and helminths in Andean countries and in Egypt. In many human hyperendemic areas of the Americas, people do not have a history of eating watercress or other water plants. In the Nile Delta region, people living in houses with piped water had a higher infection risk. Metacercariae are likely to be resistant to chlorine disinfection but should be removed by various filtration processes. For example, in Tiba, Egypt, human prevalence was markedly decreased after filtered water was supplied to specially constructed washing units. Selected bibliography Mas-Coma S (2004) Human fascioliasis. In: Cotruvo JA et al., eds. Waterborne zoonoses: Identification, causes, and controls. IWA Publishing, London, on behalf of the World Health Organization, Geneva. Mas-Coma S, Esteban JG, Bargues MD (1999) Epidemiology of human fascioliasis: A review and proposed new classification. Bulletin of the World Health Organization, 77(4):340–346. WHO (1995) Control of foodborne trematode infections. Geneva, World Health Organization (WHO Technical Report Series, No. 849). Free-living nematodes General description Nematodes are the most numerous metazoan (many-celled) animals on Earth. Many of them are parasites of insects, plants or animals, including humans. Free-living species are abundant in aquatic environments, both freshwater and saltwater, and soil habitats. Not only are the vast majority of species encountered poorly understood biologically, but there may be thousands more unknown species of nematodes yet to be discovered. Nematodes are structurally simple, with the digestive tract running from the mouth on the anterior end to the posterior opening near the tail, being characterized as a tube in a tube. Nematodes found in drinking-water systems range in size from 0.1 mm to over 0.6 mm. About 20 different orders have been distinguished within the phylum Nematoda. Four of these orders (Rhabditida, Tylenchida, Aphelenchida and Dorylaimida) are particularly common in soil. Non-pathogenic free-living nematodes that have been found in drinking-water include Cheilobus, Diplogaster, Tobrilus, Aphelenchus and Rhabditis. Human health effects The presence of free-living nematodes in drinking-water does not necessarily indicate a direct health threat. It has largely been regarded by water suppliers as an “aesthetic” problem, either directly or through their association with discoloured water. High concentrations of nematodes in drinking-water have been reported to impart an unpleasant taste to the drinking-water. The presence of free-living nematodes in drinking-water reduces its acceptability to the consumer. It has been suggested that free-living nematodes could carry pathogenic bacteria in their gut. Such bacteria would be protected from chlorine disinfection and might therefore present a health hazard. Enterobacteriaceae have been isolated from the microflora in the guts of nematodes taken from a treated water supply and from the raw water from which it was derived. However, they were of non-pathogenic genera. Opportunistic pathogens such as Nocardia and Mycobacterium may also be carried in the gut of the free-living nematodes. There is no reason to suppose that pathogens would be selectively favoured. The microorganisms present in the gut of the free-living nematodes are much more likely to reflect those in the sediments and biofilms where they are feeding. In some cases, the motile larvae of parasitic nematodes such as hookworms (Necator americanus and Ancylostoma duodenale) and threadworms (Strongyloides stercoralis) are capable of moving themselves through sand filters or may be introduced into drinking-water during distribution as the result of faecal contamination. There are also some other species of nematodes that theoretically could infect humans through ingestion of contaminated water. Such a source of infection, however, is difficult to prove. Dracunculus medinensis is a noticeable parasitic nematode that may occur in drinking-water. This parasite is reported elsewhere in this section. Source and occurrence Because free-living nematodes are ubiquitous, they, as an egg or free-living larval or adult form, can enter the drinking-water supply at the storage, treatment, distribution or household level. The concentration of free-living nematodes in the raw water source generally corresponds to the turbidity of the water. The higher the turbidity, the larger the concentration of free-living nematodes there will be. In warm or even temperate weather, slow sand filters may discharge nematodes— and Origochaetes (e.g. Aeolosoma spp.), insect larvae (e.g. Chironomus spp.) and mosquitoes (Culex spp.)—by drawdown into the filtered water. Aquatic animals that successfully penetrate drinking-water treatment processes are largely benthic species, living on the bottoms or margins of water bodies. Routes of exposure Potential health concerns arise from exposure to the nematodes through ingestion of drinking-water, during recreation and potentially through consumption of fresh vegetables fertilized with sewage that received non-lethal treatment. Distinguishing pathogenic larvae of the hookworm and threadworm from free-living non-pathogenic nematodes in water is difficult and requires special knowledge of nematology. Significance in drinking‑water Large numbers of nematodes are not normally found in well-maintained, piped drinking-water systems. Eggs or infective larvae from species parasitic to humans (Ascaris, Trichuris, Ancylostoma, Necator and Strongyloides) and the many non-pathogenic nematodes are not usually present in protected groundwater sources or are generally removed during treatment processes. In some circumstances, when the water contains a high nutrient or organic matter content and the ambient temperatures are appropriate, it may be possible for free-living nematodes to feed on microbial growth in the biofilms or slimes in treatment processes or in water mains and thus multiply within the system. This is particularly true if drinking-water sources have not been adequately protected, treatment systems are not adequate or not operated and maintained properly, the distribution system is leaking or there are many stagnant areas or “dead zones” in the distribution system. Detection of large numbers of nematodes (live and dead) in drinking-water indicates that there is a problem that needs to be resolved, without necessarily implying a direct health risk. Selected bibliography Ainsworth R, ed. (2004) Safe piped water: Managing microbial water quality in piped distribution systems. Geneva, World Health Organization. Brusca RC, Brusca GJ (2002) Invertebrates, 2nd ed. Sunderland, MA, Sinauer Associates Inc. Chang SL, Woodward RL, Kabler PW (1960) Survey of free-living nematodes and amebas in municipal supplies. Journal of the American Water Works Association, 52:613. Endo T, Morishima Y (2004) Major helminth zoonoses in water. In: Cotruvo JA et al., eds. Waterborne zoonoses: Identification, causes, and controls. IWA Publishing, London, on behalf of the World Health Organization, Geneva, pp. 291–304. Evins C, Greaves GF (1979) Penetration of water treatment works by animals. Medmenham, Water Research Centre (Technical Report TR 115). Lupi E, Ricci V, Burrini D (1995) Recovery of bacteria in nematodes from a drinking water supply. Journal of Water Supply: Research and Technology – Aqua, 44:212–218. McSorley R (2007) Soil-inhabiting nematodes, Phylum Nematoda. Gainesville, FL, University of Florida, Institute of Food and Agricultural Sciences, Department of Entomology and Nematology. Mott JB, Mulamoottil G, Harrison AD (1981) A 13-month survey of nematodes at three water treatment plants in southern Ontario, Canada. Water Research, 15:729. Tombes AS et al. (1979) The relationship between rainfall and nematode density in drinking water. Water Research, 13:619. Schistosoma spp. General description The genus Schistosoma is a member of the class Trematoda, commonly known as trematodes or blood flukes. The life cycle takes about 3–4 months and requires two hosts. There are about 20 species of Schistosoma, and adult flukes are found in humans, other mammals and birds. Unlike other trematode species, Schistosoma has two distinct sexual forms. In the most important human schistosomes, adult flukes are 12–20 mm in length and 0.3–0.6 mm in width; male flukes are shorter and thicker than the females. Adult worms of schistosomes reside in the mesenteric blood vessels of the definitive host. Once the worms mature, they mate, and the females produce eggs that are round or oval and vary in length from 50 to 200 μm. Depending on the infecting species, a large number of eggs released by the females reach either the intestine or the bladder and are excreted in faeces or urine, respectively. The eggs hatch in fresh water, and the larvae (miracidia) invade snail hosts, where they undergo asexual reproduction and develop into the infective larvae (cercariae). Cercariae have pear-shaped heads and forked tails and are 400–600 μm in length. They emerge into the water from snails and invade the final hosts, including humans. Human health effects Schistosomiasis, also known as bilharzia, is a group of infectious diseases caused by five major species of Schistosoma in humans. Intestinal schistosomiasis is caused by Schistosoma mansoni, S. japonicum, S. mekongi and S. intercalatum, whereas urinary schistosomiasis is caused by S. haematobium. Most of the symptoms of schistosomiasis are the manifestation of the body’s reaction to the eggs laid and are due to the intensity of the immune response of the host, not to the worms themselves. Therefore, the symptoms depend on the amount and location of eggs in the human host, and light infections can be asymptomatic. In some people, an initial allergic reaction (Katayama fever), including fever, chills, muscle pains and cough, can begin within 1–2 months of infection immediately before and during initial egg deposition. Chronic infections with S. mansoni, S. japonicum, S. intercalatum and S. mekongi result primarily in intestinal and hepatic symptoms, including bloody diarrhoea (bilharzial dysentery), abdominal pains and hepatosplenomegaly, whereas S. haematobium infection leads to urinary manifestation, including dysuria and haematuria. Important life-threatening complications that arise from chronic infections include liver fibrosis and portal hypertension. Later development of bladder cancer and renal failure is associated with urinary schistosomiasis. Rarely, eggs are found in the brain or spinal cord and can cause cerebral symptoms, such as seizures and paralysis. Anaemia and malnutrition are also found in young infected cases. Impaired growth, impaired development and poor cognition are signs of morbidity in infected school-age children. In total, more than 200 million people are infected in 75 countries. The number of deaths associated with schistosomiasis is estimated at 20 000 annually. Schistosomiasis is of great public health and socioeconomic importance in developing countries where it is endemic. Source and occurrence Schistosomes occur in tropical and subtropical freshwater sources. Schistosoma mansoni is found in Africa, the Arabian Peninsula, Brazil, Suriname, the Bolivarian Republic of Venezuela and some Caribbean islands; S.haematobium is found in Africa and the Middle East; S. japonicum is found in China, the Philippines and the Sulawesi Island of Indonesia; S. intercalatum is found in some countries of Central Africa; and S. mekongi is limited to the Mekong River in Cambodia and the Lao People’s Democratic Republic. Water resource development projects, including dam construction, have been identified as potential sources of elevated rates of schistosomiasis as a result of the production of increased habitats for freshwater snails. Humans are the principal reservoirs of S. haematobium, S. intercalatum and S. mansoni, although the latter has been reported in rodents. Various animals, such as humans, dogs, cats, rodents, pigs, cattle and water buffalo, are potential reservoirs of S. japonicum, whereas humans and dogs are potential reservoirs of S. mekongi. Routes of exposure Infection occurs through skin penetration when people are exposed to free-swimming cercariae in infested water used for agricultural, domestic and recreational activities. Infection does not occur through consumption of drinking-water. Cercariae of human infectious schistosomes penetrate the skin rapidly and transform into schistosomules, which migrate to the lungs through the circulatory system and develop into adult flukes in the mesenteric veins. If cercariae of non-human infectious schistosomes come in contact with human skin, they do not survive but can cause an inflammatory response, especially in hosts that have been exposed previously. Papular rash, known as schistosome cercarial dermatitis, can result at points of penetration of cercariae. The cercariae of avian schistosomes and probably bovine schistosomes are responsible for a majority of cases of this dermatitis, which has been reported throughout the world. Person-to-person transmission does not occur. Significance in drinking‑water Most infections occur in poor communities without access to safe drinking-water and adequate sanitation. Ready availability of safe drinking-water contributes to disease prevention by replacing use of infested water for domestic purposes. Within a water safety plan, control measures include prevention of source water contamination by human waste, snail control programmes and adequate treatment. Schistosoma cercariae can be removed by filtration and inactivated by chlorination. Selected bibliography Boulanger D et al. (1999) The oral route as a potential way of transmission of Schistosoma bovis in goats. Journal of Parasitology, 85:464–467. Esrey SA et al. (1991) Effects of improved water supply and sanitation on ascariasis, diarrhoea, dracunculiasis, hookworm infection, schistosomiasis, and trachoma. Bulletin of the World Health Organization, 69:609–621. Giver H et al. (1999) Peroral infection of pigs with Schistosoma japonicum cercariae. Veterinary Parasitology, 83:161–165. Hunter JM et al. (1993) Parasitic diseases in water resources development: The need for intersectoral negotiation. Geneva, World Health Organization. Noda S et al. (1997) Effect of piped water supply on human water contact patterns in a Schistosoma haematobium–endemic area in Coast Province, Kenya. American Journal of Tropical Medicine and Hygiene, 56:118–126. Steinmann P et al. (2006) Schistosomiasis and water resources development: Systematic review, meta-analysis, and estimates of people at risk. Lancet Infectious Diseases, 6:411–425. 11.5 Toxic cyanobacteria Detailed information on cyanobacteria and their toxins is available in the supporting document Toxic cyanobacteria in water (Annex 1). See also the chapter 12 fact sheets in these Guidelines for more information on the four key cyanobacterial toxin groups —that is, microcystins (MCs), cylindrospermopsins (CYNs), anatoxin-a variants (ATXs) and saxitoxins (STXs), including guideline values and their derivation. Significance in drinking‑water Cyanobacteria occur at low cell density in most surface water bodies. Where conditions promote their growth, they may attain high biomass concentrations termed “blooms”. Such biomass accumulations cause high organic loads, challenging treatment; some also have aesthetic impacts, particularly taste and odour, challenging acceptability (see section 10.1). The most notable feature of cyanobacteria in terms of public health impact is that many species may produce toxins (“cyanotoxins”). Cases of liver damage have been reported where cyanotoxins occurred in poorly treated drinking-water and where scum-forming cyanobacteria were ingested during recreational activities. Human fatalities attributable to cyanotoxins (MCs, with a possible contribution of CYNs) have also occurred through renal dialysis using inadequately treated water (including drinking-water) containing high cyanotoxin levels. Although mats of filamentous cyanobacteria growing on surfaces can contain high concentrations of neurotoxic ATXs and/or STXs within their cells and have caused numerous cases of rapid deaths of dogs that ingest such material, hazardous concentrations at sites of drinking-water abstraction from these mats have not been reported. As toxins released from such mats are rapidly diluted, hazardous concentrations in drinking-water would be expected only if substantial amounts of cell material from such mats entered the supply system and if treatment was inadequate. General description Cyanobacteria are photosynthetic bacteria that share some properties with algae. Notably, they possess chlorophyll a and liberate oxygen during photosynthesis. They also contain a blue pigment specific to cyanobacteria that can give some species a bluish-green appearance; hence, a common term for these organisms is blue-green algae. However, because they produce different pigments, a large number of cyanobacteria are not blue-green; they can range in colour from blue-green to yellow-brown to red. Most cyanobacteria are phototrophs, but some also exhibit heterotrophic growth, and some species can fix elemental nitrogen dissolved in water. Cyanobacteria can be identified by their morphology under a microscope to genus or even to species level. They may grow as separate cells or in multicellular filaments or colonies, which are sometimes large enough to be visible without a microscope. Some species float in the water, (i.e. “planktonic”), whereas other species may grow on surfaces as mats of filamentous cyanobacteria (i.e. “benthic”). Some planktonic species can regulate their buoyancy via intracellular gas vacuoles and thus can form surface scums or accumulate at the thermocline of thermally stratified reservoirs. In contrast, other planktonic species are evenly dispersed throughout the Table 11.1 Main bloom-forming planktonic freshwater cyanobacteria that produce toxinsa Current genus (previous genus) and speciesb Cyanotoxinsc Anabaena spp. MCs, ATXs, CYNs Aphanizomenon spp. CYNs, STXs, ATXs Chrysosporum (Aphanizomenon) ovalisporum CYNs Cuspidothrix (Aphanizomenon) issatschenkoi ATXs, STXs Cuspidothrix (Raphidiopsis) mediterranea Dolichospermum (Anabaena) spp. MCs, ATXs, STXs Microcystis spp. MCs Planktothrix spp. MCs Raphidiopsis spp., in particular, Raphidiopsis (Cylindrospermopsis) raciborskii CYNs, STXs a Confirmed in isolated strains using reliable analysis (high‑performance liquid chromatography, liquid chromatography–mass spectrometry, genetics). b Note that substantial taxonomic reorganization has moved some species from one genus to another; species name is given where the toxin(s) has been found only within that species or to highlight changes of the taxonomical classification, where the latter is indicated in parentheses. Cyanotoxins are given roughly in the order of their frequency of reporting. Sources: Bernard et al. (2016); chapter 2 in the supporting document Toxic cyanobacteria in water (Annex 1). mixed layers of the water body. A number of species encompass genotypes that can produce a range of toxins, as shown in Table 11.1. Human health effects Each cyanotoxin group has specific properties, with distinct concerns, including liver and kidney damage, neurotoxicity and tumour promotion. Repeated or chronic exposure is the primary concern for two groups of cyanotoxins: MCs and CYNs. For other groups, concern chiefly pertains to acute toxicity (in particular, the neurotoxic STXs and possibly ATXs). Acute symptoms reported after recreational contact with cyanobacterial blooms in fresh water include gastrointestinal disorders, fever and irritations of the skin, ears, eyes, throat and respiratory tract, but these effects are rare and usually mild. As they do not correspond to the mode of action of the known cyanotoxins, it is uncertain whether they are caused by yet unknown cyanobacterial components or by other agents in the water, possibly associated with the bloom (such as other organisms growing in the mucilage of cyanobacterial colonies). Cyanobacteria do not multiply in the human body and hence are not infectious. Source and occurrence Cyanobacteria are found in a diverse range of environments, including soils, seawater and, most notably, freshwater environments. They can occur widely in lakes, reservoirs, ponds and slow-flowing rivers. Blooms may rapidly form scums of very variable duration (hours to weeks). Blooms can occur seasonally, for several months on end, and in some climates perennially. They tend to recur in a given water body. Depending on the species, blooms may result in pronounced turbidity and greenish discoloration of water due to a high density of suspended cells. Planktonic species proliferate if concentrations of nutrients (phosphorus and nitrogen) in water bodies are sufficiently high, and thus blooms typically occur under eutrophic conditions. Other environmental conditions that can promote their growth include low turbulence and warm weather. Benthic species need clear water and thus typically occur where nutrient concentrations are too low for blooms. Where cyanobacterial blooms occur, toxins can be expected, at variable concentrations, depending on the toxin content of the cyanobacterial cells. Route of exposure The oral route is usually the most significant source of exposure to cyanotoxins. Exposure can occur through ingestion of drinking-water, during recreation, and in some situations potentially through consumption of food, particularly fish, molluscs and shellfish from bloom-ridden water bodies. Where surface water is used as the source for drinking-water, this may be the most relevant source of chronic exposure to cyanotoxins. Exposure to cyanotoxins via inhalation may also be a relevant pathway for specific recreational activities such as waterskiing or jet-skiing, and for specific occupational activities involving spraying with bloom-contaminated water. Although dermal exposure to toxins in marine cyanobacteria through recreational activities can be relevant, the compounds causing dermal effects in fresh waters are uncertain (see “Human health effects”, above). For specific population groups, exposure to cyanotoxins may occur by the parenteral route—for example, associated with the use of contaminated water for renal dialysis or infusions. Prevention and management Cyanobacteria are most effectively controlled in the context of developing a WSP (see chapter 4). Control measures to manage potential risks from cyanobacteria and their toxins in drinking-water should include adequate treatment, as well as measures to control development of cyanobacterial blooms. A variety of resource protection and source management actions are available to decrease the probability of bloom occurrence. The most sustainable and effective measure is to reduce nutrient (particularly phosphorus) inputs to water bodies to limit the amount of cyanobacterial biomass that can grow. This is achieved by controlling nutrient loads from sewage effluents, erosion and the amount of manure and fertilizers spread in the catchment. Furthermore, some measures within water bodies can render hydro-physical conditions less suitable for cyanobacteria; some can address phosphorus release from sediments; and some (“biomanipulation”) can support other biota in outcompeting cyanobacteria. Situating the site for raw water abstraction away from bays or water layers in which blooms accumulate may substantially reduce concentrations, and abstraction via bank filtration may remove both cells and toxins effectively. Drinking-water treatment that removes particles, such as conventional water treatment (coagulation, flocculation and filtration), can effectively remove cyanobacteria, including cell-bound cyanotoxins, if the process is optimized to target their removal. Care needs to be taken to avoid cell rupture and toxin release. Other water treatment techniques can remove extracellular cyanotoxins; see the cyanotoxin fact sheets in chapter 12 for more information. Measures to control cyanobacterial blooms may require years to take effect. While blooms occur, it is therefore important to implement short-term responses to prevent exposure to cyanotoxins. These need to be based on effective monitoring 313 11. MICROBIAL FACT SHEETS Note: For the alert levels mentioned, see Table 11.2; for a more detailed alert level framework scheme, see the drinking-water section in chapter 5 of the supporting document Toxic cyanobacteria in water (Annex 1). Figure 11.1 Simplified scheme of an alert level framework for cyanotoxins in drinking-water and inspection programmes that prioritize water bodies with conditions rendering blooms likely. Measures also need to be adaptive, increasing sampling and analyses when there is evidence of increasing amounts of cyanobacteria. Early-warning and short-term management responses are most effectively organized within an alert level framework (ALF) (Figure 11.1). An ALF primarily uses levels of cyanobacterial biomass to trigger responses when biomass reaches levels at which concentrations exceeding cyanotoxin alert values can no longer be excluded. Measures of biomass Table 11.2 Alert levels for cyanobacterial biomass indicators that should trigger management responses Alert values for indicators of Cyanotoxin alert valuesa cyanobacterial biomass Alert level (AL) thresholds Biovolume Chlorophyll a (with cyanobacteria dominant) MCs CYNs ATXs STXs (mm³/l) (μg/l) (μg/l) AL 1 0.3 1 1 (lifetime pGV) 0.7 (lifetime pGV) (3b) (1/10 of AL 2) (0.3b) (1/10 of AL 2) AL 2 4 12 12 (short‑term pGV) 3 (short‑term pGV) 30 (short‑term provisional reference value; also applicable for acute exposure) 3 (acute GV) GV, guideline value; pGV, provisional guideline value a See the corresponding chapter 12 fact sheets for information on how these values should be interpreted. b Note that the AL 1 thresholds for ATXs and STXs are not formal WHO values for lifetime exposure, but merely concentrations 10‑fold below those for acute exposure. used in these Guidelines (Table 11.2) are biovolume or chlorophyll a. However, any other indicator of biomass that is locally more useful may be used (e.g. fluorometry, cell counts, satellite imaging or even turbidity) if periodically “calibrated” to ensure that it adequately indicates cyanotoxin concentrations. Wherever possible, toxin analysis should be performed when blooms occur in the source water, particularly when biomass indicators approach Alert Level 2. Toxin concentrations in blooms can vary substantially, and periodically establishing local toxin:biomass ratios improves hazard analysis. Since the alert values for these biomass indicators reflect the upper range of MCs:biomass ratios found in field samples, toxin content in many blooms is much lower and calibrating toxin:biomass ratios through occasional toxin analysis from the local blooms may allow higher biomass thresholds to be used. Also, specifically for CYNs, the fraction dissolved in water may persist well after the CYN producing cyanobacteria have disappeared, and thus alert levels based on cyanobacterial biomass may not reflect the hazard from this toxin. For finished drinking-water, toxin analysis is important to assess the efficacy of removal by treatment. Selected bibliography Bernard et al (2016) Cyanobacteria associated with the production of cyanotoxins. Annex II in: Meriluoto J, Spoof L, Codd GA, eds (2017). Handbook of cyanobacterial monitoring and cyanotoxin analysis. John Wiley & Sons. Chorus I, Welker M, eds. Toxic cyanobacteria in water, 2nd edition. Boca Raton: CRC Press, on behalf of the World Health Organization, Geneva. Humpage A & Cunliffe D (2021): Drinking-water. In: Chorus I, Welker M, eds. Toxic cyanobacteria in water, 2 nd edition. Boca Raton: CRC Press, on behalf of the World Health Organization, Geneva. 11.6 Indicator organisms Indicator organisms are used for a range of purposes, including as indicators of: • faecal pollution in verification and surveillance monitoring; • the effectiveness of processes such as filtration or disinfection in validation; • integrity and cleanliness of distribution systems in operational monitoring. Further discussion on indicator organisms is contained in section 7.4 and the supporting document Assessing microbial safety of drinking water (Annex 1). Total coliform bacteria General description Total coliform bacteria include a wide range of aerobic and facultatively anaerobic, Gram-negative, non-spore-forming bacilli capable of growing in the presence of relatively high concentrations of bile salts with the fermentation of lactose and production of acid or aldehyde within 24 hours at 35–37 °C. Escherichia coli and thermotolerant coliforms are a subset of the total coliform group that can ferment lactose at higher temperatures (see below). As part of lactose fermentation, total coliforms produce the enzyme β-galactosidase. Traditionally, coliform bacteria were regarded as belonging to the genera Escherichia, Citrobacter, Klebsiella and Enterobacter, but the group is more heterogeneous and includes a wider range of genera, such as Serratia and Hafnia. The total coliform group includes both faecal and environmental species. Indicator value Total coliforms include organisms that can survive and grow in water. Hence, they are not useful as an indicator of faecal pathogens, but they can be used to assess the cleanliness and integrity of distribution systems and the potential presence of biofilms. However, there are better indicators for these purposes. It has been proposed that total coliforms could be used as a disinfection indicator. However, the test for total coliforms is far slower and less reliable than direct measurement of disinfectant residual. In addition, total coliforms are far more sensitive to disinfection than are enteric viruses and protozoa. HPC measurements detect a wider range of microorganisms and are generally considered a better indicator of distribution system integrity and cleanliness. Source and occurrence Total coliform bacteria (excluding E. coli) occur in both sewage and natural waters. Some of these bacteria are excreted in the faeces of humans and animals, but many coliforms are heterotrophic and able to multiply in water and soil environments. Total coliforms can also survive and grow in water distribution systems, particularly in the presence of biofilms. Application in practice Total coliforms are generally measured in 100 ml samples of water. A variety of relatively simple procedures are available based on the production of acid from lactose or the production of the enzyme β-galactosidase. The procedures include membrane filtration followed by incubation of the membranes on selective media at 35–37 °C and counting of colonies after 24 hours. Alternative methods include most probable number procedures using tubes or microtitre plates and presence/absence tests. Field test kits are available. Significance in drinking‑water Total coliforms should be absent immediately after disinfection, and the presence of these organisms indicates inadequate treatment. The presence of total coliforms in distribution systems and stored water supplies can reveal regrowth and possible biofilm formation or contamination through ingress of foreign material, including soil or plants. Selected bibliography Ashbolt NJ, Grabow WOK, Snozzi M (2001) Indicators of microbial water quality. In: Fewtrell L, Bartram J, eds. Water quality—Guidelines, standards and health: Assessment of risk and risk management for water-related infectious disease. London, IWA Publishing, pp. 289–315 (WHO Water Series). Grabow WOK (1996) Waterborne diseases: Update on water quality assessment and control. Water SA, 22:193–202. Sueiro RA et al. (2001) Evaluation of Coli-ID and MUG Plus media for recovering Escherichia coli and other coliform bacteria from groundwater samples. Water Science and Technology, 43:213–216. Escherichia coli and thermotolerant coliform bacteria General description Total coliform bacteria that are able to ferment lactose at 44–45 °C are known as thermotolerant coliforms. In most waters, the predominant genus is Escherichia, but some types of Citrobacter, Klebsiella and Enterobacter are also thermotolerant. Escherichia coli can be differentiated from the other thermotolerant coliforms by the ability to produce indole from tryptophan or by the production of the enzyme β-glucuronidase. Escherichia coli is present in very high numbers in human and animal faeces and is rarely found in the absence of faecal pollution, although there is some evidence for growth in tropical soils. Thermotolerant coliform species other than E. coli can include environmental organisms. Indicator value Escherichia coli is considered the most suitable indicator of faecal contamination. In most circumstances, populations of thermotolerant coliforms are composed predominantly of E. coli; as a result, this group is regarded as a less reliable but acceptable indicator of faecal pollution. Escherichia coli (or, alternatively, thermotolerant coliforms) is the first organism of choice in monitoring programmes for verification, including surveillance of drinking-water quality. These organisms are also used as disinfection indicators, but testing is far slower and less reliable than direct measurement of disinfectant residual. In addition, E. coli is far more sensitive to disinfection than are enteric viruses and protozoa. Source and occurrence Escherichia coli occurs in high numbers in human and animal faeces, sewage and water subject to recent faecal pollution. Water temperatures and nutrient conditions present in drinking-water distribution systems are highly unlikely to support the growth of these organisms. Application in practice Escherichia coli (or, alternatively, thermotolerant coliforms) are generally measured in 100 ml samples of water. A variety of relatively simple procedures are available based on the production of acid and gas from lactose or the production of the enzyme β-glucuronidase. The procedures include membrane filtration followed by incubation of the membranes on selective media at 44–45 °C and counting of colonies after 24 hours. Alternative methods include most probable number procedures using tubes or microtitre plates and presence/absence tests, some for volumes of water larger than 100 ml. Field test kits are available. Significance in drinking‑water The presence of E. coli (or, alternatively, thermotolerant coliforms) provides evidence of recent faecal contamination, and detection should lead to consideration of further action, which could include further sampling and investigation of potential sources such as inadequate treatment or breaches in distribution system integrity. Selected bibliography Ashbolt NJ, Grabow WOK, Snozzi M (2001) Indicators of microbial water quality. In: Fewtrell L, Bartram J, eds. Water quality—Guidelines, standards and health: Assessment of risk and risk management for water-related infectious disease. London, IWA Publishing, pp. 289–315 (WHO Water Series). George I et al. (2001) Use of rapid enzymatic assays to study the distribution of faecal coliforms in the Seine river (France). Water Science and Technology, 43:77–80. Grabow WOK (1996) Waterborne diseases: Update on water quality assessment and control. Water SA, 22:193–202. Sueiro RA et al. (2001) Evaluation of Coli-ID and MUG Plus media for recovering Escherichia coli and other coliform bacteria from groundwater samples. Water Science and Technology, 43:213–216. Heterotrophic plate counts A substantial review of the use of HPC is available (see the supporting document Heterotrophic plate counts and drinking-water safety; Annex 1). General description HPC measurement detects a wide spectrum of heterotrophic microorganisms, including bacteria and fungi, based on the ability of the organisms to grow on rich growth media, without inhibitory or selective agents, over a specified incubation period and at a defined temperature. The spectrum of organisms detected by HPC testing includes organisms sensitive to disinfection processes, such as coliform bacteria; organisms resistant to disinfection, such as spore formers; and organisms that rapidly proliferate in treated water in the absence of residual disinfectants. The tests detect only a small proportion of the microorganisms that are present in water. The population recovered will differ according to the method and conditions applied. Although standard methods have been developed, there is no single universal HPC measurement. A range of media is available, incubation temperatures used vary from 20 °C to 37 °C and incubation periods range from a few hours to 7 days or more. Indicator value The test has little value as an indicator of pathogen presence but can be useful in operational monitoring as a treatment and disinfectant indicator, where the objective is to keep numbers as low as possible. In addition, HPC measurement can be used in assessing the cleanliness and integrity of distribution systems and the presence of biofilms. Source and occurrence Heterotrophic microorganisms include both members of the natural (typically nonhazardous) microbial flora of water environments and organisms present in a range of pollution sources. They occur in large numbers in raw water sources. The actual organisms detected by HPC tests vary widely between locations and between consecutive samples. Some drinking-water treatment processes, such as coagulation and sedimentation, reduce the number of HPC organisms in water. However, the organisms proliferate in other treatment processes, such as biologically active carbon and sand filtration. Numbers of HPC organisms are reduced significantly by disinfection practices, such as chlorination, ozonation and UV light irradiation. However, in practice, none of the disinfection processes sterilizes water; under suitable conditions, such as the absence of disinfectant residuals, HPC organisms can grow rapidly. HPC organisms can grow in water and on surfaces in contact with water as biofilms. The principal determinants of growth or “regrowth” are temperature, availability of nutrients, including assimilable organic carbon, lack of disinfectant residual and stagnation. Application in practice No sophisticated laboratory facilities or highly trained staff are required. Results on simple aerobically incubated agar plates are available within hours to days, depending on the characteristics of the procedure used. Significance in drinking‑water After disinfection, numbers would be expected to be low; for most uses of HPC test results, however, actual numbers are of less value than changes in numbers at particular locations. In distribution systems, increasing numbers can indicate a deterioration in cleanliness, possibly stagnation and the potential development of biofilms. HPC can include potentially “opportunistic” pathogens such as Acinetobacter, Aeromonas, Flavobacterium, Klebsiella, Moraxella, Serratia, Pseudomonas and Xanthomonas. However, there is no evidence of an association of any of these organisms with gastrointestinal infection through ingestion of drinking-water in the general population. Selected bibliography Ashbolt NJ, Grabow WOK, Snozzi M (2001) Indicators of microbial water quality. In: Fewtrell L, Bartram J, eds. Water quality—Guidelines, standards and health: Assessment of risk and risk management for water-related infectious disease. London, IWA Publishing, pp. 289–315 (WHO Water Series). Bartram J et al., eds (2003) Heterotrophic plate counts and drinking-water safety: The significance of HPCs for water quality and human health. London, IWA Publishing (WHO Emerging Issues in Water and Infectious Disease Series). Intestinal enterococci General description Intestinal enterococci are a subgroup of the larger group of organisms defined as faecal streptococci, comprising species of the genus Streptococcus. These bacteria are Gram-positive and relatively tolerant of sodium chloride and alkaline pH levels. They are facultatively anaerobic and occur singly, in pairs or as short chains. Faecal streptococci including intestinal enterococci all give a positive reaction with Lancefield’s Group D antisera and have been isolated from the faeces of warm-blooded animals. The subgroup intestinal enterococci consists of the species Enterococcus faecalis, E. faecium, E. durans and E. hirae. This group was separated from the rest of the faecal streptococci because they are relatively specific for faecal pollution. However, some intestinal enterococci isolated from water may occasionally also originate from other habitats, including soil, in the absence of faecal pollution. Indicator value The intestinal enterococci group can be used as an indicator of faecal pollution. Most species do not multiply in water environments. The numbers of intestinal enterococci in human faeces are generally about an order of magnitude lower than those of E. coli. Important advantages of this group are that they tend to survive longer in water environments than E. coli (or thermotolerant coliforms), are more resistant to drying and are more resistant to chlorination. Intestinal enterococci have been used in testing of raw water as an indicator of faecal pathogens that survive longer than E. coli and in drinking-water to augment testing for E. coli. In addition, they have been used to test water quality after repairs to distribution systems or after new mains have been laid. Source and occurrence Intestinal enterococci are typically excreted in the faeces of humans and other warmblooded animals. Some members of the group have also been detected in soil in the absence of faecal contamination. Intestinal enterococci are present in large numbers in sewage and water environments polluted by sewage or wastes from humans and animals. Application in practice Enterococci are detectable by simple, inexpensive cultural methods that require basic bacteriology laboratory facilities. Commonly used methods include membrane filtration with incubation of membranes on selective media and counting of colonies after incubation at 35–37 °C for 48 hours. Other methods include a most probable number technique using microtitre plates where detection is based on the ability of intestinal enterococci to hydrolyse 4-methyl-umbelliferyl-β-D-glucoside in the presence of thallium acetate and nalidixic acid within 36 hours at 41 °C. Significance in drinking‑water The presence of intestinal enterococci provides evidence of recent faecal contamination, and detection should lead to consideration of further action, which could include further sampling and investigation of potential sources such as inadequate treatment or breaches in distribution system integrity. Selected bibliography Ashbolt NJ, Grabow WOK, Snozzi M (2001) Indicators of microbial water quality. In: Fewtrell L, Bartram J, eds. Water quality—Guidelines, standards and health: Assessment of risk and risk management for water-related infectious disease. London, IWA Publishing, pp. 289–315 (WHO Water Series). Grabow WOK (1996) Waterborne diseases: Update on water quality assessment and control. Water SA, 22:193–202. Junco TT et al. (2001) Identification and antibiotic resistance of faecal enterococci isolated from water samples. International Journal of Hygiene and Environmental Health, 203:363–368. Pinto B et al. (1999) Characterization of “faecal streptococci” as indicators of faecal pollution and distribution in the environment. Letters in Applied Microbiology, 29:258–263. Clostridium perfringens General description Clostridium spp. are Gram-positive, anaerobic, sulfite-reducing bacilli. They produce spores that are exceptionally resistant to unfavourable conditions in water environments, including UV irradiation, temperature and pH extremes, and disinfection processes, such as chlorination. The characteristic species of the genus, C. perfringens, is a member of the normal intestinal flora of 13–35% of humans and other warm-blooded animals. Other species are not exclusively of faecal origin. Like E. coli, C. perfringens does not multiply in most water environments and is a highly specific indicator of faecal pollution. Indicator value In view of the exceptional resistance of C. perfringens spores to disinfection processes and other unfavourable environmental conditions, C. perfringens has been proposed as an indicator of protozoa in treated drinking-water supplies. In addition, C. perfringens can serve as an indicator of faecal pollution that took place previously and hence can indicate sources liable to intermittent contamination. The evidence that Clostridium is a reliable indicator for enteric viruses is limited and inconsistent, largely based on one study of reductions by drinking-water treatment. Results should be treated with some caution, as the exceptionally long survival times of its spores are likely to far exceed those of enteric pathogens. Clostridium perfringens spores are smaller than protozoan (oo)cysts and may be useful indicators of the effectiveness of filtration processes. Source and occurrence Clostridium perfringens and its spores are virtually always present in sewage. The organism does not multiply in water environments. Clostridium perfringens is present more often and in higher numbers in the faeces of some animals, such as dogs, than in the faeces of humans and less often in the faeces of many other warm-blooded animals. The numbers excreted in faeces are normally substantially lower than those of E. coli. Application in practice Vegetative cells and spores of C. perfringens are usually detected by membrane filtration techniques in which membranes are incubated on selective media under strict anaerobic conditions. These detection techniques are not as simple and inexpensive as those for other indicators, such as E. coli and intestinal enterococci. Significance in drinking‑water The presence of C. perfringens in drinking-water can be an indicator of intermittent faecal contamination. Potential sources of contamination should be investigated. Filtration processes designed to remove enteric viruses or protozoa should also remove C. perfringens. Detection in water immediately after treatment should lead to investigation of filtration plant performance. Selected bibliography Araujo M et al. (2001) Evaluation of fluorogenic TSC agar for recovering Clostridium perfringens in groundwater samples. Water Science and Technology, 43:201–204. Ashbolt NJ, Grabow WOK, Snozzi M (2001) Indicators of microbial water quality. In: Fewtrell L, Bartram J, eds. Water quality—Guidelines, standards and health: Assessment of risk and risk management for water-related infectious disease. London, IWA Publishing, pp. 289–315 (WHO Water Series). Nieminski EC, Bellamy WD, Moss LR (2000) Using surrogates to improve plant performance. Journal of the American Water Works Association, 92(3):67–78. Payment P, Franco E (1993) Clostridium perfringens and somatic coliphages as indicators of the efficiency of drinking-water treatment for viruses and protozoan cysts. Applied and Environmental Microbiology, 59:2418–2424. Coliphages General description Bacteriophages (phages) are viruses that use only bacteria as hosts for replication. Coliphages use E. coli and closely related species as hosts and hence can be released by these bacterial hosts into the faeces of humans and other warm-blooded animals. Coliphages used in water quality assessment are divided into the major groups of somatic coliphages and F-RNA coliphages. Differences between the two groups include the route of infection. Somatic coliphages initiate infection by attaching to receptors permanently located on the cell wall of hosts. They replicate more frequently in the gastrointestinal tract of warm-blooded animals but can also replicate in water environments. Somatic coliphages consist of a wide range of phages (members of the phage families Myoviridae, Siphoviridae, Podoviridae and Microviridae) with a spectrum of morphological types. F-RNA coliphages initiate infection by attaching to fertility (F-, sex) fimbriae on E. coli hosts. These F-fimbriae are produced only by bacteria carrying the fertility (F-) plasmid. As F-fimbriae are produced only in the logarithmic growth phase at temperatures above 30 °C, F-RNA phages are not likely to replicate in environments other than the gastrointestinal tract of warm-blooded animals. F-RNA coliphages comprise a restricted group of closely related phages, which belong to the family Leviviridae, and consist of a single-stranded RNA genome and an icosahedral capsid that is morphologically similar to that of picornaviruses. F-RNA coliphages have been divided into serological types I–IV, which can be identified as genotypes by molecular techniques such as gene probe hybridization. Members of groups I and IV have to date been found exclusively in (non-human) animal faeces, and group III in human faeces. Group II phages have been detected in human faeces and no animal faeces other than about 28% of porcine faeces. This specificity, which is not fully understood, offers a potential tool to distinguish between faecal pollution of human and animal origin under certain conditions and limitations. Indicator value Phages share many properties with human viruses, notably composition, morphology, structure and mode of replication. As a result, coliphages are useful models or surrogates to assess the behaviour of enteric viruses in water environments and the sensitivity to treatment and disinfection processes. In this regard, they are superior to faecal bacteria and could be considered for inclusion in verification and surveillance monitoring where source waters are known to be affected by human faecal waste. However, there is no direct correlation between numbers of coliphages and numbers of enteric viruses. In addition, coliphages cannot be absolutely relied upon as an indicator for enteric viruses. This has been confirmed by the isolation of enteric viruses from treated and disinfected drinking-water supplies that yielded negative results in conventional tests for coliphages. F-RNA coliphages provide a more specific indicator of faecal pollution than somatic phages. In addition, F-RNA coliphages are better indicators of the behaviour of enteric viruses in water environments and their response to treatment and disinfection processes than are somatic coliphages. This has been confirmed by studies in which the behaviour and survival of F-RNA coliphages, somatic phages, faecal bacteria and enteric viruses have been compared. Available data indicate that the specificity of F-RNA serogroups (genotypes) for human and animal excreta may prove useful in the distinction between faecal pollution of human and animal origin. However, there are shortcomings and conflicting data that need to be resolved, and the extent to which this tool can be applied in practice remains to be elucidated. Owing to the limitations of coliphages, they are best used in laboratory investigations, pilot trials and possibly validation testing. They are not suitable for operational or verification (including surveillance) monitoring. Source and occurrence Coliphages are excreted by humans and animals in relatively low numbers. As a result of their respective modes of replication and host specificity, somatic coliphages are generally excreted by most humans and animals, whereas F-RNA coliphages are excreted by a variable and generally lower percentage of humans and animals. Available data indicate that in some communities, F-RNA phages are detectable in 10% of human, 45% of bovine, 60% of porcine and 70% of poultry faecal specimens. Somatic coliphages have been found to generally outnumber F-RNA phages in water environments by a factor of about 5 and cytopathogenic human viruses by a factor of about 500, although these ratios vary considerably. Sewage contains somatic coliphages in numbers of the order of 106–108 per litre; in one study, slaughterhouse wastewater was found to contain somatic coliphages in numbers up to 1010 per litre. There are indications that they may multiply in sewage, and somatic coliphages may multiply in natural water environments using saprophytic hosts. Somatic phages and F-RNA phages have been detected in numbers up to 105 per litre in lake and river water. Application in practice Somatic coliphages are detectable by relatively simple and inexpensive plaque assays, which yield results within 24 hours. Plaque assays for F-RNA coliphages are not quite as simple, because the culture of host bacteria has to be in the logarithmic growth phase at a temperature above 30 °C to ensure that F-fimbriae are present. Plaque assays using large petri dishes have been designed for the quantitative enumeration of plaques in 100 ml samples, and presence/absence tests have been developed for volumes of water of 500 ml or more. Significance in drinking‑water As coliphages typically replicate in the gastrointestinal tract of humans and warmblooded animals, their presence in drinking-water provides an indicator of faecal pollution and hence the potential presence of enteric viruses and possibly also other pathogens. The presence of coliphages in drinking-water also indicates shortcomings in treatment and disinfection processes designed to remove enteric viruses. F-RNA coliphages provide a more specific indicator for faecal pollution. The absence of coliphages from treated drinking-water supplies does not confirm the absence of pathogens such as enteric viruses and protozoan parasites. Selected bibliography Ashbolt NJ, Grabow WOK, Snozzi M (2001) Indicators of microbial water quality. In: Fewtrell L, Bartram J, eds. Water quality—Guidelines, standards and health: Assessment of risk and risk management for water-related infectious disease. London, IWA Publishing, pp. 289–315 (WHO Water Series). Grabow WOK (2001) Bacteriophages: Update on application as models for viruses in water. Water SA, 27:251–268. Mooijman KA et al. (2001) Optimisation of the ISO-method on enumeration of somatic coliphages (draft ISO 10705-2). Water Science and Technology, 43:205–208. Schaper M et al. (2002) Distribution of genotypes of F-specific RNA bacteriophages in human and non-human sources of faecal pollution in South Africa and Spain. Journal of Applied Microbiology, 92:657–667. Storey MV, Ashbolt NJ (2001) Persistence of two model enteric viruses (B40-8 and MS-2 bacteriophages) in water distribution pipe biofilms. Water Science and Technology, 43:133– 138. Bacteroides fragilis phages General description The bacterial genus Bacteroides inhabits the human gastrointestinal tract in greater numbers than E. coli. Faeces can contain 109–1010 Bacteroides per gram compared with 106–108 E. coli per gram. Bacteroides are rapidly inactivated by environmental oxygen levels, but Bacteroides bacteriophages are resistant to unfavourable conditions. Two groups of B. fragilis phages are used as indicators in water quality assessment. One is a restricted group of phages that specifically uses B. fragilis strain HSP40 as host. This group of phages appears unique, because it is found only in human faeces and not in faeces of animals. The numbers of these phages in sewage appear to be relatively low, and they are almost absent in some geographical areas. The B. fragilis HSP40 phages belong to the family Siphoviridae, with flexible non-contractile tails, double-stranded DNA and capsids with a diameter of up to 60 nm. The second group of Bacteroides phages used as indicators is those that use B. fragilis strain RYC2056 as a host. This group includes a substantially wider spectrum of phages, occurring in the faeces of humans and many animals. The numbers of these phages in sewage are generally substantially higher than those of B. fragilis HSP40 phages. Indicator value Bacteroides bacteriophages have been proposed as a possible indicator of faecal pollution as a result of their specific association with faecal material and exceptional resistance to environmental conditions. In particular, B. fragilis HSP40 phages are found only in human faeces. Bacteroides fragilis phage B40-8, a typical member of the group of B. fragilis HSP40 phages, has been found to be more resistant to inactivation by chlorine than poliovirus type 1, simian rotavirus SA11, coliphage f2, E. coli and Streptococcus faecalis. Bacteroides fragilis strain RYC2056 phages seem to be likewise relatively resistant to disinfection. Indicator shortcomings of B. fragilis phages include relatively low numbers in sewage and polluted water environments. This applies in particular to B. fragilis HSP40 phages. Human enteric viruses have been detected in drinking-water supplies that yielded negative results in conventional tests for B. fragilis HSP40 phages. Owing to the limitations of Bacteroides bacteriophages, they are best used in laboratory investigations, pilot trials and possibly validation testing. Source and occurrence Bacteroides fragilis HSP40 phages are excreted by about 10–20% of humans in certain parts of the world; consequently, their numbers in sewage are substantially lower than those of somatic and even F-RNA coliphages. A mean count of 67 B. fragilis HSP40 phages per litre in a sewage-polluted river has been reported. In some parts of the world, B. fragilis HSP40 phages would appear not to be detectable in sewage at all. Phages using B. fragilis RYC2056 as host are excreted in larger numbers and seem to occur more universally. On average, these phages are excreted by more than 25% of humans. In a survey of water environments, B. fragilis HSP40 phages have been found to outnumber cytopathogenic enteric viruses on average by only about 5-fold. Theoretically, wastewaters could be expected to contain higher levels of B. fragilis phages than those detected. The reason for the discrepancy may be due to failure in maintaining sufficiently anaerobic conditions during the performance of plaque assays. Improvement of detection methods may result in the recording of higher numbers of B. fragilis phages in sewage and polluted water environments. Application in practice Disadvantages of B. fragilis phages are that the detection methods are more complex and expensive than those for coliphages. Costs are increased by the need to use antibiotics for purposes of selection and to incubate cultures and plaque assays under absolute anaerobic conditions. Results of plaque assays are usually available after about 24 hours compared with about 8 hours for coliphages. Significance in drinking‑water The presence of B. fragilis phages in drinking-water is sound evidence of faecal pollution as well as shortcomings in water treatment and disinfection processes. In addition, the presence of B. fragilis HSP40 phages strongly indicates faecal pollution of human origin. However, B. fragilis phages occur in relatively low numbers in sewage, polluted water environments and drinking-water supplies. This implies that the absence of B. fragilis phages from treated drinking-water supplies does not confirm the absence of pathogens such as enteric viruses and protozoan parasites. Selected bibliography Bradley G et al. (1999) Distribution of the human faecal bacterium Bacteroides fragilis and their relationship to current sewage pollution indicators in bathing waters. Journal of Applied Microbiology, 85(Suppl.):90S–100S. Grabow WOK (2001) Bacteriophages: Update on application as models for viruses in water. Water SA, 27:251–268. Puig A et al. (1999) Diversity of Bacteroides fragilis strains in their capacity to recover phages from human and animal wastes and from fecally polluted wastewater. Applied and Environmental Microbiology, 65:1772–1776. Storey MV, Ashbolt NJ (2001) Persistence of two model enteric viruses (B40-8 and MS-2 bacteriophages) in water distribution pipe biofilms. Water Science and Technology, 43:133– 138. Tartera C, Lucena F, Jofre J (1989) Human origin of Bacteroides fragilis bacteriophages present in the environment. Applied and Environmental Microbiology, 10:2696–2701. Enteric viruses General description The viruses referred to here are a combined group of those that infect the human gastrointestinal tract and are predominantly transmitted by the faecal–oral route. Well-known members of this group include the enteroviruses, astroviruses, enteric adenoviruses, orthoreoviruses, rotaviruses, caliciviruses and hepatitis A and E viruses. The enteric viruses cover a wide spectrum of viruses, members of which are a major cause of morbidity and mortality worldwide. Members of the group of enteric viruses differ with regard to structure, composition, nucleic acid and morphology. There are also differences in the numbers and frequency of excretion, survival in the environment and resistance to water treatment processes. Enteric viruses have robust capsids that enable them to survive unfavourable conditions in the environment as well as allowing passage through the acidic and proteolytic conditions in the stomach on their way to the duodenum, where they infect susceptible epithelial cells. Indicator value The use of enteric viruses as indicator organisms is based on the shortcomings of the existing choices. The survival of faecal bacteria in water environments and the sensitivity to treatment and disinfection processes differ substantially from those of enteric viruses. Monitoring based on one or more representatives of the large group of enteric viruses themselves would therefore be more valuable for assessment of the presence of any of the enteric viruses in water and the response to control measures. Source and occurrence Enteric viruses are excreted by individuals worldwide at a frequency and in numbers that result in many of these viruses being universally present in substantial numbers in wastewater. However, the prevalence of individual members may vary to a large extent as a result of variations in rates of infection and excretion. Much higher numbers would be present during outbreaks. Application in practice Practical methods are not yet available for the routine monitoring of water supplies for a broad spectrum of enteric viruses. Viruses that are more readily detectable include members of the enterovirus, adenovirus and orthoreovirus groups. These viruses occur in polluted environments in relatively high numbers and can be detected by reasonably practical and moderate-cost techniques based on cytopathogenic effect in cell culture that yield results within 3–12 days (depending on the type of virus). In addition, progress in technology and expertise is decreasing costs. The cost for the recovery of enteric viruses from large volumes of drinking-water has been reduced extensively. Some techniques—for instance, those based on glass wool adsorption–elution—are inexpensive. The cost of cell culture procedures has also been reduced. Consequently, the cost of testing drinking-water supplies for cytopathogenic viruses has become acceptable for certain purposes. Testing could be used to validate the effectiveness of treatment processes and, in certain circumstances, as part of specific investigations to verify the performance of processes. The incubation times, cost and relative complexity of testing mean that enteric virus testing is not suitable for operational or verification (including surveillance) monitoring. Orthoreoviruses, and at least the vaccine strains of polioviruses detected in many water environments, also have the advantage of not constituting a health risk to laboratory workers. Significance in drinking‑water The presence of any enteric viruses in drinking-water should be regarded as an indicator for the potential presence of other enteric viruses, is conclusive evidence of faecal pollution and also provides evidence of shortcomings in water treatment and disinfection processes. Selected bibliography Ashbolt NJ, Grabow WOK, Snozzi M (2001) Indicators of microbial water quality. In: Fewtrell L, Bartram J, eds. Water quality—Guidelines, standards and health: Assessment of risk and risk management for water-related infectious disease. London, IWA Publishing, pp. 289–315 (WHO Water Series). Grabow WOK, Taylor MB, de Villiers JC (2001) New methods for the detection of viruses: Call for review of drinking-water quality guidelines. Water Science and Technology, 43:1–8. 12 Chemical fact sheets TThe background documents referred to in this chapter (as the principal reference for each fact sheet) may be found on the Water, Sanitation, Hygiene and Health web site at https://www.who.int/ teams/environment-climate-change-and-health/ water-sanitation-andhealth/chemical-haz ards-in-drinking-water. A complete list of references cited in this chapter, including the background documents for each chemical, is provided in Annex 2. 12.1 Chemical contaminants in drinking-water Acrylamide Residual acrylamide monomer occurs in polyacrylamide coagulants used in the treatment of drinking-water. In general, the maximum authorized dose of polymer is 1 mg/l. At a monomer content of 0.05%, this corresponds to a maximum theoretical concentration of 0.5 μg/l of the monomer in water. Practical concentrations may be lower by a factor of 2–3. This applies to the anionic and non-ionic polyacrylamides, but residual levels from cationic polyacrylamides may be higher. Polyacrylamides are also used as grouting agents in the construction of drinking-water reservoirs and wells. Human ex329 Guideline value 0.0005 mg/l (0.5 μg/l) Occurrence Concentrations up to a few micrograms per litre occasionally detected in tap water Basis of guideline value Combined mammary, thyroid and uterine tumours observed in female rats derivation in a drinking‑water study, and using the linearized multistage model Limit of detection 0.032 μg/l by gas chromatography (GC); 0.2 μg/l by high‑performance liquid chromatography (HPLC); 10 μg/l by HPLC with ultraviolet (UV) detection Treatment performance Conventional treatment processes do not remove acrylamide. Acrylamide concentrations in drinking‑water are usually controlled by limiting either the acrylamide content of polyacrylamide flocculants or the dose used, or both. Advances in analytical techniques are also beginning to allow control by direct measurement (see background document). Additional comments Every effort should be made to limit free acrylamide monomer in polyacrylamide used for water treatment, and water suppliers should also make every effort to ensure that residual acrylamide in drinking‑water is kept as low as is technically feasible. In particular, if acrylamide is controlled by limiting the amount dosed, overdosing should always be avoided. Assessment date 2011 Principal references FAO/WHO (2011) Evaluation of certain contaminants in food WHO (2011) Acrylamide in drinking-water posure is much greater from food than from drinking-water, owing to the formation of acrylamide in foods (e.g. breads, fried and roasted foods) cooked at high temperatures. Following ingestion, acrylamide is readily absorbed from the gastrointestinal tract and widely distributed in body fluids. Acrylamide can cross the placenta. It is neurotoxic, affects germ cells and impairs reproductive function. In mutagenicity assays, acrylamide was negative in the Ames test but induced gene mutations in mammalian cells and chromosomal aberrations in vitro and in vivo. In a long-term carcinogenicity study in rats exposed via drinking-water, acrylamide induced scrotal, thyroid and adrenal tumours in males and mammary, thyroid and uterine tumours in females. The International Agency for Research on Cancer (IARC) has placed acrylamide in Group 2A (probably carcinogenic to humans). The Joint Food and Agriculture Organization of the United Nations (FAO)/World Health Organization (WHO) Expert Committee on Food Additives (JECFA) has recently noted concerns regarding the carcinogenicity and neurotoxicity of acrylamide and concluded that dietary exposure should be reduced to as low a level as technically achievable. Recent data have shown that exposure to acrylamide from cooked food is much higher than previously thought. As it is difficult to control the intake of acrylamide from food, it is very important that the acrylamide content of polyacrylamide used as a coagulant aid in water treatment, the most important source of drinking-water contamination by acrylamide, be as low as possible and that polyacrylamide not be overdosed in an attempt to take a shortcut to improving coagulation. Alachlor Alachlor (Chemical Abstracts Service [CAS] No. 15972-60-8) is a pre-emergence and post-emergence herbicide used to control annual grasses and many broad-leaved weeds in maize and a number of other crops. It is lost from soil mainly through volatilization, photodegradation and biodegradation. Many alachlor degradation products have been identified in soil. Alachlor was included in the Prior Informed Consent procedure of the Rotterdam Convention on the basis of the final regulatory actions taken by the European Community and by Canada to ban alachlor as a pesticide. Guideline value 0.02 mg/l (20 μg/l) Occurrence Has been detected in groundwater and surface water; has also been detected in drinking‑water at levels below 0.002 mg/l Basis of guideline value Calculated by applying the linearized multistage model to data on the derivation incidence of nasal tumours in rats Limit of detection 0.1 μg/l by gas–liquid chromatography with electrolytic conductivity detection in the nitrogen mode or by capillary column GC with a nitrogen– phosphorus detector Treatment performance 0.001 mg/l should be achievable using granular activated carbon (GAC) Assessment date 1993 Principal reference WHO (2003) Alachlor in drinking-water On the basis of available experimental data, evidence for the genotoxicity of alachlor is considered to be equivocal. However, a metabolite of alachlor, 2,6-diethylaniline, has been shown to be mutagenic. Available data from two studies in rats clearly indicate that alachlor is carcinogenic, causing benign and malignant tumours of the nasal turbinate, malignant stomach tumours and benign thyroid tumours. Aldicarb Aldicarb (CAS No. 116-06-3) is a systemic pesticide used to control nematodes in soil and insects and mites on a variety of crops. It is very soluble in water and highly mobile in soil. It degrades mainly by biodegradation and hydrolysis, persisting for weeks to months. Guideline value 0.01 mg/l (10 μg/l) Occurrence Frequently found as a contaminant in groundwater in the vicinity of application areas, particularly when associated with sandy soil; concentrations in well water as high as 500 μg/l have been measured; aldicarb sulfoxide and aldicarb sulfone residues are found in an approximately 1:1 ratio in groundwater Acceptable daily intake (ADI) 0–0.003 mg/kg body weight based on cholinesterase depression in a single oral dose study in human volunteers Limit of detection 0.001 mg/l by reversed‑phase HPLC with fluorescence detection Treatment performance 0.001 mg/l should be achievable using GAC or ozonation Guideline value derivation • allocation to water • weight • consumption 10% of upper limit of ADI 60 kg adult 2 litres/day Additional comments The guideline value derived from the 1992 assessment of the Joint FAO/WHO Meeting on Pesticide Residues (JMPR) was very similar to the guideline value derived in the second edition, which was therefore retained. Assessment date 2003 Principal references FAO/WHO (1993) Pesticide residues in food—1992 evaluations WHO (2003) Aldicarb in drinking-water Aldicarb is one of the most acutely toxic pesticides in use, although the only consistently observed toxic effect with both long-term and single-dose administration is acetylcholinesterase inhibition. It is converted to the sulfoxide and sulfone. Aldicarb sulfoxide is a more potent inhibitor of acetylcholinesterase than aldicarb itself, whereas aldicarb sulfone is considerably less toxic than either aldicarb or the sulfoxide. The weight of evidence indicates that aldicarb, aldicarb sulfoxide and aldicarb sulfone are not genotoxic or carcinogenic. IARC has concluded that aldicarb is not classifiable as to its carcinogenicity (Group 3). Aldrin and dieldrin Aldrin (CAS No. 309-00-2) and dieldrin (CAS No. 60-57-1) are chlorinated pesticides that are used against soil-dwelling pests, for wood protection and, in the case of dieldrin, against insects of public health importance. Since the early 1970s, many countries have either severely restricted or banned the use of both compounds, particularly in agriculture. The two compounds are closely related with respect to their toxicology and mode of action. Aldrin is rapidly converted to dieldrin under most environmental conditions and in the body. Dieldrin is a highly persistent organochlorine compound that has low mobility in soil, can be lost to the atmosphere and bioaccumulates. Dietary exposure to aldrin/dieldrin is very low and decreasing. Guideline value Aldrin and dieldrin (combined): 0.000 03 mg/l (0.03 μg/l) Occurrence Seldom detected in drinking water; concentrations of aldrin and dieldrin in drinking‑water normally less than 0.01 μg/l; rarely present in groundwater Provisional tolerable daily 0.1 μg/kg body weight (combined total for aldrin and dieldrin), based on intake (PTDI) no‑observed‑adverse‑effect levels (NOAELs) of 1 mg/kg diet in the dog and 0.5 mg/kg diet in the rat, which are equivalent to 0.025 mg/kg body weight per day in both species, and applying an uncertainty factor of 250 based on concern about carcinogenicity observed in mice Limit of detection 0.003 μg/l for aldrin and 0.002 μg/l for dieldrin by GC with electron capture detector (ECD) Treatment performance 0.02 μg/l should be achievable using coagulation, GAC or ozonation Guideline value derivation • allocation to water • weight • consumption 1% of PTDI (In view of the reduction in exposure from food this value is probably very conservative.) 60 kg adult 2 litres/day Additional comments Aldrin and dieldrin are listed under the Stockholm Convention on Persistent Organic Pollutants. Hence, monitoring may occur in addition to that required by drinking‑water guidelines. Assessment date 2003 Principal references FAO/WHO (1995) Pesticide residues in food—1994 evaluations WHO (2003) Aldrin and dieldrin in drinking-water Both compounds are highly toxic in experimental animals, and cases of poisoning in humans have occurred. Aldrin and dieldrin have more than one mechanism of toxicity. The target organs are the central nervous system and the liver. In long-term studies, dieldrin was shown to produce liver tumours in both sexes of two strains of mice. It did not produce an increase in tumours in rats and does not appear to be genotoxic. IARC has classified aldrin and dieldrin in Group 3 (not classifiable as to its carcinogenicity to humans). Exposure through food has decreased significantly with the dramatic reduction in use. Aluminium Aluminium is the most abundant metallic element and constitutes about 8% of Earth’s crust. Aluminium salts are widely used in water treatment as coagulants to reduce organic matter, colour, turbidity and microorganism levels. Such use may lead to increased concentrations of aluminium in finished water. Where residual concentrations are high, undesirable colour and turbidity may ensue. Concentrations of aluminium at which such problems may occur are highly dependent on a number of water quality parameters and operational factors at the water treatment plant. Aluminium intake from foods, particularly those containing aluminium compounds used as food additives, represents the major route of aluminium exposure for the general public. The contribution of drinking-water to the total oral exposure to aluminium is usually less than 5% of the total intake. Reason for not establishing a guideline value A health‑based value of 0.9 mg/l could be derived from the JECFA provisional tolerable weekly intake (PTWI), but this value exceeds practicable levels based on optimization of the coagulation process in drinking‑water plants using aluminium‑based coagulants: 0.1 mg/l or less in large water treatment facilities and 0.2 mg/l or less in small facilities Assessment date 2009 Principal references FAO/WHO (2007) Aluminium (from all sources, including food additives) IPCS (1997) Aluminium WHO (2010) Aluminium in drinking-water There is little indication that orally ingested aluminium is acutely toxic to humans despite the widespread occurrence of the element in foods, drinking-water and many antacid preparations. It has been hypothesized that aluminium exposure is a risk factor for the development or acceleration of onset of Alzheimer disease in humans. The 1997 WHO Environmental Health Criteria document for aluminium concludes that: On the whole, the positive relationship between aluminium in drinking-water and AD [Alzheimer disease], which was demonstrated in several epidemiological studies, cannot be totally dismissed. However, strong reservations about inferring a causal relationship are warranted in view of the failure of these studies to account for demonstrated confounding factors and for total aluminium intake from all sources. Taken together, the relative risks for AD from exposure to aluminium in drinking-water above 100 μg/l, as determined in these studies, are low (less than 2.0). But, because the risk estimates are imprecise for a variety of methodological reasons, a population-attributable risk cannot be calculated with precision. Such imprecise predictions may, however, be useful in making decisions about the need to control exposures to aluminium in the general population. In 2007, JECFA developed a PTWI for aluminium from all sources of 1 mg/kg body weight. JECFA concluded the following: … the available studies have many limitations and are not adequate for defining the dose– response relationships. The Committee therefore based its evaluation on the combined evidence from several studies. The relevance of studies involving administration of aluminium compounds by gavage was unclear because the toxicokinetics after gavage were expected to differ from toxicokinetics after dietary administration, and the gavage studies generally did not report total aluminium exposure including basal levels in the feed. The studies conducted with dietary administration of aluminium compounds were considered most appropriate for the evaluation. The lowest LOELs [lowest-observed-effect levels] for aluminium in a range of different dietary studies in mice, rats and dogs were in the region of 50–75 mg/kg bw [body weight] per day expressed as Al. The Committee applied an uncertainty factor of 100 to the lower end of this range of LOELs (50 mg/kg bw per day expressed as Al) to allow for inter- and intraspecies differences. There are deficiencies in the database, notably the absence of NOELs [no-observedeffect levels] in the majority of the studies evaluated and the absence of long-term studies on the relevant toxicological end-points. The deficiencies are counterbalanced by the probable lower bioavailability of the less soluble aluminium species present in food. Overall, an additional uncertainty factor of three was considered to be appropriate. The Committee confirmed that the resulting health-based guidance value should be expressed as a PTWI, because of the potential for bioaccumulation. The Committee established a PTWI of 1 mg/kg bw for Al, which applies to all aluminium compounds in food, including additives. A health-based value derived from the JECFA PTWI would be 0.9 mg/l (rounded value), based on an allocation of 20% of the PTWI to drinking-water and assuming a 60 kg adult drinking 2 litres of water per day. However, there remain uncertainties as to the extent of aluminium absorption from drinking-water, which depends on a number of parameters, such as the aluminium salt administered, pH (for aluminium speciation and solubility), bioavailability and dietary factors. The beneficial effects of the use of aluminium as a coagulant in water treatment are recognized. Taking this into account, and considering the health concerns about aluminium (i.e. its potential neurotoxicity), a practicable level is derived, based on optimization of the coagulation process in drinking-water plants using aluminiumbased coagulants, to minimize aluminium levels in finished water. Several approaches are available for minimizing residual aluminium concentrations in treated water. These include use of optimum pH in the coagulation process, avoiding excessive aluminium dosage, good mixing at the point of application of the coagulant, optimum paddle speeds for flocculation and efficient filtration of the aluminium floc. Under good operating conditions, concentrations of aluminium of 0.1 mg/l or less are achievable in large water treatment facilities. Small facilities (e.g. those serving fewer than 10 000 people) might experience some difficulties in attaining this level, because the small size of the plant provides little buffering for fluctuation in operation; moreover, such facilities often have limited resources and limited access to the expertise needed to solve specific operational problems. For these small facilities, 0.2 mg/l or less is a practicable level for aluminium in finished water. As indicated above, a health-based value derived from the JECFA PTWI would be 0.9 mg/l (rounded value) based on an allocation of 20% of the PTWI to drinking-water and assuming a 60 kg adult drinking 2 litres of water per day. However, as also noted above, practicable levels based on optimization of the coagulation process in drinking-water plants using aluminium-based coagulants are less than 0.1 mg/l in large water treatment facilities and less than 0.2 mg/l in small facilities. In view of the importance of optimizing coagulation to prevent microbial contamination and the need to minimize deposition of aluminium floc in distribution systems, it is important to ensure that average residuals do not exceed these values. Ammonia The term ammonia includes the non-ionized (NH3) and ionized (NH4+) species. Ammonia in the environment originates from metabolic, agricultural and industrial processes and from disinfection with chloramine. Natural levels in groundwater and surface water are usually below 0.2 mg/l. Anaerobic groundwaters may contain up to 3 mg/l. Intensive rearing of farm animals can give rise to much higher levels in surface water. Ammonia contamination can also arise from cement mortar pipe linings. Ammonia in water is an indicator of possible bacterial, sewage and animal waste pollution. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 1993 Principal reference WHO (2003) Ammonia in drinking-water Ammonia is a major component of the metabolism of mammals. Exposure from environmental sources is insignificant in comparison with endogenous synthesis of ammonia. Toxicological effects are observed only at exposures above about 200 mg/kg body weight. Ammonia in drinking-water is not of immediate health relevance, and therefore no health-based guideline value is proposed. However, ammonia can compromise disinfection efficiency, result in nitrite formation in distribution systems, cause the failure of filters for the removal of manganese and cause taste and odour problems (see also chapter 10). Anatoxins (cyanobacterial toxins)1 Anatoxin-a, homoanatoxin-a and their dihydro derivatives (ATXs) are naturally occurring alkaloids produced by strains of various species of cyanobacteria, primarily in freshwater environments.2 ATXs have been found in many countries but generally have been reported less often than MCs or CYNs. They have been reported from a number of cyanobacterial genera, including Anabaena, Dolichospermum, Aphanizomenon and Cuspidothrix, many of which are primarily benthic (i.e. grow on sediments or other submerged surfaces) (see also section 11.5). ATXs, like MCs and STXs, usually occur bound to cyanobacterial cells. Drinking-water is the most likely route of exposure to ATXs where surface water with cyanobacterial blooms is the drinking-water source. Recreational activities in lakes with cyanobacterial blooms may also be a relevant exposure pathway, potentially to high concentrations (see WHO Guidelines on recreational water quality, 2021). Reason for not establishing Available data inadequate to permit derivation of health‑based a guideline value guideline value Provisional reference value Total ATXs (sum of all congeners, free plus cell-bound): 0.03 mg/l (short‑term)* The reference value is based on data for anatoxin‑a only Occurrence Concentrations reported usually range well below 1 mg/l; outside of scum areas, they rarely exceed several μg/l. ATXs largely occur cell‑bound unless cell damage causes release. TDI A formal TDI could not be derived because of database limitations. However, 98 μg/kg bw per day, based on a 28‑day study in mice, was selected as a NOAEL. An uncertainty factor of 100 (10 each for inter‑and intra‑species variability) was applied. An uncertainty factor for database limitations was not applied because of the conservative assumptions used to select the NOAEL (see text following the table for further detail). Limit of detection 0.05 μg/L by LC‑MS/MS and <30 μg/L by HPLC coupled with postderivatization fluorescence. LC‑MS/MS requires quantitative reference standards, which are available for anatoxin‑a and for dihydro‑anatoxin‑a. A receptor‑binding assay that is commercially available circumvents this problem and provides more reliable results than HPLC. Prior extraction of cells with freeze–thaw cycles and acidified water or acidified mixtures of methanol/water is necessary for cell‑bound ATX; neglecting extraction from cells will lead to dramatic underestimation of concentrations. 1 As cyanobacteria and their toxins are a concern in many areas and considering the complexities in their management, this chemical fact sheet has been expanded. 2 ATXs do not include anatoxin-a(S), a naturally occurring organophosphate with a different mode of neurotoxicity. 0.15 μg/L for anatoxin‑a and 10 μg/L for homoanatoxin‑a by commercially available immunoassay kits (ELISA); although these are less precise than LC with the above‑mentioned detection methods, they probably capture all anatoxin‑a congeners and thus are useful for most monitoring purposes. Monitoring The likelihood of blooms can be assessed by understanding water body conditions (in particular, nutrient concentrations, water body depth, water retention time, patterns of mixing and stratification; see section 11.5). Where conditions render blooms likely, visual monitoring of source water (including microscopy for potentially ATX‑containing genera) for evidence of increasing cyanobacterial biomass (blooms) is important because biomass can increase rapidly. Exceeding alert values of biomass indicators or ATX concentrations should trigger management responses to prevent exposure to elevated toxin concentrations (see the alert level framework in section 11.5). Analysis of cyanotoxins is particularly useful for validating and optimizing the efficacy of control measures such as riverbank filtration or treatment. Prevention and treatment Actions to decrease the probability of bloom occurrence include catchment and source water management, such as reducing nutrient loading or changing reservoir stratification and mixing. Filtration is effective for removing intact cyanobacterial cells, but some dissolved ATX may be present. For this dissolved fraction, oxidation with ozone at sufficient concentrations and contact times, as well as GAC and some PAC applications, are effective. Chlorination is not reliably effective for oxidation of ATXs (see chapters 7–10 of Toxic cyanobacteria in water; Annex 1). Reference value derivation • allocation to water • weight • consumption 100% 60 kg adult 2 litres/day Additional comments Total ATXs as gravimetric or molar equivalents should be evaluated against the reference value since these toxins can occur as mixtures. Although the reference value is based on anatoxin‑a, limited evidence suggests that all ATXs are similarly toxic. Because ATX is acutely toxic, avoiding any exposure above the reference value is recommended. It is recommended, as a precautionary measure, that bottle‑fed infants and small children be provided with an alternative safe drinking‑water source (e.g. bottled water that is certified by the responsible authorities) if concentrations are greater than 6 μg/L, even for short periods. Assessment date 2020 Principal references WHO (2020) Cyanobacterial toxins: anatoxin-a and analogues Chorus & Welker (2021) Toxic cyanobacteria in water * Although a formal guideline value cannot be established, the provisional reference value was derived based on limited available studies, recognizing that a “bounding value” may be useful, to provide guidance to Member States in the event of need. Reference values are too uncertain to be used for developing regulations or standards. ATXs have been much investigated because of the rapid animal deaths that have been observed upon ingestion of cyanobacterial cells in beached scum material or dislodged benthic mats—in particular, dogs that ingest significant amounts. In contrast, high concentrations of dissolved ATXs near drinking-water abstraction sites have not been reported, presumably because ATX released from the cells in benthic mats or scums is usually rapidly diluted. The (+) enantiomer of anatoxin-a binds with high affinity to nicotinic acetylcholine receptors of nerve cells, causing chronic overstimulation. This can lead to increases in heart rate and blood pressure, as well as fatigue and eventual paralysis of muscles, which can cause death when it occurs in respiratory muscles. Although anatoxin-a is the best studied analogue, limited evidence suggests that homoanatoxin-a and the dihydro derivatives of these compounds bind to the same receptor and may have similar potency to anatoxin-a when administered orally. The toxicological database on ATXs is not adequate to support derivation of a formal guideline value since a nonlethal dose that caused significant adverse effects was not identified in the available repeated-dose studies. Nevertheless, a “bounding value” may be useful to risk assessors. Based on the limited available studies of acute and subchronic anatoxin-a toxicity, a health-based reference value is provided that is unlikely to cause adverse effects in exposed adults. The reference value is based on a 28-day mouse study that found no effects that were clearly ATX related in any of the doses tested. However, as a mouse died from unexplained causes at each of two lower doses, the dose below these two has been selected as the NOAEL. Two other studies indicate that this selection is very conservative. Practical considerations Where nutrient (phosphorus and nitrogen) concentrations are elevated in lakes, reservoirs or slowly flowing rivers, cyanobacteria occur widely. Where their excessive growth leads to high biomass, sometimes termed “bloom” events, ATXs can reach concentrations in raw water that are potentially hazardous to human health. Such blooms tend to recur in the same water bodies. Cells of some cyanobacterial species (e.g. Anabaena, Cuspidothrix, Dolichspermum) may accumulate at the surface as scums. Such accumulations may develop rapidly and may be of very variable duration (hours to weeks). In many circumstances, blooms and accumulations are seasonal, whereas others occur perennially. Cyanobacteria are most effectively controlled in the context of developing a WSP (see chapter 4). Control measures to manage potential risks from cyanobacteria, and in particular from their toxins, in drinking-water should include not only adequate treatment, but also measures to control cyanobacterial bloom development. See section 11.5 for more information on cyanobacteria, including further details on monitoring cyanobacterial blooms, the alert level framework, and prevention and management of cyanobacteria in source waters. Effectively minimizing the formation of blooms and locating the raw water intake away from blooms reduce the treatment steps required to remove cyanotoxins. Drinking-water treatment that removes particles—that is, soil, slow sand or riverbank filtration, conventional water treatment (coagulation, flocculation and filtration) or dissolved air flotation—can remove cell-bound ATXs effectively. Soil, slow sand and riverbank filtration can also remove dissolved cyanotoxins. For all these processes, it is important that they are optimized to target the removal of cells and dissolved toxins. Chlorination is not reliably effective, whereas ozonation at sufficiently high doses and contact times is effective for degrading dissolved ATXs; however, elevated organic carbon in bloom situations will substantially increase the disinfectant demand. Chlorine dioxide and chloramine are ineffective for degrading ATXs. Both for pre-oxidation and conventional treatment, cell rupture and toxin release should be avoided. GAC and PAC can be effective for removing dissolved ATXs, with efficacy dependent on several factors, including the type of activated carbon, contact times (PAC), flow rates (GAC) and water quality. As the challenges that blooms present for treatment are complex, periodic validation of efficacy during bloom situations and under the specific local conditions is particularly important. Avoiding bloom occurrence and intake is therefore the preferred option. Antimony Elemental antimony forms very hard alloys with copper, lead and tin. Antimony compounds have various therapeutic uses. Antimony is used in solders as a replacement for lead, but there is little evidence of any significant contribution to drinking-water concentrations from this source. Total exposure from environmental sources, food and drinking-water is very low compared with occupational exposure. Guideline value 0.02 mg/l (20 μg/l) Occurrence Concentrations in groundwater less than 0.001 μg/l; concentrations in surface water less than 0.2 μg/l; concentrations in drinking‑water appear to be less than 5 μg/l Tolerable daily intake (TDI) 6 μg/kg body weight, based on a NOAEL of 6.0 mg/kg body weight per day for decreased body weight gain and reduced food and water intake in a 90‑day study in which rats were administered potassium antimony tartrate in drinking‑water, using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation, 10 for the short duration of the study) Limit of detection 0.01 μg/l by electrothermal atomic absorption spectrometry (AAS); 0.1–1 μg/l by inductively coupled plasma mass spectrometry (ICP‑MS); 0.8 μg/l by graphite furnace AAS; 5 μg/l by hydride generation AAS Treatment performance Conventional treatment processes do not remove antimony. However, antimony is not normally a raw water contaminant. As the most common source of antimony in drinking‑water appears to be dissolution from metal plumbing and fittings, control of antimony from such sources would be by product control. Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Assessment date 2003 Principal reference WHO (2003) Antimony in drinking-water There has been a significant increase in the toxicity data available since the previous review, although much of it pertains to the intraperitoneal route of exposure. The form of antimony in drinking-water is a key determinant of the toxicity, and it would appear that antimony leached from antimony-containing materials would be in the form of the antimony(V) oxo-anion, which is the less toxic form. The subchronic toxicity of antimony trioxide is lower than that of potassium antimony tartrate, which is the most soluble form. Antimony trioxide, owing to its low bioavailability, is genotoxic only in some in vitro tests, but not in vivo, whereas soluble antimony(III) salts exert genotoxic effects in vitro and in vivo. Animal experiments from which the carcinogenic potential of soluble or insoluble antimony compounds may be quantified are not available. IARC has concluded that antimony trioxide is possibly carcinogenic to humans (Group 2B) on the basis of an inhalation study in rats, but that antimony trisulfide was not classifiable as to its carcinogenicity to humans (Group 3). However, chronic oral uptake of potassium antimony tartrate may not be associated with an additional carcinogenic risk, as antimony after inhalation exposure was carcinogenic only in the lung but not in other organs and is known to cause direct lung damage following chronic inhalation as a consequence of overload with insoluble particulates. Although there is some evidence for the carcinogenicity of certain antimony compounds by inhalation, there are no data to indicate carcinogenicity by the oral route. Arsenic1 Arsenic is found widely in Earth’s crust in oxidation states of –3, 0, +3 and +5, often as sulfides or metal arsenides or arsenates. In water, it is mostly present as arsenate (+5), but in anaerobic conditions, it is likely to be present as arsenite (+3). It is usually present in natural waters at concentrations of less than 1–2 μg/l. However, in waters, particularly groundwaters, where there are sulfide mineral deposits and sedimentary deposits deriving from volcanic rocks, the concentrations can be significantly elevated. Arsenic is found in the diet, particularly in fish and shellfish, in which it is found mainly in the less toxic organic form. There are only limited data on the proportion of inorganic arsenic in food, but these indicate that approximately 25% is present in the inorganic form, depending on the type of food. Apart from occupational exposure, the most important routes of exposure are through food and drinking-water, including beverages that are made from drinking-water. Where the concentration of arsenic in drinking-water is 10 μg/l or greater, this will be the dominant source of intake. In circumstances where soups or similar dishes are a staple part of the diet, the drinking-water contribution through preparation of food will be even greater. Provisional guideline value 0.01 mg/l (10 μg/l) The guideline value is designated as provisional on the basis of treatment performance and analytical achievability. 1 As arsenic is one of the chemicals of greatest health concern in some natural waters, its chemical fact sheet has been expanded. Occurrence Levels in natural waters generally range between 1 and 2 μg/l, although concentrations may be elevated (up to 12 mg/l) in areas containing natural sources Basis of guideline value derivation There remains considerable uncertainty over the actual risks at low concentrations, and available data on mode of action do not provide a biological basis for using either linear or non‑linear extrapolation. In view of the practical difficulties in removing arsenic from drinking‑water, as well as the practical quantification limit in the region of 1–10 μg/l, the guideline value of 10 μg/l is retained and designated as provisional. Limit of detection 0.1 μg/l by ICP‑MS; 2 μg/l by hydride generation AAS or flame AAS Treatment performance Assessment date It is technically feasible to achieve arsenic concentrations of 5 μg/l or lower using any of several possible treatment methods. However, this requires careful process optimization and control, and a more reasonable expectation is that 10 μg/l should be achievable by conventional treatment (e.g. coagulation). 2011 Principal references FAO/WHO (2011) Evaluation of certain contaminants in food IARC (1987) Overall evaluations of carcinogenicity IPCS (2001) Arsenic and arsenic compounds ISO (1982) Water quality—determination of total arsenic USNRC (2001) Arsenic in drinking water, 2001 update WHO (2011) Arsenic in drinking-water Both pentavalent and trivalent soluble arsenic compounds are rapidly and extensively absorbed from the gastrointestinal tract. Metabolism is characterized by 1) reduction of pentavalent to trivalent arsenic and 2) oxidative methylation of trivalent arsenic to form monomethylated, dimethylated and trimethylated products. Methylation of inorganic arsenic facilitates the excretion of inorganic arsenic from the body, as the end-products monomethylarsonic acid and dimethylarsinic acid are readily excreted in urine. There are major qualitative and quantitative interspecies differences in methylation, but in humans and most common laboratory animals, inorganic arsenic is extensively methylated, and the metabolites are excreted primarily in the urine. There is large interindividual variation in arsenic methylation in humans, probably due to a wide difference in the activity of methyltransferases and possible polymorphism. Ingested organoarsenicals are much less extensively metabolized and more rapidly eliminated in urine than inorganic arsenic. Arsenic has not been demonstrated to be essential in humans. The acute toxicity of arsenic compounds in humans is predominantly a function of their rate of removal from the body. Arsine is considered to be the most toxic form, followed by the arsenites, the arsenates and organic arsenic compounds. Acute arsenic intoxication associated with the ingestion of well water containing very high concentrations (21.0 mg/l) of arsenic has been reported. Signs of chronic arsenicism, including dermal lesions such as hyperpigmentation and hypo-pigmentation, peripheral neuropathy, skin cancer, bladder and lung cancers and peripheral vascular disease, have been observed in populations ingesting arsenic-contaminated drinking-water. Dermal lesions were the most commonly observed symptom, occurring after minimum exposure periods of approximately 5 years. Effects on the cardiovascular system were observed in children consuming arsenic-contaminated water (mean concentration 0.6 mg/l) for an average of 7 years. Numerous epidemiological studies have examined the risk of cancers associated with arsenic ingestion through drinking-water. Many are ecological-type studies, and many suffer from methodological flaws, particularly in the measurement of exposure. However, there is overwhelming evidence that consumption of elevated levels of arsenic through drinking-water is causally related to the development of cancer at several sites. Nevertheless, there remain considerable uncertainty and controversy over both the mechanism of carcinogenicity and the shape of the dose–response curve at low intakes. The International Programme on Chemical Safety (IPCS) concluded that long-term exposure to arsenic in drinking-water is causally related to increased risks of cancer in the skin, lungs, bladder and kidney, as well as other skin changes, such as hyperkeratosis and pigmentation changes. These effects have been demonstrated in many studies using different study designs. Exposure–response relationships and high risks have been observed for each of these end-points. The effects have been most thoroughly studied in Taiwan, China, but there is considerable evidence from studies on populations in other countries as well. Increased risks of lung and bladder cancer and of arsenic-associated skin lesions have been reported to be associated with ingestion of drinking-water at concentrations below 50 μg of arsenic per litre. There is a need for more analytical epidemiological studies to determine the dose–time response for skin lesions, as well as cancer, in order to assist in developing suitable interventions and determining practical intervention policies. Inorganic arsenic compounds are classified by IARC in Group 1 (carcinogenic to humans) on the basis of sufficient evidence for carcinogenicity in humans and limited evidence for carcinogenicity in animals. Although there is a substantial database on the association between both internal and skin cancers and the consumption of arsenic in drinking-water, there remains considerable uncertainty over the actual risks at low concentrations. In its updated evaluation, the United States National Research Council concluded that “the available mode-of-action data on arsenic do not provide a biological basis for using either a linear or nonlinear extrapolation”. The maximum likelihood estimates, using a linear extrapolation, for bladder and lung cancer for populations in the United States of America (USA) exposed to arsenic at concentrations of 10 μg/l in drinking-water are, respectively, 12 and 18 per 10 000 population for females and 23 and 14 per 10 000 population for males. The actual numbers indicated by these estimated risks would be very difficult to detect by current epidemiological methods. There is also uncertainty over the contribution of arsenic in food—a higher intake of inorganic arsenic from food would lead to a lower risk estimate for water—and the impact of factors such as variation in the metabolism of arsenic and nutritional status. Some studies in areas with arsenic concentrations somewhat above 50 μg/l have not detected arsenic-related adverse effects in the residents. It remains possible that the estimates of cancer risk associated with various arsenic intakes are overestimates. The concentration of arsenic in drinking-water below which no effects can be observed remains to be determined, and there is an urgent need for identification of the mechanism by which arsenic causes cancer, which appears to be the most sensitive toxicity end-point. The practical quantification limit for arsenic is in the region of 1–10 μg/l, and removal of arsenic to concentrations below 10 μg/l is difficult in many circumstances. In view of the practical difficulties in removing arsenic from drinking-water, particularly from small supplies, and the practical quantification limit for arsenic, the guideline value of 10 μg/l is retained as a goal and designated as provisional. The provisional guideline value of 10 μg/l was previously supported by a JECFA PTWI of 15 μg/kg body weight, assuming an allocation of 20% to drinking-water. However, JECFA recently re-evaluated arsenic and concluded that the existing PTWI was very close to the lower confidence limit on the benchmark dose for a 0.5% response (BMDL0.5) calculated from epidemiological studies and was therefore no longer appropriate. The PTWI was therefore withdrawn. Nevertheless, given that, in many countries, even the provisional guideline value may not be attainable, it is retained on the basis of treatment performance and analytical achievability with the proviso that every effort should be made to keep concentrations as low as reasonably possible. Practical considerations A silver diethyldithiocarbamate spectrophotometric method (ISO 6595:1982) is available for the determination of arsenic; the detection limit is about 1 μg/l. Graphite furnace AAS, hydride generation AAS and ICP-MS are more sensitive. HPLC in combination with ICP-MS can also be used to determine various arsenic species. It is technically feasible to achieve arsenic concentrations of 5 μg/l or lower using any of several possible treatment methods. However, this requires careful process optimization and control, and a more reasonable expectation is that 10 μg/l should be achievable by conventional treatment (e.g. coagulation). For local non-piped water supplies, the first option is often substitution by, or dilution with, microbially safe low-arsenic sources. It may also be appropriate to use alternative sources for drinking and cooking but to use the contaminated sources for purposes such as washing and laundry. There are also an increasing number of effective small-scale treatment techniques, usually based around coagulation and precipitation or adsorption, available at relatively low cost for removal of arsenic from small supplies. For further guidance on identifying and managing arsenic contamination of drinking-water, see Arsenic primer (Annex 1). Asbestos Asbestos is introduced into water by the dissolution of asbestos-containing minerals and ores, as well as from industrial effluents and atmospheric pollution. However, the main source of asbestos in drinking-water is asbestos–cement pipes in the distribution system. Rainwater harvesting can contribute to exposure where water runoff is collected from asbestos–cement roofing that is inadequately sealed. Exfoliation of asbestos fibres from asbestos–cement pipes is related to the aggressiveness of the water supply, although, where pipes are already degraded, actions to to control water corrosivity will not prevent asbestos fibre release from the pipes. Exposure to airborne asbestos released from tap water during showers or humidification is unlikely to be significant for human health. Reason for not establishing No consistent evidence that ingested asbestos is hazardous to health. a guideline value Further, epidemiological studies have a number of limitations that would preclude their use for deriving a guideline value. Occurrence Concentrations in drinking‑water are typically less than 1 million fibres per litre (MFL) and rarely exceed 10 MFL. However, much higher concentrations have been reported in drinking‑water supplies where asbestos–cement piping is used, although occurrence data are limited. Limit of detection Less than 0.1 MFL in water by transmission electron microscopy Treatment performance Conventional treatment, including coagulation and filtration, can effectively remove asbestos fibres (>99%) if operation is optimized. Additional comments Where existing asbestos–cement pipes are still in active use, suppliers should map and record their location, assess their condition (including related to water aggressiveness) and determine the most appropriate risk reduction strategies. Degradation of asbestos–cement materials in drinking‑water supplies should be minimized. As these materials fail or deteriorate significantly, the asbestos–cement materials should be replaced with non‑asbestoscontaining materials. No new sources of asbestos fibres in drinking‑water should be introduced. Investigative monitoring of asbestos–cement pipes should be considered to provide additional information on the contribution of older asbestos–cement pipes to numbers, types, sizes and shape of fibres in drinking‑water. Assessment date 2021 Principal reference WHO (2021) Asbestos in drinking-water Although asbestos is a known human carcinogen by the inhalation route, findings from epidemiological studies that evaluated the correlation between asbestos exposure via drinking-water and incidence of cancers of the stomach and gastrointestinal tract have been inconsistent, with some studies suggesting a weak positive correlation and others finding no evidence of a correlation. However, in extensive studies in experimental animal species, asbestos has not consistently increased the incidence of tumours of the gastrointestinal tract. Thus, the overall weight of evidence from epidemiological and animal studies does not support the hypothesis that oral exposure to asbestos in drinking-water is associated with an increased risk of developing cancer. Further, uncertainties associated with the epidemiological data preclude the establishment of a health-based guideline value for asbestos in drinking-water. The primary issue surrounding asbestos–cement in drinking-water is for people working on the outside of the pipes (e.g. cutting pipe) or those repairing roof tiles, because of the risk of inhalation of asbestos dust. Where replacement or repair of pipes is required, it is essential that appropriate measures are taken to prevent any worker exposure to asbestos dust, including during transport and disposal. Similar to asbestos–cement pipes, it is important that appropriate measures are taken to prevent worker and public exposure to asbestos dust generated during work on asbestos– cement roof tiles. Atrazine and its metabolites Atrazine is a selective systemic herbicide of the chlorotriazine class, used for the control of annual broadleaf and grassy weeds. Atrazine and its chloro-s-triazine metabolites— deethyl-atrazine, deisopropyl-atrazine and diaminochlorotriazine—have been found in surface water and groundwater as a result of the use of atrazine as a pre-emergent or early post-emergent herbicide. The metabolite hydroxyatrazine is more commonly detected in groundwater than in surface water. Guideline values Atrazine and its chloro-s-triazine metabolites: 0.1 mg/l (100 μg/l) Hydroxyatrazine: 0.2 mg/l (200 μg/l) Occurrence Concentrations rarely exceed 2 μg/l and are commonly well below 0.1 μg/l Group ADI for atrazine 0–0.02 mg/kg body weight based on the NOAEL for atrazine of 1.8 mg/ and its chloro‑s‑triazine kg body weight per day identified on the basis of luteinizing hormone metabolites surge suppression and subsequent disruption of the estrous cycle seen at 3.6 mg/kg body weight per day in a 6‑month study in rats, using a safety factor of 100 ADI for hydroxyatrazine 0–0.04 mg/kg body weight based on the NOAEL of 1.0 mg/kg body weight per day identified on the basis of kidney toxicity at 7.8 mg/kg body weight per day in a 24‑month study in rats, using a safety factor of 25, based on kinetic considerations Limit of detection Atrazine: 1 ng/l, isotope dilution MS with solid‑phase extraction; 10 ng/l, GC‑MS with solid‑phase extraction; 50 ng/l, liquid chromatography (LC)– MS with solid‑phase extraction; 100 ng/l, GC with nitrogen–phosphorus detection Metabolites: 5 ng/l, capillary GC with nitrogen thermionic specific detection and HPLC with photodiode array absorption detection following extraction with styrene‑divinylbenzene sorbents and elution with acetone Treatment performance 0.1 μg/l can be achieved using GAC or powdered activated carbon (PAC); bankside filtration and nanofiltration are also effective Guideline value derivation • allocation to water 20% of upper limit of ADI• body weight 60 kg adult• consumption 2 litres/day Additional comments JMPR considered that the NOAEL for atrazine is protective for the consequences of neuroendocrine and other adverse effects caused by prolonged exposure to atrazine and its chloro‑s‑triazine metabolites. JMPR was not able to assess the source allocation of atrazine to drinking‑water. As such, the default 20% allocation was chosen, as it will be very conservative in most countries; in addition, it is expected that exposure of the public will be primarily through drinking‑water. Assessment date 2011 Principal references FAO/WHO (2009) Pesticide residues in food—2007 evaluations WHO (2011) Atrazine and its metabolites in drinking-water JMPR agreed that it is unlikely that atrazine is genotoxic and concluded that atrazine is not likely to pose a carcinogenic risk to humans, as the mode of carcinogenic action in certain susceptible rat strains is not relevant for human risk assessment. The weight of evidence from the epidemiological studies also did not support a causal association between exposure to atrazine and the occurrence of cancer in humans. In special studies of reproductive toxicity, exposure of rats during early pregnancy (i.e. the luteinizing hormone–dependent period) caused increased pre-implantation or post-implantation losses, including full-litter resorptions. Attenuation of the luteinizing hormone surge and subsequent disruption of the estrous cycle (characterized by an increase in days in estrus) were observed at and above 3.65 mg/kg body weight per day, with a NOAEL of 1.8 mg/kg body weight per day. The effects on the luteinizing hormone surge and disruption of the estrous cycle were further supported by a number of short-term mechanistic studies. Additional experiments suggested that the effects of atrazine on luteinizing hormone and prolactin secretion are mediated via a hypothalamic site of action. JMPR concluded that atrazine was not teratogenic. Studies using a variety of test systems in vitro and in vivo indicated that modulation of the immune system occurs after exposure to atrazine. However, effects suggestive of impaired function of the immune system were observed only at doses greater than those shown to affect neuroendocrine function, leading to disruption of the estrous cycle or developmental effects. The toxicity profiles and mode of action of the chloro-s-triazine metabolites are similar to those of atrazine; the potency of these metabolites with regard to their neuroendocrine-disrupting properties appeared to be similar to that of the parent compound. The metabolite hydroxyatrazine does not have the same mode of action or toxicity profile as atrazine and its chloro-s-triazine metabolites. The main effect of hydroxyatrazine was kidney toxicity (owing to its low solubility in water, resulting in crystal formation and a subsequent inflammatory response), and there was no evidence that hydroxyatrazine has neuroendocrine-disrupting properties. There was no evidence of carcinogenicity, and hydroxyatrazine did not show genotoxicity in an adequate range of tests in vitro and in vivo. Barium Barium compounds are present in nature as ore deposits and in igneous and sedimentary rocks, and are used in a variety of industrial applications. Barium in water comes primarily from natural sources, although barium also enters the environment from industrial emissions and anthropogenic uses. Food is the primary source of intake for the non-occupationally exposed population. However, where barium concentrations in water are high, drinking-water may contribute significantly to total intake. Guideline value 1.3 mg/l (1300 μg/l) Occurrence Concentrations in drinking‑water are generally below 100 μg/l, although concentrations above 1 mg/l have been measured in drinking‑water derived from groundwater TDI 0.21 mg/kg bw per day, derived by applying an uncertainty factor of 300 to account for intraspecies variation (10), interspecies variation (10) and database deficiencies (3 for the lack of a developmental toxicity study) to a BMDL05 of 63 mg/kg bw per day for nephropathy in mice in a 2‑year study Limit of detection 0.004–0.8 μg/l by ICP‑MS; 1.0 μg/l by ICP‑AES Treatment performance Ion exchange, lime softening or direct filtration with chemical precipitation may be able to remove barium to below 1 mg/l Guideline value derivation • allocation to water 20% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments As rounding can have significant practical implications at milligram per litre levels, it was concluded that a guideline value with two significant figures was reasonable in this case. The guideline value derived based on the long‑term mouse study is not inconsistent with health‑based values that could be derived from limited human studies. Assessment date 2016 Principal references IPCS (2001). Barium and barium compounds USEPA (2005). Toxicological review of barium and compounds. In support of summary information on the Integrated Risk Information System (IRIS). WHO (2016). Barium in drinking-water There is no evidence that barium is carcinogenic or genotoxic. Acute hypertension has been observed in case reports, but the effects may be secondary to hypokalaemia. The critical study that had been identified previously for deriving the guideline value has several limitations (e.g. no effect observed at the single dose evaluated, limitations in the exposure methodology and design, no control for important risk factors for hypertension). Another human study that reported no effects on hypertension at 10 mg/l is limited by the small study size and short exposure duration. Barium has been shown to cause nephropathy in laboratory animals, and this was selected as the toxicological end-point of concern for the current guideline. Bentazone Bentazone (CAS No. 25057-89-0) is a post-emergence herbicide used for selective control of broadleaf weeds and sedges occurring among a variety of crops. It is highly soluble in water and very resistant to hydrolysis; it is also very mobile in soil. However, photodegradation occurs in both soil and water. Bentazone may leach from soil into groundwater, particularly during heavy rainfall, and may contaminate surface water through effluents from production plants, drainage waters and actual use in the water (rice fields). Exposure from food is likely to be low. Reason for not establishing Occurs in drinking‑water or drinking‑water sources at concentrations a guideline value well below those of health concern Health‑based value* 0.5 mg/l Acute health‑based value** 20 mg/l Occurrence Concentrations up to 120 μg/l in groundwater and up to 14 μg/l in surface water have been measured ADI 0–0.09 mg/kg bw, based on a NOAEL of 9 mg/kg bw per day for prolonged blood coagulation and clinical chemistry changes indicative of effects on liver and kidney from a 2‑year toxicity and carcinogenicity study in rats and application of a safety factor of 100 ARfD 0.5 mg/kg bw, based on a NOAEL of 50 mg/kg bw for decreased motor activity observed in male rats on day 0 in an acute neurotoxicity study and application of a safety factor of 100 Limit of detection 0.1 μg/l by GC with ECD after liquid–liquid extraction; limit of quantification of 0.01 μg/l by LC‑MS/MS Treatment performance Conventional treatment, including coagulation and filtration, not effective; activated carbon may be effective under certain circumstances Health‑based value derivation • allocation to water 20% of upper bound of ADI• weight 60 kg adult• consumption 2 litres/day Acute health‑based value derivation • allocation to water 100% of ARfD (0.5 mg/kg bw) • weight 60 kg adult• consumption 2 litres/day Additional comments The default allocation factor of 20% has been used to account for the fact that the available food exposure data, which suggest that exposure via this route is low, do not generally include information from developing countries, where exposure via this route may be higher As a general principle, the concentration of pesticides in water, including bentazone, should be kept as low as possible and concentrations should not be allowed to increase up to the health‑based value. Further guidance on interpreting the health‑based value and deciding when to monitor can be found in section 8.5.3 Assessment date 2016 and 2020 Principal references WHO (2013). Pesticide residues in food – 2012 evaluations FAO/WHO (2016) Pesticide residues in food – 2016 evaluations WHO (2020). Bentazone in drinking-water * When a formal guideline value is not established, a “health‑based value” may be determined in order to provide guidance to Member States when there is reason for local concern. Establishing a formal guideline value for such substances may encourage Member States to incorporate a value into their national standards when this may be unnecessary. ** For more information on acute health‑based values, see section 8.7.5. Bentazone is not carcinogenic in rats or mice, and showed no evidence of genotoxicity in a range of in vitro and in vivo assays. Consistent observations in repeated-dose toxicity studies in mice, rats and dogs are effects on haematology and blood coagulation (e.g. prolongation of prothrombin time and partial thromboplastin time). Benzene Benzene is used principally in the production of other organic chemicals. It is present in petrol, and vehicular emissions constitute the main source of benzene in the environment. Benzene may be introduced into water by industrial effluents and atmospheric pollution. Guideline value 0.01 mg/l (10 μg/l) Occurrence Concentrations in drinking‑water, when present, generally much less than 5 μg/l Basis of guideline value Robust linear extrapolation model (because of statistical lack of fit of some derivation of the data with the linearized multistage model) applied to leukaemia and lymphomas in female mice and oral cavity squamous cell carcinomas in male rats in a 2‑year gavage study Limit of detection 0.2 μg/l by GC with photoionization detection and confirmation by MS Treatment performance 0.01 mg/l should be achievable using GAC or air stripping Additional comments Lower end of estimated range of concentrations in drinking‑water corresponding to an upper‑bound excess lifetime cancer risk of 10−5 (10–80 μg/l) corresponds to the estimate derived from data on leukaemia from epidemiological studies involving inhalation exposure, which formed the basis for the previous guideline value. The previous guideline value is therefore retained. Assessment date 1993 Principal reference WHO (2003) Benzene in drinking-water Acute exposure of humans to high concentrations of benzene primarily affects the central nervous system. At lower concentrations, benzene is toxic to the haematopoietic system, causing a continuum of haematological changes, including leukaemia. Because benzene is carcinogenic to humans, IARC has classified it in Group 1. Haematological abnormalities similar to those observed in humans have been observed in experimental animal species exposed to benzene. In animal studies, benzene was shown to be carcinogenic following both inhalation and ingestion. It induced several types of tumours in both rats and mice in a 2-year carcinogenesis bioassay by gavage in corn oil. Benzene has not been found to be mutagenic in bacterial assays, but it has been shown to cause chromosomal aberrations in vivo in a number of species, including humans, and to be positive in the mouse micronucleus test. Beryllium The primary source of beryllium compounds in water appears to be release from coal burning and other industries using beryllium. Other sources of beryllium in surface water include deposition of atmospheric beryllium and weathering of rocks and soils containing beryllium. Beryllium is not likely to be found in natural water above trace levels as a result of the insolubility of beryllium oxides and hydroxides in the normal pH range. Reason for not establishing Rarely found in drinking‑water at concentrations of health concern a guideline value Assessment date 2009 Principal references IPCS (2001) Beryllium and beryllium compounds WHO (2009) Beryllium in drinking-water As beryllium is rarely, if ever, found in drinking-water at concentrations of concern, it is not considered necessary to set a formal guideline value. A health-based value for beryllium in drinking-water of 12 μg/l can be calculated based on an allocation of 20% of the TDI of 2 μg/kg body weight, derived from a longterm study in which dogs exhibited lesions of the small intestine, to drinking-water and assuming a 60 kg adult drinking 2 litres of water per day. This allocation is probably conservative, as the limited data on food indicate that exposure from this source is likely to be well below the TDI. Although beryllium appears to be found in drinking-water sources and drinking-water at low concentrations, the database on occurrence is limited, and there may be specific circumstances in which concentrations can be elevated due to natural sources where the pH is either below 5 or above 8 or there is high turbidity. Boron Boron compounds are used in the manufacture of glass, soaps and detergents and as flame retardants. Naturally occurring boron is present in groundwater primarily as a result of leaching from rocks and soils containing borates and borosilicates. The borate content of surface water can be increased as a result of wastewater discharges, but this use has decreased significantly, and levels of boron in wastewater discharges continue to fall. Guideline value 2.4 mg/l (2400 μg/l) Occurrence Concentrations vary widely and depend on the surrounding geology and wastewater discharges; for most of the world, the concentration of boron in drinking‑water is judged to be below 0.5 mg/l TDI 0.17 mg/kg body weight, based on a BMDL05 of 10.3 mg/kg body weight per day for developmental toxicity (decreased fetal body weight in rats) and an uncertainty factor of 60 (10 for interspecies variation and 6 for intraspecies variation) Limit of detection 0.15 μg/l by ICP‑MS; 6–10 μg/l by ICP–atomic emission spectrometry (AES) Treatment performance Conventional water treatment (coagulation, sedimentation, filtration) does not significantly remove boron, and special methods need to be used in order to remove boron from waters with high boron concentrations. Ion exchange and reverse osmosis processes may enable substantial reduction but are likely to be prohibitively expensive. Blending with low‑boron supplies may be the only economical method to reduce boron concentrations in waters where these concentrations are high. Guideline value derivation • allocation to water • weight • consumption 40% of TDI (because intake from other sources is low) 60 kg adult 2 litres/day Additional comments Because it will be difficult to achieve the guideline value of 2.4 mg/l in some desalinated supplies and in areas with high natural boron levels, local regulatory and health authorities should consider a value in excess of 2.4 mg/l by assessing exposure from other sources. Assessment date 2009 Principal reference WHO (2009) Boron in drinking-water Short- and long-term oral exposures to boric acid or borax in laboratory animals have demonstrated that the male reproductive tract is a consistent target of toxicity. Testicular lesions have been observed in rats, mice and dogs given boric acid or borax in food or drinking-water. Developmental toxicity has been demonstrated experimentally in rats, mice and rabbits. Negative results in a large number of mutagenicity assays indicate that boric acid and borax are not genotoxic. In long-term studies in mice and rats, boric acid and borax caused no increase in tumour incidence. Bromate Sodium and potassium bromate are powerful oxidizers used mainly in permanent wave neutralizing solutions and the dyeing of textiles using sulfur dyes. Potassium bromate has also been used as an oxidizer to mature flour during milling, in treating barley in beer making and in fish paste products, although JECFA has concluded that the use of potassium bromate in food processing is not appropriate. Bromate is not normally found in water, but can occur as a result of pollution from industrial sources, sometimes as a consequence of its presence in contaminated soil. However, the main source in drinking-water is its formation during ozonation when the bromide ion is present in water. Bromate may also be formed in hypochlorite solutions produced by electrolysis of bromide-containing salt. Provisional guideline value 0.01 mg/l (10 μg/l) The guideline value is provisional because of limitations in available analytical and treatment methods. Occurrence Has been reported in drinking‑water with a variety of source water characteristics after ozonation at concentrations ranging from less than 2 to 293 μg/l, depending on bromide ion concentration, ozone dosage, pH, alkalinity and dissolved organic carbon; can also be formed in the electrolytic generation of chlorine and hypochlorite from brine with a high level of bromide contamination Basis of guideline value derivation Upper‑bound estimate of cancer potency for bromate is 0.19 per mg/ kg body weight per day, based on low‑dose linear extrapolation (a one‑stage Weibull time‑to‑tumour model was applied to the incidence of mesotheliomas, renal tubule tumours and thyroid follicular tumours in male rats given potassium bromate in drinking‑water, using the 12‑, 26‑, 52‑and 77‑week interim kill data). A health‑based value of 2 μg/l is associated with the upper‑bound excess cancer risk of 10−5. A similar conclusion may be reached through several other methods of extrapolation, leading to values in the range 2–6 μg/l. Limit of detection 0.2 μg/l by ion chromatography with UV/visible absorbance detection; 0.3 μg/l by ion chromatography with detection by ICP‑MS; 1.5 μg/l by ion chromatography with suppressed conductivity detection Treatment performance Bromate is difficult to remove once formed. By appropriate control of disinfection conditions, it is possible to achieve bromate concentrations below 0.01 mg/l. Assessment date 2003 Principal reference WHO (2003) Bromate in drinking-water IARC has concluded that although there is inadequate evidence of carcinogenicity in humans, there is sufficient evidence for the carcinogenicity of bromate from high-dose studies in experimental animals; IARC has classified bromate in Group 2B (possibly carcinogenic to humans). Bromate is mutagenic both in vitro and in vivo. At this time, there is not sufficient evidence to conclude as to the mode of carcinogenic action for bromate. Observation of tumours at a relatively early time and the positive response of bromate in a variety of genotoxicity assays suggest that the predominant mode of action at low doses is due to oxidative deoxyribonucleic acid (DNA) damage. Although there is evidence to suggest that the DNA reactivity in kidney tumours may have a non-linear dose–response relationship, there is no evidence to suggest that this same dose–response relationship operates in the development of mesotheliomas or thyroid tumours. Oxidative stress may play a role in the formation of kidney tumours, but the evidence is insufficient to establish lipid peroxidation and free radical production as key events responsible for the induction of kidney tumours. However, emerging evidence points to rapid decomposition of bromate in the gastrointestinal tract, blood and liver, which supports a non-linear dose–response relationship at low doses. Bromide Bromide is commonly found in nature along with sodium chloride, owing to their similar physical and chemical properties, but in smaller quantities. Bromide concentrations in seawater range from 65 mg/l to well over 80 mg/l, in fresh water from trace amounts to about 0.5 mg/l and in desalinated waters up to 1 mg/l. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2009 Principal reference WHO (2009) Bromide in drinking-water Inorganic bromide was evaluated in 1966 by JMPR, which recommended an ADI of 0–1 mg/kg body weight, based on a minimum pharmacologically effective dosage in humans of about 900 mg of potassium bromide, equivalent to 600 mg of bromide ion. The JMPR ADI was reaffirmed with new data in 1988. The results of human studies suggest a conservative no-observed-effect level (NOEL) (for marginal effect within normal limits of electroencephalograms in females) of 4 mg/kg body weight per day, giving an ADI of 0–0.4 mg/kg body weight, including a safety factor of 10 for population diversity. The upper limit of the ADI of 0–0.4 mg/kg body weight yields an acceptable total daily intake of 24 mg/person for a 60 kg person. Assuming a relative source contribution of 50%, the drinking-water value for a 60 kg adult consuming 2 litres/day would be up to 6 mg/l; for a 10 kg child consuming 1 litre/day, the value would be up to 2 mg/l. However, the dietary bromide contribution for a 10 kg child would probably be less than that for an adult. These are reasonably conservative values, and they are unlikely to be encountered in drinking-water supplies. Bromide can be involved in the reaction between chlorine and naturally occurring organic matter in drinking-water, forming brominated and mixed chloro-bromo by-products, such as trihalomethanes (THMs) and halogenated acetic acids (HAAs), or it can react with ozone to form bromate. The levels of bromide that can result in the formation of these substances are well below the health-based values suggested above. This guidance applies specifically to inorganic bromide ion and not to bromate or organohalogen compounds, for which individual health-based guideline values have been developed. Brominated acetic acids Brominated acetic acids are formed during disinfection of water that contains bromide ions and organic matter. Bromide ions occur naturally in surface water and groundwater and exhibit seasonal fluctuations in levels. Bromide ion levels can increase as a result of either saltwater intrusion resulting from drought conditions or pollution. Brominated acetates are generally present in surface water and groundwater distribution systems at mean concentrations below 5 μg/l. Reason for not establishing Available data inadequate to permit derivation of health‑based guideline values guideline values Assessment date 2003 Principal references IPCS (2000) Disinfectants and disinfectant by-products WHO (2004) Brominated acetic acids in drinking-water The database for dibromoacetic acid is considered inadequate for the derivation of a guideline value. There are no systemic toxicity studies of subchronic duration or longer. The database also lacks suitable toxicokinetic studies, a carcinogenicity study, a developmental study in a second species and a multigeneration reproductive toxicity study. Available mutagenicity data suggest that dibromoacetate is genotoxic. Data are also limited on the oral toxicity of monobromoacetic acid and bromochloroacetic acid. Limited mutagenicity and genotoxicity data give mixed results for monobromoacetic acid and generally positive results for bromochloroacetic acid. Data gaps include subchronic or chronic toxicity studies, multigeneration reproductive toxicity studies, standard developmental toxicity studies and carcinogenicity studies. The available data are considered inadequate to establish guideline values for these chemicals. Cadmium Cadmium metal is used in the steel industry and in plastics. Cadmium compounds are widely used in batteries. Cadmium is released to the environment in wastewater, and diffuse pollution is caused by contamination from fertilizers and local air pollution. Contamination in drinking-water may also be caused by impurities in the zinc of galvanized pipes and solders and some metal fittings. Food is the main source of daily exposure to cadmium. The daily oral intake is 10–35 μg. Smoking is a significant additional source of cadmium exposure. Guideline value 0.003 mg/l (3 μg/l) Occurrence Levels in drinking‑water usually less than 1 μg/l PTMI 25 μg/kg body weight, based on the relationship between β2‑microglobulin excretion in urine and cadmium excretion in urine for individuals who are 50 years of age and older Limit of detection 0.01 μg/l by ICP‑MS; 2 μg/l by flame AAS Treatment performance 0.002 mg/l should be achievable using coagulation or precipitation softening Guideline value derivation • allocation to water 10% of provisional tolerable monthly intake (PTMI) because of high intake from food • weight 60 kg adult• consumption 2 litres/day Additional comments Although new information indicates that a proportion of the general population may be at increased risk for tubular dysfunction when exposed at the current PTMI, the risk estimates that can be made at present are imprecise. It is recognized that the margin between the PTMI and the actual monthly intake of cadmium by the general population is small and that this margin may be even smaller in smokers. Assessment date 2011 Principal references FAO/WHO (2011) Evaluation of certain food additives and contaminants WHO (2003) Cadmium in drinking-water Absorption of cadmium compounds is dependent on the solubility of the compounds. Cadmium accumulates primarily in the kidneys and has a long biological half-life in humans of 10–35 years. There is evidence that cadmium is carcinogenic by the inhalation route, and IARC has classified cadmium and cadmium compounds in Group 2A (probably carcinogenic to humans). However, there is no evidence of carcinogenicity by the oral route and no clear evidence for the genotoxicity of cadmium. The kidney is the main target organ for cadmium toxicity. In its recent evaluation of cadmium, JECFA found that data relating excretion of the biomarker β2-microglobulin in urine to cadmium excretion in urine for individuals who are 50 years of age and older provided the most reliable basis on which to determine a critical concentration of cadmium in the urine. Urinary excretion of less than 5.24 μg of cadmium per gram creatinine was not associated with an increased excretion of β2-microglobulin, and the dietary exposure that would result in a urinary cadmium concentration at the breakpoint of 5.24 μg/g creatinine was estimated to be 0.8 μg/kg body weight per day or about 25 μg/kg body weight per month. Because of cadmium’s exceptionally long half-life, the previous PTWI of 7 μg/kg body weight was withdrawn, and a PTMI of 25 μg/kg body weight was established. Carbaryl Carbaryl (CAS No. 63-25-2) is a broad-spectrum carbamate insecticide that is used to control insect pests in crops, trees and ornamental plants. It also has some uses in public health and veterinary practice. Carbaryl has not been reported in drinking-water; however, it could occur following overspraying or spillage into surface water. Exposure through drinking-water is therefore considered to be low unless in exceptional circumstances. The major route of carbaryl intake for the general population is food, but residues are considered to be relatively low. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2006 Principal references FAO/WHO (2002) Pesticide residues in food—2001 evaluations WHO (2008) Carbaryl in drinking-water Carbaryl acts through inhibition of brain cholinesterase, and this is also its primary mode of toxicity. However, carbaryl is also considered to be a non-genotoxic carcinogen in mice, in which it causes vascular tumours in males. On this basis, JMPR established an ADI of 0–0.008 mg/kg body weight. This was based on a lowest-observedadverse-effect level (LOAEL) of 15 mg/kg body weight per day and application of a safety factor of 2000 (10 for interspecies variation, 10 for intraspecies variation and 20 to reflect the occurrence of the rare and malignant tumour for which a no-effect level could not be identified). A health-based value of 50 μg/l (rounded value) can be determined from the JMPR ADI of 0–0.008 mg/kg body weight, assuming a 60 kg adult drinking 2 litres of water per day and allowing 20% of the upper limit of the ADI from drinkingwater. However, carbaryl does not appear to be found in drinking-water at significant concentrations, and so it is not considered necessary to propose a formal guideline value. Carbofuran Carbofuran (CAS No. 1563-66-2) is used worldwide as a pesticide for many crops. Residues in treated crops are generally very low or not detectable. The physicochemical properties of carbofuran and the few data on occurrence indicate that drinking-water from both groundwater and surface water sources is potentially the major route of exposure. Guideline value 0.007 mg/l (7 μg/l) Occurrence Has been detected in surface water, groundwater and drinking‑water, generally at levels of a few micrograms per litre or lower; highest concentration (30 μg/l) measured in groundwater ADI 0–0.002 mg/kg body weight based on a NOAEL of 0.22 mg/kg body weight per day for acute (reversible) effects in dogs in a short‑term (4‑week) study conducted as an adjunct to a 13‑week study in which inhibition of erythrocyte acetylcholinesterase activity was observed, and using an uncertainty factor of 100 Limit of detection 0.1 μg/l by GC with a nitrogen–phosphorus detector; 0.9 μg/l by reversed‑phase HPLC with a fluorescence detector Treatment performance 1 μg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of upper limit of ADI• weight 60 kg adult• consumption 2 litres/day Additional comments Use of a 4‑week study was considered appropriate because the NOAEL is based on a reversible acute effect; the NOAEL will also be protective for chronic effects. Assessment date 1998 Principal references FAO/WHO (1997) Pesticide residues in food—1996 evaluations WHO (2004) Carbofuran in drinking-water Carbofuran is highly toxic after acute oral administration. The main systemic effect of carbofuran poisoning in short-term and long-term toxicity studies appears to be cholinesterase inhibition. No evidence of teratogenicity has been found in reproductive toxicity studies. On the basis of available studies, carbofuran does not appear to be carcinogenic or genotoxic. Carbon tetrachloride Carbon tetrachloride is used mainly in the production of chlorofluorocarbon refrigerants, foam-blowing agents and solvents. However, since the Montreal Protocol on Substances that Deplete the Ozone Layer (1987) and its amendments (1990 and 1992) established a timetable for the phase-out of the production and consumption of carbon tetrachloride, manufacture and use have dropped and will continue to drop. Carbon tetrachloride is released mostly into the atmosphere but also into industrial wastewater. Although it readily migrates from surface water to the atmosphere, levels in anaerobic groundwater may remain elevated for months or even years. Although available data on concentrations in food are limited, the intake from air is expected to be much greater than that from food or drinking-water. Guideline value 0.004 mg/l (4 μg/l) Occurrence Concentrations in drinking‑water generally less than 5 μg/l TDI 1.4 μg/kg body weight, based on a NOAEL of 1 mg/kg body weight per day for hepatotoxic effects in a 12‑week oral gavage study in rats, adjusting for daily dosing and applying an uncertainty factor of 500 (100 for interspecies and intraspecies variation, 10 for the duration of the study and a modifying factor of 0.5 because it was a bolus study) Limit of detection 0.1–0.3 μg/l by GC‑ECD or GC‑MS Treatment performance 0.001 mg/l should be achievable using air stripping Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments The guideline value is lower than the range of values associated with upper‑bound lifetime excess cancer risks of 10−4, 10−5 and 10−6 calculated by linear extrapolation. Assessment date 2003 Principal references IPCS (1999) Carbon tetrachloride WHO (2004) Carbon tetrachloride in drinking-water The primary targets for carbon tetrachloride toxicity are liver and kidney. In experiments with mice and rats, carbon tetrachloride proved to be capable of inducing hepatomas and hepatocellular carcinomas. The doses inducing hepatic tumours were higher than those inducing cell toxicity. It is likely that the carcinogenicity of carbon tetrachloride is secondary to its hepatotoxic effects. On the basis of available data, carbon tetrachloride can be considered to be a non-genotoxic compound. Carbon tetrachloride is classified by IARC as being possibly carcinogenic to humans (Group 2B): there is sufficient evidence that carbon tetrachloride is carcinogenic in laboratory animals, but inadequate evidence in humans. Chloral hydrate Chloral hydrate, or trichloroacetaldehyde, can be formed as a by-product of the chlorination of water containing organic precursor material, such as fulvic and humic acids. It has been found in drinking-water at concentrations of up to 100 μg/l, but concentrations are usually below 10 μg/l. Concentrations are generally higher in surface water than in groundwater, and concentrations appear to increase during distribution. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2004 Principal references IPCS (2000) Chloral hydrate IPCS (2000) Disinfectants and disinfectant by-products WHO (2005) Chloral hydrate in drinking-water Chloral hydrate is used as an intermediate in the production of insecticides, herbicides and hypnotic drugs. It has also been widely used as a sedative or hypnotic drug in humans at oral doses of up to about 750–1000 mg/day. Although intake from clinical use is considerably higher than intake from drinking-water, clinical exposure is of shorter-term duration. No epidemiological or carcinogenic studies were found in humans that associated exposure to chloral hydrate with cancer, despite the fact that chloral hydrate has been used for many decades (and still is used) as a sedative and hypnotic drug in adults and children (specifically for dental procedures). IARC classified chloral hydrate as not classifiable as to its carcinogenicity to humans (Group 3), based on inadequate evidence in humans and limited evidence in experimental animals. There is equivocal evidence for the genotoxicity of chloral hydrate. A health-based value of 0.1 mg/l (rounded figure) can be calculated on the basis of a TDI of 0.0045 mg/kg body weight derived based on an increased incidence of liver histopathology observed in mice in a 2-year drinking-water study, allocating 80% of the TDI to drinking-water (because most exposure to chloral hydrate is from drinking-water) and assuming a 60 kg adult consuming 2 litres of water per day. However, because chloral hydrate usually occurs in drinking-water at concentrations well below those of health concern, it is not considered necessary to derive a guideline value. Chloral hydrate levels in drinking-water can be controlled by changes to disinfection practice (e.g. enhanced coagulation and softening to remove organic precursor compounds, moving the point of disinfection to reduce the reaction between chlorine and precursor compounds and using chloramines for residual disinfection instead of chlorine) and by GAC treatment. Chloramines (monochloramine, dichloramine, trichloramine) Monochloramine, dichloramines and trichloramines are considered by-products of drinking-water chlorination, being formed when chlorine and ammonia are added to water. Monochloramine may also be added to maintain residual disinfection activity in potable water distribution systems. Because higher chloramines are formed only occasionally and cause taste and odour problems at concentrations lower than those at which monochloramine causes taste and odour problems, only monochloramine has been considered for development of a health-based guideline value. Chloramine is rapidly decomposed in the stomach by gastric juice. The use of chloramines for disinfection instead of chlorine reduces the formation of THMs in drinking-water supplies. However, formation of other by-products, such as haloketones, chloropicrin, cyanogen chloride, HAAs, haloacetonitriles, aldehydes and chlorophenols, has been reported. Monochloramine, the most abundant chloramine, is recognized as a less effective disinfectant than chlorine and is used as a secondary disinfectant to maintain a residual in distribution systems. Guideline value Monochloramine: 3 mg/l (3000 μg/l) Occurrence Typical chloramine concentrations of 0.5–2 mg/l are found in drinking‑water supplies where chloramine is used as a primary disinfectant or to provide a chlorine residual in the distribution system TDI 94 μg/kg body weight, based on a NOAEL of 9.4 mg/kg body weight per day, the highest dose administered to male rats in a 2‑year United States National Toxicology Program (NTP) drinking‑water study (although mean body weights of rats given the highest dose were lower than those of their respective control groups, it is probable that the lower body weights were caused by the unpalatability of the drinking‑water) Limit of detection 10 μg/l by colorimetric methods Treatment performance Guideline value derivati• allocation to water • weight • consumption It is possible to reduce the concentration of chloramine effectively to zero (< 0.1 mg/l) by reduction; however, it is normal practice to supply water with a chloramine residual of a few tenths of a milligram per litre to provide some protection against microbial recontamination and limit microbial growth problems during distribution. on 100% of TDI 60 kg adult 2 litres/day Additional comments An additional uncertainty factor for possible carcinogenicity was not applied because equivocal cancer effects reported in the NTP study in only one species and in only one sex were within the range observed in historical controls. Most individuals are able to taste chloramines at concentrations below 5 mg/l, and some at levels as low as 0.3 mg/l. Assessment date 2003 Principal references IPCS (2000) Disinfectants and disinfectant by-products WHO (2004) Monochloramine in drinking-water Reason for not establishing Available data inadequate to permit derivation of health‑based guideline guideline values values for dichloramine and trichloramine Assessment date 1993 Principal references IPCS (2000) Disinfectants and disinfectant by-products Monochloramine Although monochloramine has been shown to be mutagenic in some in vitro studies, it has not been found to be genotoxic in vivo. IARC has classified chloramine in Group 3 (not classifiable as to its carcinogenicity to humans). In the NTP bioassay in two species, the incidence of mononuclear cell leukaemias in female rats was increased, but no other increases in tumour incidence were observed. IPCS did not consider the increase in mononuclear cell leukaemia to be treatment related. Dichloramine and trichloramine Dichloramine and trichloramine have not been extensively studied, and available data are inadequate to permit derivation of health-based guideline values for either of these chemicals. However, these substances can cause taste and odour problems (see chapter 10) if formation of monochloramine is not controlled adequately. Chlordane Chlordane (CAS No. 57-47-9) is a broad-spectrum insecticide that has been used since 1947. Its use has recently been increasingly restricted in many countries, and it is now used mainly to destroy termites by subsurface injection into soil. Chlordane may be a low-level source of contamination of groundwater when applied by subsurface injection. Technical chlordane is a mixture of compounds, with the cis and trans forms of chlordane predominating. It is very resistant to degradation, highly immobile in soil and unlikely to migrate to groundwater, where it has only rarely been found. It is readily lost to the atmosphere. Although levels of chlordane in food have been decreasing, it is highly persistent and has a high bioaccumulation potential. Guideline value 0.0002 mg/l (0.2 μg/l) Occurrence Has been detected in both drinking‑water and groundwater, usually at levels below 0.1 μg/l PTDI 0.5 μg/kg body weight based on a NOAEL of 50 μg/kg body weight per day for increased liver weights, serum bilirubin levels and incidence of hepatocellular swelling, derived from a long‑term dietary study in rats, and using an uncertainty factor of 100 (10 each for interspecies and intraspecies variation) Limit of detection 0.014 μg/l by GC with ECD Treatment performance 0.1 μg/l should be achievable using GAC Guideline value derivation • allocation to water 1% of PTDI • weight 60 kg adult• consumption 2 litres/day Additional comments Chlordane is listed under the Stockholm Convention on Persistent Organic Pollutants. Hence, monitoring may occur in addition to that required by drinking‑water guidelines. Assessment date 2003 Principal references FAO/WHO (1995) Pesticide residues in food—1994 evaluations WHO (2003) Chlordane in drinking-water In experimental animals, prolonged exposure in the diet causes liver damage. Chlordane produces liver tumours in mice, but the weight of evidence indicates that it is not genotoxic. Chlordane can interfere with cell communication in vitro, a characteristic of many tumour promoters. IARC re-evaluated chlordane in 1991 and concluded that there is inadequate evidence for its carcinogenicity in humans and sufficient evidence for its carcinogenicity in animals, classifying it in Group 2B. Chloride Chloride in drinking-water originates from natural sources, sewage and industrial effluents, urban runoff containing de-icing salt and saline intrusion. The main source of human exposure to chloride is the addition of salt to food, and the intake from this source is usually greatly in excess of that from drinking-water. Reason for not establishing Not of health concern at levels found in drinking‑water a guideline value Additional comments May affect acceptability of drinking‑water Assessment date 1993 Principal reference WHO (2003) Chloride in drinking-water Excessive chloride concentrations increase rates of corrosion of metals in the distribution system, depending on the alkalinity of the water. This can lead to increased concentrations of metals in the supply. No health-based guideline value is proposed for chloride in drinking-water. However, chloride concentrations in excess of about 250 mg/l can give rise to detectable taste in water (see chapter 10). Chlorine Chlorine is produced in large amounts and widely used both industrially and domestically as an important disinfectant and bleach. In particular, it is widely used in the disinfection of swimming pools and is the most commonly used disinfectant and oxidant in drinking-water treatment. In water, chlorine reacts to form hypochlorous acid and hypochlorites. Concentrations of chlorate and some perchlorates increase in hypochlorite solutions upon storage at high ambient temperatures or when new hypochlorite is added to old hypochlorite. Guideline value 5 mg/l (5000 μg/l) for free chlorine Occurrence Present in most disinfected drinking‑water at concentrations of 0.2–1 mg/l TDI 150 μg/kg body weight, derived from a NOAEL for the absence of toxicity in rodents ingesting chlorine in drinking‑water for 2 years Limit of detection 0.01 μg/l following pre‑column derivatization to 4‑bromoacetanilide by HPLC; 10 μg/l as free chlorine by colorimetry; 200 μg/l by ion chromatography Treatment performance It is possible to reduce the concentration of chlorine effectively to zero (< 0.1 mg/l) by reduction. However, it is normal practice to supply water with a chlorine residual of a few tenths of a milligram per litre to provide some protection against microbial recontamination and limit microbial growth problems during distribution. Guideline value derivation • allocation to water 100% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments The guideline value is conservative, as no adverse effect level was identified in the critical study. Most individuals are able to taste chlorine at the guideline value. Assessment date 1993 Principal reference WHO (2003) Chlorine in drinking-water In humans and experimental animals exposed to chlorine in drinking-water, no specific adverse treatment-related effects have been observed. IARC has classified hypochlorite in Group 3 (not classifiable as to its carcinogenicity to humans). Chlorine dioxide, chlorite and chlorate Chlorite and chlorate are DBPs resulting from the use of chlorine dioxide as a disinfectant and for odour and taste control in water. Sodium chlorite and sodium chlorate are both used in the production of chlorine dioxide as well as for other commercial purposes. Chlorite and chlorate are also formed during the decomposition of hypochlorite solutions that are stored for long periods, particularly at warm temperatures. Where hypochlorite or chlorine dioxide is used as a disinfectant, the major route of environmental exposure to chlorite and chlorate is expected to be through drinking-water. Provisional guideline values Chlorite: 0.7 mg/l (700 μg/l) Chlorate: 0.7 mg/l (700 μg/l) The guideline values for chlorite and chlorate are designated as provisional because use of aged hypochlorite or of chlorine dioxide as disinfectants may result in the chlorite and chlorate guideline values being exceeded, and difficulties in meeting the guideline values must never be a reason for compromising adequate disinfection Occurrence When chlorine dioxide is used as the final disinfectant at typical doses, the resulting chlorite concentration would normally be less than 0.2 mg/l. Chlorate concentrations above 1 mg/l have been reported when hypochlorite was used, but such high concentrations would be unusual unless hypochlorite is stored under adverse conditions. ADIs Chlorite: 0–0.03 mg/kg bw based on a NOAEL of 3 mg/kg bw per day for reduced liver weight of F0 females and F1 males and females in a two‑generation reproductive toxicity study in rats and using a safety factor of 100 (10 each for interspecies and intraspecies variability) Chlorate: 0–0.01 mg/kg bw based on a BMDL10 of 1.1 mg/kg bw per day for non‑neoplastic effects on the thyroid of male rats in a carcinogenicity study and using a safety factor of 100 (10 to allow for intraspecies variability and an additional factor of 10 to allow for the deficiencies in the database; a safety factor for interspecies variation was not considered necessary because humans are likely to be less sensitive than rats to these effects) Limit of detection MDLs as low as 0.45 μg/l for chlorite and 0.78 μg/l for chlorate (IC with conductivity detection) and 78 μg/l for chlorine dioxide (UV/visible spectrophotometric method) Prevention and treatment When using hypochlorite, the following control approach is recommended to minimize formation of chlorite and chlorate: purchase fresh solutions that are of an appropriate quality, store them in a cool place and out of direct sunlight, and use the hypochlorite as soon as possible after purchase (e.g. within a month, if possible). Further, new hypochlorite solutions should not be added to containers containing old hypochlorite solutions, as this will accelerate chlorate formation. It is possible to reduce the concentration of chlorine dioxide and chlorite effectively to zero (<0.1 mg/l) by reduction; however, it is normal practice to supply water with a chlorine dioxide residual of a few tenths of a milligram per litre to provide some protection against microbial regrowth and limit microbial growth problems during distribution. With chlorine dioxide disinfection, the concentrations of chlorate and chlorite depend on process conditions (in both the chlorine dioxide generator and the water treatment plant) and applied dose of chlorine dioxide. As there is no low‑cost option for reducing concentrations of chlorate once it is formed, control of chlorate concentration must rely on preventing its addition (from sodium hypochlorite) or formation (from chlorine dioxide). If chlorine dioxide is used as a pre‑oxidant, the resulting chlorite concentration may need to be reduced using ferrous iron, sulfur reducing agents or activated carbon. Guideline value derivation • allocation to water 80% of ADI • weight 60 kg adult• consumption 2 litres/day Additional comments Concentrations should be maintained as low as reasonably practical, without compromising adequate disinfection. Although a health‑based value of 0.3 mg/l could be derived from the ADI for chlorate, in some circumstances, it may not be possible to adequately disinfect potable water and maintain chlorate concentrations at or below the health‑based value as chlorate is a byproduct of hypochlorite. Therefore, the previous provisional guideline value is retained. Moreover, even this provisional guideline value may be exceeded when aged hypochlorite is used and difficulties in meeting the guideline value must never be a reason for compromising adequate disinfection. Assessment date 2016 Principal references IPCS (2000). Disinfectants and disinfectant by-products WHO (2008). Acidified sodium chlorite WHO (2016). Chlorine dioxide, chlorate and chlorite in drinking-water Chlorine dioxide Any chlorine dioxide remaining at the consumer’s tap will be reduced to chlorite and chloride upon ingestion. Consequently, a guideline value for chlorine dioxide has not been established. The provisional guideline values for chlorite and chlorate are adequately protective for potential toxicity from chlorine dioxide. The taste and odour threshold for chlorine dioxide is 0.2–0.4 mg/l. Chlorite IARC has concluded that chlorite is not classifiable as to its carcinogenicity to humans. The primary and most consistent finding arising from exposure to chlorite in a number of species was oxidative stress resulting in changes in the red blood cells. This observation was supported by a number of biochemical studies conducted in vitro. Studies with human volunteers for up to 12 weeks did not identify any effect on blood parameters at the highest dose tested, 36 μg/kg bw per day. Chlorate Although chlorate has also been reported to have effects on red blood cells, the most sensitive effects observed in rats administered sodium chlorate in drinking-water for 21 or 90 days were changes in thyroid histology (e.g. colloid depletion, hypertrophy, incidence and severity of hyperplasia) and in thyroid hormones. As with chlorite, a chlorate dose of 36 μg/kg bw per day for 12 weeks did not result in any adverse effects in human volunteers. Chloroacetones 1,1-Dichloroacetone is formed from the reaction between chlorine and organic precursors and has been detected in chlorinated drinking-water. Concentrations are estimated to be less than 10 μg/l and usually less than 1 μg/l. Reason for not establishing Available data inadequate to permit derivation of health‑based guideline values guideline values for any of the chloroacetones Assessment date 1993 Principal reference WHO (2003) Chloroacetones in drinking-water The toxicological data on 1,1-dichloroacetone are very limited, although studies with single doses indicate that it affects the liver. There are insufficient data at present to permit the setting of guideline values for 1,1-dichloroacetone or any of the other chloroacetones. Chlorophenols (2-chlorophenol, 2,4-dichlorophenol, 2,4,6-trichlorophenol) Chlorophenols are present in drinking-water as a result of the chlorination of phenols, as by-products of the reaction of hypochlorite with phenolic acids, as biocides or as degradation products of phenoxy herbicides. Those most likely to occur in drinking-water as by-products of chlorination are 2-chlorophenol, 2,4-dichlorophenol and 2,4,6-trichlorophenol. The taste thresholds for chlorophenols in drinking-water are low. Guideline value 2,4,6-Trichlorophenol: 0.2 mg/l (200 μg/l) Occurrence Concentrations of chlorophenols in drinking‑water usually less than 1 μg/l Basis of guideline value Applying the linearized multistage model to leukaemias in male rats derivation observed in a 2‑year feeding study (hepatic tumours found in this study were not used for risk estimation because of the possible role of contaminants in their induction) Limit of detection 0.5–5 μg/l by formation of pentafluorobenzyl ether derivatives; 0.01 μg/l using GC with ECD Treatment performance 2,4,6‑Trichlorophenol concentrations can be reduced using GAC Additional comments The guideline value for 2,4,6‑trichlorophenol exceeds its lowest reported taste threshold. Assessment date 1993 Principal reference WHO (2003) Chlorophenols in drinking-water Reason for not Available data inadequate to permit derivation of health‑based guideline establishing guideline values for 2-chlorophenol and 2,4-dichlorophenol values Assessment date 1993 Principal reference WHO (2003) Chlorophenols in drinking-water 2‑Chlorophenol Data on the toxicity of 2-chlorophenol are limited. Therefore, no health-based guideline value has been derived. 2,4‑Dichlorophenol Data on the toxicity of 2,4-dichlorophenol are limited. Therefore, no health-based guideline value has been derived. 2,4,6‑Trichlorophenol 2,4,6-Trichlorophenol has been reported to induce lymphomas and leukaemias in male rats and hepatic tumours in male and female mice. The compound has not been shown to be mutagenic in the Ames test but has shown weak mutagenic activity in other in vitro and in vivo studies. IARC has classified 2,4,6-trichlorophenol in Group 2B (possibly carcinogenic to humans). Chloropicrin Chloropicrin, or trichloronitromethane, is formed by the reaction of chlorine with humic and amino acids and with nitrophenols. Its formation is increased in the presence of nitrates. Limited data from the USA indicate that concentrations in drinking-water are usually less than 5 μg/l. Reason for not establishing Available data inadequate to permit derivation of health‑based a guideline value guideline value Assessment date 1993 Principal reference WHO (2003) Chloropicrin in drinking-water Decreased survival and body weights have been reported following long-term oral exposure in laboratory animals. Chloropicrin has been shown to be mutagenic in bacterial tests and in in vitro assays in lymphocytes. Because of the high mortality in a carcinogenesis bioassay and the limited number of end-points examined in the 78-week toxicity study, the available data were considered inadequate to permit the establishment of a guideline value for chloropicrin. Chlorotoluron Chlorotoluron (CAS No. 15545-48-9) is a pre emergence or early post emergence herbicide that is slowly biodegradable and mobile in soil. There is only very limited exposure to this compound from food. Guideline value 0.03 mg/l (30 μg/l) Occurrence Detected in drinkingwater at concentrations of less than 1 μg/l TDI 11.3 μg/kg body weight, derived from a NOAEL of 11.3 mg/kg body weight per day for systemic effects in a 2‑year feeding study in mice using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for evidence of carcinogenicity) Limit of detection 0.1 μg/l by separation by reversed‑phase HPLC followed by UV and electrochemical detection Treatment performance 0.1 μg/l should be achievable using GAC Guideline value derivation • allocation to water • weight • consumption 10% of TDI 60 kg adult 2 litres/day Assessment date 1993 Principal reference WHO (2003) Chlorotoluron in drinking-water Chlorotoluron is of low toxicity following single, short-term and long-term exposures in experimental animals, but it has been shown to cause an increase in adenomas and carcinomas of the kidneys of male mice given high doses for 2 years. As no carcinogenic effects were reported in a 2-year study in rats, it has been suggested that chlorotoluron has a carcinogenic potential that is both species and sex specific. Chlorotoluron and its metabolites have shown no evidence of genotoxicity. Chlorpyrifos Chlorpyrifos (CAS No. 2921-88-2) is a broad-spectrum organophosphorus insecticide used for the control of mosquitoes, flies, various crop pests in soil and on foliage, household pests and aquatic larvae. Although it is not recommended for addition to water for public health purposes by the WHO Pesticide Evaluation Scheme (WHOPES), it may be used in some countries as an aquatic larvicide for the control of mosquito larvae. Chlorpyrifos is strongly absorbed by soil and does not readily leach from it, degrading slowly by microbial action. It has a low solubility in water and great tendency to partition from aqueous phases into organic phases in the environment. Guideline value 0.03 mg/l (30 μg/l) Occurrence Detected in surface waters in the USA, usually at concentrations below 0.1 μg/l; also detected in groundwater in less than 1% of the wells tested, usually at concentrations below 0.01 μg/l ADI 0–0.01 mg/kg body weight on the basis of a NOAEL of 1 mg/kg body weight per day for inhibition of brain acetylcholinesterase activity in studies in mice, rats and dogs, using a 100‑fold uncertainty factor, and on the basis of a NOAEL of 0.1 mg/kg body weight per day for inhibition of erythrocyte acetylcholinesterase activity in a study of human subjects exposed for 9 days, using a 10‑fold uncertainty factor Limit of detection 1 μg/l by GC using ECD or flame photometric detection Treatment performance No data available; should be amenable to treatment by coagulation (10– 20% removal), activated carbon adsorption and ozonation Guideline value derivation • allocation to water 10% of upper limit of ADI• weight 60 kg adult• consumption 2 litres/day Assessment date 2003 Principal references FAO/WHO (2000) Pesticide residues in food—1999 evaluations WHO (2003) Chlorpyrifos in drinking-water JMPR concluded that chlorpyrifos is unlikely to pose a carcinogenic risk to humans. Chlorpyrifos was not genotoxic in an adequate range of studies in vitro and in vivo. In long-term studies, inhibition of cholinesterase activity was the main toxicological finding in all species. Chromium Chromium is widely distributed in Earth’s crust. It can exist in valences of –2 to +6 but is primarily in its trivalent (III) and hexavalent (VI) forms. In general, with the exception of individuals living close to a point source of contamination, food appears to be the major source of intake. Guideline value Total chromium: 0.05 mg/l (50 μg/l) Occurrence Total chromium concentrations in drinking‑water are usually low (<5 μg/l), although there may be elevated concentrations in areas containing natural sources or as a result of anthropogenic contamination. Few data are available on the speciation of chromium in drinking‑water. Basis of guideline value derivation The guideline value is based on achievability by available treatment technologies, measurability by analytical methods, and toxicology. The guideline value is for total chromium because of limitations in currently available analytical methods that preclude reliable speciation of chromium in water. Hyperplasia in the small intestine was considered the most sensitive end‑point of tumour formation, and is protective of both noncancer (in the case of chromium(III) and chromium(VI)) and cancer (in the case of chromium(VI)) effects. Limit of detection 0.08–7 μg/l for total chromium by ICP‑AES, ICP‑MS, AES or graphite furnace AAS; 0.5 mg/ for total chromium by FAAS. FAAS and electrothermal atomization AAS methods are recommended when concentrations are 0.5–20 mg/l and <0.1 mg/ml, respectively. 0.0044–0.015 μg/l for chromium(VI) by IC with post‑column derivatization and UV‑visible spectroscopy; 0.5 μg/l for chromium (III) and chromium(VI) using ion chromatography followed by ICP‑MS. Determination of chromium species remains difficult; reliable and validated methods to separate analysis of chromium(III) and chromium(VI) are still required. Treatment performance Effective central treatment technologies for chromium(III) and chromium(VI) include conventional water treatment (coagulation, sedimentation, filtration—requires chromium(VI) reduction to chromium(III)), adsorption by iron oxides, ion exchange, reverse osmosis and nanofiltration. Additional comments As chromium is usually found in drinking‑water below the guideline value, in general, monitoring and inclusion in drinking‑water standards would only be necessary if there were indications that a problem might exist. Monitoring can usually be limited to treatment works; however, specific pollution events may need to be considered on a case‑by‑case basis. Assessment date 2020 Principal reference WHO (2020) Chromium in drinking-water Chromium toxicity is dependent on its valence state; chromium(VI) is known to be more toxic than chromium(III). Following oral exposure, chromium(VI) is rapidly and efficiently reduced in the gastrointestinal tract to chromium(III), although a proportion of chromium(VI) may remain available for absorption. IARC has classified chromium(VI) compounds as “carcinogenic to humans” (Group 1) by the inhalation route of exposure; however, data on human carcinogenicity via the oral route are lacking. In a 2-year chronic drinking-water study, increased incidence of tumours of the oral cavity squamous epithelium and of the small intestinal epithelium were reported in rats and mice (respectively) exposed to chromium(VI), at doses of 0.77 mg/kg bw per day in rats and 0.38 mg/kg bw/day in mice. These tumours have been attributed to a threshold mode of action, recognizing an absence of mutagenicity in highly proliferative intestinal tissue following drinking-water exposure, lack of concordance of mutagenicity and tumour development, and early onset of crypt cell proliferation, which is unlikely to result from a fixed mutation. Concentrations in drinking-water rarely approach the guideline value, and evaluation of drinking-water carcinogenicity studies indicate that most environmental concentrations of chromium(VI) are orders of magnitude below the lowest concentrations tested in the rodent cancer bioassays; therefore, it was determined that the existing guideline value continues to be adequately health-protective. Copper Copper is both an essential nutrient and a drinking-water contaminant. It is used to make pipes, valves and fittings and is present in alloys and coatings. Copper sulfate pentahydrate is sometimes added to surface water for the control of algae. Copper concentrations in drinking-water vary widely, with the primary source most often being the corrosion of interior copper plumbing. Levels in running or fully flushed water tend to be low, whereas those in standing or partially flushed water samples are more variable and can be substantially higher (frequently above 1 mg/l). Copper concentrations in treated water often increase during distribution, especially in systems with an acid pH or high-carbonate waters with an alkaline pH. Food and water are the primary sources of copper exposure in developed countries. Consumption of standing or partially flushed water from a distribution system that includes copper pipes or fittings can considerably increase total daily copper exposure, especially for infants fed formula reconstituted with tap water. Guideline value 2 mg/l (2000 μg/l) Occurrence Concentrations in drinking‑water range from ≤ 0.005 to > 30 mg/l, primarily as a result of the corrosion of interior copper plumbing Basis of guideline value To be protective against acute gastrointestinal effects of copper and derivation provide an adequate margin of safety in populations with normal copper homeostasis Limit of detection 0.02–0.1 μg/l by ICP‑MS; 0.3 μg/l by ICP–optical emission spectroscopy; 0.5 μg/l by flame AAS Treatment performance Copper is not removed by conventional treatment processes. However, copper is not normally a raw water contaminant. Additional comments For adults with normal copper homeostasis, the guideline value should permit consumption of 2 or 3 litres of water per day, use of a nutritional supplement and copper from foods without exceeding the tolerable upper intake level of 10 mg/day or eliciting an adverse gastrointestinal response. Staining of laundry and sanitary ware occurs at copper concentrations above 1 mg/l. At levels above 2.5 mg/l, copper imparts an undesirable bitter taste to water; at higher levels, the colour of water is also impacted. In most instances where copper tubing is used as a plumbing material, concentrations of copper will be below the guideline value. However, there are some conditions, such as highly acidic or aggressive waters, that will give rise to much higher copper concentrations, and the use of copper tubing may not be appropriate in such circumstances. Assessment date 2003 Principal references IPCS (1998) Copper WHO (2004) Copper in drinking-water IPCS concluded that the upper limit of the acceptable range of oral intake in adults is uncertain but is most likely in the range of several (more than 2 or 3 mg/day), but not many, milligrams per day in adults. This evaluation was based solely on studies of gastrointestinal effects of copper-contaminated drinking-water. The available data on toxicity in experimental animals were not considered helpful in establishing the upper limit of the acceptable range of oral intake owing to uncertainty about an appropriate model for humans, but they help to establish a mode of action for the response. The data on the gastrointestinal effects of copper must be used with caution, as the effects observed are influenced by the concentration of ingested copper to a greater extent than the total mass or dose ingested in a 24-hour period. Recent studies have delineated the threshold for the effects of copper in drinking-water on the gastrointestinal tract, but there is still some uncertainty regarding the long-term effects of copper on sensitive populations, such as carriers of the gene for Wilson disease and other metabolic disorders of copper homeostasis. Cyanazine Cyanazine (CAS No. 21725-46-2) is a member of the triazine family of herbicides. It is used as a pre-emergence and post-emergence herbicide for the control of annual grasses and broadleaf weeds. It can be degraded in soil and water by microorganisms and by hydrolysis. Guideline value 0.0006 mg/l (0.6 μg/l) Occurrence Has been detected in surface water and groundwater, usually at concentrations of a few micrograms per litre, although levels as high as 1.3 and 3.5 mg/l have been measured in surface water and groundwater, respectively TDI 0.198 μg/kg body weight based on a NOAEL of 0.198 mg/kg body weight for hyperactivity in male rats in a 2‑year toxicity/carcinogenicity study, using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for limited evidence of carcinogenicity) Limit of detection 0.01 μg/l by GC‑MS Treatment performance 0.1 μg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Assessment date 1998 Principal reference WHO (2003) Cyanazine in drinking-water On the basis of the available mutagenicity data on cyanazine, evidence for genotoxicity is equivocal. Cyanazine causes mammary gland tumours in rats but not in mice. The mechanism of mammary gland tumour development in rats is currently under investigation and may prove to be hormonal. Cyanazine is also teratogenic in rats at dose levels of 25 mg/kg body weight per day and higher. Cyanide Cyanides can be found in some foods, particularly in some developing countries, and they are occasionally found in drinking-water, but usually only at very low concentrations. However, there are occasions on which large spills of cyanide, associated with industry, occur, and these can give rise to very high concentrations in drinking-water source waters, particularly surface waters. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern, except in emergency situations following a spill to a water source Assessment date 2009 Principal references IPCS (2004) Hydrogen cyanide and cyanides WHO (2009) Cyanide in drinking-water Cyanide is highly acutely toxic. It is detoxified in the liver by first-pass metabolism following oral exposure. As a consequence, exposure to a dose spread over a longer period, through a day, for example, will result in lower toxicity, or higher tolerance, than the same dose given in a single bolus dose. Exposure to high doses can give rise to thyroid toxicity as a secondary effect of exposure due to the inhibition of iodine uptake from the thiocyanate generated through the detoxifying action of rhodanese. It is difficult to interpret human data in view of the difficulty in assessing the actual absorbed dose in humans following acute fatal intoxication and the lack of well-conducted studies on sublethal toxicity. There is a need for guidance regarding concentrations that would not be of concern for public health following short-term exposure to cyanide. However, because cyanide is unlikely to occur in drinking-water at concentrations of health concern, it is considered unnecessary to derive a formal guideline value for short-term exposure to cyanide. The data on acute exposure to cyanide are unsuitable for use in deriving a health-based value for short-term exposure because of the high uncertainty surrounding the data. Using the NOAEL for effects on the reproductive organs of male rats in a 13week study and an uncertainty factor of 100, a TDI of 0.045 mg/kg body weight can be derived. Because this health-based value is intended for short-term use and exposure would not exceed 5 days, it is considered to be acceptable to allocate 40% of the TDI to drinking-water to allow for exposure to cyanogenic glycosides in food. Therefore, assuming a 60 kg adult drinking 2 litres of water per day with an allocation of 40% of the TDI to drinking-water, a health-based value of 0.5 mg/l (rounded value) for short-term exposure can be calculated. This health-based value is well below the level that is normally considered to be of health concern for humans. Cyanide is rapidly detoxified, and exposure spread throughout the day will further reduce the potential for effects. This health-based value would be suitable for use for a limited period of up to 5 days, which is the longest period likely to be required under the circumstances of such an emergency. However, it is probable that, in most circumstances, this value will be highly conservative for short-term exposure. It should be noted that the lowest reported odour threshold for cyanide in drinking-water is 0.17 mg/l, which is below the short-term health-based value. It is therefore possible that a small number of individuals will detect cyanide by odour at concentrations below the health-based value. The health-based value relates to total cyanide concentration at the tap, including cyanide from cyanogen chloride in drinking-water as a by-product of disinfection with chlorine. Cyanogen chloride rapidly breaks down to cyanide in the distribution system or when ingested. As the low levels of cyanide normally found in drinking-water are mostly a consequence of the presence of cyanogen chloride, it is not considered necessary to develop a guideline value for long-term exposure to cyanide. Cyanogen chloride Cyanogen chloride may be formed as a by-product of chloramination or chlorination of water. It is also formed by the chlorination of cyanide ion present in raw water. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2009 Principal references IPCS (2004) Hydrogen cyanide and cyanides WHO (2009) Cyanogen chloride in drinking-water Cyanogen chloride is rapidly metabolized to cyanide in the body. There are few data available on the oral toxicity of cyanogen chloride. As cyanogen chloride is unlikely to be found in drinking-water at concentrations that are of health concern, it is considered unnecessary to develop a formal guideline value for cyanogen chloride. Instead, for guidance purposes, a health-based value is derived based on cyanide. Using a NOAEL for cyanide of 4.5 mg/kg body weight per day for minor changes in the testis in a subchronic study in which rats were exposed through their drinking-water and an uncertainty factor of 100, a TDI for cyanide of 0.045 mg/kg body weight (corresponding to a cyanogen chloride dose of 0.11 mg/kg body weight) can be derived. In view of the minor nature of the changes observed and the NOAEL in a previous chronic study, it is not considered necessary to include an additional uncertainty factor to allow for the length of the study. Further, it appears that a dose that may be toxic in acute poisoning would certainly be tolerated under chronic conditions, owing to efficient detoxification. Assuming a 60 kg adult drinking 2 litres of water per day and allowing 20% of the TDI to come from water because of the potential for exposure to cyanogenic glycosides in food, the health-based value for long-term exposure is 0.3 mg/l for cyanide or 0.6 mg/l for cyanogen chloride (rounded values). Although low concentrations of cyanide in raw waters will be converted to cyanogen chloride by chlorination, cyanogen chloride may also be formed during the production of chloramines in situ as a residual disinfectant to maintain the hygienic condition of the distribution system. It is important that treatment be optimized to minimize the formation of cyanogen chloride while maintaining adequate chloramine residuals where chloramination is practised. Cylindrospermopsins (cyanobacterial toxins)1 Cylindrospermopsin (CYN) and its four variants are naturally occurring alkaloids produced by strains of various species of cyanobacteria, primarily in freshwater environments. They have been found in cyanobacteria around the globe and have most frequently been reported from the cyanobacterial genera Raphidiopsis (formerly Cylindrospermopsis), Aphanizomenon and Chrysosporum (see also section 11.5). Unlike MCs, a major fraction of CYNs is often found dissolved in water; particularly at lower temperatures, these toxins may persist even after the producing cyanobacteria are no longer present. Drinking-water is the most likely route of exposure to CYNs where surface water with cyanobacterial blooms is the drinking-water source. Recreational activities in lakes with cyanobacterial blooms may also be a relevant exposure pathway, potentially to high, usually intermittent concentrations (see WHO Guidelines on recreational water quality, 2021). Limited data suggest that CYNs may also accumulate in some food items. Provisional guideline Total CYNs (sum of all congeners, free plus cell-bound): 0.0007 mg/l (0.7 μg/l) value (lifetime) The guideline value is provisional because of the high level of uncertainty— it is based on data for only CYN and the database is limited, as reflected in the composite uncertainty factor of 1000 Provisional short‑term Total CYNs (sum of all congeners, free plus cell-bound): 0.003 mg/l (3 μg/l) guideline value Occurrence Concentrations reported usually range well below 1 mg/l; outside of scum areas, they rarely exceed several μg/l. Major fractions (up to 90%) can occur dissolved in water TDI 0.03 μg/kg bw, based on a NOAEL of 30 μg/kg bw per day for renal pathology observed in an 11‑week study in mice and applying an uncertainty factor of 1000 (10 each for inter‑ and intra‑species variability and 10 for database deficiencies,* taking into consideration limitations in the database—in particular, limited data on chronic toxicity, reproductive toxicity and carcinogenicity) Limit of detection 0.065 μg/L by LC‑MS/MS or LC (including HPLC) followed by UV/PDA detection. LC‑MS/MS has the highest specificity and sensitivity but requires quantitative reference standards for each CYN in the sample. For UV/PDA detection, the signal from one can be used to estimate the concentrations of each. Prior extraction of cells with freeze–thaw cycles and water or methanol/ water is necessary for cell‑bound CYNs; neglecting extraction from cells will lead to dramatic underestimation of concentrations. 0.05 μg/l by commercially available immunoassay kits (ELISA); although these are less precise than LC with the above‑mentioned detection methods, they capture all CYNs and thus are useful for most monitoring purposes. 1 As cyanobacteria and their toxins are a concern in many areas and considering the complexities in their management, this chemical fact sheet has been expanded. Monitoring The likelihood of blooms can be assessed by understanding water body conditions (in particular, nutrient concentrations, water body depth, water retention time, patterns of mixing and stratification; see section 11.5). Where conditions render blooms likely, visual monitoring of source water (including microscopy for potentially CYN‑containing genera) for evidence of increasing cyanobacterial biomass (blooms) is important because biomass can increase rapidly. Exceeding alert values for biomass indicators or CYN concentrations should trigger responses to prevent exposure to elevated toxin concentrations (see the alert level framework in section 11.5). As a major fraction of CYN may occur dissolved in water and be persistent, if blooms of potentially CYN‑producing genera have been observed, monitoring should also include toxin analysis for CYNs, if possible. Analysis of cyanotoxins is particularly useful for validating and optimizing the efficacy of control measures such as riverbank filtration or treatment. Prevention and treatment Actions to decrease the probability of bloom occurrence include catchment and source water management, such as reducing nutrient loading or changing reservoir stratification and mixing. Filtration is effective for removing intact cyanobacterial cells. For the often large dissolved fraction of CYNs, oxidation with chlorine or ozone at sufficient concentrations and contact times, as well as GAC and some PAC applications, are effective (see chapters 7–10 of Toxic cyanobacteria in water; Annex 1). Guideline value derivation • allocation to water 80% of TDI; for short‑term exposure, 100% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments Total CYNs as gravimetric or molar equivalents should be evaluated against the guideline values since CYNs usually occur as mixtures. Although the guideline values are based on CYN, limited evidence suggests that other CYN congeners have similar toxicity to CYN. The provisional short‑term drinking‑water guideline value is intended to indicate the extent to which the lifetime value can be exceeded for periods of up to about 2 weeks until water treatment can be augmented to bring the concentration of CYNs back under control. It is not intended to allow repeated seasonal exceedances of the lifetime value. It is recommended, as a precautionary measure, that bottle‑fed infants and small children be provided with an alternative safe drinking‑water source (e.g. bottled water that is certified by the responsible authorities) if concentrations are greater than 0.7 μg/L, even for short periods. Assessment date 2020 Principal references WHO (2020) Cyanobacterial toxins: cylindrospermopsins Chorus & Welker (2021) Toxic cyanobacteria in water * For the short‑term guideline value, a database uncertainty factor was applied due to the limited database, including lack of data on reproductive effects after oral exposure, and evidence of potential in vivo genotoxicity of CYNs. An uncertainty factor of 3 was applied because lack of chronic toxicity data does not affect derivation of a guideline value for short‑term exposures. CYN is a potent inhibitor of protein synthesis, and also has cytochrome P450dependent effects on other processes—for example, DNA damage and induction of cellular stress responses. CYN was the likely cause of a mass human poisoning incident in Australia in 1979. The provisional guideline values for CYNs are based on a study in male mice using CYN, corroborated by two other studies in both sexes of different strains of mice. The studies demonstrated that a range of organs were adversely affected. The kidneys were identified as the most sensitive organs in these studies, and one study found gender-based differences in the sensitivity of the liver and kidneys. Other studies in mice have demonstrated CYN-induced DNA damage in various organs. Practical considerations Where nutrient (phosphorus and nitrogen) concentrations are elevated in lakes, reservoirs or slowly flowing rivers, cyanobacteria occur widely. Where their excessive growth leads to high biomass, sometimes termed “bloom” events, CYNs can reach concentrations in raw water that are potentially hazardous to human health. Such blooms tend to recur in the same water bodies and to be seasonal, whereas others occur perennially. Although some CYN-producing cyanobacteria form scums or accumulate at the thermocline of thermally stratified reservoirs, they tend to not be as pronounced as the scums and accumulations of MC-producing cyanobacteria. Cyanobacteria are most effectively controlled in the context of developing a WSP (see chapter 4). Control measures to manage potential risks from cyanobacteria, and in particular from their toxins, in drinking-water should include not only adequate treatment, but also measures to control cyanobacterial bloom development. See section 11.5 for more information on cyanobacteria, including further details on monitoring cyanobacterial blooms, the alert level framework, and prevention and management of cyanobacteria in source waters. Effectively minimizing the formation of blooms and locating the raw water intake away from blooms reduce the treatment steps required to remove cyanotoxins. Drinking-water treatment that removes particles—that is, soil, slow sand or riverbank filtration, conventional water treatment (coagulation, flocculation and filtration or dissolved air flotation) or dissolved air flotation—can remove cell-bound CYNs effectively. Soil, slow sand and riverbank filtration can also remove dissolved cyanotoxins. For all these processes it is important that they are optimized to target the removal of cells and dissolved toxins. While for both pre-oxidation and conventional treatment, cell rupture and toxin release should be avoided, treatment also needs to target the typically large fraction of dissolved CYN. Chlorination and ozonation at sufficiently high doses and contact times are effective for degrading dissolved CYNs; however, elevated organic carbon in bloom situations will substantially increase the disinfectant demand. Chlorine dioxide and chloramine are ineffective for degrading CYNs. GAC and PAC can be effective for removing dissolved CYNs, with efficacy dependent on several factors, including the type of activated carbon, contact times (PAC), flow rates (GAC) and water quality. As the challenges that blooms present for treatment are complex, periodic validation of efficacy during bloom situations and under the specific local conditions is particularly important. Avoiding bloom occurrence and intake is therefore the preferred option. 2,4-D The term 2,4-D is used here to refer to the free acid, 2,4-dichlorophenoxyacetic acid (CAS No. 94-75-7). Commercial 2,4-D products are marketed as the free acid, alkali and amine salts and ester formulations. 2,4-D itself is chemically stable, but its esters are rapidly hydrolysed to the free acid. 2,4-D is a systemic herbicide used for control of broad-leaved weeds, including aquatic weeds. 2,4-D is rapidly biodegraded in the environment. Residues of 2,4-D in food rarely exceed a few tens of micrograms per kilogram. Guideline value 0.03 mg/l (30 μg/l) Occurrence Levels in water usually below 0.5 μg/l, although concentrations as high as 30 μg/l have been measured ADI 0–0.01 mg/kg body weight for the sum of 2,4‑D and its salts and esters, expressed as 2,4‑D, on the basis of a NOAEL of 1 mg/kg body weight per day in a 1‑year study of toxicity in dogs (for a variety of effects, including histopathological lesions in kidneys and liver) and a 2‑year study of toxicity and carcinogenicity in rats (for renal lesions) Limit of detection 0.1 μg/l by gas–liquid chromatography with electrolytic conductivity detection Treatment performance 1 μg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of upper limit of ADI• weight 60 kg adult• consumption 2 litres/day Additional comments The guideline value applies to 2,4‑D, as salts and esters of 2,4‑D are rapidly hydrolysed to the free acid in water. Assessment date 1998 Principal references FAO/WHO (1997) Pesticide residues in food—1996 evaluations WHO (2003) 2,4-D in drinking-water Epidemiological studies have suggested an association between exposure to chlorophenoxy herbicides, including 2,4-D, and two forms of cancer in humans: soft tissue sarcomas and non-Hodgkin lymphoma. The results of these studies, however, are inconsistent; the associations found are weak, and conflicting conclusions have been reached by the investigators. Most of the studies did not provide information on exposure specifically to 2,4-D, and the risk was related to the general category of chlorophenoxy herbicides, a group that includes 2,4,5-trichlorophenoxyacetic acid (2,4,5-T), which was potentially contaminated with dioxins. JMPR concluded that it was not possible to evaluate the carcinogenic potential of 2,4-D on the basis of the available epidemiological studies. JMPR also concluded that 2,4-D and its salts and esters are not genotoxic. The toxicity of the salts and esters of 2,4-D is comparable to that of the acid. 2,4-DB The half-lives for degradation of chlorophenoxy herbicides, including 2,4-DB, or 2,4dichlorophenoxybutyric acid (CAS No. 94-82-6), in the environment are in the order of several days. Chlorophenoxy herbicides are not often found in food. Guideline value 0.09 mg/l (90 μg/l) Occurrence Chlorophenoxy herbicides not frequently found in drinking‑water; when detected, concentrations usually no greater than a few micrograms per litre TDI 30 μg/kg body weight, based on a NOAEL of 3 mg/kg body weight per day for effects on body and organ weights, blood chemistry and haematological parameters in a 2‑year study in rats, with an uncertainty factor of 100 (for interspecies and intraspecies variation) Limit of detection 1 μg/l to 1 mg/l for various methods commonly used for the determination of chlorophenoxy herbicides in water, including solvent extraction, separation by GC, gas–liquid chromatography, thin‑layer chromatography or HPLC, with ECD or UV detection Treatment performance 0.1 μg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments The NOAEL used in the guideline value derivation is similar to the NOAEL of 2.5 mg/kg body weight per day obtained in a short‑term study in dogs and the NOAEL for hepatocyte hypertrophy of 5 mg/kg body weight per day obtained in a 3‑month study in rats. Assessment date 1993 Principal reference WHO (2003) Chlorophenoxy herbicides (excluding 2,4-D and MCPA) in drinking-water Chlorophenoxy herbicides, as a group, have been classified in Group 2B (possibly carcinogenic to humans) by IARC. However, the available data from studies in exposed populations and experimental animals do not permit assessment of the carcinogenic potential to humans of any specific chlorophenoxy herbicide. Therefore, drinking-water guidelines for these compounds are based on a threshold approach for other toxic effects. DDT and metabolites The structure of dichlorodiphenyltrichloroethane, or DDT (CAS No. 107917-42-0), permits several different isomeric forms; commercial products consist predominantly of p,p′-DDT. Its use has been restricted or banned in several countries, although DDT is still used in some countries for the control of vectors that transmit yellow fever, sleeping sickness, typhus, malaria and other insect-transmitted diseases. DDT and its metabolites are persistent in the environment and resistant to complete degradation by microorganisms. Food is the major source of intake of DDT and related compounds for the general population, although exposure has significantly decreased as a consequence of the greatly reduced use of DDT for all except specialist applications. Guideline value 0.001 mg/l (1 μg/l) Occurrence Detected in surface water at concentrations below 1 μg/l; also detected in drinking‑water at 100‑fold lower concentrations PTDI 0.01 mg/kg body weight based on a NOAEL of 1 mg/kg body weight per day for developmental toxicity in rats, applying an uncertainty factor of 100 (for interspecies and intraspecies variation) Limit of detection 0.011 μg/l by GC using ECD Treatment performance 0.1 μg/l should be achievable using coagulation or GAC Guideline value derivation • allocation to water • weight • consumption 1% of PTDI 10 kg child 1 litre/day Additional comments DDT is listed under the Stockholm Convention on Persistent Organic Pollutants. Hence, monitoring may occur in addition to that required by drinking‑water guidelines. It should be noted that the level of DDT and its metabolites in food has been falling steadily, and the allocation of 1% of the PTDI may be very conservative. The guideline value is derived on the basis of a 10 kg child consuming 1 litre of drinking‑water per day, because infants and children may be exposed to greater amounts of chemicals in relation to their body weight and because of concern over the bioaccumulation of DDT. It should be emphasized that the benefits of DDT use in malaria and other vector control programmes outweigh any health risk from the presence of DDT in drinking‑water. Assessment date 2003 Principal references FAO/WHO (2001) Pesticide residues in food—2000 evaluations WHO (2004) DDT and its derivatives in drinking-water A working group convened by IARC classified the DDT complex (the mixture of the various isomers of DDT and associated compounds) as a non-genotoxic carcinogen in rodents and a potent promoter of liver tumours. IARC has concluded that there is insufficient evidence in humans and sufficient evidence in experimental animals for the carcinogenicity of DDT (Group 2B) based upon liver tumours observed in rats and mice. The results of epidemiological studies of pancreatic cancer, multiple myeloma, non-Hodgkin lymphoma and uterine cancer did not support the hypothesis of an association with environmental exposure to the DDT complex. Conflicting data were obtained with regard to some genotoxic end-points. In most studies, DDT did not induce genotoxic effects in rodent or human cell systems, nor was it mutagenic to fungi or bacteria. The United States Agency for Toxic Substances and Disease Registry concluded that the DDT complex could impair reproduction and development in several species. Hepatic effects of DDT in rats include increased liver weights, hypertrophy, hyperplasia, induction of microsomal enzymes, including cytochrome P450, cell necrosis, increased activity of serum liver enzymes and mitogenic effects, which might be related to a regenerative liver response to high doses of DDT. 1,2-Dibromo-3-chloropropane 1,2-Dibromo-3-chloropropane (CAS No. 96-12-8), or DBCP, is a soil fumigant that is highly soluble in water. It has a taste and odour threshold in water of 10 μg/l. DBCP was detected in vegetables grown in treated soils, and low levels have been detected in air. Guideline value 0.001 mg/l (1 μg/l) Occurrence Limited survey found levels of up to a few micrograms per litre in drinking‑water Basis of guideline value Linearized multistage model was applied to the data on the incidence of derivation stomach, kidney and liver tumours in the male rat in a 104‑week dietary study Limit of detection 0.02 μg/l by GC with ECD Treatment performance 1 μg/l should be achievable using air stripping followed by GAC Additional comments The guideline value of 1 μg/l should be protective for the reproductive toxicity of DBCP. Assessment date 1993 Principal reference WHO (2003) 1,2-Dibromo-3-chloropropane in drinking-water On the basis of data from different strains of rats and mice, DBCP was determined to be carcinogenic in both sexes by the oral, inhalation and dermal routes. DBCP was also determined to be a reproductive toxicant in humans and several species of laboratory animals. DBCP was found to be genotoxic in a majority of in vitro and in vivo assays. IARC has classified DBCP in Group 2B based upon sufficient evidence of carcinogenicity in animals. Recent epidemiological evidence suggests an increase in cancer mortality in individuals exposed to high levels of DBCP. 1,2-Dibromoethane 1,2-Dibromoethane (CAS No. 106-93-4), or ethylene dibromide, is used as a lead scavenger in tetraalkyl lead petrol and antiknock preparations and as a fumigant for soils, grains and fruits. However, with the phasing out of leaded petrol and of the use of 1,2-dibromoethane in agricultural applications in many countries, use of this substance has declined significantly. In addition to its continued use as a petrol additive in some countries, 1,2-dibromoethane is currently used principally as a solvent and as an intermediate in the chemical industry. Provisional guideline value 0.0004 mg/l (0.4 μg/l) The guideline value is provisional owing to serious limitations of the critical studies. Occurrence Detected in groundwater following its use as a soil fumigant at concentrations as high as 100 μg/l Basis of guideline value derivation Lower end of the range (and thus more conservative estimate) of lifetime low‑dose cancer risks calculated by linearized multistage modelling of the incidences of haemangiosarcomas and tumours in the stomach, liver, lung and adrenal cortex (adjusted for the observed high early mortality, where appropriate, and corrected for the expected rate of increase in tumour formation in rodents in a standard bioassay of 104 weeks) of rats and mice exposed by gavage Limit of detection 0.01 μg/l by microextraction GC‑MS; 0.03 μg/l by purge‑and‑trap GC with halogen‑specific detector; 0.8 μg/l by purge‑and‑trap capillary column GC with photoionization and electrolytic conductivity detectors in series Treatment performance 0.1 μg/l should be achievable using GAC Assessment date 2003 Principal references IPCS (1995) Report of the 1994 meeting of the Core Assessment Group IPCS (1996) 1,2-Dibromoethane WHO (2003) 1,2-Dibromoethane in drinking-water 1,2-Dibromoethane has induced an increased incidence of tumours at several sites in all carcinogenicity bioassays identified in which rats or mice were exposed to the compound by gavage, ingestion in drinking-water, dermal application and inhalation. However, many of these studies were characterized by high early mortality, limited histopathological examination, small group sizes or use of only one exposure level. The substance acted as an initiator of liver foci in an initiation/promotion assay but did not initiate skin tumour development. 1,2-Dibromoethane was consistently genotoxic in in vitro assays, although results of in vivo assays were mixed. Biotransformation to active metabolites, which have been demonstrated to bind to DNA, is probably involved in the induction of tumours. Available data do not support the existence of a non-genotoxic mechanism of tumour induction. The available data thus indicate that 1,2-dibromoethane is a genotoxic carcinogen in rodents. Data on the potential carcinogenicity in humans are inadequate; however, it is likely that 1,2dibromoethane is metabolized similarly in rodent species and in humans (although there may be varying potential for the production of active metabolites in humans, owing to genetic polymorphism). IARC classified 1,2-dibromoethane in Group 2A (probably carcinogenic to humans). Dichloroacetic acid Chlorinated acetic acids, including dichloroacetic acid (DCA), are formed from organic material during water chlorination. DCA has been used as a therapeutic agent to treat lactic acidosis, diabetes and familial hyperlipidaemia in humans. Provisional guideline value 0.05 mg/l (50 μg/l) The guideline value is designated as provisional on the basis of technical achievability. Occurrence Found in groundwater and surface water distribution systems at concentrations up to about 100 μg/l, with mean concentrations below 20 μg/l Basis of guideline value Linear multistage model applied to combined data for carcinomas and derivation adenomas in male mice exposed to doses up to 429 mg/kg body weight per day for up to 2 years Limit of detection < 0.1–0.4 μg/l by GC with ECD; practical quantification limit 1 μg/l Treatment performance Concentrations may be reduced by installing or optimizing coagulation to remove precursors or by controlling the pH during chlorination. Additional comments The concentration associated with a 10−5 upper‑bound excess lifetime cancer risk is 40 μg/l. In some circumstances, however, it may not be possible to adequately disinfect potable water and maintain DCA levels below 40 μg/l, so the provisional guideline value of 50 μg/l is retained. Assessment date 2004 Principal reference WHO (2005) Dichloroacetic acid in drinking-water IARC reclassified DCA as Group 2B (possibly carcinogenic to humans) in 2002, based on the absence of data on human carcinogenicity and sufficient evidence of its carcinogenicity in experimental animals. This classification was based primarily on findings of liver tumours in rats and mice. Genotoxicity data are considered to be inconclusive, particularly at lower doses. Glycogen deposition, peroxisome proliferation, changes in signal transduction pathways and DNA hypomethylation have all been observed following DCA exposure and have been hypothesized to be involved in its carcinogenicity. However, the available data are not sufficient to establish a cancer mode of action with reasonable certainty, especially at the very low exposure levels expected to apply to humans ingesting chlorinated drinking-water. Recent data suggest that there may be more than one mechanism leading to tumours, as altered hepatic foci from treated mice were found to have three different types of cellular characteristics. Dichlorobenzenes (1,2-dichlorobenzene, 1,3-dichlorobenzene, 1,4-dichlorobenzene) The dichlorobenzenes (DCBs) are widely used in industry and in domestic products such as odour-masking agents, chemical dyestuffs and pesticides. Sources of human exposure are predominantly air and food. Guideline values 1,2-Dichlorobenzene: 1 mg/l (1000 μg/l) 1,4-Dichlorobenzene: 0.3 mg/l (300 μg/l) Occurrence Have been found in raw water sources at levels as high as 10 μg/l and in drinking‑water at concentrations up to 3 μg/l; much higher concentrations (up to 7 mg/l) present in contaminated groundwater TDIs 1,2-Dichlorobenzene: 429 μg/kg body weight, based on a NOAEL of 60 mg/kg body weight per day for tubular degeneration of the kidney identified in a 2‑year mouse gavage study, adjusting for daily dosing and using an uncertainty factor of 100 (for interspecies and intraspecies variation) 1,4-Dichlorobenzene: 107 μg/kg body weight, based on a LOAEL of 150 mg/kg body weight per day for kidney effects identified in a 2‑year rat study, adjusting for daily dosing and using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for the use of a LOAEL instead of a NOAEL and the carcinogenicity end‑point) Limit of detection 0.01–0.25 μg/l by gas–liquid chromatography with ECD; 3.5 μg/l by GC using a photoionization detector Treatment performance 0.01 mg/l should be achievable using air stripping Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments Guideline values for both 1,2‑ and 1,4‑DCB far exceed their lowest reported taste thresholds in water of 1 and 6 μg/l, respectively. Assessment date 1993 Principal reference WHO (2003) Dichlorobenzenes in drinking-water Reason for not establishing Available data inadequate to permit derivation of health‑based a guideline value guideline value for 1,3-dichlorobenzene Assessment date 1993 Principal reference WHO (2003) Dichlorobenzenes in drinking-water 1,2‑Dichlorobenzene 1,2-DCB is of low acute toxicity by the oral route of exposure. Oral exposure to high doses of 1,2-DCB affects mainly the liver and kidneys. The balance of evidence suggests that 1,2-DCB is not genotoxic, and there is no evidence for its carcinogenicity in rodents. 1,3‑Dichlorobenzene There are insufficient toxicological data on this compound to permit a guideline value to be proposed, but it should be noted that it is rarely found in drinking-water. 1,4‑Dichlorobenzene 1,4-DCB is of low acute toxicity, but there is evidence that it increases the incidence of renal tumours in rats and of hepatocellular adenomas and carcinomas in mice after long-term exposure. IARC has placed 1,4-DCB in Group 2B (possibly carcinogenic to humans). 1,4-DCB is not considered to be genotoxic, and the relevance for humans of the tumours observed in experimental animals is doubtful. 1,1-Dichloroethane 1,1-Dichloroethane is used as a chemical intermediate and solvent. There are limited data showing that it can be present at concentrations of up to 10 μg/l in drinking-water. It is primarily of concern for groundwater. Reason for not establishing Available data inadequate to permit derivation of health‑based a guideline value guideline value Assessment date 1993 Principal reference WHO (2003) 1,1-Dichloroethane in drinking-water 1,1-Dichloroethane is rapidly metabolized by mammals to acetic acid and a variety of chlorinated compounds. It is of relatively low acute toxicity, and limited data are available on its toxicity from short-term and long-term studies. There is limited in vitro evidence of genotoxicity. One carcinogenicity study by gavage in mice and rats provided no conclusive evidence of carcinogenicity, although there was some evidence of an increased incidence of haemangiosarcomas in treated animals. In view of the very limited database on toxicity and carcinogenicity, it was concluded that no guideline value should be proposed. 1,2-Dichloroethane 1,2-Dichloroethane is used mainly as an intermediate in the production of vinyl chloride and other chemicals and to a lesser extent as a solvent. It was used as a scavenger for tetraethyl lead in gasoline. It may enter surface waters via effluents from industries that manufacture or use the substance. It may also enter groundwater, where it may persist for long periods, following disposal in waste sites. It is found in urban air. Guideline value 0.03 mg/l (30 μg/l) Occurrence Has been found in drinking‑water at levels of up to a few micrograms per litre Basis of guideline value Applying the linearized multistage model to haemangiosarcomas derivation observed in male rats in a 78‑week gavage study Limit of detection 0.03 μg/l by GC with photoionization detection; 0.03–0.2 μg/l by GC with electrolytic conductivity detector; 0.06–2.8 μg/l by GC‑MS; 5 μg/l by GC with flame ionization detection (FID) Treatment performance 0.0001 mg/l should be achievable using GAC Additional comments The guideline value of 0.03 mg/l is consistent with the value derived from IPCS (1998), based on a 10−5 risk level. Assessment date 2003 Principal references IPCS (1995) 1,2-Dichloroethane, 2nd ed. IPCS (1998) 1,2-Dichloroethane WHO (2003) 1,2-Dichloroethane in drinking-water IARC has classified 1,2-dichloroethane in Group 2B (possible human carcinogen). It has been shown to produce statistically significant increases in a number of tumour types in laboratory animals, including the relatively rare haemangiosarcoma, and the balance of evidence indicates that it is potentially genotoxic. Targets of 1,2dichloroethane toxicity in orally exposed animals included the immune system, central nervous sytem, liver and kidney. Data indicate that 1,2-dichloroethane is less potent when inhaled. 1,1-Dichloroethene 1,1-Dichloroethene, or vinylidene chloride, is used mainly as a monomer in the production of polyvinylidene chloride co-polymers and as an intermediate in the synthesis of other organic chemicals. It is an occasional contaminant of drinking-water, usually being found together with other chlorinated hydrocarbons. There are no data on levels in food, but levels in air are generally less than 40 ng/m3 except at some manufacturing sites. 1,1-Dichloroethene is detected in finished drinking-water taken from groundwater sources at median concentrations of 0.28–1.2 μg/l and in public drinking-water supplies at concentrations up to 0.5 μg/l. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2004 Principal references IPCS (2003) 1,1-Dichloroethene (vinylidene chloride) WHO (2005) 1,1-Dichloroethene in drinking-water 1,1-Dichloroethene is a central nervous system depressant and may cause liver and kidney toxicity in occupationally exposed humans. It causes liver and kidney damage in laboratory animals. IARC has placed 1,1-dichloroethene in Group 3 (not classifiable as to its carcinogenicity to humans). It was found to be genotoxic in a number of test systems in vitro but was not active in the dominant lethal and micro-nucleus assays in vivo. It induced kidney tumours in mice in one inhalation study but was reported not to be carcinogenic in a number of other studies, including several in which it was given in drinking-water. A health-based value of 140 μg/l (rounded value) can be calculated on the basis of a TDI of 0.046 mg/kg body weight, derived using the benchmark dose (BMD) approach from a study in which the critical effect was minimal hepatocellular mid-zonal fatty change in female rats. However, this value is significantly higher than the concentrations of 1,1-dichloroethene normally found in drinking-water. It is therefore considered unnecessary to set a formal guideline value for 1,1-dichloroethene in drinking-water. 1,2-Dichloroethene 1,2-Dichloroethene exists in a cis and a trans form. The cis form is more frequently found as a water contaminant. The presence of these two isomers, which are metabolites of other unsaturated halogenated hydrocarbons in wastewater and anaerobic groundwater, may indicate the simultaneous presence of other organochlorine chemicals, such as vinyl chloride. Accordingly, their presence indicates that more intensive monitoring should be conducted. There are no data on exposure from food. Concentrations in air are low, with higher concentrations, in the microgram per cubic metre range, near production sites. The cis isomer was previously used as an anaesthetic. Guideline value 0.05 mg/l (50 μg/l) Occurrence Has been found in drinking‑water supplies derived from groundwater at levels up to 120 μg/l TDI 17 μg/kg body weight, based on a NOAEL (for increases in serum alkaline phosphatase levels and increased thymus weight) of 17 mg/ kg body weight from a 90‑day study in mice administered trans‑1,2dichloroethene in drinking‑water, using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for the short duration of the study) Limit of detection 0.17 μg/l by GC‑MS Treatment performance 0.01 mg/l should be achievable using GAC or air stripping Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments Data on the trans isomer were used to calculate a joint guideline value for both isomers because toxicity for the trans isomer occurred at a lower dose than for the cis isomer and because data suggest that the mouse is a more sensitive species than the rat. Assessment date 1993 Principal reference WHO (2003) 1,2-Dichloroethene in drinking-water There is little information on the absorption, distribution or excretion of 1,2dichloroethene. However, by analogy with 1,1-dichloroethene, 1,2-dichloroethene would be expected to be readily absorbed, distributed mainly to the liver, kidneys and lungs and rapidly excreted. The cis isomer is more rapidly metabolized than the trans isomer in in vitro systems. Both isomers have been reported to cause increased serum alkaline phosphatase levels in rodents. In a 3-month study in mice given the trans isomer in drinking-water, there was a reported increase in serum alkaline phosphatase and reduced thymus and lung weights. Transient immunological effects were also reported, the toxicological significance of which is unclear. Trans-1,2-dichloroethene also caused reduced kidney weights in rats, but at higher doses. Only one rat toxicity study is available for the cis isomer, which produced toxic effects in rats similar in magnitude to those induced by the trans isomer in mice, but at higher doses. There are limited data to suggest that both isomers may possess some genotoxic activity. There is no information on carcinogenicity. Dichloromethane Dichloromethane, or methylene chloride, is widely used as a solvent for many purposes, including coffee decaffeination and paint stripping. Exposure from drinking-water is likely to be insignificant compared with that from other sources. Guideline value 0.02 mg/l (20 μg/l) Occurrence Has been found in surface water samples at concentrations ranging from 0.1 to 743 μg/l; levels usually higher in groundwater because volatilization is restricted, with concentrations as high as 3600 μg/l reported; mean concentrations in drinking‑water less than 1 μg/l TDI 6 μg/kg body weight, derived from a NOAEL of 6 mg/kg body weight per day for hepatotoxic effects in a 2‑year drinking‑water study in rats, using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for concern about carcinogenic potential) Limit of detection 0.3 μg/l by purge‑and‑trap GC with MS detection (note that dichloromethane vapour readily penetrates tubing during the procedure) Treatment performance 20 μg/l should be achievable using air stripping Guideline value derivation • allocation to water • weight • consumption 10% of TDI 60 kg adult 2 litres/day Assessment date 1993 Principal reference WHO (2003) Dichloromethane in drinking-water Dichloromethane is of low acute toxicity. An inhalation study in mice provided conclusive evidence of carcinogenicity, whereas drinking-water studies in rats and mice provided only suggestive evidence. IARC has placed dichloromethane in Group 2B (possible human carcinogen); however, the balance of evidence suggests that it is not a genotoxic carcinogen and that genotoxic metabolites are not formed in relevant amounts in vivo. 1,2-Dichloropropane 1,2-Dichloropropane (CAS No. 78-87-5), or 1,2-DCP, is used as an insecticide fumigant on grain and soil and to control peach tree borers. It is also used as an intermediate in the production of tetrachloroethene and other chlorinated products and as a solvent. 1,2-DCP is relatively resistant to hydrolysis, is poorly adsorbed onto soil and can migrate into groundwater. Provisional guideline value 0.04 mg/l (40 μg/l) The guideline value is provisional owing to limitations of the toxicological database. Occurrence Detected in groundwater and drinking‑water, usually at concentrations below 20 μg/l, although levels as high as 440 μg/l have been measured in well water TDI 14 μg/kg body weight based on a LOAEL of 71.4 mg/kg body weight per day (100 mg/kg body weight per day adjusted for daily dosing) for changes in haematological parameters in a 13‑week study in male rats, with an uncertainty factor of 5000 (100 for interspecies and intraspecies variation, 10 for use of a LOAEL and 5 to reflect limitations of the database, including the limited data on in vivo genotoxicity and use of a subchronic study) Limit of detection 0.02 μg/l by purge‑and‑trap GC with an electrolytic conductivity detector or GC‑MS Treatment performance 1 μg/l should be achievable using GAC Guideline value derivation • allocation to water • weight • consumption 10% of TDI 60 kg adult 2 litres/day Assessment date 1998 Principal reference WHO (2003) 1,2-Dichloropropane (1,2-DCP) in drinking-water 1,2-DCP was evaluated by IARC in 1986 and 1987. The substance was classified in Group 3 (not classifiable as to its carcinogenicity to humans) on the basis of limited evidence for its carcinogenicity in experimental animals and insufficient data with which to evaluate its carcinogenicity in humans. Results from in vitro assays for mutagenicity were mixed. The in vivo studies, which were limited in number and design, were negative. In accordance with the IARC evaluation, the evidence from the long-term carcinogenicity studies in mice and rats was considered limited, and it was concluded that the use of a threshold approach for the toxicological evaluation of 1,2-DCP was appropriate. 1,3-Dichloropropane 1,3-Dichloropropane (CAS No. 142-28-9) has several industrial uses and may be found as a contaminant of soil fumigants containing 1,3-dichloropropene. It is rarely found in water. Reason for not establishing Available data inadequate to permit derivation of health‑based a guideline value guideline value Assessment date 1993 Principal reference WHO (2003) 1,3-Dichloropropane in drinking-water 1,3-Dichloropropane is of low acute toxicity. There is some indication that it may be genotoxic in bacterial systems. No short-term, long-term, reproductive or developmental toxicity data pertinent to exposure via drinking-water could be located in the literature. The available data are considered insufficient to permit recommendation of a guideline value. 1,3-Dichloropropene 1,3-Dichloropropene (CAS Nos. 542-75-6 isomer mixture; 10061-01-5 cis isomer; 10061-02-6 trans isomer) is a soil fumigant, the commercial product being a mixture of cis and trans isomers. It is used to control a wide variety of soil pests, particularly nematodes in sandy soils. Notwithstanding its high vapour pressure, it is soluble in water at the gram per litre level and can be considered a potential water contaminant. Guideline value 0.02 mg/l (20 μg/l) Occurrence Has been found in surface water and groundwater at concentrations of a few micrograms per litre Basis of guideline value Calculated by applying the linearized multistage model to the derivation observation of lung and bladder tumours in female mice in a 2‑year gavage study Limit of detection 0.34 and 0.20 μg/l by purge‑and‑trap packed column GC using an electrolytic conductivity detector or microcoulometric detector for the cis and trans isomers, respectively Treatment performance No information found on removal from water Assessment date 1993 Principal reference WHO (2003) 1,3-Dichloropropene in drinking-water 1,3 Dichloropropene is a direct-acting mutagen that has been shown to produce forestomach tumours following long term oral gavage exposure in rats and mice. Tumours have also been found in the bladder and lungs of female mice and the liver of male rats. Long term inhalation studies in the rat have proved negative, whereas some benign lung tumours have been reported in inhalation studies in mice. IARC has classified 1,3 dichloropropene in Group 2B (possible human carcinogen). Dichlorprop The half-lives for degradation of chlorophenoxy herbicides, including dichlorprop (CAS No. 120-36-5), or 2,4-DP, in the environment are in the order of several days. Chlorophenoxy herbicides are not often found in food. Guideline value 0.1 mg/l (100 μg/l) Occurrence Chlorophenoxy herbicides not frequently found in drinking‑water; when detected, concentrations usually no greater than a few micrograms per litre TDI 36.4 μg/kg body weight, based on a NOAEL of 3.64 mg/kg body weight per day for renal toxicity in a 2‑year dietary study in rats, applying an uncertainty factor of 100 (for intraspecies and interspecies variation) Limit of detection 1 μg/l to 1 mg/l for various methods commonly used for the determination of chlorophenoxy herbicides in water, including solvent extraction, separation by GC, gas–liquid chromatography, thin‑layer chromatography or HPLC, with ECD or UV detection Treatment performance No information found on removal from water Guideline value derivation • allocation to water • weight • consumption 10% of TDI 60 kg adult 2 litres/day Assessment date 1993 Principal reference WHO (2003) Chlorophenoxy herbicides (excluding 2,4-D and MCPA) in drinking-water Chlorophenoxy herbicides, as a group, have been classified in Group 2B (possible human carcinogen) by IARC. However, the available data from studies in exposed populations and experimental animals do not permit assessment of the carcinogenic potential to humans of any specific chlorophenoxy herbicide. Therefore, drinking-water guidelines for these compounds are based on a threshold approach for other toxic effects. In dietary studies in rats, slight liver hypertrophy was observed in a 3-month study, and effects in a 2-year study included hepatocellular swelling, mild anaemia, increased incidence of brown pigment in the kidneys (possibly indicative of slight degeneration of the tubular epithelium) and decreased urinary specific gravity and protein. Dichlorvos Dichlorvos (CAS No. 62-73-7) is a broad-spectrum organophosphorus insecticide used primarily for controlling household pests and for protecting stored products from insects. It is no longer approved for use in some jurisdictions because of concerns over its acute toxicity. Dichlorvos is expected to be very mobile in soils. It is rapidly degraded by microbial activity and hydrolysis in soil, and does not adsorb to sediments. Degradation in water occurs primarily through hydrolysis. There are relatively few studies on its occurrence in source waters. Exposure from food varies widely, depending on local circumstances and usage. Dichlorvos can be inhaled from its use as a domestic insecticide. Reason for not establishing Occurs in drinking‑water or drinking‑water sources at concentrations a guideline value well below those of health concern Health‑based value* 0.02 mg/l Acute health‑based value** 3 mg/l Occurrence Concentrations in surface water in the range 10–50 ng/l, but sometimes as high as 1500 ng/l, have been measured ADI 0–0.004 mg/kg bw, based on a NOAEL of 0.04 mg/kg bw per day for the inhibition of erythrocyte acetylcholinesterase activity in a 21‑day study in male volunteers and application of a safety factor of 10 ARfD 0.1 mg/kg bw, based on a NOAEL of 1 mg/kg bw for erythrocyte acetylcholinesterase inhibition in an acute oral study in male volunteers and application of a safety factor of 10 Limit of detection 0.01 μg/l (limit of quantification) based on solvent extraction and GC analysis; 0.1 μg/l (reporting limit) based on GC‑MS Treatment performance Conventional treatment, including coagulation, filtration and chlorination, not effective; removal by membranes depends on membrane type and operational conditions. Removal by nanofiltration membranes has variable effectiveness (removal rates from 4 to 60%). Reverse osmosis would be expected to be effective (removal rates > 85%) based on removal studies and predictions. Health‑based value derivation • allocation to water 20% of upper bound of ADI• weight 60 kg adult• consumption 2 litres/day Acute health‑based value derivation • allocation to water 100% of ARfD • weight 60 kg adult• consumption 2 litres/day Additional comments The default allocation factor of 20% has been used to account for the fact that the available food exposure data, which suggest that exposure via this route is low, do not generally include information from developing countries, where exposure via this route may be higher, and as potential exposure via inhalation from indoor air resulting from use of dichlorvos as a domestic insecticide is unknown As a general principle, the concentration of pesticides in water, including dichlorvos, should be kept as low as possible and concentrations should not be allowed to increase up to the health‑based value. Further guidance on interpreting the health‑based value and deciding when to monitor can be found in section 8.5.3 Assessment date 2016 Principal references WHO (2012). Pesticide residues in food – 2011 evaluations WHO (2016). Dichlorvos in drinking-water * When a formal guideline value is not established, a “health‑based value” may be determined in order to provide guidance to Member States when there is reason for local concern. Establishing a formal guideline value for such substances may encourage Member States to incorporate a value into their national standards when this may be unnecessary. ** For more information on acute health‑based values, see section 8.7.5. As with other organophosphorus insecticides, the inhibition of cholinesterase activity, causing neurotoxicity, is the most sensitive toxicological end-point following acute or repeated exposures to dichlorvos. Dichlorvos is unlikely to be genotoxic in vivo or to pose a carcinogenic risk to humans. Some reproductive toxicity has been observed in rats, but dichlorvos was not found to cause developmental toxicity or to be teratogenic. Dicofol Dicofol (CAS No. 115-32-2) is an organochlorine acaricide that has been registered for broad-spectrum contact, non-systemic control of plant-eating mites in cotton, tea and a wide variety of fruit, vegetable and ornamental crops. Products containing dicofol, which is manufactured from DDT, are being phased out in the USA and are no longer approved for use in the European Union. Dicofol is unlikely to reach water, but may do so if bound to particulate matter subject to runoff. Dicofol is only slightly soluble in water and binds strongly to soil. There are few data on the occurrence of dicofol in water. Exposure from food varies widely, depending on local circumstances and usage. Dicofol has been proposed as a persistent organic pollutant under the Stockholm Convention. Reason for not establishing Unlikely to be found in drinking‑water or drinking‑water sources* a guideline value Health‑based value** 0.01 mg/l Acute health‑based value*** 6 mg/l Occurrence Not detected in limited groundwater monitoring ADI 0–0.002 mg/kg bw, based on a NOAEL of 0.22 mg/kg bw per day for histopathological changes in the liver and adrenal gland in a 2‑year toxicity and carcinogenicity study in rats and application of a safety factor of 100 ARfD 0.2 mg/kg bw, based on a NOAEL of 15 mg/kg bw for decreased body weight and decreased feed intake in an acute neurotoxicity study in rats and application of a safety factor of 100 Limit of detection Solvent extraction followed by GC‑ECD may be effective (limit of quantification 5 ng/l) Treatment performance Should be removed by adsorption onto activated carbon, and any dicofol adsorbed onto particulate matter would likely be removed during coagulation Health‑based value derivation • allocation to water 20% of the upper bound of the ADI• weight 60 kg adult• consumption 2 litres/day Acute health‑based value derivation • allocation to water 100% of the ARfD • weight 60 kg adult• consumption 2 litres/day Additional comments The default allocation factor of 20% has been used to account for the fact that the available food exposure data, which suggest that exposure via this route is low, do not generally include information from developing countries, where exposure via this route may be higher As a general principle, the concentration of pesticides in water, including dicofol, should be kept as low as possible and concentrations should not be allowed to increase up to the health‑based value. Further guidance on interpreting the health‑based value and deciding when to monitor can be found in section 8.5.3 Assessment date 2016 Principal references WHO (2012). Pesticide residues in food – 2011 evaluations WHO (2016). Dicofol in drinking-water * Although dicofol does not fulfil one of the three criteria for evaluation in the Guidelines, a background document has been prepared, and a health‑based value has been established, in response to a request from Member States for guidance. ** When a formal guideline value is not established, a “health‑based value” may be determined in order to provide guidance to Member States when there is reason for local concern. Establishing a formal guideline value for such substances may encourage Member States to incorporate a value into their national standards when this may be unnecessary. *** For more information on acute health‑based values, see section 8.7.5. The primary effects of dicofol after short- or long-term exposure of experimental animals were body weight reduction associated with decreased feed intake, and increased liver weight accompanied by changes in liver enzyme activities. Dicofol caused liver tumours in male mice at doses associated with significant enzyme induction and liver hypertrophy. However, on the basis of the absence of genotoxicity in an adequate range of in vitro genotoxicity and in vivo chromosomal aberration tests, the absence of carcinogenic effects in rats and the expectation that the adenomas present in mice will exhibit a threshold, dicofol is unlikely to pose a carcinogenic risk to humans at anticipated dietary exposure levels. There is a margin of 20 000 between the upper bound of the ADI and the LOAEL for liver adenomas in the male mouse. Di(2-ethylhexyl)adipate Di(2-ethylhexyl)adipate (DEHA) is used mainly as a plasticizer for synthetic resins such as polyvinyl chloride (PVC). Reports of the presence of DEHA in surface water and drinking-water are scarce, but DEHA has occasionally been identified in drinking-water at levels of a few micrograms per litre. As a consequence of its use in PVC films, food is the most important source of human exposure (up to 20 mg/day). Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal reference WHO (2003) Di(2-ethylhexyl)adipate in drinking-water DEHA is of low short-term toxicity; however, dietary levels above 6000 mg/kg of feed induce peroxisomal proliferation in the liver of rodents. This effect is often associated with the development of liver tumours. DEHA induced liver carcinomas in female mice at very high doses, but not in male mice or rats. It is not genotoxic. IARC has placed DEHA in Group 3 (not classifiable as to its carcinogenicity to humans). A health-based value of 80 μg/l can be calculated for DEHA on the basis of a TDI of 280 μg/kg body weight, based on fetotoxicity in rats, and allocating 1% of the TDI to drinking-water. However, because DEHA occurs at concentrations well below those of health concern, it is not considered necessary to derive a formal guideline value. Di(2-ethylhexyl)phthalate Di(2-ethylhexyl)phthalate (DEHP) is used primarily as a plasticizer. Exposure among individuals may vary considerably because of the broad nature of products into which DEHP is incorporated. In general, food will be the main exposure route. Guideline value 0.008 mg/l (8 μg/l) Occurrence Found in surface water, groundwater and drinking‑water in concentrations of a few micrograms per litre; in polluted surface water and groundwater, concentrations of hundreds of micrograms per litre have been reported TDI 25 μg/kg body weight, based on a NOAEL of 2.5 mg/kg body weight per day for peroxisomal proliferation in the liver in rats, using an uncertainty factor of 100 for interspecies and intraspecies variation Limit of detection 0.1 μg/l by GC‑MS Treatment performance No information found on removal from water Guideline value derivation • allocation to water 1% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments The reliability of some data on environmental water samples is questionable because of secondary contamination during sampling and working‑up procedures. Concentrations that exceed the solubility more than 10‑fold have been reported. Assessment date 1993 Principal reference WHO (2003) Di(2-ethylhexyl)phthalate in drinking-water In rats, DEHP is readily absorbed from the gastrointestinal tract. In primates (including humans), absorption after ingestion is lower. Species differences are also observed in the metabolic profile. Most species excrete primarily the conjugated mono-ester in urine. Rats, however, predominantly excrete terminal oxidation products. DEHP is widely distributed in the body, with highest levels in liver and adipose tissue, without showing significant accumulation. The acute oral toxicity is low. The most striking effect in short-term toxicity studies is the proliferation of hepatic peroxisomes, indicated by increased peroxisomal enzyme activity and histopathological changes. The available information suggests that primates, including humans, are far less sensitive to this effect than rodents. In long-term oral carcinogenicity studies, hepatocellular carcinomas were found in rats and mice. IARC has concluded that DEHP is possibly carcinogenic to humans (Group 2B). In 1988, JECFA evaluated DEHP and recommended that human exposure to this compound in food be reduced to the lowest level attainable. JECFA considered that this might be achieved by using alternative plasticizers or alternatives to plastic material containing DEHP. In a variety of in vitro and in vivo studies, DEHP and its metabolites have shown no evidence of genotoxicity, with the exception of induction of aneuploidy and cell transformation. Dimethoate Dimethoate (CAS No. 60-51-5) is an organophosphorus insecticide used to control a broad range of insects in agriculture, as well as the housefly. It has a half-life of 18 hours to 8 weeks and is not expected to persist in water, although it is relatively stable at pH 2–7. A total daily intake from food of 0.001 μg/kg body weight has been estimated. Guideline value 0.006 mg/l (6 μg/l) Occurrence Detected at trace levels in a private well in Canada, but not detected in a Canadian survey of surface water or drinking‑water supplies ADI 0–0.002 mg/kg body weight based on an apparent NOAEL of 1.2 mg/kg body weight per day for reproductive performance in a study of reproductive toxicity in rats, applying an uncertainty factor of 500 (100 for interspecies and intraspecies variation, 5 to take into consideration concern regarding whether the NOAEL could be a LOAEL) Assessment date 2003 Principal references FAO/WHO (1997) Pesticide residues in food—1996 evaluations WHO (2004) Dimethoate in drinking-water In studies with human volunteers, dimethoate has been shown to be a cholinesterase inhibitor and a skin irritant. Dimethoate is not carcinogenic to rodents. JMPR concluded that although in vitro studies indicate that dimethoate has mutagenic potential, this potential does not appear to be expressed in vivo. In a multi-generation study of reproductive toxicity in rats, the NOAEL appeared to be 1.2 mg/kg body weight per day, but there was some indication that reproductive performance may have been affected at lower doses. No data were available to assess whether the effects on reproductive performance were secondary to inhibition of cholinesterase. JMPR concluded that it was not appropriate to base the ADI on the results of the studies of volunteers, as the crucial end-point (reproductive performance) has not been assessed in humans. It was suggested that there may be a need to re-evaluate the toxicity of dimethoate after the periodic review of the residue and analytical aspects of dimethoate has been completed if it is determined that omethoate is a major residue. 1,4-Dioxane 1,4-Dioxane is used as a stabilizer in chlorinated solvents and as a solvent for resins, oils and waxes, for agricultural and biochemical intermediates and for adhesives, sealants, cosmetics, pharmaceuticals, rubber chemicals and surface coatings. Guideline value 0.05 mg/l (50 μg/l) Occurrence Has been measured in surface water at concentrations up to 40 μg/l and in groundwater at concentrations up to 80 μg/l TDI 16 μg/kg body weight, based on a NOAEL of 16 mg/kg body weight per day for hepatocellular tumours observed in a long‑term drinking‑water study in rats, using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for non‑genotoxic carcinogenicity) Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Basis of guideline value Linear multistage model applied to data for hepatic tumours from derivation based on drinking‑water studies in rats carcinogenicity Limit of detection 0.1–50 μg/l by GC‑MS Treatment performance Not removed using conventional water treatment processes; effectively removed by biological activated carbon treatment Additional comments Similar guideline values were derived using the TDI approach (assuming 1,4‑dioxane is not genotoxic in humans at low doses) and linear multistage modelling (because the compound clearly induces multiple tumours in various organs). Assessment date 2004 Principal reference WHO (2005) 1,4-Dioxane in drinking-water 1,4-Dioxane caused hepatic and nasal cavity tumours in rodents in most longterm oral studies conducted. Tumours in peritoneum, skin and mammary gland were also observed in rats given a high dose. Lung tumours were specifically detected after intraperitoneal injection. Although cohort studies of workers did not reveal any elevation in the incidence of death by cancer, a significant increase in the incidence of liver cancer was found in a comparative mortality study. However, the evidence is inadequate for human carcinogenicity assessment because of small samples or lack of exposure data. A possibly weak genotoxic potential of 1,4-dioxane has been suggested. IARC has classified 1,4-dioxane in Group 2B (possibly carcinogenic to humans). Diquat Diquat (CAS No. 85-00-7; CAS No. 2764-72-9 for diquat ion) is a non-selective, quick-acting contact herbicide that is used for weed control on several food crops, for residential weed control on lawns and ornamental plants, and as an aquatic herbicide for the control of free-floating and submerged aquatic weeds in ponds and irrigation ditches. It is highly soluble in water but is strongly adsorbed to soil and is resistant to degradation in the sorbed state. Photochemical degradation in soil and water occurs in the presence of sunlight. Exposure from food is likely to be low. Reason for not establishing Occurs in drinking‑water or drinking‑water sources at concentrations a guideline value well below those of health concern Health‑based value* 0.03 mg/l Acute health‑based value** 20 mg/l Occurrence Rarely detected in surface water ADI 0–0.006 mg/kg bw (expressed as the diquat ion), based on a NOAEL of 0.58 mg/kg bw per day for cataracts in a 2‑year toxicity and carcinogenicity study in rats and application of a safety factor of 100 ARfD 0.8 mg/kg bw (expressed as the diquat ion), based on a NOAEL of 75 mg/kg bw for clinical signs and decreased body weight gain in the 1st week and decreased feed consumption in a neurotoxicity study in rats and application of a safety factor of 100 Limit of detection 1 μg/l using HPLC with UV absorbance detection after solid sorbent cartridge extraction; practical quantification limit of 1 μg/l using LC‑MS analysis after solid‑phase extraction Treatment performance Conventional treatment, including coagulation and filtration, not effective; activated carbon may be effective Health‑based value derivation • allocation to water 20% of upper bound of unrounded ADI (0.0058 mg/kg bw) • weight 60 kg adult• consumption 2 litres/day Acute health‑based value derivation • allocation to water 100% of unrounded ARfD (0.75 mg/kg bw) • weight 60 kg adult• consumption 2 litres/day Additional comments The default allocation factor of 20% has been used to account for the fact that the available food exposure data, , which suggest that exposure via this route is low, do not generally include information from developing countries, where exposure via this route may be higher As a general principle, the concentration of pesticides in water, including diquat, should be kept as low as possible and concentrations should not be allowed to increase up to the health‑based value. Further guidance on interpreting the health‑based value and deciding when to monitor can be found in section 8.5.3 Assessment date 2016 Principal references WHO (2014). Pesticide residues in food – 2013 evaluations WHO (2016). Diquat in drinking-water * When a formal guideline value is not established, a “health‑based value” may be determined in order to provide guidance to Member States when there is reason for local concern. Establishing a formal guideline value for such substances may encourage Member States to incorporate a value into their national standards when this may be unnecessary. ** For more information on acute health‑based values, see section 8.7.5. The eye is the main target organ following short-term repeated exposure in rats and dogs. Effects on kidney, liver and haematological parameters are also observed. Diquat is not carcinogenic in mice or rats. In tests for genotoxicity, diquat gave equivocal or positive responses in the mammalian cell cytogenetic assay, but was negative in the in vivo mouse micronucleus assay and dominant lethal assay. No reproductive effects were observed in a two-generation reproductive toxicity study in rats, and diquat was not teratogenic in rats or rabbits. Edetic acid Human exposure to edetic acid, also known as ethylenediaminetetraacetic acid or EDTA, arises directly from its use in food additives, medicines and personal care and hygiene products. Exposure to EDTA from drinking-water will be mostly very low in comparison with that from other sources. Once EDTA is present in the aquatic environment, its speciation will depend on the water quality and the presence of trace metals with which it will combine. The removal of EDTA from communal wastewater by biodegradation in sewage purification plants is very limited. Guideline value EDTA (as the free acid): 0.6 mg/l (600 μg/l) Occurrence Present in surface waters generally at concentrations below 70 μg/l, although higher concentrations (900 μg/l) have been measured; detected in drinking‑water prepared from surface waters at concentrations of 10–30 μg/l ADI 0–1.9 mg/kg body weight as the free acid (ADI of 0–2.5 mg/kg body weight proposed by JECFA for calcium disodium edetate as a food additive) Limit of detection 1 μg/l by potentiometric stripping analyis Treatment performance 0.01 mg/l using GAC plus ozonation Guideline value derivation • allocation to water • weight • consumption 1% of upper limit of ADI 60 kg adult 2 litres/day Additional comments Concern has been expressed over the ability of EDTA to complex and therefore reduce the availability of zinc. However, this is of significance only at elevated doses substantially in excess of those encountered in the environment. Assessment date 1998 Principal reference WHO (2003) Edetic acid (EDTA) in drinking-water Calcium disodium edetate is poorly absorbed from the gut. The long-term toxicity of EDTA is complicated by its ability to chelate essential and toxic metals. Those toxicological studies that are available indicate that the apparent toxicological effects of EDTA have in fact been due to zinc deficiency as a consequence of complexation. EDTA does not appear to be teratogenic or carcinogenic in experimental animals. The vast clinical experience of the use of EDTA in the treatment of metal poisoning has demonstrated its safety in humans. Endosulfan Endosulfan (CAS No. 115-29-7) is an insecticide used in countries throughout the world to control pests on fruit, vegetables and tea and on non-food crops such as tobacco and cotton. In addition to its agricultural use, it is used in the control of the tsetse fly, as a wood preservative and for the control of home garden pests. Endosulfan contamination does not appear to be widespread in the aquatic environment, but the chemical has been found in agricultural runoff and rivers in industrialized areas where it is manufactured or formulated, as well as in surface water and groundwater samples collected from hazardous waste sites in the USA. Surface water samples in the USA generally contain less than 1 μg/l. The main source of exposure of the general population is food, but residues have generally been found to be well below the FAO/ WHO maximum residue limits. Another important route of exposure to endosulfan for the general population is the use of tobacco products. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal references FAO/WHO (1999) Pesticide residues in food—1998 evaluations WHO (2004) Endosulfan in drinking-water JMPR concluded that endosulfan is not genotoxic, and no carcinogenic effects were noted in long-term studies using mice and rats. The kidney is the target organ for toxicity. Several recent studies have shown that endosulfan, alone or in combination with other pesticides, may bind to estrogen receptors and perturb the endocrine system. A health-based value of 20 μg/l can be calculated for endosulfan on the basis of an ADI of 0–0.006 mg/kg body weight, based on results from a 2-year dietary study of toxicity in rats and supported by a 78-week study in mice, a 1-year study in dogs and a developmental toxicity study in rats. However, because endosulfan occurs at concentrations well below those of health concern, it is not considered necessary to derive a formal guideline value. Endrin Endrin (CAS No. 72-20-8) is a broad-spectrum foliar insecticide that acts against a wide range of agricultural pests. It is also used as a rodenticide. There is now very little use of endrin. Small amounts of endrin are present in some foods, but the total intake from food has decreased significantly. Guideline value 0.0006 mg/l (0.6 μg/l) Occurrence Traces of endrin found in the drinking‑water supplies of several countries PTDI 0.2 μg/kg body weight, based on a NOAEL of 0.025 mg/kg body weight per day in a 2‑year study in dogs and applying an uncertainty factor of 100 for interspecies and intraspecies variation Limit of detection 0.002 μg/l by GC with ECD Treatment performance 0.2 μg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of PTDI • weight 60 kg adult• consumption 2 litres/day Additional comments Endrin is listed under the Stockholm Convention on Persistent Organic Pollutants. Hence, monitoring may occur in addition to that required by drinking‑water guidelines. Assessment date 2003 Principal references FAO/WHO (1995) Pesticide residues in food—1994 evaluations IPCS (1992) Endrin WHO (2004) Endrin in drinking-water Toxicological data are insufficient to indicate whether endrin is a carcinogenic hazard to humans. The primary site of action of endrin is the central nervous system. Epichlorohydrin Epichlorohydrin is used for the manufacture of glycerol, unmodified epoxy resins and water treatment coagulant polymers and some ion exchange resins. No quantitative data are available on its occurrence in food or drinking-water. Epichlorohydrin is slowly hydrolysed in aqueous media. Provisional guideline value 0.0004 mg/l (0.4 μg/l) The guideline value is considered to be provisional because of the uncertainties surrounding the toxicity of epichlorohydrin and the use of a large uncertainty factor in deriving the guideline value. Occurrence No quantitative data available TDI 0.14 μg/kg body weight, on the basis of a LOAEL of 2 mg/kg body weight per day for forestomach hyperplasia observed in a 2‑year gavage study in rats, adjusting for daily dosing and using an uncertainty factor of 10 000 to take into consideration interspecies and intraspecies variation (100), the use of a LOAEL instead of a NOAEL (10) and carcinogenicity (10) Limit of detection 0.01 μg/l by GC with ECD; 0.1 and 0.5 μg/l by GC‑MS; 10 μg/l by GC with FID Treatment performance Conventional treatment processes do not remove epichlorohydrin. Epichlorohydrin concentrations in drinking‑water are controlled by limiting either the epichlorohydrin content of polyamine flocculants or the dose used, or both. Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments Although epichlorohydrin is a genotoxic carcinogen, the use of the linearized multistage model for estimating cancer risk was considered inappropriate because tumours are seen only at the site of administration, where epichlorohydrin is highly irritating. Assessment date 2003 Principal reference WHO (2004) Epichlorohydrin in drinking-water Epichlorohydrin is rapidly and extensively absorbed following oral, inhalation or dermal exposure. It binds easily to cellular components. Major toxic effects are local irritation and damage to the central nervous system. It induces squamous cell carcinomas in the nasal cavity by inhalation and forestomach tumours by the oral route. It has been shown to be genotoxic in vitro and in vivo. IARC has placed epichlorohydrin in Group 2A (probably carcinogenic to humans). Ethylbenzene The primary sources of ethylbenzene in the environment are the petroleum industry and the use of petroleum products. Because of its physicochemical properties, more than 96% of ethylbenzene in the environment can be expected to be present in air. Values of up to 26 μg/m3 in air have been reported. Ethylbenzene is found in trace amounts in surface water, groundwater, drinking-water and food. Guideline value 0.3 mg/l (300 μg/l) Occurrence Concentrations in drinking‑water generally below 1 μg/l; levels up to 300 μg/l have been reported in groundwater contaminated by point emissions TDI 97.1 μg/kg body weight, based on a NOAEL of 136 mg/kg body weight per day for hepatotoxicity and nephrotoxicity observed in a limited 6‑month study in rats, adjusting for daily dosing and using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for the limited database and short duration of the study) Limit of detection 0.002–0.005 μg/l by GC with photoionization detector; 0.03–0.06 μg/l by GC‑MS Treatment performance 0.001 mg/l should be achievable using air stripping Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments The guideline value exceeds the lowest reported odour threshold for ethylbenzene in drinking‑water (0.002 mg/l). Assessment date 1993 Principal reference WHO (2003) Ethylbenzene in drinking-water Ethylbenzene is readily absorbed by the oral, inhalation or dermal route. In humans, storage in fat has been reported. Ethylbenzene is almost completely converted to soluble metabolites, which are excreted rapidly in urine. The acute oral toxicity is low. No definite conclusions can be drawn from limited teratogenicity data. No data on reproduction, long-term toxicity or carcinogenicity are available. Ethylbenzene has shown no evidence of genotoxicity in in vitro or in vivo systems. Fenitrothion Fenitrothion (CAS No. 122-14-5) is mainly used in agriculture for controlling insects on rice, cereals, fruits, vegetables, stored grains and cotton and in forest areas. It is also used for the control of flies, mosquitoes and cockroaches in public health programmes and indoor use. Fenitrothion is stable in water only in the absence of sunlight or microbial contamination. In soil, biodegradation is the primary route of degradation, although photolysis may also play a role. Fenitrothion residues detected in water were low (maximum 1.30 μg/l) during the spruce budworm spray programme. Following the spraying of forests to control spruce budworm, water samples did not contain detectable amounts of fenitrothion; post-spray samples contained less than 0.01 μg/l. Levels of fenitrothion residues in fruits, vegetables and cereal grains decline rapidly after treatment, with a half-life of 1–2 days. Intake of fenitrothion appears to be primarily (95%) from food. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal references FAO/WHO (2001) Pesticide residues in food—2000 evaluations WHO (2004) Fenitrothion in drinking-water On the basis of testing in an adequate range of studies in vitro and in vivo, JMPR concluded that fenitrothion is unlikely to be genotoxic. It also concluded that fenitrothion is unlikely to pose a carcinogenic risk to humans. In long-term studies of toxicity, inhibition of cholinesterase activity was the main toxicological finding in all species. A health-based value of 8 μg/l can be calculated for fenitrothion on the basis of an ADI of 0–0.005 mg/kg body weight, based on a NOAEL of 0.5 mg/kg body weight per day for inhibition of brain and erythrocyte cholinesterase activity in a 2-year study of toxicity in rats and supported by a NOAEL of 0.57 mg/kg body weight per day for inhibition of brain and erythrocyte cholinesterase activity in a 3-month study of ocular toxicity in rats and a NOAEL of 0.65 mg/kg body weight per day for reduced food consumption and body weight gain in a study of reproductive toxicity in rats, and allocating 5% of the upper limit of the ADI to drinking-water. However, because fenitrothion occurs at concentrations well below those of health concern, it is not considered necessary to derive a formal guideline value. Fenoprop The half-lives for degradation of chlorophenoxy herbicides, including fenoprop (CAS No. 93-72-1), also known as 2,4,5-trichlorophenoxy propionic acid or 2,4,5-TP, in the environment are in the order of several days. Chlorophenoxy herbicides are not often found in food. Guideline value 0.009 mg/l (9 μg/l) Occurrence Chlorophenoxy herbicides not frequently found in drinking‑water; when detected, concentrations usually no greater than a few micrograms per litre TDI 3 μg/kg body weight, based on a NOAEL of 0.9 mg/kg body weight for adverse effects on the liver in a study in which dogs were administered fenoprop in the diet for 2 years, with an uncertainty factor of 300 (100 for interspecies and intraspecies variation and 3 for limitations of the database) Limit of detection 0.2 μg/l by either packed or capillary column GC with ECD Treatment performance 0.001 mg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Assessment date 1993 Principal reference WHO (2003) Chlorophenoxy herbicides (excluding 2,4-D and MCPA) in drinking-water Chlorophenoxy herbicides, as a group, have been classified in Group 2B (possibly carcinogenic to humans) by IARC. However, the available data from studies in exposed populations and experimental animals do not permit assessment of the carcinogenic potential to humans of any specific chlorophenoxy herbicide. Therefore, drinking-water guidelines for these compounds are based on a threshold approach for other toxic effects. Effects observed in long-term studies with dogs given fenoprop in the diet include mild degeneration and necrosis of hepatocytes and fibroblastic proliferation in one study and severe liver pathology in another study. In rats, increased kidney weight was observed in two long-term dietary studies. Fluoride1 Fluorine is a common element that is widely distributed in Earth’s crust and exists in the form of fluorides in a number of minerals, such as fluorspar, cryolite and fluorapatite. Traces of fluorides are present in many waters, with higher concentrations often associated with groundwaters. In some areas rich in fluoride-containing minerals, well water may contain up to about 10 mg of fluoride per litre, although much higher concentrations can be found. High fluoride concentrations can be found in many parts of the world, particularly in parts of India, China, Central Africa and South America, but high concentrations can be encountered locally in most parts of the world. Virtually all foodstuffs contain at least traces of fluorine. All vegetation contains some fluoride, which is absorbed from soil and water. Tea in particular can contain high fluoride concentrations, and levels in dry tea are on average 100 mg/kg. Fluoride is widely used in dental preparations to combat dental caries, particularly in areas of high sugar intake. These can be in the form of tablets, mouthwashes, toothpaste, varnishes or gels for local application. In some countries, fluoride may also be added to table salt or drinking-water in order to provide protection against dental caries. The amounts added to drinking-water are such that final concentrations are usually between 0.5 and 1 mg/l. The fluoride in final water is always present as fluoride ions, whether from natural sources or from artificial fluoridation. 1 As fluoride is one of the chemicals of greatest health concern in some natural waters, its chemical fact sheet has been expanded. Total daily fluoride exposure can vary markedly from one region to another. This will depend on the concentration of fluoride in drinking-water and the amount drunk, levels in foodstuffs and the use of fluoridated dental preparations. In addition, fluoride exposure in some areas is considerably higher as a consequence of a range of practices, including the consumption of brick tea and the cooking and drying of food with high-fluoride coal. Guideline value 1.5 mg/l (1500 μg/l) Occurrence In groundwater, concentrations vary with the type of rock through which the water flows but do not usually exceed 10 mg/l; highest natural level reported is 2800 mg/l Basis of guideline value Epidemiological evidence that concentrations above this value carry an derivation increasing risk of dental fluorosis and that progressively higher concentrations lead to increasing risks of skeletal fluorosis. The value is higher than that recommended for artificial fluoridation of water supplies, which is usually 0.5–1.0 mg/l. Limit of detection 0.01 mg/l by ion chromatography; 0.1 mg/l by ion‑selective electrodes or the sulfo phenyl azo dihydroxy naphthalene disulfonic acid colorimetric method Treatment performance 1 mg/l should be achievable using activated alumina (not a “conventional” treatment process, but relatively simple to install filters) Additional comments A management guidance document on fluoride is available. In setting national standards for fluoride or in evaluating the possible health consequences of exposure to fluoride, it is essential to consider the intake of water by the population of interest and the intake of fluoride from other sources (e.g. from food, air and dental preparations). Where the intakes from other sources are likely to approach, or be greater than, 6 mg/day, it would be appropriate to consider setting standards at concentrations lower than the guideline value. In areas with high natural fluoride levels in drinking‑water, the guideline value may be difficult to achieve, in some circumstances, with the treatment technology available. Assessment date 2003 Principal references Fawell et al. (2006) Fluoride in drinking-water IPCS (2002) Fluorides USNRC (2006) Fluoride in drinking water WHO (2004) Fluoride in drinking-water After oral uptake, water-soluble fluorides are rapidly and almost completely absorbed from the gastrointestinal tract, although this may be reduced by complex formation with aluminium, phosphorus, magnesium or calcium. There is no difference in absorption between natural or added fluoride in drinking-water. Fluoride in inhaled particles—from high-fluoride coal, for example—is also absorbed, depending on the particle size and solubility of the fluoride compounds present. Absorbed fluoride is rapidly distributed throughout the body, where it is incorporated into teeth and bones, with virtually no storage in soft tissues. Fluoride in teeth and bone can be mobilized after external exposure has ceased or been reduced. Fluoride is excreted via urine, faeces and sweat. Fluoride may be an essential element for humans; however, essentiality has not been demonstrated unequivocally. Meanwhile, there is evidence of fluoride being a beneficial element with regard to the prevention of dental caries. To produce signs of acute fluoride intoxication, minimum oral doses of about 1 mg of fluoride per kilogram of body weight were required. Many epidemiological studies of possible adverse effects of the long-term ingestion of fluoride via drinking-water have been carried out. These studies clearly establish that high fluoride intakes primarily produce effects on skeletal tissues (bones and teeth). Low concentrations provide protection against dental caries, both in children and in adults. The protective effects of fluoride increase with concentration up to about 2 mg of fluoride per litre of drinking-water; the minimum concentration of fluoride in drinking-water required to produce it is approximately 0.5 mg/l. However, fluoride can also have an adverse effect on tooth enamel and may give rise to mild dental fluorosis (prevalence: 12–33%) at drinking-water concentrations between 0.9 and 1.2 mg/l, depending on drinking-water intake and exposure to fluoride from other sources. Mild dental fluorosis may not be detectable except by specialist examination. The risk of dental fluorosis will depend on the total intake of fluoride from all sources and not just the concentration in drinking-water. Elevated fluoride intakes can have more serious effects on skeletal tissues. Skeletal fluorosis (with adverse changes in bone structure) may be observed when drinking-water contains 3–6 mg of fluoride per litre, particularly with high water consumption. Crippling skeletal fluorosis usually develops only where drinking-water contains over 10 mg of fluoride per litre. IPCS concluded that there is clear evidence from India and China that skeletal fluorosis and an increased risk of bone fractures occur at a total intake of 14 mg of fluoride per day. This conclusion was supported by a review by the United States National Research Council in 2006. The relationship between exposure and response for adverse effects in bone is frequently difficult to ascertain because of inadequacies in most of the epidemiological studies. IPCS concluded from estimates based on studies from China and India that for a total intake of 14 mg/day, there is a clear excess risk of skeletal adverse effects; and there is suggestive evidence of an increased risk of effects on the skeleton at total fluoride intakes above about 6 mg/day. Several epidemiological studies are available on the possible association between fluoride in drinking-water and cancer. IPCS evaluated these studies and concluded that, overall, the evidence of carcinogenicity in laboratory animals is inconclusive and that the available evidence does not support the hypothesis that fluoride causes cancer in humans; however, the data on bone cancer are limited. The results of several epidemiological studies on the possible adverse effects of fluoride in drinking-water on pregnancy outcome indicate that there is no relationship between the rates of Down syndrome or congenital malformation and the consumption of fluoridated drinking-water. There is no evidence to suggest that the guideline value of 1.5 mg/l set in 1984 and reaffirmed in 1993 needs to be revised. Concentrations above this value carry an increasing risk of dental fluorosis, and much higher concentrations lead to skeletal fluorosis. The value is higher than that recommended for artificial fluoridation of water supplies, which is usually 0.5–1.0 mg/l. In setting national standards or local guidelines for fluoride or in evaluating the possible health consequences of exposure to fluoride, it is essential to consider the average daily intake of water by the population of interest and the intake of fluoride from other sources (e.g. from food and air). Where the intakes are likely to approach, or be greater than, 6 mg/day, it would be appropriate to consider setting a standard or local guideline at a concentration lower than 1.5 mg/l. Practical considerations Fluoride is usually determined by means of an ion-selective electrode, which makes it possible to measure the total amount of free and complex-bound fluoride dissolved in water. The method can detect fluoride concentrations in water well below the guideline value. However, appropriate sample preparation is a critical step in the accurate quantification of fluoride, especially where only the free fluoride ion is measured. A range of treatment technologies are available for both large and small supplies. Different methods for small supplies are favoured in different countries; these are based on bone charcoal, contact precipitation, activated alumina and clay. However, in some areas with high natural fluoride levels in drinking-water, the guideline value may be difficult to achieve in some circumstances with the treatment technology available. Large supplies tend to rely on activated alumina or advanced treatment processes such as reverse osmosis. Formaldehyde Formaldehyde occurs in industrial effluents and is emitted into air from plastic materials and resin glues. Formaldehyde in drinking-water results primarily from the oxidation of natural organic matter during ozonation and chlorination. Concentrations of up to 30 μg/l have been found in ozonated drinking-water. Formaldehyde can also be found in drinking-water as a result of release from polyacetal plastic fittings. Formaldehyde’s physicochemical properties suggest that it is unlikely to volatilize from water, so exposure by inhalation during showering is expected to be low. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2004 Principal references IPCS (2002) Formaldehyde WHO (2005) Formaldehyde in drinking-water Rats and mice exposed to formaldehyde by inhalation exhibited an increased incidence of carcinomas of the nasal cavity at doses that caused irritation of the nasal epithelium. Ingestion of formaldehyde in drinking-water for 2 years caused stomach irritation in rats. Papillomas of the stomach associated with severe tissue irritation were observed in one study. IARC has classified formaldehyde in Group 1 (carcinogenic to humans). The weight of evidence indicates that formaldehyde is not carcinogenic by the oral route. Owing to formaldehyde’s high reactivity, effects in the tissue of first contact following ingestion are more likely to be related to the concentration of the formaldehyde consumed than to its total intake. A tolerable concentration of 2.6 mg/l for ingested formaldehyde has been established based on a NOEL of 260 mg/l for histopathological effects in the oral and gastric mucosa of rats administered formaldehyde in their drinking-water for 2 years, using an uncertainty factor of 100 (for interspecies and intraspecies variation). In view of the significant difference between the expected concentrations of formaldehyde in drinking-water and the tolerable concentration, it is not considered necessary to set a formal guideline value for formaldehyde. Glyphosate and AMPA Glyphosate (CAS No. 1071-83-6) is a broad-spectrum herbicide used in both agriculture and forestry and for aquatic weed control. Microbial biodegradation of glyphosate occurs in soil, aquatic sediment and water, the major metabolite being aminomethylphosphonic acid (AMPA) (CAS No. 1066-51-9). Glyphosate is chemically stable in water and is not subject to photochemical degradation. The low mobility of glyphosate in soil indicates minimal potential for the contamination of groundwater. Glyphosate can, however, enter surface and subsurface waters after direct use near aquatic environments or by runoff or leaching from terrestrial applications. Reason for not establishing Occur in drinking‑water at concentrations well below those of health guideline values concern Assessment date 2003 Principal references FAO/WHO (1998) Pesticide residues in food—1997 evaluations IPCS (1994) Glyphosate WHO (2005) Glyphosate and AMPA in drinking-water Glyphosate and AMPA have similar toxicological profiles, and both are considered to exhibit low toxicity. A health-based value of 0.9 mg/l can be derived based on the group ADI for AMPA alone or in combination with glyphosate of 0–0.3 mg/ kg body weight, based upon a NOAEL of 32 mg/kg body weight per day, the highest dose tested, identified in a 26-month study of toxicity in rats fed technical-grade glyphosate and using an uncertainty factor of 100 (for interspecies and intraspecies variation). Because of their low toxicity, the health-based value derived for AMPA alone or in combination with glyphosate is orders of magnitude higher than concentrations of glyphosate or AMPA normally found in drinking-water. Under usual conditions, therefore, the presence of glyphosate and AMPA in drinking-water does not represent a hazard to human health. For this reason, the establishment of a formal guideline value for glyphosate and AMPA is not deemed necessary. Halogenated acetonitriles (dichloroacetonitrile, dibromoacetonitrile, bromochloroacetonitrile, trichloroacetonitrile) Halogenated acetonitriles are produced during water chlorination or chloramination from naturally occurring substances, including algae, fulvic acid and proteinaceous material. In general, increasing temperature or decreasing pH is associated with increasing concentrations of halogenated acetonitriles. Ambient bromide levels appear to influence, to some degree, the speciation of halogenated acetonitrile compounds. Dichloroacetonitrile is by far the most predominant halogenated acetonitrile species detected in drinking-water. Provisional guideline Dichloroacetonitrile: 0.02 mg/l (20 μg/l) value The guideline value for dichloroacetonitrile is provisional owing to limitations of the toxicological database. Guideline value Dibromoacetonitrile: 0.07 mg/l (70 μg/l) Occurrence Concentrations of individual halogenated acetonitriles can exceed 0.01 mg/l, although levels of 0.002 mg/l or less are more usual TDIs Dichloroacetonitrile: 2.7 μg/kg body weight based on a LOAEL of 8 mg/ kg body weight per day for increased relative liver weight in male and female rats in a 90‑day study, using an uncertainty factor of 3000 (taking into consideration intraspecies and interspecies variation, the short duration of the study, the use of a minimal LOAEL and database deficiencies) Dibromoacetonitrile: 11 μg/kg body weight, based on a NOAEL of 11.3 mg/kg body weight per day for decreased body weight in male rats in a 90‑day drinking‑water study and an uncertainty factor of 1000 (accounting for interspecies and intraspecies variation, subchronic to chronic extrapolation and database insufficiencies) Limit of detection 0.03 μg/l by GC with ECD Treatment performance Reduction of organic precursors will reduce the formation of halogenated acetonitriles. Guideline value derivation • allocation to water • weight • consumption 20% of TDI 60 kg adult 2 litres/day Assessment date 2003 Principal references IPCS (2000) Disinfectants and disinfectant by-products WHO (2004) Halogenated acetonitriles in drinking-water Reason for not establishing Available data inadequate to permit derivation of health‑based guideline guideline values values for bromochloroacetonitrile and trichloroacetonitrile Assessment date 2003 Principal references IPCS (2000) Disinfectants and disinfectant by-products WHO (2004) Halogenated acetonitriles in drinking-water IARC has concluded that dichloroacetonitrile, dibromoacetonitrile, bromochloroacetonitrile and trichloroacetonitrile are not classifiable as to their carcinogenicity in humans. Dichloroacetonitrile and bromochloroacetonitrile have been shown to be mutagenic in bacterial assays, whereas results for dibromoacetonitrile and trichloroacetonitrile were negative. All four of these halogenated acetonitriles induced sister chromatid exchange and DNA strand breaks and adducts in mammalian cells in vitro but were negative in the mouse micronucleus test. The majority of reproductive and developmental toxicity studies of the halogenated acetonitriles were conducted using tricaprylin as a vehicle for gavage administration of the compound under study. As tricaprylin was subsequently demonstrated to be a developmental toxicant that potentiated the effects of trichloroacetonitrile and, presumably, other halogenated acetonitriles, results reported for developmental studies using tricaprylin as the gavage vehicle are likely to overestimate the developmental toxicity of these halogenated acetonitriles. Dichloroacetonitrile Dichloroacetonitrile induced decreases in body weight and increases in relative liver weight in short-term studies. Although developmental toxicity has been demonstrated, the studies used tricaprylin as the vehicle for gavage administration. Dibromoacetonitrile Dibromoacetonitrile is currently under analysis for chronic toxicity in mice and rats. None of the available reproductive or developmental studies were adequate to use in the quantitative dose–response assessment. The data gap may be particularly relevant because cyanide, a metabolite of dibromoacetonitrile, induces male reproductive system toxicity and because of uncertainty regarding the significance of the testes effects observed in a 14-day NTP rat study. Bromochloroacetonitrile Available data are insufficient to serve as a basis for derivation of a guideline value for bromochloroacetonitrile. Trichloroacetonitrile Available data are also insufficient to serve as a basis for derivation of a guideline value for trichloroacetonitrile. The previous provisional guideline value of 1 μg/l was based on a developmental toxicity study in which trichloroacetonitrile was administered by gavage in tricaprylin vehicle, and a re-evaluation judged this study to be unreliable in light of the finding in a more recent study that tricaprylin potentiates the developmental and teratogenic effects of halogenated acetonitriles and alters the spectrum of malformations in the fetuses of treated dams. Hardness Hardness in water is caused by a variety of dissolved polyvalent metallic ions, predominantly calcium and magnesium cations. It is usually expressed as milligrams of calcium carbonate per litre. Hardness is the traditional measure of the capacity of water to react with soap, hard water requiring considerably more soap to produce a lather. Reason for not establishing Not of health concern at levels found in drinking‑water a guideline value Additional comments May affect acceptability of drinking‑water Assessment date 1993, revised in 2011 Principal reference WHO (2011) Hardness in drinking-water Natural and treated waters have a wide range of mineral content, from very low levels in rainwater and naturally soft and softened water to higher levels in naturally hard waters. Bottled and packaged waters can be naturally mineralized or naturally soft or demineralized. Thus, the mineral consumption from drinking-water and cooking water will vary widely, depending upon location, treatment and water source. The degree of hardness of drinking-water is important for aesthetic acceptability by consumers (see chapter 10) and for economic and operational considerations. Many hard waters are softened for those reasons using several applicable technologies. The choice of the most appropriate conditioning technology will depend on local circumstances (e.g. water quality issues, piping materials, corrosion) and will be applied either centrally or in individual homes as a consumer preference. Consumers should be informed of the mineral composition of their water, whether or not it is modified. The contribution of drinking-water minerals to mineral nutrition should be considered where changes in supply are proposed or where less traditional sources, such as recycled water, seawater or brackish water, are processed and exploited for drinking-water. The treatments used remove most minerals, and stabilization of water is always necessary prior to distribution. Drinking-water can be a contributor to calcium and magnesium intake and could be important for those who are marginal for calcium and magnesium. Where drinking-water supplies are supplemented with or replaced by demineralized water that requires conditioning, consideration should be given to adding calcium and magnesium salts to achieve concentrations similar to those that the population received from the original supply. Modification of calcium and magnesium concentrations in drinking-water for health reasons should comply with the technical requirements to provide water suitable for distribution. Although there is evidence from epidemiological studies for a protective effect of magnesium or hardness on cardiovascular mortality, the evidence is being debated and does not prove causality. Further studies are being conducted. There are insufficient data to suggest either minimum or maximum concentrations of minerals at this time, as adequate intake will depend on a range of other factors. Therefore, no guideline values are proposed. Heptachlor and heptachlor epoxide Heptachlor (CAS No. 76-44-8) is a broad-spectrum insecticide, the use of which has been banned or restricted in many countries. At present, the major use of heptachlor is for termite control by subsurface injection into soil. Heptachlor is quite persistent in soil, where it is mainly transformed to its epoxide. Heptachlor epoxide (CAS No. 102457-3) is very resistant to further degradation. Heptachlor and heptachlor epoxide bind to soil particles and migrate very slowly. Heptachlor and heptachlor epoxide have been found in drinking-water at nanogram per litre levels. Diet is considered to represent the major source of exposure to heptachlor, although intake is decreasing significantly, as its use has substantially declined. Reason for not establishing Occur in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal references FAO/WHO (1992) Pesticide residues in food—1991 evaluations FAO/WHO (1995) Pesticide residues in food—1994 evaluations WHO (2003) Heptachlor and heptachlor epoxide in drinking-water Prolonged exposure to heptachlor has been associated with damage to the liver and central nervous system toxicity. In 1991, IARC reviewed the data on heptachlor and concluded that the evidence for carcinogenicity was sufficient in animals and inadequate in humans, classifying it in Group 2B (possibly carcinogenic to humans). A health-based value of 0.03 μg/l can be calculated for heptachlor and heptachlor epoxide on the basis of a PTDI of 0.1 μg/kg body weight, based on a NOAEL for heptachlor of 0.025 mg/kg body weight per day from two studies in the dog, taking into consideration inadequacies of the database and allocating 1% of the PTDI to drinking-water. However, because heptachlor and heptachlor epoxide occur at concentrations well below those of health concern, it is not considered necessary to derive a formal guideline value. It should also be noted that concentrations below 0.1 μg/l are generally not achievable using conventional treatment technology. Hexachlorobenzene The major agricultural application for hexachlorobenzene (CAS No. 118-74-1), or HCB, was as a seed dressing for crops to prevent the growth of fungi, but its use is now uncommon. At present, it appears mainly as a by-product of several chemical processes or an impurity in some pesticides. HCB is distributed throughout the environment because it is mobile and resistant to degradation. It bioaccumulates in organisms because of its physicochemical properties and its slow elimination. HCB is commonly detected at low levels in food, and it is generally present at low concentrations in ambient air. It has been detected only infrequently, and at very low concentrations (below 0.1 μg/l), in drinking-water supplies. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal references IPCS (1997) Hexachlorobenzene WHO (2004) Hexachlorobenzene in drinking-water IARC has evaluated the evidence for the carcinogenicity of HCB in animals and humans and assigned it to Group 2B (possibly carcinogenic to humans). HCB has been shown to induce tumours in three animal species and at a variety of sites. A health-based value of 1 μg/l can be derived for HCB by applying the linearized multistage low-dose extrapolation model to liver tumours observed in female rats in a 2-year dietary study. Using an alternative (tumorigenic dose05, or TD05) approach, a TDI of 0.16 μg/kg body weight can be calculated, which corresponds to a health-based value of approximately 0.05 μg/l, if one assumes a 1% allocation of the TDI to drinking-water. It should be noted that concentrations in food have been falling steadily, and this allocation factor may be considered very conservative. Because the health-based values derived from both of these approaches are considerably higher than the concentrations at which HCB is detected in drinking-water (i.e. sub-nanograms per litre), when it is detected, it is not considered necessary to establish a formal guideline value for HCB in drinking-water. HCB is listed under the Stockholm Convention on Persistent Organic Pollutants. Hexachlorobutadiene Hexachlorobutadiene, or HCBD, is used as a solvent in chlorine gas production, a pesticide, an intermediate in the manufacture of rubber compounds and a lubricant. Concentrations of up to 6 μg/l have been reported in the effluents from chemical manufacturing plants. HCBD is also found in air and food. Guideline value 0.0006 mg/l (0.6 μg/l) Occurrence Has been detected in surface water at concentrations of a few micrograms per litre and in drinking‑water at concentrations below 0.5 μg/l TDI 0.2 μg/kg body weight, based on a NOAEL of 0.2 mg/kg body weight per day for renal toxicity in a 2‑year feeding study in rats, using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for limited evidence of carcinogenicity and genotoxicity of some metabolites) Limit of detection 0.01 μg/l by GC‑MS; 0.18 μg/l by GC with ECD Treatment performance 0.001 mg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments The practical quantification limit for HCBD is of the order of 2 μg/l, but concentrations in drinking‑water can be controlled by specifying the HCBD content of products coming into contact with it. Assessment date 2003 Principal references IPCS (1994) Hexachlorobutadiene WHO (2003) Hexachlorobutadiene in drinking-water HCBD is easily absorbed and metabolized via conjugation with glutathione. This conjugate can be further metabolized to a nephrotoxic derivative. Kidney tumours were observed in a long-term oral study in rats. HCBD has not been shown to be carcinogenic by other routes of exposure. IARC has placed HCBD in Group 3 (not classifiable as to its carcinogenicity to humans). Positive and negative results for HCBD have been obtained in bacterial assays for point mutation; however, several metabolites have given positive results. Hydrogen sulfide Hydrogen sulfide is a gas with an offensive “rotten eggs” odour that is detectable at very low concentrations, below 0.8 μg/m3 in air. It is formed when sulfides are hydrolysed in water. However, the level of hydrogen sulfide found in drinking-water will usually be low, because sulfides are readily oxidized in well-aerated or chlorinated water. Reason for not establishing Not of health concern at levels found in drinking‑water a guideline value Additional comments May affect acceptability of drinking‑water Assessment date 1993 Principal reference WHO (2003) Hydrogen sulfide in drinking-water The acute toxicity to humans of hydrogen sulfide following inhalation of the gas is high; eye irritation can be observed at concentrations of 15–30 mg/m3. Although oral toxicity data are lacking, it is unlikely that a person could consume a harmful dose of hydrogen sulfide from drinking-water. Consequently, no guideline value is proposed. However, hydrogen sulfide can be easily detected in drinking-water by taste or odour (see chapter 10). Inorganic tin Tin is used principally in the production of coatings used in the food industry. Food, particularly canned food, therefore represents the major route of human exposure to tin. For the general population, drinking-water is not a significant source of tin, and levels in drinking-water greater than 1–2 μg/l are exceptional. However, there is increasing use of tin in solder, which may be used in domestic plumbing, and tin has been proposed for use as a corrosion inhibitor. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal reference WHO (2004) Inorganic tin in drinking-water Tin and inorganic tin compounds are poorly absorbed from the gastrointestinal tract, do not accumulate in tissues and are rapidly excreted, primarily in faeces. No increased incidence of tumours was observed in long-term carcinogenicity studies conducted in mice and rats fed tin(II) chloride. Tin has not been shown to be teratogenic or fetotoxic in mice, rats or hamsters. In rats, the NOAEL in a long-term feeding study was 20 mg/kg body weight per day. The main adverse effect on humans of excessive levels of tin in canned beverages (above 150 mg/kg) or other canned foods (above 250 mg/kg) has been acute gastric irritation. There is no evidence of adverse effects in humans associated with chronic exposure to tin. In 1989, JECFA established a PTWI of 14 mg/kg body weight from a TDI of 2 mg/kg body weight on the basis that the problem with tin is associated with acute gastrointestinal irritancy, the threshold for which is about 200 mg/kg in food. This was reaffirmed by JECFA in 2000. In view of its low toxicity, the presence of tin in drinking-water does not, therefore, represent a hazard to human health. For this reason, the establishment of a guideline value for inorganic tin is not deemed necessary. Iodine Iodine occurs naturally in water in the form of iodide. Traces of iodine are produced by oxidation of iodide during water treatment. Iodine is occasionally used for water disinfection in the field or in emergency situations. The diet is the major source of exposure to iodine for the general human population; the contribution to total exposure from drinking-water is assumed to be low (around 5%). Reason for not establishing Available data inadequate to permit derivation of health‑based a guideline value guideline value. Additionally, occurrence in drinking‑water is usually low. Although higher levels of exposure may occur when iodine is used as a drinking‑water disinfectant at the point of use, extended periods of exposure to iodine through water disinfection are unlikely. Occurrence Average concentrations have ranged from 0.5 to 20 μg/l in rivers and lakes Limit of detection Treatment performance 10 μg/l by a leuco crystal violet method Not applicable as occurrence in drinking‑water is usually low Additional comments Iodine is not recommended for use as a primary disinfectant of drinking‑water but it can be used as a point‑of‑use disinfectant. Caution should be exercised for longer‑term point‑of‑use disinfection in susceptible individuals (see Part II of the supporting document Alternative drinking-water disinfectants: Bromine, iodine and silver; Annex 1). Assessment date 2020 Principal reference WHO (2020) Iodine in drinking-water Iodine is an essential element for the synthesis of thyroid hormones. Various national and international organizations have established recommended daily intakes, with recommendations by WHO/FAO set in 2004 at 90 μg/day for infants, 120 μg/day for children, 150 μg/day for adults and 200 μg/day for pregnant or lactating women. In many parts of the world, dietary deficiencies in iodine are a health issue, which can adversely affect neurological development. In 1989, JECFA set a provisional maximum tolerable daily intake (PMTDI) for iodine of 1000 μg/day (17 μg/kg body weight per day) from all sources, based primarily on data on the effects of iodide. Various national and international organizations have also established recommended upper intake levels for iodine, ranging from 500 to 1100 μg/day. Exposure to excess iodine can lead to hypothyroidism (with or without goitre—enlargement of the thyroid), hyperthyroidism, and changes in the incidence and types of thyroid malignancies. However, available data are inadequate to establish a dose–response relationship between iodine exposure and changes in thyroid levels, and to clearly identify a threshold associated with thyrotoxicosis. Further, a guideline value cannot be derived using the more robust toxicological dataset for iodide because the effects of iodine and iodide on thyroid hormone concentrations in the blood may differ. Because iodine is generally not recommended for long-term disinfection, lifetime exposure to iodine concentrations such as might occur from water disinfection is unlikely. For these reasons, a guideline value for iodine has not been established at this time. Disinfection of iodide-containing water by adding chlorine or chloramine can result in the production of iodinated DBPs; most often, these are formed during chloramination when complete oxidation is prevented. Iodinated DBPs are occasionally detected in drinking-water from treatment plants located in coastal saltwater areas. As with all DBPs, their concentrations in drinking-water can be reduced at the treatment plant by removing the natural organic matter from the water before the disinfection process occurs. It is critical that any method used to control DBP levels does not compromise the effectiveness of disinfection. Iron Iron is one of the most abundant metals in Earth’s crust. It is found in natural fresh waters at levels ranging from 0.5 to 50 mg/l. Iron may also be present in drinking-water as a result of the use of iron coagulants or the corrosion of steel and cast iron pipes during water distribution. Reason for not establishing Not of health concern at levels found in drinking‑water a guideline value Additional comments May affect acceptability of drinking‑water Assessment date 1993 Principal reference WHO (2003) Iron in drinking‑water Iron is an essential element in human nutrition, particularly in the iron(II) oxidation state. Estimates of the minimum daily requirement for iron depend on age, sex, physiological status and iron bioavailability and range from about 10 to 50 mg/ day. As a precaution against storage in the body of excessive iron, in 1983, JECFA established a PMTDI of 0.8 mg/kg body weight, which applies to iron from all sources except for iron oxides used as colouring agents and iron supplements taken during pregnancy and lactation or for specific clinical requirements. An allocation of 10% of this PMTDI to drinking-water gives a value of about 2 mg/l, which does not present a hazard to health. The taste and appearance of drinking-water will usually be affected below this level (see chapter 10). No guideline value for iron in drinking-water is proposed. Isoproturon Isoproturon (CAS No. 34123-59-6) is a selective, systemic herbicide used in the control of annual grasses and broad-leaved weeds in cereals. It can be photodegraded, hydrolysed and biodegraded and persists for periods ranging from days to weeks. It is mobile in soil. There is evidence that exposure to this compound through food is low. Isoproturon is of low acute toxicity and low to moderate toxicity following short-Guideline value 0.009 mg/l (9 μg/l) Occurrence Has been detected in surface water and groundwater, usually at concentrations below 0.1 μg/l; levels above 0.1 μg/l have occasionally been detected in drinking‑water TDI 3 μg/kg body weight based on a NOAEL of approximately 3 mg/kg body weight in a 90‑day study in dogs and a 2‑year feeding study in rats, with an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for evidence of non‑genotoxic carcinogenicity in rats) Limit of detection 10–100 ng/l by reversed‑phase HPLC followed by UV or electrochemical detection Treatment performance 0.1 μg/l should be achievable using ozonation Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Assessment date 1993 Principal reference WHO (2003) Isoproturon in drinking-water term and long-term exposures. It does not possess significant genotoxic activity, but it causes marked enzyme induction and liver enlargement. Isoproturon caused an increase in hepatocellular tumours in male and female rats, but this was apparent only at doses that also caused liver toxicity. Isoproturon appears to be a tumour promoter rather than a complete carcinogen. Lead Lead is used principally in the production of lead-acid batteries, solder and alloys. The organolead compounds tetraethyl and tetramethyl lead have also been used extensively as antiknock and lubricating agents in petrol, although their use for these purposes in many countries has largely been phased out. Owing to the decreasing use of lead-containing additives in petrol and of lead-containing solder in the food processing industry, concentrations in air and food are declining; in most countries, lead levels in blood are also declining unless there are specific sources, such as dust from leaded paint or occupational/household recycling of lead-containing materials. Lead is rarely present in tap water as a result of its dissolution from natural sources; rather, its presence is primarily from corrosive water effects on household plumbing systems containing lead in pipes, solder or fittings (including alloy fittings with high lead content), or from the service connections to homes. The amount of lead dissolved from the plumbing system depends on several factors, including pH, temperature, alkalinity, scale in pipe and standing time of the water, with soft, acidic water being the most plumbosolvent. Free chlorine residuals in drinking-water tend to form more insoluble lead-containing deposits, whereas chloramine residuals may form more soluble sediments in lead pipe. Accordingly, significant changes in the water quality of a supply, resulting from, for example, changes in treatment or changes of source, can result in changes in plumbosolvency or solubilization of lead deposits, or both. Provisional guideline value 0.01 mg/l (10 μg/l) The guideline value is designated as provisional on the basis of treatment performance and analytical achievability. As this is no longer a health‑based guideline value, concentrations should be maintained as low as reasonably practical. New sources of lead, such as service connections and lead solder, should not be introduced into any system, and low lead alloy fittings should be used in repairs and new installations. Occurrence Concentrations in drinking‑water are generally below 5 μg/l, although much higher concentrations (above 100 μg/l) have been measured where lead service connections or fittings are present. The primary source of lead is from service connections and plumbing in buildings; therefore, lead should be measured at the tap. Lead concentrations can also vary according to the period in which the water has been in contact with the lead‑containing materials. Basis of guideline derivation The guideline value was previously based on a JECFA PTWI, which has since been withdrawn, and no new PTWI has been established, on the basis that there does not appear to be a threshold for the key effects of lead. However, substantial efforts have been made to reduce lead exposure from a range of sources, including drinking‑water. The guideline value is maintained at 10 μg/l but is designated as provisional on the basis of treatment performance and analytical achievabiilty because it is extremely difficult to achieve a lower concentration than this by central conditioning, such as phosphate dosing. Limit of detection 1 μg/l by AAS; practical quantification limit in the region of 1–10 μg/l Treatment performance Not a raw water contaminant; treatment not applicable Additional comments Infants and children are considered to be the most sensitive subgroups of the population Lead is exceptional compared with other chemical hazards, in that most lead in drinking‑water arises from lead service connections and plumbing in buildings, and the remedy consists principally of removing service connections, plumbing and fittings containing lead. This requires much time and money, and it is recognized that not all water will meet the guideline value immediately. Meanwhile, all other practical measures to reduce total exposure to lead, including corrosion control, should be implemented. In new installations or repairs, lead‑free service connections and solder and low lead alloy fittings should be used to prevent the introduction of contamination. The sampling protocol adopted – e.g. first draw, random daytime sampling or flushed – will depend on the objective of taking the samples. Where there is a need to verify that lead solder and/or high‑lead fittings have not been installed in new or repaired systems, the approach used is to take a worst‑case sample that reflects an extended period of stagnation, to maximize the chance of identifying the presence of lead. Assessment date 2011, revised 2016 Principal references FAO/WHO (2011) Evaluation of certain food additives and contaminants WHO (2016) Lead in drinking-water Exposure to lead is associated with a wide range of effects, including various neurodevelopmental effects, mortality (mainly due to cardiovascular diseases), impaired renal function, hypertension, impaired fertility and adverse pregnancy outcomes. Impaired neurodevelopment in children is generally associated with lower blood lead concentrations than the other effects, the weight of evidence is greater for neurodevelopmental effects than for other health effects and the results across studies are more consistent than those for other effects. For adults, the adverse effect associated with lowest blood lead concentrations for which the weight of evidence is greatest and most consistent is a lead-associated increase in systolic blood pressure. JECFA concluded that the effects on neurodevelopment and systolic blood pressure provided the appropriate bases for dose–response analyses. Based on the dose–response analyses, JECFA estimated that the previously established PTWI of 25 μg/kg body weight is associated with a decrease of at least 3 intelligence quotient (IQ) points in children and an increase in systolic blood pressure of approximately 3 mmHg (0.4 kPa) in adults. These changes are important when viewed as a shift in the distribution of IQ or blood pressure within a population. JECFA therefore concluded that the PTWI could no longer be considered health protective, and it was withdrawn. Because the dose–response analyses do not provide any indication of a threshold for the key effects of lead, JECFA concluded that it was not possible to establish a new PTWI that would be considered to be health protective. JECFA reaffirmed that because of the neurodevelopmental effects, fetuses, infants and children are the subgroups that are most sensitive to lead. It needs to be recognized that lead is exceptional compared with other chemical hazards, in that most lead in drinking-water arises from lead service connections and plumbing in buildings, and the remedy consists principally of removing plumbing and fittings containing lead, which requires much time and money. It is therefore emphasized that all other practical measures to reduce total exposure to lead, including corrosion control, should be implemented. New sources of lead, such as lead service connections and solder, should not be introduced into any system, and low lead alloy fittings should be used in repairs and new installations. In terms of monitoring, if the monitoring objective is to identify the presence of lead in the internal plumbing of a building, then the sample should be from the tap. The sampling protocols also depend on the objective of taking the samples. First-draw samples typically will have the highest lead concentrations, but this may not be reflected in normal use if the same system provides water for toilet flushing, etc. Flushed samples, in contrast, give consistent values, but reflect the minimum contact time between the water and the lead-containing material. The random daytime samples, although most truly reflecting the water that the consumer drinks, give the most variable levels; hence, it is necessary to collect more samples to determine the mean level of exposure. Where there is a need to verify that lead service connections, lead solder and/or high-lead fittings have not been installed in new or repaired systems, the approach used is to take a worst-case sample that reflects an extended period of stagnation and to maximize the chance of identifying the presence of lead. Extended stagnation with sequential volume can also be used to identify sources or locations of lead as an investigative activity. Lindane Lindane (γ-hexachlorocyclohexane; γ-HCH) (CAS No. 58-89-9) is used as an insecticide on fruit and vegetable crops, for seed treatment and in forestry. It is also used as a therapeutic pesticide in humans and animals. Several countries have restricted the use of lindane. Lindane can be degraded in soil and rarely leaches to groundwater. In surface waters, it can be removed by evaporation. Exposure of humans occurs mainly via food, but this is decreasing. There may also be exposure from its use in public health and as a wood preservative. Guideline value 0.002 mg/l (2 μg/l) Occurrence Has been detected in both surface water and groundwater, usually at concentrations below 0.1 μg/l, although concentrations as high as 12 μg/l have been measured in wastewater‑contaminated rivers ADI 0–0.005 mg/kg body weight on the basis of a NOAEL of 0.47 mg/kg body weight per day in a 2‑year toxicity/carcinogenicity study in rats in which an increased incidence of periacinar hepatocellular hypertrophy, increased liver and spleen weights and increased mortality occurred at higher doses, using an uncertainty factor of 100 (for interspecies and intraspecies variation) Limit of detection 0.01 μg/l using GC Treatment performance 0.1 μg/l should be achievable using GAC Guideline value derivat• allocation to water • weight • consumption Additional comments Assessment date Principal references ion 1% of upper limit of ADI 60 kg adult 2 litres/day It should be noted that concentrations in food have been falling steadily, and the 1% allocation factor may be considered very conservative. 2003 FAO/WHO (2003) Pesticide residues in food—2002 evaluations WHO (2003) Lindane in drinking-water Lindane was toxic to the kidney and liver after administration orally, dermally or by inhalation in short-term and long-term studies of toxicity and reproductive toxicity in rats. The renal toxicity of lindane was specific to male rats and was considered not to be relevant to human risk assessment, as it is a consequence of accumulation of α2u-globulin, a protein that is not found in humans. Hepatocellular hypertrophy was observed in a number of studies in mice, rats and rabbits and was reversed only partially after recovery periods of up to 6 weeks. Lindane did not induce a carcinogenic response in rats or dogs, but it caused an increased incidence of adenomas and carcinomas of the liver in agouti and pseudoagouti mice, but not in black or any other strains of mice, in a study of the role of genetic background in the latency and incidence of tumorigenesis. JMPR concluded that there was no evidence of genotoxicity. In the absence of genotoxicity and on the basis of the weight of the evidence from the studies of carcinogenicity, JMPR concluded that lindane is not likely to pose a carcinogenic risk to humans. Further, in an epidemiological study designed to assess the potential association between breast cancer and exposure to chlorinated pesticides, no correlation with lindane was found. Malathion Malathion (CAS No. 121-75-5) is commonly used to control mosquitoes and a variety of insects that attack fruits, vegetables, landscaping plants and shrubs. It can also be found in other pesticide products used indoors, on pets to control ticks and insects and to control human head and body lice. Under least favourable conditions (i.e. low pH and little organic content), malathion may persist in water with a half-life of months or even years. However, under most conditions, the half-life appears to be roughly 7–14 days. Malathion has been detected in surface water and drinking-water at concentrations below 2 μg/l. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal references FAO/WHO (1998) Pesticide residues in food—1997 evaluations WHO (2003) Malathion in drinking-water Malathion inhibits cholinesterase activity in mice, rats and human volunteers. It increased the incidence of liver adenomas in mice when administered in the diet. Most of the evidence indicates that malathion is not genotoxic, although some studies indicate that it can produce chromosomal aberrations and sister chromatid exchange in vitro. JMPR has concluded that malathion is not genotoxic. A health-based value of 0.9 mg/l can be calculated for malathion based on an allocation of 10% of the upper limit of the JMPR ADI—based on a NOAEL of 29 mg/kg body weight per day in a 2-year study of toxicity and carcinogenicity in rats, using an uncertainty factor of 100 for interspecies and intraspecies variation and supported by a NOAEL of 25 mg/kg body weight per day in a developmental toxicity study in rabbits—to drinking-water. However, intake of malathion from all sources is generally low and well below the upper limit of the ADI. As the chemical occurs in drinking-water at concentrations much lower than the health-based value, the presence of malathion in drinking-water under usual conditions is unlikely to represent a hazard to human health. For this reason, it is considered unnecessary to derive a formal guideline value for malathion in drinking-water. Manganese1 Manganese is one of the most abundant metals in Earth’s crust, usually occurring with iron. It can exist in 11 oxidation states, often as chloride, oxides and sulfates. The most common oxidation states for manganese in natural water are manganese(II) and manganese(IV). Manganese is used principally in the manufacture of iron and steel alloys, and manganese compounds such as potassium and sodium permanganate are ingredients in various products used for cleaning, bleaching and disinfection. Manganese compounds are additionally used in some locations for potable water treatment and can also be an impurity in coagulants used during water treatment. Manganese occurs naturally in many surface water and groundwater sources; although naturally occurring manganese is usually the most important source for drinking-water, anthropogenic activities can also contribute to high levels of manganese in water. Manganese also occurs naturally in many food sources, and the greatest exposure to manganese is usually from food. Provisional guideline value Total manganese: 0.08 mg/l (80 μg/l), to be protective against neurological effects in the most sensitive subpopulation—bottle‑fed infants—and consequently the general population This guideline value is provisional because of the high level of uncertainty, as reflected in the composite uncertainty factor of 1000 Occurrence Levels in fresh waters vary widely. They are typically in the range 1–200 μg/l. Higher levels are usually associated with groundwater, lakes and reservoirs under acidic or reducing conditions, or in aerobic waters with industrial pollution. Very high concentrations (up to 10 mg/l) have been reported in acidic groundwater. In treated drinking‑water, concentrations are typically less than 50 μg/l. TDI 0.025 mg/kg bw, derived by applying an uncertainty factor of 1000 to a LOAEL of 25 mg/kg bw per day identified from studies that reported neurological effects in rats exposed to manganese from birth to postnatal day 21. The uncertainty factor takes into account interpecies variation (10), intraspecies variation (10) and database uncertainties (10, including use of a LOAEL). Limit of detection 0.002 μg/l by ICP‑MS; 0.005–50 μg/l by ICP‑AES and GFAA spectrometry; and 10–70 μg/l by colorimetric methods. None of these methods distinguish between the different oxidation states of manganese. 1 As naturally occurring manganese in drinking-water is a chemical of concern in many areas, its chemical fact sheet has been expanded. Treatment performance Manganese concentrations in drinking‑water can be easily lowered to less than 0.05 mg/l using several treatment methods, including oxidation/filtration, adsorption/oxidation, softening/ion exchange and biological filtration. Selection of the appropriate treatment system for manganese removal depends on the form of manganese (dissolved or particulate) in the source water. Guideline value derivation • allocation to water 50% of TDI • weight 5 kg body weight for a bottle‑fed infant • consumption 0.75 litres per day for a bottle‑fed infant Additional comments Risks to infants arising from exceedance of the provisional guideline value may be mitigated by following the WHO recommendation for exclusive breastfeeding, or by using an alternative safe source of drinking‑water (e.g. bottled water that is certified by the responsible authorities) to prepare formula. The presence of particulate manganese in drinking‑water systems can cause acceptability problems; concentrations above 0.02 mg/l have caused complaints about discoloured water and staining of plumbing fixtures and laundry. Therefore, aesthetic as well as health aspects should be considered in the management of manganese in drinking‑water, and when setting regulations and standards for drinking‑water quality. Assessment date 2020 Principal references WHO (2021) Manganese in drinking-water Manganese is an essential trace element. It is a necessary component of a number of enzymes and activates several others. The central nervous system is the primary concern for manganese toxicity in mammals, including humans. Neurodevelopmental toxicity studies in manganese-exposed juvenile rats revealed behavioural and sensorimotor effects, and corresponding neurostructural and neurochemical changes. Several epidemiological studies have also reported neurological effects (including reduced cognitive ability) in adult populations and children following ingestion of manganese-contaminated water. The epidemiological studies have limited utility in risk assessment due to uncertain manganese exposure levels, unclear temporality of effects and other potential confounding factors. However, collectively, they provide qualitative support that the neurological effects reported in animal studies are relevant in humans. Existing studies and reports do not provide adequate evidence to assess potential carcinogenicity from oral exposure to manganese in humans. Absorption of manganese from the gastrointestinal (GI) tract has been suggested to take place through both an active transport mechanism and passive diffusion. GI absorption is influenced by several factors, including dietary factors: absorption is negatively correlated with intake of dietary fibre, oxalic acids and phytic acids. Some studies and reports have suggested that absorption and bioavailability of manganese are greater from drinking-water than from food, although other studies have reported no differences. However, absorption from drinking-water may be influenced by fasting conditions and the chemical form of manganese. Following GI tract absorption, manganese is distributed via the systemic circulation to all tissues. Levels of manganese can increase in several tissues following oral exposure, including some regions of the brain in infants and adults. The main route of elimination of manganese from the body is faecal elimination via hepatobiliary excretion. Manganese absorption from the GI tract may be higher in infants than in adults. Infants also retain higher levels of manganese than adults during the early neonatal period, possibly because of the incomplete development of the biliary excretion system. Along with the important neurodevelopmental processes occurring in neonates, this may render them particularly susceptible to toxicity from exposure to manganese. Further, there is potential for increased exposure to manganese in bottle-fed infants compared with breastfed infants—from the concentrated or powdered formula itself as well as the tap water used to prepare the formula. Practical considerations Manganese levels in drinking-water can be an issue in both high- and low-income countries, and should be considered in establishing national standards and local guidance. Resource-limited suppliers, in particular, may have difficulty in achieving the provisional guideline value; in such cases, incremental improvements towards meeting the provisional guideline value are encouraged. This is a particular problem for groundwater, for which treatment may be minimal and prohibitively expensive. In such instances, benefits from a reliable, microbiologically safe groundwater source should be assessed against the risks posed by an alternative source that may be subject to faecal contamination. Issues of acceptability of the drinking-water (which varies between different populations) should also be considered, since reduced acceptability may lead consumers to turn to more aesthetically acceptable but less microbiologically safe water supplies. It is vital that a sufficient supply of acceptable, microbiologically safe water is always available, even if some guidelines or standards for chemicals such as manganese cannot be immediately met. Manganese should be evaluated and managed in the context of developing a WSP (see chapter 4). Surface waters prone to high and variable concentrations of manganese may require more frequent and targeted monitoring than groundwater. Where manganese is present at concentrations close to the provisional guideline value or the water is treated to remove manganese, routine monitoring should be conducted after treatment. If manganese is detected at the point of collection or use, or aesthetic issues related to manganese are reported by consumers, this indicates that treatment for manganese removal is not optimized or that the distribution system is not appropriately managed. Options for controlling levels in groundwater include drilling a new well or blending water from different wells. For lake and reservoir sources where there is a thermocline and lower water levels become anoxic, management of the sources to prevent release of manganese from sediment is important. Selection of the appropriate treatment system for manganese removal depends on the form of manganese (dissolved or particulate) in the source water. Dissolved manganese(II) is most often the predominant form present in anoxic and acidic groundwater or lakes. However, depending on the pH and the dissolved oxygen content of the water, a combination of dissolved and particulate manganese can be present. In general, treatment methods used for manganese rely on a combination of processes (e.g. oxidation, adsorption, filtration) to remove both the dissolved and particulate forms. At the point of use, reverse osmosis is the most effective and reliable treatment technology; however, point-of-use units using ion exchange media are also moderately effective. To reduce water discoloration and staining of laundry and fixtures, ion exchange and greensand filtration with careful operation and maintenance can be used at the point of entry. Low levels of manganese in source or treated water can accumulate in the distribution system. Periodic release of manganese can then occur, resulting in high levels at the tap. Releases can occur as a result of physical or hydraulic disturbances to the system (e.g. mains breaks, hydrant flushing) or changes in water chemistry (e.g. changes in pH, temperature, chlorine residual, source water type/blending). Physical and hydraulic disturbances most often release particulate manganese and can cause discoloured water and consumer complaints. Chemical releases can go unnoticed if manganese occurs predominantly in the dissolved form. Other contaminants (e.g. arsenic, barium, chromium, lead, uranium) that deposit with manganese oxides in the distribution system may also be released into the water and reach consumers’ taps. Control measures to minimize manganese release events include maintaining stable water chemistry and minimizing manganese levels entering the distribution system, the amount of manganese oxide deposits in the distribution system (through best practices for water mains cleaning), and physical or hydraulic disturbances. MCPA MCPA is a phenoxyacetic acid herbicide that is found in various formulations: as the free acid (CAS No. 94-74-6), as a dimethylamine salt (CAS No. 2039-46-5), as a sodium salt (CAS No. 3653-48-3) and as a 2-ethylhexyl ester (CAS No. 29450-45-1). It is a post-emergence herbicide that is widely used against broadleaf weeds in agriculture and horticulture and on grassland and lawns. All forms of MCPA will dissociate in water to the acid (anion) form. MCPA is highly soluble in water. Biological degradation is an important process in determining MCPA’s environmental fate. Chlorophenols and chlorocresols are potential soil metabolites and may, if present in water, give rise to unacceptable tastes. Surface water may be contaminated via spray drift and runoff, whereas groundwater may be contaminated via leaching from soil. Exposure from food is likely to be low. Reason for not establishing Occurs in drinking‑water or drinking‑water sources at concentrations a guideline value well below those of health concern Health‑based value* 0.7 mg/l Acute health‑based value** 20 mg/l Occurrence Concentrations in surface water usually less than 1 μg/l; concentrations in drinking‑water usually below 0.1 μg/l ADI 0–0.1 mg/kg bw for MCPA ion, based on an overall NOAEL of 12 mg/ kg bw per day for changes in clinical chemistry parameters indicative of effects on the kidneys from four subchronic studies in rats and application of a safety factor of 100 ADI established for the sum of MCPA and its salts and esters, expressed as MCPA acid equivalents ARfD 0.6 mg/kg bw for MCPA ion, based on the overall NOAEL of 60 mg/kg bw for maternal and developmental toxicity in rats and application of a safety factor of 100 ARfD established for the sum of MCPA and its salts and esters, expressed as MCPA acid equivalents Limit of detection 0.8 μg/L using HPLC with a photodiode array UV detector; 0.09 μg/l using derivatization and GC with ECD; limit of quantification of 0.0005 μg/l for LC‑MS/MS Treatment performance Conventional treatment not effective; activated carbon adsorption and/ or ozonation and advanced oxidation processes (e.g. UV with hydrogen peroxide) are effective; membrane filtration processes (e.g. reverse osmosis) may be effective Health‑based value derivation • allocation to water 20% of upper bound of unrounded ADI (0.12 mg/kg bw) • weight 60 kg adult• consumption 2 litres/day Acute health‑based value derivation • allocation to water 100% of ARfD • weight 60 kg adult• consumption 2 litres/day Additional comments The default allocation factor of 20% has been used to account for the fact that the available food exposure data, which suggest that exposure via this route is low, do not generally include information from developing countries, where exposure via this route may be higher. As a general principle, the concentration of pesticides in water, including MCPA, should be kept as low as possible and concentrations should not be allowed to increase up to the health‑based value. Further guidance on interpreting the health‑based value and deciding when to monitor can be found in section 8.5.3. Assessment date 2016 Principal references WHO (2013). Pesticide residues in food – 2012 evaluations WHO (2016). MCPA in drinking-water * When a formal guideline value is not established, a “health‑based value” may be determined in order to provide guidance to Member States when there is reason for local concern. Establishing a formal guideline value for such substances may encourage Member States to incorporate a value into their national standards when this may be unnecessary. ** For more information on acute health‑based values, see section 8.7.5. The target organs for the MCPA ion are the kidney, liver and blood. MCPA is not carcinogenic in mice or rats, and the MCPA ion exhibits no genotoxic potential. In multigeneration studies in rats, there was no evidence of reproductive toxicity up to the highest dose tested. The MCPA ion was not teratogenic in rats or rabbits. Mecoprop The half-lives for degradation of chlorophenoxy herbicides, including mecoprop (CAS No. 93-65-2; 7085-19-0 racemic mixture), also known as 2(2-methyl-chlorophenoxy) propionic acid or MCPP, in the environment are in the order of several days. Chlorophenoxy herbicides are not often found in food. Guideline value 0.01 mg/l (10 μg/l) Occurrence Chlorophenoxy herbicides not frequently found in drinking‑water; when detected, concentrations usually no greater than a few micrograms per litre TDI 3.33 μg/kg body weight, based on a NOAEL of 1 mg/kg body weight for effects on kidney weight in 1‑ and 2‑year studies in rats, with an uncertainty factor of 300 (100 for interspecies and intraspecies variation and 3 for limitations in the database) Limit of detection 0.01 μg/l by GC‑MS; 0.01–0.02 μg/l by GC with ECD Treatment performance 0.1 μg/l should be achievable using GAC or ozonation Guideline value derivation • allocation to water • weight • consumption 10% of TDI 60 kg adult 2 litres/day Assessment date 1993 Principal reference WHO (2003) Chlorophenoxy herbicides (excluding 2,4-D and MCPA) in drinking-water Chlorophenoxy herbicides, as a group, have been classified in Group 2B (possibly carcinogenic to humans) by IARC. However, the available data from studies in exposed populations and experimental animals do not permit assessment of the carcinogenic potential to humans of any specific chlorophenoxy herbicide. Therefore, drinking-water guidelines for these compounds are based on a threshold approach for other toxic effects. Effects of dietary administration of mecoprop in short-term and long-term studies include decreased relative kidney weight (rats and dogs), increased relative liver weight (rats), effects on blood parameters (rats and dogs) and depressed body weight gain (dogs). Mercury Mercury is used in the electrolytic production of chlorine, in electrical appliances, in dental amalgams and as a raw material for various mercury compounds. Methylation of inorganic mercury has been shown to occur in fresh water and in seawater, although almost all mercury in uncontaminated drinking-water is thought to be in the form of Hg2+. Thus, it is unlikely that there is any direct risk of the intake of organic mercury compounds, especially of alkylmercurials, as a result of the ingestion of drinking-water. However, there is a possibility that methylmercury will be converted into inorganic mercury. Food is the main source of mercury in non-occupationally exposed populations; the mean dietary intake of mercury in various countries ranges from 2 to 20 μg/day per person. Guideline value 0.006 mg/l (6 μg/l) for inorganic mercury Occurrence Mercury is present in the inorganic form in surface water and groundwater at concentrations usually below 0.5 μg/l, although local mineral deposits may produce higher levels in groundwater TDI 2 μg/kg body weight for inorganic mercury based on a NOAEL of 0.23 mg/kg body weight per day for kidney effects in a 26‑week study in rats and applying an uncertainty factor of 100 (for interspecies and intraspecies variation) after adjusting for daily dosing Limit of detection 0.05 μg/l by cold vapour AAS; 0.6 μg/l by ICP; 5 μg/l by flame AAS Treatment performance It should be possible to achieve a concentration below 1 μg/l by treatment of raw waters that are not grossly contaminated with mercury using methods that include coagulation/sedimentation/filtration, PAC and ion exchange. Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments A similar TDI may be obtained by applying an uncertainty factor of 1000 (an additional uncertainty factor of 10 for adjustment from a LOAEL to a NOAEL) to the LOAEL for renal effects of 1.9 mg/kg body weight per day in a 2‑year NTP study in rats. The current guideline value applies to inorganic mercury, which is the form found in drinking‑water, whereas the previous guideline value applied to total (inorganic and organic) mercury. Assessment date 2004 Principal references IPCS (2003) Elemental mercury and inorganic mercury compounds WHO (2005) Mercury in drinking-water The toxic effects of inorganic mercury compounds are seen mainly in the kidney in both humans and laboratory animals following short-term and long-term exposure. In rats, effects include increased absolute and relative kidney weights, tubular necrosis, proteinuria and hypoalbuminaemia. In humans, acute oral poisoning results primarily in haemorrhagic gastritis and colitis; the ultimate damage is to the kidney. The overall weight of evidence is that mercury(II) chloride has the potential to increase the incidence of some benign tumours at sites where tissue damage is apparent and that it possesses weak genotoxic activity but does not cause point mutations. Methoxychlor Methoxychlor (CAS No. 72-43-5) is an insecticide used on vegetables, fruit, trees, fodder and farm animals. It is poorly soluble in water and highly immobile in most agricultural soils. Under normal conditions of use, methoxychlor does not seem to be of environmental concern. Daily intake from food and air is expected to be below 1 μg per person. Environmental metabolites are formed preferentially under anaerobic rather than aerobic conditions and include mainly the dechlorinated and demethylated products. There is some potential for the accumulation of the parent compound and its metabolites in surface water sediments. Guideline value 0.02 mg/l (20 μg/l) Occurrence Detected occasionally in drinking‑water, at concentrations as high as 300 μg/l in rural areas TDI 5 μg/kg body weight, based on a systemic NOAEL of 5 mg/kg body weight in a teratology study in rabbits, with an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 reflecting concern for threshold carcinogenicity and the limited database) Limit of detection 0.001–0.01 μg/l by GC Treatment performance 0.1 μg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Assessment date 1993 Principal reference WHO (2004) Methoxychlor in drinking-water The genotoxic potential of methoxychlor appears to be negligible. In 1979, IARC assigned methoxychlor to Group 3. Subsequent data suggest a carcinogenic potential of methoxychlor for liver and testes in mice. This may be due to the hormonal activity of proestrogenic mammalian metabolites of methoxychlor and may therefore have a threshold. The study, however, was inadequate, because only one dose was used and because this dose may have been above the maximum tolerated dose. The database for studies on long-term, short-term and reproductive toxicity is inadequate. A teratology study in rabbits reported a systemic NOAEL of 5 mg/kg body weight per day, which is lower than the LOAELs and NOAELs from other studies. This NOAEL was therefore selected for use in the derivation of a TDI. Methyl parathion Methyl parathion (CAS No. 298-00-0) is a non-systemic insecticide and acaricide that is produced throughout the world and has been registered for use on many crops, in particular cotton. It partitions mainly to air and soil in the environment. There is virtually no movement through soil, and neither the parent compound nor its breakdown products will reach groundwater. By far the most important route for the environmental degradation of methyl parathion is microbial degradation. Half-lives of methyl parathion in water are in the order of weeks to months. Concentrations of methyl parathion in natural waters of agricultural areas in the USA ranged up to 0.46 μg/l, with highest levels in summer. The general population can come into contact with methyl parathion via air, water or food. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal references FAO/WHO (1996) Pesticide residues in food—1995 evaluations. IPCS (1992) Methyl parathion WHO (2004) Methyl parathion in drinking-water A NOAEL of 0.3 mg/kg body weight per day was derived from the combined results of several studies conducted in humans, based on the depression of erythrocyte and plasma cholinesterase activities. Methyl parathion decreased cholinesterase activities in long-term studies in mice and rats, but did not induce carcinogenic effects. Methyl parathion was mutagenic in bacteria, but there was no evidence of genotoxicity in a limited range of studies in mammalian systems. A health-based value of 9 μg/l can be calculated for methyl parathion on the basis of an ADI of 0–0.003 mg/kg body weight, based on a NOAEL of 0.25 mg/kg body weight per day in a 2-year study in rats for retinal degeneration, sciatic nerve demyelination, reduced body weight, anaemia and decreased brain acetylcholinesterase activity, using an uncertainty factor of 100 for interspecies and intraspecies variation. As the toxicological end-points seen in experimental animals were other than acetylcholinesterase inhibition, it was considered more appropriate to use these data rather than the NOAEL derived for cholinesterase inhibition in humans. Intake of methyl parathion from all sources is generally low and well below the upper limit of the ADI. As the health-based value is much higher than concentrations of methyl parathion likely to be found in drinking-water, the presence of methyl parathion in drinking-water under usual conditions is unlikely to represent a hazard to human health. For this reason, the establishment of a formal guideline value for methyl parathion is not deemed necessary. Methyl tertiary-butyl ether The major use of methyl tert-butyl ether, or MTBE, is as a gasoline additive. Surface water can be contaminated by gasoline spills; however, owing to the high volatility of MTBE, most is lost to evaporation. Spills and leaking storage tanks can cause more serious problems in groundwater, where MTBE is more persistent. MTBE has been detected in groundwater and drinking-water at concentrations in the nanogram to microgram per litre range. Reason for not establishing Any guideline that would be derived would be significantly higher than a guideline value concentrations at which MTBE would be detected by odour Assessment date 2004 Principal references IPCS (1998) Methyl tertiary-butyl ether WHO (2005) Methyl tertiary-butyl ether (MTBE) in drinking-water No human cancer studies have been published for either the general population or occupationally exposed cohorts. There have been a number of human studies of neurological and clinical effects of exposure to MTBE by inhalation, with mixed results. In general, no objective changes could be seen at levels of MTBE normally found, even in such microenvironments as gasoline filling stations. The weight of evidence suggests that MTBE is not genotoxic. A large number of studies using in vitro and in vivo mammalian and non-mammalian systems have been conducted to assess the mutagenicity of MTBE, almost all of which have produced negative results. These results suggest that the mechanism of action of MTBE is more likely to be non-genotoxic than genotoxic, although no one mechanism appears to explain all of the observed effects. It has been concluded that MTBE should be considered a rodent carcinogen but that it is not genotoxic, and the carcinogenic response is evident only at high levels of exposure that also induce other adverse effects. The available data are therefore considered inconclusive and prohibit their use for human carcinogenic risk assessment. A health-based guideline value has not been derived for MTBE, owing to the fact that any guideline value that would be derived would be significantly higher than the concentration at which it would be detected by odour (15 μg/l is the lowest level eliciting a response in a study using taste- and odour-sensitive participants). Metolachlor Metolachlor (CAS No. 51218-45-2) is a selective pre-emergence herbicide used on a number of crops. It can be lost from the soil through biodegradation, photodegradation and volatilization. It is fairly mobile and under certain conditions can contaminate groundwater, but it is mostly found in surface water. Guideline value 0.01 mg/l (10 μg/l) Occurrence Detected in surface water and groundwater at concentrations that can exceed 10 μg/l TDI 3.5 μg/kg body weight, based on a NOAEL of 3.5 mg/kg body weight for an apparent decrease in kidney weight at the two highest dose levels in a 1‑year dog study, with an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 reflecting some concern regarding carcinogenicity) Limit of detection 0.75–0.01 μg/l by GC with nitrogen–phosphorus detection Treatment performance 0.1 μg/l should be achievable using GAC Guideline value derivation • allocation to water • weight • consumption 10% of TDI 60 kg adult 2 litres/day Assessment date 1993 Principal reference WHO (2003) Metolachlor in drinking-water In a 1-year study in dogs, administration of metolachlor resulted in decreased kidney weight at the two highest dose levels. In 2-year studies with rodents fed metolachlor in the diet, the only toxicological effects observed in mice were decreased body weight gain and decreased survival in females at the highest dose level, whereas rats showed decreased body weight gain and food consumption at the highest dose level. There is no evidence from available studies that metolachlor is carcinogenic in mice. In rats, an increase in liver tumours in females as well as a few nasal tumours in males have been observed. Metolachlor is not genotoxic. Microcystins (cyanobacterial toxins)1 Microcystins (MCs) are naturally occurring heptapeptides produced by strains of various species of cyanobacteria, primarily in freshwater environments. They occur widely around the globe, particularly with strains of the frequently occurring cyanobacterial genera Microcystis, Planktothrix and Dolichspermum (see also section 11.5). Among the more than 250 MCs identified to date, only a few occur frequently and in high concentrations. MCs are usually cell-bound, and substantial amounts are released to the surrounding water only during cell rupture (i.e. lysis). One of the most common MCs, and the one most studied, is microcystin-LR (MC-LR). Drinking-water is the most likely route of exposure to MCs where surface water with cyanobacterial blooms is the drinking-water source. Recreational activities in lakes with cyanobacterial blooms may also be a relevant exposure pathway, potentially to high, usually intermittent concentrations (see WHO Guidelines on recreational water quality, 2021). Limited data suggest that MCs may also accumulate in some food items. Provisional guideline Total MCs (sum of all congeners, free plus cell-bound): 0.001 mg/l (1 μg/l) value (lifetime) The guideline value is provisional because of the high level of uncertainty—it is based on data for only MC‑LR, and the database is limited, as reflected in the composite uncertainty factor of 1000 Provisional short‑term Total MCs (sum of all congeners, free plus cell-bound): 0.012 mg/l (12 μg/l) guideline value Occurrence Although concentrations in scums can reach the range of mg/l (even up to 100 mg/L), outside of scum areas they rarely exceed several tens of μg/l. MCs largely occur cell‑bound unless cell damage causes release TDI 0.04 μg/kg bw, based on a NOAEL of 40 μg/kg bw per day for liver pathology observed in a 13‑week study in mice and applying an uncertainty factor of 1000 (10 each for inter‑ and intra‑species variability and 10 for database deficiencies,* taking into consideration limitations in the database—in particular, limited data on chronic toxicity, reproductive toxicity and carcinogenicity) Limit of detection <1 μg/l by LC‑MS/MS, or LC (including HPLC) followed by UV/PDA detection. LC‑MS/MS has the highest specificity and sensitivity but requires quantitative reference standards for all relevant MCs in the sample. For UV/PDA detection, standards for a few MCs whose signals are representative of all are sufficient. Prior extraction of cells with freeze–thaw cycles and 75% aqueous methanol is necessary for cell‑bound MCs; neglecting extraction from cells will lead to dramatic underestimation of concentrations. 1 As cyanobacteria and their toxins, in particular MCs, are a concern in many areas and considering the complexities in their management, its chemical fact sheet has been expanded <1 μg/l by commercially available immunoassay (ELISA) or enzyme assay (PPA) kits; although these are less precise than LC with the above‑mentioned detection methods, they capture all MCs and thus are useful for most monitoring purposes. Monitoring The likelihood of blooms can be assessed by understanding water body conditions (in particular, nutrient concentrations, water body depth, water retention time, patterns of mixing and stratification; see section 11.5). Where conditions render blooms likely, visual monitoring of source water (including microscopy for potentially MC‑containing genera) for evidence of increasing cyanobacterial biomass (blooms) is important because biomass can increase rapidly. Exceeding alert values for biomass indicators or MC concentrations should trigger management responses to prevent exposure to elevated toxin concentrations (see the alert level framework in section 11.5). Analysis of cyanotoxins is particularly useful for validating and optimizing the efficacy of control measures such as riverbank filtration or treatment. Prevention and treatment Actions to decrease the probability of bloom occurrence include catchment and source water management, such as reducing nutrient loading or changing reservoir stratification and mixing. Filtration is effective for removing intact cyanobacterial cells, and in most cases this removes the major fraction of MCs. For dissolved MCs, oxidation with chlorine or ozone at sufficient concentrations and contact times, as well as GAC and some PAC applications, are effective (see chapters 7–10 of Toxic cyanobacteria in water; Annex 1). Guideline value derivation • allocation to water 80% of TDI; for short‑term exposure, 100% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments Total MCs as gravimetric or molar equivalents should be evaluated against the guideline values since MCs usually occur as mixtures. The guideline values are based on MC‑LR, which is one of the most toxic and common MCs. The provisional short‑term drinking‑water guideline value is intended to indicate the extent to which the lifetime value can be exceeded for periods of up to about 2 weeks until water treatment can be augmented to bring the concentration of MCs back under control. It is not intended to allow repeated seasonal exceedances of the lifetime value. It is recommended, as a precautionary measure, that bottle‑fed infants and small children be provided with an alternative safe drinking‑water source (e.g. bottled water that is certified by the responsible authorities) if concentrations are greater than 3 μg/L, even for short periods. Assessment date 2020 Principal references WHO (2020) Cyanobacterial toxins: microcystins Chorus & Welker (2021) Toxic cyanobacteria in water * For the short‑term guideline value, a database uncertainty factor was not applied because chronic toxicity, carcinogenicity (tumour promotion) and the limited evidence for reproductive toxicity following subchronic dosing are not relevant to short‑term exposures. MCs are actively transported into cells by specific organic anion transport proteins (OATPs). Because the liver has high expression of OATPs, MCs are considered to be primarily hepatotoxins, although distribution to other organs and tissues expressing OATPs does occur. Once in the cell, MCs bind with high affinity to certain protein phosphatases that are involved in a wide range of regulatory pathways, including those responsible for cytoskeletal structure, cell replication, stress responses and DNA repair. Acute effects of MC poisoning include intrahepatic haemorrhage. Chronic effects include a reduced efficacy of proliferation control mechanisms within tumour cells. Cases of liver damage have been reported from MCs in drinking-water that was inadequately treated. As well, human fatalities clearly attributable to MCs have occurred through renal dialysis using inadequately treated water, including drinking-water that contained high MC levels. In 2006, IARC classified MC-LR as a possible human carcinogen (Group 2B) based on evidence for tumour promotion. The provisional guideline values for MCs are based on a study in mice using MC-LR (the only congener with sufficiently comprehensive oral toxicity data for guideline value derivation), corroborated by a similar study in rats. The liver was identified as the most sensitive organ in these studies. Although some recent research has suggested that certain reproductive organs may be affected by prolonged exposure to lower concentrations, these reports suffer from a number of methodological and reporting deficiencies and, further, are contradicted by earlier studies. Therefore, these findings of adverse effects require corroboration. Practical considerations Where nutrient (phosphorus and nitrogen) concentra¬tions are elevated in lakes, reservoirs or slowly flowing rivers, cyanobacteria occur widely. Where their excessive growth leads to high biomass, sometimes termed “bloom” events, MCs can reach concentrations in raw water that are potentially hazardous to human health. Such blooms tend to recur in the same water bodies. Cells of some cyanobacterial species may accumulate at the surface as scums (particularly of Microcystis) or at the thermocline of thermally stratified reservoirs (i.e. Planktothrix rubescens). Such accumulations may develop rapidly and may be of very variable duration (hours to weeks). In many circumstances, blooms and accumulations are seasonal, whereas others occur perennially. Cyanobacteria are most effectively controlled in the context of developing a WSP (see chapter 4). Control measures to manage potential risks from cyanobacteria, and in particular from their toxins, in drinking-water should include not only adequate treatment, but also measures to control cyanobacterial bloom development. See section 11.5 for more information on cyanobacteria, including further details on monitoring cyanobacterial blooms, the alert level framework, and prevention and management of cyanobacteria in source waters. Effectively minimizing the formation of blooms and locating the raw water intake away from blooms reduce the treatment steps required to remove cyanotoxins. Drinking-water treatment that removes particles—that is, soil, slow sand or riverbank filtration, conventional water treatment (coagulation, flocculation and filtration or dissolved air flotation)—can remove cell-bound MCs effectively. Soil, slow sand and riverbank filtration can also remove dissolved cyanotoxins. For all these processes, it is important that they are optimized to target the removal of cells and dissolved toxins. Chlorination and ozonation at sufficiently high doses and contact times are effective for degrading dissolved MCs; however, elevated organic carbon in bloom situations will substantially increase the disinfectant demand. Chlorine dioxide and chloramine are ineffective for degrading MCs. Both for pre-oxidation and conventional treatment, cell rupture and toxin release should be avoided. GAC and PAC can be effective for removing dissolved MCs, with efficacy dependent on several factors, including the type of activated carbon, contact times (PAC), flow rates (GAC) and water quality. As the challenges that blooms present for treatment are complex, periodic validation of efficacy during bloom situations and under the specific local conditions is particularly important. Avoiding bloom occurrence and intake is therefore the preferred option. Molinate Molinate (CAS No. 2212-67-1) is a herbicide used to control broad-leaved and grassy weeds in rice. The available data suggest that groundwater pollution by molinate is restricted to some rice growing regions. Data on the occurrence of molinate in the environment are limited. Molinate is of low persistence in water and soil, with a half life of about 5 days. Guideline value 0.006 mg/l (6 μg/l) Occurrence Concentrations in water rarely exceed 1 μg/l TDI 2 μg/kg body weight, based on a NOAEL for reproductive toxicity in the rat of 0.2 mg/kg body weight, with an uncertainty factor of 100 (for interspecies and intraspecies variation) Limit of detection 0.01 μg/l by GC‑MS Treatment performance 0.001 mg/l should be achievable using GAC Guideline value derivation • allocation to water • weight • consumption 10% of TDI 60 kg adult 2 litres/day Assessment date 1993 Principal reference WHO (2003) Molinate in drinking-water On the basis of the limited information available, molinate does not seem to be carcinogenic or mutagenic in experimental animals. Evidence suggests that impairment of the reproductive performance of the male rat represents the most sensitive indicator of molinate exposure. However, epidemiological data based on the examination of workers involved in molinate production do not indicate any effect on human fertility. Molybdenum Molybdenum is found naturally in soil and is used in the manufacture of special steels and in the production of tungsten and pigments, and molybdenum compounds are used as lubricant additives and in agriculture to prevent molybdenum deficiency in crops. Concentrations in drinking-water are usually less than 0.01 mg/l, although concentrations as high as 200 μg/l have been reported in areas near mining sites. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 1993, revised in 2011 Principal references WHO (2011) Molybdenum in drinking-water Molybdenum is considered to be an essential element, with an estimated daily requirement of 0.1–0.3 mg for adults. As molybdenum occurs at very low concentrations in drinking-water, it is not considered necessary to set a formal guideline value. For guidance purposes, a health-based value can be derived. In a 2-year study of humans exposed via drinking-water, the NOAEL was found to be 0.2 mg/l, but there are some concerns about the quality of this study. As molybdenum is an essential element, a factor of 3 is considered to be adequate to reflect intraspecies variation. This gives a health-based value of 0.07 mg/l (rounded figure), which is in the same range as that derived on the basis of the results of toxicological studies in experimental animals and is consistent with the essential daily requirement for molybdenum. Monochloroacetic acid Chlorinated acetic acids are formed from organic material during water chlorination. Guideline value 0.02 mg/l (20 μg/l) Occurrence Present in surface water–derived drinking‑water at concentrations up to 82 μg/l (mean 2.1 μg/l) TDI 3.5 μg/kg body weight, based on a LOAEL of 3.5 mg/kg body weight per day from a study in which increased absolute and relative spleen weights were observed in male rats exposed to monochloroacetic acid in drinking‑water for 2 years, using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for use of a minimal LOAEL instead of a NOAEL and database deficiencies, including the lack of a multigeneration reproductive toxicity study) Limit of detection 2 μg/l by GC with ECD; 5 μg/l by GC‑MS Treatment performance No information available Guideline value derivation • allocation to water 20% of TDI • weight 60 kg adult• consumption 2 litres/day Assessment date 2003 Principal reference WHO (2004) Monochloroacetic acid in drinking-water No evidence of carcinogenicity of monochloroacetate was found in 2-year gavage bioassays with rats and mice. Monochloroacetate has given mixed results in a limited number of mutagenicity assays and has been negative for clastogenicity in genotoxicity studies. IARC has not classified the carcinogenicity of monochloroacetic acid. Monochlorobenzene Releases of monochlorobenzene (MCB) to the environment are thought to be mainly due to volatilization losses associated with its use as a solvent in pesticide formulations, as a degreasing agent and from other industrial applications. MCB has been detected in surface water, groundwater and drinking-water; mean concentrations were less than 1 μg/l in some potable water sources (maximum 5 μg/l) in Canada. The major source of human exposure is probably air. Reason for not Occurs in drinking‑water at concentrations well below those of health establishing a guideline concern, and health‑based value would far exceed lowest reported taste value and odour threshold Assessment date 2003 Principal reference WHO (2004) Monochlorobenzene in drinking-water MCB is of low acute toxicity. Oral exposure to high doses of MCB results in effects mainly on the liver, kidneys and haematopoietic system. There is limited evidence of carcinogenicity in male rats, with high doses increasing the occurrence of neoplastic nodules in the liver. The majority of evidence suggests that MCB is not mutagenic; although it binds to DNA in vivo, the level of binding is low. A health-based value of 300 μg/l can be calculated for MCB on the basis of a TDI of 85.7 μg/kg body weight, based on neoplastic nodules identified in a 2-year rat study with dosing by gavage, and taking into consideration the limited evidence of carcinogenicity. However, because MCB occurs at concentrations well below those of health concern, it is not considered necessary to derive a formal guideline value. It should also be noted that the health-based value far exceeds the lowest reported taste and odour threshold for MCB in water. MX MX, which is the common name for 3-chloro-4-dichloromethyl-5-hydroxy-2(5H)-furanone, is formed by the reaction of chlorine with complex organic matter in drinking-water. It has been identified in chlorinated humic acid solutions and drinking-water in Finland, the United Kingdom and the USA and was found to be present in 37 water sources at levels of 2–67 ng/l. Five drinking-water samples from different Japanese cities contained MX at concentrations ranging from less than 3 to 9 ng/l. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal references IPCS (2000) Disinfectants and disinfectant by-products WHO (2003) MX in drinking-water MX is a potent mutagen in bacteria and in cells in vitro and has undergone a lifetime study in rats in which some tumorigenic responses were observed. These data indicate that MX induces thyroid and bile duct tumours. IARC has classified MX in Group 2B (possibly carcinogenic to humans) on the basis of rat tumorigenicity and its strong mutagenicity. A health-based value of 1.8 μg/l can be calculated for MX on the basis of the increase in cholangiomas and cholangiocarcinomas in female rats using the linearized multistage model (without a body surface area correction). However, this is significantly above the concentrations that would be found in drinking-water, and, in view of the analytical difficulties in measuring this compound at such low concentrations, it is considered unnecessary to propose a formal guideline value for MX in drinking-water. Nickel Nickel is a naturally occurring element, which is used mainly in the production of stainless steel and nickel alloys. Food is the dominant source of nickel exposure in the non-smoking, non-occupationally exposed population; water is generally a minor contributor to the total daily oral intake. However, the nickel contribution from water may be significant where there is heavy pollution, where nickel that occurs naturally in groundwater is mobilized, or where nickel leaches from nickel- or chromium-plated taps or stainless steel devices or materials that are in contact with water. The primary source of nickel in drinking-water is leaching from metals that are in contact with drinking-water. Guideline value 0.07 mg/l (70 μg/l) Occurrence Concentration in drinking‑water are typically less than 25 μg/l, although concentrations may be elevated (up to 5 mg/l) where nickel is released from metal alloys that are in contact with drinking‑water, such as fittings, including taps, or as a result of anthropogenic contamination or mobilization from natural deposits Basis of guideline value derivation The guideline value is based on achievability by available source control measures and treatment technologies, measurability by analytical methods and toxicology. Reproductive toxicity (increased incidence of rat litters with post‑implantation loss) was considered the most sensitive end‑point and is therefore the basis for the TDI derivation of 13 μg/kg body weight. An uncertainty factor of 100 was applied to account for interspecies differences (10) and intraspecies variation (10) to the BDML10 of 1.3 mg/kg bw per day, from a two‑generation study in rats. Although the 2021 risk assessment supports a health‑based value of 80 μg/l based on this TDI, the guideline value was retained at 70 μg/l considering the above factors. Further, 80 μg/l is only slightly higher than the previous guideline value of 70 μg/l (established in 2014); factoring in the imprecision inherent in risk assessment procedures, this difference is not judged significant enough to warrant a revised, minimally relaxed guideline value. Limit of detection 0.5–5 μg/l for ICP‑MS, ICP‑AES and graphite furnace AAS; 0.1 mg/l for flame AAS Prevention and treatment For surface water, conventional water treatment (coagulation, sedimentation, filtration) may be effective under certain circumstances, depending on a number of factors, including the coagulant dosage and pH. For groundwater, ion‑exchange resins have been shown to be effective. The most important means of control is by product specifications through an appropriate certification scheme for materials in contact with drinking‑water. Consumers, particularly nickel‑sensitive people, should flush chromium‑ or nickel‑plated taps before using the water, particularly after periods of stagnation. Health‑based value derivation • allocation to water 20% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments The guideline value is also protective of any possible acute effects, including systemic contact dermatitis As nickel is usually found in drinking‑water below the guideline value, monitoring and inclusion in drinking‑water regulations and standards would usually only be necessary if there were indications that a specific pollution or problem might exist Assessment date 2021 Principal reference WHO (2021) Nickel in drinking-water EFSA (2020) Update of the risk assessment of nickel in food IARC concluded that inhaled nickel compounds are carcinogenic to humans (Group 1) and that metallic nickel is possibly carcinogenic (Group 2B). However, there is a lack of evidence of a carcinogenic risk from oral exposure to nickel. In a well-conducted two-generation reproductive study in rats administered nickel by gavage, an increase in the frequency of litters with post-implantation loss was reported; thus, reproductive and developmental toxicity was identified as the relevant and sensitive end-point for derivation of the health-based value. Exposure to nickel through the skin or by inhalation may lead to nickel sensitization. Whereas oral ingestion of nickel is not known to lead to systemic contact dermatitis (SCD) in the general population, it can elicit SCD in nickel-sensitized individuals. However, the guideline value of 70 μg/L is considered adequately protective of SCD that may result from drinking-water exposure based on a margin of exposure (MOE) assessment. Acute consumption of water containing nickel at the chronic health-based value of 80 μg/l would result in an MOE of approximately 16. The MOE of 16 was calculated from an acute LOAEL for SCD of 4.3 μg/kg bw and an acute exposure of 0.27 μg/kg bw (based on a scenario of a person weighing 60 kg, drinking a glass of tap water (about 200 ml) containing nickel at the health-based value of 80 μg/l). The guideline value is further considered adequately protective considering that SCD elicitation was associated with a bolus exposure, in contrast to the intermittent nature of a drinking-water exposure scenario. Nitrate and nitrite1 Nitrate (NO3−) is found naturally in the environment and is an important plant nutrient. It is present at varying concentrations in all plants and is a part of the nitrogen cycle. Nitrite (NO2−) is not usually present in significant concentrations except in a reducing environment, because nitrate is the more stable oxidation state. It can be formed by the microbial reduction of nitrate and in vivo by reduction from ingested nitrate. Nitrite can also be formed chemically in distribution pipes by Nitrosomonas bacteria during stagnation of nitrate-containing and oxygen-poor drinking-water in galvanized steel pipes, or if chloramination is used to provide a residual disinfectant. An excess of free ammonia entering the distribution system can lead to nitrification and the potential increase of nitrate and nitrite in drinking-water. Nitrate can reach both surface water and groundwater as a consequence of agricultural activity (including excess application of inorganic nitrogenous fertilizers and manures), from wastewater disposal and from oxidation of nitrogenous waste products in human and other animal excreta, including septic tanks. Nitrate can also occasionally reach groundwater as a consequence of natural vegetation. Surface water nitrate concentrations can change rapidly owing to surface runoff of fertilizer, uptake by phytoplankton and denitrification by bacteria, but groundwater concentrations generally show relatively slow changes. Nitrate and nitrite can also be produced as a result of nitrification in source water or distribution systems. In general, the most important source of human exposure to nitrate and nitrite is through vegetables (nitrate and nitrite) and through meat in the diet (nitrite is used as a preservative in many cured meats). In some circumstances, however, drinking-water can make a significant contribution to nitrate and, occasionally, nitrite intake. In the case of bottle-fed infants, drinking-water can be the major external source of exposure to nitrate and nitrite. 2 Guideline values2 Nitrate: 50 mg/l as nitrate ion, to be protective against methaemoglobinaemia and thyroid effects in the most sensitive subpopulation, bottle‑fed infants, and, consequently, other population subgroups Nitrite: 3 mg/l as nitrite ion, to be protective against methaemoglobinaemia induced by nitrite from both endogenous and exogenous sources in bottle‑fed infants, the most sensitive subpopulation, and, consequently, the general population Combined nitrate plus nitrite: The sum of the ratios of the concentrations of each of nitrate and nitrite to its guideline value should not exceed 1 Occurrence Nitrate levels vary significantly, but levels in well water are often higher than those in surface water and, unless heavily influenced by surface water, are less likely to fluctuate. Concentrations often approach or exceed 50 mg/l where there are significant sources of contamination. Nitrite levels are normally lower, less than a few milligrams per litre. 1 As nitrate and nitrite are chemicals of significant concern in some natural waters, the chemical fact sheet on nitrate and nitrite has been expanded. 2 Conversion factors: 1 mg/l as nitrate = 0.226 mg/l as nitrate-nitrogen; 1 mg/l as nitrite = 0.304 mg/l as nitrite-nitrogen. Basis of guideline value Nitrate (bottle-fed infants): In epidemiological studies, no adverse health derivation effects (methaemoglobinaemia or thyroid effects) were reported in infants in areas where drinking‑water consistently contained nitrate at concentrations below 50 mg/l Nitrite (bottle-fed infants): Based on: 1) no incidence of methaemoglobinaemia at nitrate concentrations below 50 mg/l (as nitrate ion) in drinking‑water for bottle‑fed infants less than 6 months of age (assuming body weight of 2 kg); 2) converting 50 mg/l as nitrate to corresponding molar concentration for nitrite; 3) multiplying by a factor of 0.1 to account for the estimated conversion rate of nitrate to nitrite in infants where nitrite is formed endogenously from nitrate at a rate of 5–10%; and 4) multiplying by a source allocation factor for drinking‑water of 100% or 1, as a bottle‑fed infant’s primary exposure to nitrite is through consumption of formula reconstituted with drinking‑water that contains nitrate or nitrite. As the guideline value is based on the most sensitive subgroup of the population (bottle‑fed infants less than 6 months of age), application of an uncertainty factor is not deemed necessary. Combined nitrate plus nitrite: To account for the possibility of the simultaneous occurrence of nitrate and nitrite in drinking‑water Limit of detection MDLs of 0.009 mg/l as nitrate ion and 0.013 mg/l as nitrite ion by IC; MDL of 0.04–4.4 mg/l as nitrate ion by automated cadmium reduction with colorimetry (recommended for the analysis of nitrate at concentrations below 0.4 mg/l) Treatment performance Nitrate: Effective central treatment technologies involve the physical/ chemical and biological removal of nitrate and include ion exchange, reverse osmosis, biological denitrification and electrodialysis, which are capable of removing over 80% of nitrate from water to achieve effluent nitrate concentrations as low as 13 mg/l; conventional treatment processes (coagulation, sedimentation, filtration and chlorination) are not effective Nitrite: Treatment usually focuses on nitrate, because nitrite is readily converted to nitrate by many disinfectants Additional comments The guideline values for both nitrate and nitrite are based on short‑term effects; however, they are also considered protective for any possible longterm effects . Methaemoglobinaemia is complicated by the presence of microbial contamination and subsequent gastrointestinal infection, which can increase the risk for bottle‑fed infants significantly. Authorities should therefore be all the more vigilant that water to be used for bottle‑fed infants is microbiologically safe when nitrate is present at concentrations near or above the guideline value. It is particularly important to ensure that these infants are not currently exhibiting symptoms of gastrointestinal infection (diarrhoea). Also, as excessive boiling of water to ensure microbiological safety can concentrate levels of nitrate in the water, care should be taken to ensure that water is heated only until it reaches a rolling boil. In extreme situations, alternative sources of water (e.g. bottled water) can be used. Nitrite is relatively unstable and can be rapidly oxidized to nitrate. Nitrite can occur in the distribution system at higher concentrations when chloramination is used, but the occurrence is almost invariably intermittent. Methaemoglobinaemia is therefore the most important consideration, and the guideline value derived for protection against methaemoglobinaemia would be the most appropriate under these circumstances, allowing for any nitrate that may also be present. All water systems that practise chloramination should closely and regularly monitor their systems to verify disinfectant levels, microbiological quality and nitrite levels. If nitrification is detected (e.g. reduced disinfectant residuals and increased nitrite levels), steps can be taken to modify the treatment train or water chemistry in order to minimize nitrite formation. Effective disinfection must never be compromised. Excessively high levels may occur in small supplies; where this is suspected from the risk assessment, testing may be appropriate. Assessment date 2016 Principal references Health Canada (2013). Guidelines for Canadian Drinking Water Quality: Guideline Technical Document – Nitrate and nitrite WHO (2016). Nitrate and nitrite in drinking-water Absorption of nitrate ingested from vegetables, meat or water is rapid and in excess of 90%; final excretion is in the urine. In humans, about 25% of ingested nitrate is recirculated in saliva, of which about 20% is converted to nitrite by the action of bacteria in the mouth. There is also endogenous formation of nitrate from nitric oxide and protein breakdown as part of normal metabolism. In normal healthy adults, this endogenous synthesis leads to the excretion of about 62 mg of nitrate ion per day in the urine. Endogenous formation of nitrate or nitrite can be significantly increased in the presence of infections, particularly gastrointestinal infections. When nitrate intake is low, endogenous formation may be the major source of nitrate in the body. Nitrate metabolism is different in humans and rats, as rats may not actively secrete nitrate in their saliva. Nitrate probably has a role in protecting the gastrointestinal tract against a variety of gastrointestinal pathogens, as nitrous oxide and acidified nitrite have antibacterial properties. It may have other beneficial physiological roles. Hence, there may be a benefit from exogenous nitrate uptake, and there remains a need to balance the potential risks with the potential benefits. Significant bacterial reduction of nitrate to nitrite does not normally take place in the stomach, except in individuals with low gastric acidity or with gastrointestinal infections. These may include individuals using antacids, particularly those that block acid secretion. In humans, methaemoglobinaemia is a consequence of the reaction of nitrite with haemoglobin in the red blood cells to form methaemoglobin, which binds oxygen tightly and does not release it, thus blocking oxygen transport. Although most absorbed nitrite is oxidized to nitrate in the blood, residual nitrite can react with haemoglobin. High levels of methaemoglobin (>10%) formation in infants can give rise to cyanosis, referred to as blue-baby syndrome. Although clinically significant methaemoglobinaemia can occur as a result of extremely high nitrate intake in adults and children, the most familiar situation is its occurrence in bottle-fed infants. This was considered to be primarily a consequence of high levels of nitrate in water, although there have been cases of methaemoglobinaemia in weaned infants, associated with high nitrate intake from vegetables. Bottle-fed infants are considered to be at greater risk because the intake of water in relation to body weight is high and, in infants, the development of repair enzymes is limited. In clinical epidemiological studies of methaemoglobinaemia and subclinical increases in methaemoglobin levels associated with drinking-water nitrate, 97% of cases occurred at concentrations in excess of 44.3 mg/l, with clinical symptoms associated with the higher concentrations. The affected individuals were almost exclusively under 3 months of age. Although drinking-water nitrate may be an important risk factor for methaemoglobinaemia in bottle-fed infants, there is compelling evidence that the risk of methaemoglobinaemia is primarily increased in the presence of simultaneous gastrointestinal infections, which increase endogenous nitrite formation, may increase reduction of nitrate to nitrite and may also increase the intake of water in combat-ting dehydration. Cases have been described in which gastrointestinal infection seems to have been the primary cause of methaemoglobinaemia. Most cases of methaemoglobinaemia reported in the literature are associated with contaminated private wells (predominantly when the drinking-water is anaerobic) that also have a high probability of microbial contamination, which should not occur if it is properly disinfected. Although numerous epidemiological studies have investigated the relationship between exposure to nitrate or nitrite in drinking-water and cancer occurrence, the weight of evidence does not support an association between cancer and exposure to nitrate or nitrite per se. Nitrite can react with nitrosatable compounds, primarily secondary amines, in the body to form N-nitroso compounds. A number of these are considered to be carcinogenic to humans, whereas others, such as N-nitrosoproline, are not. Several studies have been carried out on the formation of N-nitroso compounds in relation to nitrate intake in humans, but there is large variation in the intake of nitrosatable compounds and in gastric physiology. Higher mean levels of N-nitroso compounds, along with high nitrate levels, have been found in the gastric juice of individuals who are achlorhydric (i.e. have very low levels of hydrochloric acid in the stomach). However, other studies have been largely inconclusive, and there appears to be no clear relationship with drinking-water nitrate compared with overall nitrate intake in relation to formation of N-nitroso compounds. Moderate consumption of a number of dietary antioxidant components, such as ascorbic acid and green tea, appears to reduce endogenous N-nitrosamine formation. A significant number of epidemiological studies have been carried out on the association of nitrate intake with primarily gastric cancers. Although the epidemiological data are considered to be inadequate to allow definitive conclusions to be drawn regarding all cancers, there is no convincing evidence of a causal association with any cancer site. The weight of evidence indicates that there is unlikely to be a causal association between gastric cancer and nitrate in drinking-water. This is consistent with the conclusion by IARC that ingested nitrate or nitrite under conditions that result in endogenous nitrosation is probably carcinogenic to humans (Group 2A), but not nitrate alone. There have been suggestions that nitrate in drinking-water could be associated with congenital malformations, but the overall weight of evidence does not support this. Nitrate appears to competitively inhibit iodine uptake, with the potential for an adverse effect on the thyroid. Current evidence also suggests that exposure to nitrate in drinking-water may alter human thyroid gland function by competitively inhibiting thyroidal iodide uptake, leading to altered thyroid hormone concentrations and functions. Although studies found that exposure to nitrate concentrations above 50 mg/l are weakly associated with altered thyroid function, the evidence is limited, conflicting and based on studies with important methodological limitations. Mode of action data suggest that pregnant women and infants are the most sensitive populations, owing primarily to the importance of adequate thyroid hormones for normal neurodevelopment in the fetus and infant, but also to increased thyroid hormone turnover and low intrathyroidal stores in fetal and early life. There have been suggestions of an association between nitrate in drinking-water and the incidence of childhood diabetes mellitus. However, subsequent studies have not found a significant relationship, and no mechanism has been identified. In some studies on rats treated with high doses of nitrite, a dose-related hypertrophy of the zona glomerulosa of the adrenal was seen; one strain of rats appeared to be more sensitive than others. However, this minimal hyperplasia was considered to be due to physiological adaptation to small fluctuations in blood pressure in response to high nitrite doses. Nitrate is not carcinogenic in laboratory animals. Nitrite has been frequently studied, and there have been suggestions of carcinogenic activity, but only at very high doses. The most recent long-term studies have shown only equivocal evidence of carcinogenicity in the forestomach of female mice, but not in rats or male mice. In view of the lack of evidence for genotoxicity, this led to the conclusion that sodium nitrite was not carcinogenic in mice and rats. In addition, as humans do not possess a forestomach and the doses were high, the significance of these data for humans is very doubtful. The guideline value for nitrate of 50 mg/l, as nitrate ion, is based on an absence of health effects (methaemoglobinaemia and thyroid effects) in epidemiological studies and is protective for bottle-fed infants and, consequently, other parts of the population. Methaemoglobinaemia is complicated by the presence of microbial contamination and subsequent gastrointestinal infection, which can increase the risk for this group significantly. Authorities should therefore be all the more vigilant that water to be used for bottle-fed infants is microbiologically safe when nitrate is present at concentrations near the guideline value. It is particularly important to ensure that these infants are not currently exhibiting symptoms of significant gastrointestinal infection (diarrhoea). Also, as excessive boiling of water to ensure microbiological safety can concentrate levels of nitrate in the water, care should be taken to ensure that water is heated only until it reaches a rolling boil. In extreme situations, alternative sources of water (e.g. bottled water) can be used. The guideline for nitrite of 3 mg/l, as nitrite ion, is based on: 1) no incidence of methaemoglobinaemia at nitrate concentrations below 50 mg/l in drinking-water for bottle-fed infants less than 6 months of age (assuming body weight of 2 kg), 2) converting 50 mg/l nitrate to the corresponding molar concentration for nitrite, 3) multiplying by a factor of 0.1 to account for the estimated conversion rate of nitrate to nitrite in infants where nitrite is formed endogenously from nitrate at a rate of 5–10% and 4) multiplying by a source allocation factor for drinking water of 100% or 1, as a bottle-fed infant’s primary exposure to nitrite is through consumption of formula reconstituted with nitrate- or nitrite-containing drinking-water. As the health-based value is based on the most sensitive subgroup of the population (bottle-fed infants less than 6 months of age), application of an uncertainty factor is not deemed necessary. Because of the possibility of the simultaneous occurrence of nitrate and nitrite in drinking-water, the sum of the ratios of the concentration (C) of each to its guideline value (GV) should not exceed 1: Cnitrate Cnitrite + ≤1GVnitrate GVnitrite The guideline values are based on short-term effects; however, they are also considered protective for long-term effects. Practical considerations The most appropriate means of controlling nitrate concentrations, particularly in groundwater, is the prevention of contamination. This may take the form of appropriate management of agricultural practices (e.g. management of fertilizer and manure application and storage of animal manures) and sanitation practices (e.g. the careful siting of pit latrines and septic tanks, sewer leakage control). Methaemoglobinaemia has most frequently been associated with private wells. It is particularly important to ensure that septic tanks and pit latrines are not sited near a well or where a well is to be dug and to ensure that animal manure is kept at a sufficient distance to ensure that runoff cannot enter the well or the ground near the well. It is particularly important that the household use of manures and fertilizers on small plots near wells should be managed with care to avoid potential contamination. The well should be sufficiently protected to prevent runoff from entering the well. Where there are elevated concentrations of nitrate or where inspection of the well indicated that there are sources of nitrate close by that could be causing contamination, particularly where there are also indications that microbiological quality might also be poor, a number of actions can be taken. As noted above, water should be heated only until the water reaches a rolling boil or disinfected by an appropriate means before consumption. Where alternative supplies are available for bottle-fed infants, these can be used, taking care to ensure that they are microbiologically safe. Steps should then be taken to protect the well and ensure that sources of both nitrate and microbial contamination are removed from the vicinity of the well. In areas where household wells are common, health authorities may wish to take a number of steps to ensure that nitrate contamination is not or does not become a problem. Such steps could include targeting mothers, particularly expectant mothers, with appropriate information about water safety, assisting with visual inspection of wells to determine whether a problem may exist, providing testing facilities where a problem is suspected, providing guidance on disinfecting water or, where nitrate levels are particularly high, providing bottled water from safe sources or providing advice as to where such water can be obtained. With regard to piped supplies, where nitrate is present, the first potential approach to treatment of drinking-water supplies, if source substitution is not feasible, is to dilute the contaminated water with a low-nitrate source. Where blending is not feasible, a number of treatment techniques are available for drinking-water. The first is disinfection, which may serve to oxidize nitrite to the less toxic nitrate as well as minimize the pathogenic and non-pathogenic reducing bacterial population in the water. Nitrate removal methods include ion exchange, biological denitrification, reverse osmosis and electrodialysis. However, there are disadvantages associated with all of these approaches, including cost, operational complexities and the need for disposal of resin, brine or reject water. Conventional municipal water treatment processes (coagulation, sedimentation, filtration and chlorination) are not effective for nitrate removal, as nitrate is a stable and highly soluble ion with low potential for co-precipitation and adsorption. In systems with a water source containing naturally occurring ammonia or that add ammonia for chloramination, free ammonia entering the distribution system can be one of the causative factors of nitrification and the potential increase of nitrate and nitrite in the distribution system. Care should be taken with the use of chloramination for providing a residual disinfectant in the distribution system. It is important to manage this to minimize nitrite formation, either in the main distribution system or in the distribution systems of buildings. Nitrilotriacetic acid Nitrilotriacetic acid, or NTA, is used primarily in laundry detergents as a replacement for phosphates and in the treatment of boiler water to prevent accumulation of mineral scale. Guideline value 0.2 mg/l (200 μg/l) Occurrence Concentrations in drinking‑water usually do not exceed a few micrograms per litre, although concentrations as high as 35 μg/l have been measured TDI 10 μg/kg body weight, based on nephritis and nephrosis in a 2‑year study in rats and using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for carcinogenic potential at high doses) Limit of detection 0.2 μg/l using GC with a nitrogen‑specific detector Treatment performance No information found on removal from water Guideline value derivation • allocation to water 50% of TDI • weight 60 kg adult• consumption 2 litres/day Assessment date 1993 Principal reference WHO (2003) Nitrilotriacetic acid in drinking-water NTA is not metabolized in experimental animals and is rapidly eliminated, although some may be briefly retained in bone. It is of low acute toxicity to experimental animals, but it has been shown to produce kidney tumours in rodents following longterm exposure to doses higher than those required to produce nephrotoxicity. IARC has placed NTA in Group 2B (possibly carcinogenic to humans). It is not genotoxic, and the reported induction of tumours is believed to be due to cytotoxicity resulting from the chelation of divalent cations such as zinc and calcium in the urinary tract, leading to the development of hyperplasia and subsequently neoplasia. Nitrobenzene Nitrobenzene is used primarily in the production of aniline, but it is also used as a solvent, as an ingredient of metal polishes and soaps and in the synthesis of other organic compounds, including acetaminophen. Nitrobenzene can be released to water during these production processes. Concentrations of nitrobenzene in environmental samples, such as surface water, groundwater and air, are generally low, except in areas with industrial pollution. Based on limited data, it appears that the potential for contamination is greater for groundwater than for surface water. The general population can be exposed to variable concentrations of nitrobenzene in air and possibly drinking-water. Only populations in the vicinity of manufacturing activities and petroleum refining plants are likely to have any significant exposure to nitrobenzene; however, people living in and around abandoned hazardous waste sites may also have potential for higher exposure, due to possible groundwater and soil contamination and uptake of nitrobenzene by plants. Reason for not establishing Rarely found in drinking‑water at concentrations of health concern a guideline value Assessment date 2009 Principal reference WHO (2009) Nitrobenzene in drinking-water Nitrobenzene is toxic to humans by the inhalation, dermal and oral routes of exposure. The main systemic effect associated with human exposure to nitrobenzene is methaemoglobinaemia. Although some recent studies have reported positive results in mutagenicity tests, it cannot be excluded that nitrobenzene is a non-genotoxic chemical. No long-term oral administration studies are available. Based on inhalation studies, IARC concluded that there was inadequate evidence in humans but sufficient evidence in experimental animals for the carcinogenicity of nitrobenzene and classified nitrobenzene in Group 2B (possibly carcinogenic to humans). Because nitrobenzene occurrence in drinking-water at concentrations above trace levels is infrequent, it is not considered necessary to derive a formal guideline value. However, health-based values can be calculated to provide guidance in the event of spills and where there are higher concentrations in industrial areas. Two health-based values are derived based on the limited available information: one for short-term exposure (30 μg/l) and the other for long-term exposure (8–63 μg/l, depending on end-point and approach used). It should be emphasized that the derivation of the long-term health-based values includes large uncertainties because of the dose metric conversion from inhalation studies and the possibility of increased metabolism to aniline in the gastrointestinal tract. It should be emphasized that nitrobenzene is a potent methaemoglobinaemic agent in humans, which is of particular concern for bottle-fed infants. Currently, data are not adequate to determine a separate health-based value for this end-point. It should also be noted that the reported odour threshold for nitrobenzene in water is 30–110 μg/l. N-Nitrosodimethylamine N-Nitrosodimethylamine, or NDMA, can occur in drinking-water through the degradation of dimethylhydrazine (a component of rocket fuel) as well as from several other industrial processes. It is also a contaminant of certain pesticides. NDMA has recently been identified as a disinfection by-product of chloramination (by the reaction of monochloramine with dimethylamine, a ubiquitous component of waters affected by wastewater discharges) and, to some extent, chlorination. NDMA can also be formed as a by-product of anion exchange treatment of water. Guideline value 0.0001 mg/l (0.1 μg/l) Occurrence Where chloramination is used, distribution system samples can have much higher levels of NDMA than the finished water at the treatment plant; levels as high as 0.16 μg/l have been measured in the distribution system, but concentrations in water at the treatment plant are generally less than 0.01 μg/l Basis of guideline value Hepatic biliary cystadenomas in female rats, the most sensitive carcinogenic derivation end‑point, observed in a drinking‑water study, using a multistage model Limit of detection 0.028 ng/l by capillary column GC and chemical ionization tandem MS; 0.4 ng/l by capillary column GC and high‑resolution MS; 0.7–1.6 ng/l by GC‑MS and ammonia positive chemical ionization detection Treatment performance The most common process for NDMA removal is UV irradiation. A concentration below 0.005 μg/l should be achievable by UV irradiation provided that the water is not grossly contaminated. NDMA is not removable by air stripping, activated carbon adsorption, reverse osmosis or biodegradation. Additional comments Potential methods for reducing the formation of NDMA during disinfection include avoiding the use of chloramination, use of breakpoint chlorination and removal of ammonia prior to chlorination. Assessment date 2006 Principal references IPCS (2002) N-Nitrosodimethylamine WHO (2008) N-Nitrosodimethylamine in drinking-water There is conclusive evidence that NDMA is a potent carcinogen in experimental animals by several routes of exposure, including through ingestion of drinking-water. NDMA has been classified by IARC as probably carcinogenic to humans. The mechanism by which NDMA produces cancer is well understood to involve biotransformation by liver microsomal enzymes, generating the methyldiazonium ion. This reactive metabolite forms DNA adducts, with most evidence pointing to O6-methylguanine as the likely proximal carcinogenic agent. As a consequence of the clear evidence of carcinogenicity, there have been few studies of other possible toxicity end-points. There is also ample evidence that NDMA is genotoxic both in vivo and in vitro. Activation by liver microsomal S9 fractions is necessary for a positive in vitro result. The recent observation that human S9 fractions are much more active in promoting genotoxicity in the Ames test than rat S9 fractions suggests that humans may be especially sensitive to the carcinogenicity of NDMA. Although there have been several case–control studies and one cohort study of NDMA in humans, none of them can be used to derive a quantitative risk of cancer. The results are supportive of the assumption that NDMA consumption is positively associated with either gastric or colorectal cancer. However, none of the studies focused on drinking-water as the route of exposure; instead, they used estimations of total dietary intake of NDMA. Organotins The group of chemicals known as the organotins is composed of a large number of compounds with differing properties and applications. Uses for the mono- and disubstituted compounds include as heat stabilizers in plastics, including PVC and chlorinated PVC (CPVC) water pipes. The tri-substituted compounds have been widely used as biocides and as antifouling agents in paint. Reason for not establishing TBT, TPT, DBT and DOT occur in drinking‑water or drinking‑water a guideline value sources at concentrations well below those of health concern Establishing a guideline value for MMT, DMT and DMTC is unnecessary because their use as stabilizers in PVC and CPVC is normally controlled by product specifications For other organotins, available data inadequate to permit derivation of health‑based guideline values Health‑based value* Sum of TBT, TPT, DBT and DOT: 1.5 μg/l (equivalent to approximately 0.6 μg/l tin) Occurrence Concentrations in drinking‑water from PVC pipes are usually below a few hundred nanograms. However, many organotins have been detected as contaminants in environmental waters, including fresh waters that are possible drinking‑water sources; the concentrations were mostly between the detection limits and hundreds of nanograms per litre. TDI Sum of TBT, TPT, DBT and DOT: 0.25 μg/kg bw (0.1 μg/kg bw as tin) derived by applying an uncertainty factor of 100 to account for interspecies (10) and intraspecies (10) variation to the NOAEL for tributyltin oxide (the reference organotin), based on chronic immunotoxicity studies in rats Limit of detection 24–51 pg as tin for six organotin compounds (chlorides of DMT, DBT, TMT, TBT, DPT and TPT) by ICP‑MS; 0.2–0.4 ng/l for DBT, TBT, DPT and TPT; and 2 ng/l for MPT, by derivatization–liquid extraction followed by GC and MS Prevention and treatment The effectiveness of water treatment appears to be significantly different for different compounds. For example, conventional water treatment (coagulation, sedimentation, filtration) has been shown to be effective in removing TBT in shipyard waters under optimal conditions, whereas it was ineffective in removing DPT and TPT. In contrast, advanced treatment processes were effective in removing these compounds but ineffective in removing other organotin compounds. However, available data and quantification information are still limited. Where the organotins originate from plastic service water pipes and fittings, particularly monoalkyltins and dialkyltins in PVC and CPVC pipes and fittings, the most important means of control is by product specifications through an appropriate certification scheme. Health‑based value derivation • allocation to water 20% of TDI • weight 60 kg adult• consumption 2 litres/day Assessment date 2020 Principal reference WHO (2020) Organotins in drinking-water DBT, dibutyltin; DMT, dimethyltin; DMTC, dimethyltin dichloride; DOT, di‑n‑octyltin; DPT, diphenyltin; MMT, monomethyltin; MPT, monophenyltin; TBT, tributyltin; TMT, trimethyltin; TPT, triphenyltin * When a formal guideline value is not established, a “health‑based value” may be determined in order to provide guidance to Member States when there is reason for local concern. Establishing a formal guideline value for such substances may encourage Member States to incorporate a value into their national standards when this may be unnecessary. Reliable lifetime TDI values for monomethyltin, dimethyltin and dimethyltin dichloride could not be derived because of a lack of long-term studies with systematic experimental data. However, there is no need to establish a guideline value for these organotins because their use as stabilizers in PVC and CPVC is normally controlled by product specifications. The data available are insufficient to permit the proposal of guideline or health-based values for other organotins, including trimethyltin, tetrabutyltin, mono-n-octyltin, tetraocyltin, monophenyltin, diphenyltin and tetraphenyltin. Parathion Parathion (CAS No. 56-38-2) is a non-systemic insecticide that is used in many countries throughout the world. It is used as a fumigant and acaricide and as a pre-harvest soil and foliage treatment on a wide variety of crops, both outdoors and in greenhouses. Parathion released to the environment will adsorb strongly to the top layer of soil and is not likely to leach significantly. Parathion disappears from surface waters in about a week. The general population is not usually exposed to parathion from air or water. Parathion residues in food are the main source of exposure. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal references FAO/WHO (1996) Pesticide residues in food—1995 evaluations WHO (2004) Parathion in drinking-water Parathion inhibits cholinesterase activity in all species tested. There has been no evidence of carcinogenicity in 2-year rat studies. JMPR concluded that parathion is not genotoxic. A health-based value of 10 μg/l can be calculated for parathion on the basis of an ADI of 0–0.004 mg/kg body weight based on a NOAEL of 0.4 mg/kg body weight per day in a 2-year study in rats for retinal atrophy and inhibition of brain acetylcholinesterase at the next higher dose, and using an uncertainty factor of 100 for interspecies and intraspecies variation. Lower NOAELs in experimental animals, based only on inhibition of erythrocyte or brain acetylcholinesterase, were not considered relevant because of the availability of a NOAEL for erythrocyte acetylcholinesterase inhibition in humans, which was 0.1 mg/kg body weight per day. Intake of parathion from all sources is generally low and well below the upper limit of the ADI. As the health-based value is much higher than concentrations of parathion likely to be found in drinking-water, the presence of parathion in drinking-water under usual conditions is unlikely to represent a hazard to human health. For this reason, the establishment of a formal guideline value for parathion is not deemed necessary. Pendimethalin Pendimethalin (CAS No. 40487-42-1) is a pre-emergence herbicide that is fairly immobile and persistent in soil. It is used in large amounts in Japan (5000 tonnes per year). It is lost through photodegradation, biodegradation and volatilization. The leaching potential of pendimethalin appears to be very low, but little is known about its more polar degradation products. Guideline value 0.02 mg/l (20 μg/l) Occurrence Rarely found in drinking‑water in the limited studies available TDI 5 μg/kg body weight, based on evidence of slight liver toxicity even at the lowest dose tested (5 mg/kg body weight) in a long‑term rat feeding study, with an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for a combination of the use of a LOAEL instead of a NOAEL and limitations of the database) Limit of detection 0.01 μg/l by GC‑MS Treatment performance 1 μg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Assessment date 1993 Principal reference WHO (2003) Pendimethalin in drinking-water In a short-term dietary study in rats, a variety of indications of hepatotoxicity as well as increased kidney weights in males were observed at the highest dose level. In a long-term dietary study, some toxic effects (hyperglycaemia in the mouse and hepatotoxicity in the rat) were present even at the lowest dose level. On the basis of available data, pendimethalin does not appear to have significant mutagenic activity. Long-term studies in mice and rats have not provided evidence of carcinogenicity; however, these studies have some important methodological limitations. Pentachlorophenol Pentachlorophenol (CAS No. 87-86-5), or PCP, and other chlorophenols are used primarily for protecting wood from fungal growth. Food is usually the major source of exposure to PCP unless there is a specific local contamination of drinking-water by PCP or exposure from log homes treated with PCP. Provisional guideline 0.009 mg/l (9 μg/l) value The guideline value is considered provisional because of the variations in metabolism between experimental animals and humans. Occurrence Concentrations in water samples are usually below 10 μg/l, although much higher concentrations in groundwater may be measured under certain conditions Basis of guideline value Multistage modelling of tumour incidence in an NTP bioassay without derivation incorporation of a body surface area correction, recognizing that there are interspecies differences in metabolism between experimental animals and humans, with an important metabolite formed in rats being only a minor metabolite in humans Limit of detection 0.005–0.01 μg/l by GC with ECD Treatment performance 0.4 μg/l should be achievable using GAC Additional comments The concentration of PCP associated with a 10−5 upper‑bound excess lifetime cancer risk is similar to the guideline value established in the second edition, so that guideline value is retained. Assessment date 1998 Principal reference WHO (2003) Pentachlorophenol in drinking-water IARC classified PCP in Group 2B (possibly carcinogenic to humans) on the basis of inadequate evidence of carcinogenicity in humans but sufficient evidence in experimental animals. There is suggestive, although inconclusive, evidence of the carcinogenicity of PCP from epidemiological studies of populations exposed to mixtures that include PCP. Conclusive evidence of carcinogenicity has been obtained in one animal species (mice). Although there are notable variations in metabolism between experimental animals and humans, it was considered prudent to treat PCP as a potential carcinogen. Perchlorate Perchlorate is a naturally occurring anion that is frequently detected in the environment. It is used primarily as an oxidizer for solid rocket fuels, automotive airbags, fireworks and road flares. Perchlorate is found in water due to contamination from perchlorate manufacturing or use, natural deposits of perchlorate, use of fertilizers containing natural deposits of perchlorate, and natural formation of perchlorate in the atmosphere and its deposition during rain or snow events. It also forms in hypochlorite solutions to varying degrees, depending on the hypochlorite concentration, age and storage conditions. Guideline value 0.07 mg/l (70 μg/l) Occurrence Generally found in drinking‑water at concentrations below 10 μg/l, although concentrations above 40 μg/l have been measured PMTDI 0.01 mg/kg bw, based on a BMDL50 of 0.11 mg/kg bw per day for 50% inhibition of iodide uptake, derived from a human clinical study on healthy adult volunteers administered perchlorate in drinking‑water, and using an uncertainty factor of 10 to account for inter‑individual differences Limit of detection 20–50 ng/l (method reporting limits) by LC‑MS; 4 μg/l (method reporting limit) by IC with suppressed conductivity detection Treatment performance The perchlorate anion is highly stable in water and is difficult to remove using conventional water treatment technologies. Treatment technologies that have been shown to effectively remove perchlorate from water include nanofiltration and reverse osmosis membranes, anaerobic biodegradation and ion exchange. Guideline value derivation • allocation to water 20% of unrounded PMTDI (0.011 mg/kg bw) • weight 60 kg adult• consumption 2 litres/day Assessment date 2016 Principal references EFSA (2014). Scientific opinion on the risks to public health related to the presence of perchlorate in food, in particular fruits and vegetables FAO/WHO (2011). Safety evaluation of certain contaminants in food WHO (2016). Perchlorate in drinking-water The primary effect of perchlorate is its ability to competitively inhibit uptake of iodide by the thyroid gland. Inhibition of iodide uptake by perchlorate reduces the amount of iodide available for the synthesis of thyroid hormones. Sustained reduction in iodide uptake by the thyroid may result in hypothyroidism, which has adverse implications for structural and functional brain development in the fetus, infant and child, and for metabolism and the functioning of the cardiovascular, gastrointestinal, skeletal, neuromuscular and reproductive systems in adults. As the rat is not a good model for humans for substances known to affect the thyroid and having a mode of action involving inhibition of the uptake of iodide, the guideline value was derived from human studies. Petroleum products Petroleum products are used in large quantities, primarily as fuels. They are complex mixtures of chemicals derived from crude oil by distillation and fractionation. They consist primarily of a wide range of aliphatic and aromatic hydrocarbons, many of which are of extremely low solubility in water. Petroleum products are widely stored and handled and are often spilt. The primary concern for drinking-water is the potential for spills into source water, penetration of distribution systems and contamination of drinking-water treatment works. Reason for not establishing Taste and odour will in most cases be detectable at concentrations a guideline value below those of health concern, particularly with short‑term exposure Assessment date 2004 Principal reference WHO (2008) Petroleum products in drinking-water Exposure to the constituents of petroleum products through drinking-water is frequently short term, as the result of an accidental spill or short-term incident. Such incidents may lead to high concentrations of total petroleum hydrocarbons. However, a number of the most soluble aromatic hydrocarbons will be detectable by taste or odour at concentrations below those concentrations of concern for health, particularly for short-term exposure. Substances such as the alkyl benzenes and the alkyl naphthalenes have taste and odour thresholds of a few micrograms per litre. In view of the above, it is not considered appropriate to set a formal health-based guideline value for petroleum products in drinking-water. In the event of a spill, it may be necessary to carry out a context-specific assessment of the risk to health. The fact that petroleum products are complex mixtures of many individual hydrocarbons is a complicating factor in determining the potential risks to consumers. The traditional approach of evaluating individual chemicals in assessing the risks from drinking-water is therefore largely inappropriate. In order to overcome this difficulty, it is more practical to consider a series of hydrocarbon fractions and to determine appropriate tolerable concentrations for those fractions. The most widely accepted approach is that developed by the Total Petroleum Hydrocarbons Criteria Working Group in the USA, which divided total petroleum hydrocarbons into a series of aliphatic and aromatic fractions based on the number of carbon atoms and the boiling point, to give equivalent carbon numbers. This pragmatic approach provides a suitable basis for assessing the potential health risks associated with larger-scale contamination of drinking-water by petroleum products. The allocation of 10% of each of the reference doses, equivalent to TDIs, for the various fractions to drinking-water provides a conservative assessment of the risks. Although the approach is based on the analysis of hydrocarbon fractions, most are of low solubility, and the most soluble fractions, consisting largely of lower molecular weight aromatic hydrocarbons, will be present in the greatest concentration. pH No health-based guideline value is proposed for pH. Although pH usually has no direct impact on consumers, it is one of the most important operational water quality parameters (see chapter 10). Reason for not establishing Not of health concern at levels found in drinking‑water a guideline value Additional comments An important operational water quality parameter Assessment date 1993 Principal reference WHO (2007) pH in drinking-water 2-Phenylphenol and its sodium salt 2-Phenylphenol (CAS No. 90-43-7) is used as a disinfectant, bactericide and virucide. In agriculture, it is used in disinfecting fruits, vegetables and eggs. It is also used as a general surface disinfectant in hospitals, nursing homes, veterinary hospitals, poultry farms, dairy farms, commercial laundries, barbershops and food processing plants. 2-Phenylphenol is readily degraded in surface waters, with a half-life of about 1 week in river water. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal references FAO/WHO (2000) Pesticide residues in food—1999 evaluations WHO (2003) 2-Phenylphenol and its sodium salt in drinking-water 2-Phenylphenol has been determined to be of low toxicity. Both 2-phenylphenol and its sodium salt are carcinogenic in male rats, and 2-phenylphenol is carcinogenic in male mice. However, urinary bladder tumours observed in male rats and liver tumours observed in male mice exposed to 2-phenylphenol appear to be threshold phenomena that are species and sex specific. JMPR concluded that 2-phenylphenol is unlikely to represent a carcinogenic risk to humans. Although a working group convened by IARC classified 2-phenylphenol, sodium salt, in Group 2B (possibly carcinogenic to humans) and 2-phenylphenol in Group 3 (not classifiable as to its carcinogenicity to humans), JMPR noted that the IARC classification is based on hazard identification, not risk assessment, and is furthermore limited to published literature, excluding unpublished studies on toxicity and carcinogenicity. JMPR also concluded that there are unresolved questions about the genotoxic potential of 2-phenylphenol. A health-based value of 1 mg/l can be calculated for 2-phenylphenol on the basis of an ADI of 0–0.4 mg/kg body weight, based on a NOAEL of 39 mg/kg body weight per day in a 2-year toxicity study on the basis of decreased body weight gain and hyperplasia of the urinary bladder and carcinogenicity of the urinary bladder in male rats, using an uncertainty factor of 100 for interspecies and intraspecies variation. Because of its low toxicity, however, the health-based value derived for 2-phenylphenol is much higher than concentrations of 2-phenylphenol likely to be found in drinking-water. Under usual conditions, therefore, the presence of 2-phenylphenol in drinking-water is unlikely to represent a hazard to human health. For this reason, the establishment of a formal guideline value for 2-phenylphenol is not deemed necessary. Polynuclear aromatic hydrocarbons Polynuclear aromatic hydrocarbons, or PAHs, form a class of diverse organic compounds each containing two or more fused aromatic rings of carbon and hydrogen atoms. Most PAHs enter the environment via the atmosphere from a variety of combustion processes and pyrolysis sources. Owing to their low solubility and high affinity for particulate matter, they are not usually found in water in notable concentrations. The main source of PAH contamination in drinking-water is usually the coal tar coating of drinking-water distribution pipes, used to protect the pipes from corrosion. Fluoranthene is the most commonly detected PAH in drinking-water and is associated primarily with coal tar linings of cast iron or ductile iron distribution pipes. PAHs have been detected in a variety of foods as a result of the deposition of airborne PAHs and in fish from contaminated waters. PAHs are also formed during some methods of food preparation, such as char-broiling, grilling, roasting, frying or baking. For the general population, the major routes of exposure to PAHs are from food and ambient and indoor air. The use of open fires for heating and cooking, which is common especially in developing countries, may increase PAH exposure. Where there are elevated levels of contamination by coal tar coatings of water pipes, PAH intake from drinking-water could equal or even exceed that from food. Guideline value Benzo[a]pyrene: 0.0007 mg/l (0.7 μg/l) Occurrence PAH levels in uncontaminated groundwater usually in range 0–5 ng/l; concentrations in contaminated groundwater may exceed 10 μg/l; typical concentration range for sum of selected PAHs in drinking‑water is from about 1 ng/l to 11 μg/l Basis of guideline value derivation Based on an oral carcinogenicity study in mice and calculated using a two‑stage birth–death mutation model, which incorporates variable dosing patterns and time of killing; quantification of dose–response for tumours, on the basis of new studies in which the carcinogenicity of benzo[a]pyrene was examined following oral administration in mice, but for which the number of dose groups was smaller, confirms this value Limit of detection 0.01 μg/l by GC‑MS and reversed‑phase HPLC with a fluorescence detector Treatment performance 0.05 μg/l should be achievable using coagulation Additional comments The presence of significant concentrations of benzo[a]pyrene in drinking‑water in the absence of very high concentrations of fluoranthene indicates the presence of coal tar particles, which may arise from seriously deteriorating coal tar pipe linings. It is recommended that the use of coal tar–based and similar materials for pipe linings and coatings on storage tanks be discontinued. Assessment date 1998 Principal reference WHO (2003) Polynuclear aromatic hydrocarbons in drinking-water Reason for not establishing Fluoranthene: Occurs in drinking‑water at concentrations well below a guideline value those of health concern Assessment date 1998 Principal reference WHO (2003) Polynuclear aromatic hydrocarbons in drinking-water Evidence that mixtures of PAHs are carcinogenic to humans comes primarily from occupational studies of workers following inhalation and dermal exposure. No data are available for humans for the oral route of exposure. There are few data on the oral toxicity of PAHs other than benzo[a]pyrene, particularly in drinking-water. Relative potencies of carcinogenic PAHs have been determined by comparison of data from dermal and other studies. The order of potencies is consistent, and this scheme therefore provides a useful indicator of PAH potency relative to that of benzo[a]pyrene. A health-based value of 4 μg/l can be calculated for fluoranthene on the basis of a NOAEL of 125 mg/kg body weight per day for increased serum glutamate–pyruvate transaminase levels, kidney and liver pathology, and clinical and haematological changes in a 13-week oral gavage study in mice, using an uncertainty factor of 10 000 (100 for interspecies and intraspecies variation, 10 for the use of a subchronic study and inadequate database and 10 because of clear evidence of co-carcinogenicity with benzo[a]pyrene in mouse skin painting studies). However, this health-based value is significantly above the concentrations normally found in drinking-water. Under usual conditions, therefore, the presence of fluoranthene in drinking-water does not represent a hazard to human health. For this reason, the establishment of a formal guideline value for fluoranthene is not deemed necessary. Potassium Potassium is an essential element in humans and is seldom, if ever, found in drinking-water at levels that could be a concern for healthy humans. The recommended daily requirement is greater than 3000 mg. Potassium occurs widely in the environment, including all natural waters. It can also occur in drinking-water as a consequence of the use of potassium permanganate as an oxidant in water treatment. In some countries, potassium chloride is being used in ion exchange for household water softening in place of, or mixed with, sodium chloride, so potassium ions would exchange with calcium and magnesium ions. Possible replacement or partial replacement of sodium salts with potassium salts for conditioning desalinated water has been suggested. The latter seems to be an unlikely development at this stage, in view of the cost difference. Reason for not establishing Occurs in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2009 Principal reference WHO (2009) Potassium in drinking-water Currently, there is no evidence that potassium levels in municipally treated drinking-water, even water treated with potassium permanganate, are likely to pose any risk for the health of consumers. It is not considered necessary to establish a health-based guideline value for potassium in drinking-water. Although potassium may cause some health effects in susceptible individuals, potassium intake from drinking-water is well below the level at which adverse health effects may occur. Health concerns would be related to the consumption of drinking-water treated by potassium-based water treatment (principally potassium chloride for regeneration of ion exchange water softeners), affecting only individuals in high-risk groups (i.e. individuals with kidney dysfunction or other diseases, such as heart disease, coronary artery disease, hypertension, diabetes, adrenal insufficiency, pre-existing hyperkalaemia; people taking medications that interfere with normal potassium-dependent functions in the body; and older individuals or infants). It is recommended that susceptible individuals seek medical advice to determine whether they should avoid the consumption of water (for drinking or cooking) treated by water softeners using potassium chloride. When high-risk individuals have been advised by a physician to avoid elevated potassium intake from water, the recommended strategy is to limit the addition of potassium to water that will be ingested or to avoid ingesting such water. This can be done by having a proportion of the water bypass the softener altogether; this approach is recommended by several countries. Although technologies are available to remove potassium, they are generally more expensive and redundant when combined with the softening treatment. Propanil Propanil (CAS No. 709-98-8) is a contact post-emergence herbicide used to control broad-leaved and grassy weeds, mainly in rice. It is a mobile compound with affinity for the water compartment. Propanil is not, however, persistent, being easily transformed under natural conditions to several metabolites. Two of these metabolites, 3,4-dichloroaniline and 3,3′,4,4′-tetrachloroazobenzene, are more toxic and more persistent than the parent compound. Although used in a number of countries, propanil has only occasionally been detected in groundwater. Reason for not establishing Readily transformed into metabolites that are more toxic; a guideline a guideline value value for the parent compound is considered inappropriate, and there are inadequate data to enable the derivation of guideline values for the metabolites Assessment date 2003 Principal reference WHO (2003) Propanil in drinking-water Although a health-based value for propanil can be derived, this has not been done, because propanil is readily transformed into metabolites that are more toxic. Therefore, a guideline value for the parent compound is considered inappropriate, and there are inadequate data on the metabolites to allow the derivation of guideline values for them. Authorities should consider the possible presence in water of more toxic environmental metabolites. Saxitoxins (cyanobacterial toxins)1 Saxitoxins (STXs) are naturally occurring alkaloids produced by some marine dinoflagellates and by strains of various species of freshwater cyanobacteria. They have 1 As cyanobacteria and their toxins are a concern in many areas and considering the complexities in their management, this chemical fact sheet has been expanded. been found in cyanobacterial blooms around the globe and from a number of cyanobacterial genera, including Dolichospermum, Raphidiopsis and Aphanizomenon (see also section 11.5). Whereas STX-contaminated marine shellfish are the most likely cause of the severe illness known as paralytic shellfish poisoning, drinking-water is the most likely exposure route where surface water with cyanobacterial blooms is the drinking-water source. However, in locations where marine shellfish are contaminated with STXs, they are likely to be the greater source of exposure. Limited data suggest that STXs may also accumulate in some freshwater food items. Recreational activities may also be a relevant exposure pathway, potentially to high concentrations (see WHO Guidelines on recreational water quality, 2021). Acute guideline value Total STXs (sum of all congeners, free plus cell-bound): 0.003 mg/l (3 μg/l) Occurrence Although concentrations usually range well below 1 μg/l, several μg/l are occasionally reported, and up to almost 200 μg/l has been found in scums. STXs largely occur cell‑bound unless cell damage causes release ARfD FAO (2004) and EFSA (2009) reviewed a large dataset of human STX poisoning cases. Exposure was to a range of STX analogues. An ARfD for saxitoxins of 0.5 μg/kg bw was derived based on a LOAEL for mild symptoms of 1.5 μg/kg bw of STX equivalents (EFSA, 2009). A database uncertainty factor of 3 was applied for use of a LOAEL. Limit of detection <3 μg/l by LC‑MS or HPLC methods with either pre‑ or post‑column derivatisation and fluorescence detection; LC‑MS methods require quantitative reference standards, which are available for only some congeners. A commercially available receptor‑binding assay circumvents this problem and provides more reliable results than HPLC. Prior extraction of cells with freeze–thaw cycles and acetic acid, hydrochloric acid or acidified aqueous methanol is necessary for cell‑bound STX; neglecting extraction from cells will lead to dramatic underestimation of concentrations. STX analysis requires particular care in the laboratory because congeners may interconvert; furthermore, the chemical variability of congeners presents challenges. 0.02 μg/l for STX alone by commercially available immunoassay kits (ELISA), but cross‑reactivity with other congeners is highly variable. Monitoring The likelihood of blooms can be assessed by understanding water body conditions (in particular, nutrient concentrations, water body depth, water retention time, patterns of mixing and stratification; see section 11.5). Where conditions render blooms likely, visual monitoring of source water (including microscopy for potentially STX‑containing genera) is important because biomass can increase rapidly. Exceeding alert values for biomass indicators or STX concentrations should trigger management responses to prevent exposure to elevated toxin concentrations (see the alert level framework in section 11.5). Analysis of cyanotoxins is particularly useful for validating and optimizing the efficacy of control measures such as riverbank filtration or treatment. Prevention and treatment Actions to decrease the probability of bloom occurrence include catchment and source water management, such as reducing nutrient loading or changing reservoir stratification and mixing. Filtration is effective for removing intact cyanobacterial cells, but some dissolved STX may be present. For this dissolved fraction, oxidation with chlorine or ozone at sufficient concentrations and contact times, as well as GAC and some PAC applications, are effective (see chapters 7–10 of Toxic cyanobacteria in water; Annex 1). Guideline value derivation • allocation to water 100% of ARfD • allocation to water 5 kg infant • consumption 0.75 litres/day Additional comments The total amount of STX and its structural analogues as gravimetric or molar equivalents should be evaluated against the guideline value. The drinking‑water guideline value was derived to protect bottle‑fed infants because the guideline value is for acute exposure, and a guideline value based on adult exposure could allow exposure of infants to a concentration of STXs close to the LOAEL. For a 60 kg adult consuming 2 litres of drinking‑water per day, a 5‑fold higher concentration than the acute guideline value would be tolerable. In locations where exposure to STXs may occur through both marine shellfish and drinking‑water, health authorities may need to consider the combined exposure in their risk assessments. Assessment date 2020 Principal references FAO (2004) Marine biotoxins EFSA (2009) Marine biotoxins in shellfish: saxitoxin group WHO (2020) Cyanobacterial toxins: saxitoxins Chorus & Welker (2021) Toxic cyanobacteria in water STXs act by blocking sodium channels in nerve cell axons, which inhibits propagation of an action potential along the axon. When this occurs in sensory neurons, symptoms such as tingling and numbness are induced; in motor neurons, muscle weakness or paralysis ensues. The effects of human intoxication by STXs have been well described from numerous cases of paralytic shellfish poisoning after consumption of marine shellfish. They range from numbness and tingling in the tongue and mouth through muscular weakness in the limbs to, in severe cases, respiratory failure and death. If a person recovers from the acute toxicity, they appear to make a full recovery and no long-term adverse effects are known. STXs were the cause of massive animal deaths along 1000 km of the Murray–Darling river system in Australia during a heavy bloom of Dolichospermum in 1991. Unlike the other cyanotoxins considered in these Guidelines (MCs, CYNs and ATXs), a guideline value for STXs could be derived from human data that are available from incidents of shellfish poisoning. Practical considerations Where nutrient (phosphorus and nitrogen) concentrations are elevated in lakes, reservoirs or slowly flowing rivers, cyanobacteria occur widely. Where their excessive growth leads to high biomass, sometimes termed “bloom” events, STXs can reach concentrations in raw water that are potentially hazardous to human health. Such blooms tend to recur in the same water bodies. Cells of some cyanobacterial species (e.g. Raphidiopsis, Aphanizomenon, Dolichospermum) may accumulate at the surface as scums. Such accumulations may develop rapidly and may be of very variable duration (hours to weeks). In many circumstances, blooms and accumulations are seasonal, whereas others occur perennially. Cyanobacteria are most effectively controlled in the context of developing a WSP (see chapter 4). Control measures to manage potential risks from cyanobacteria, and in particular from their toxins, in drinking-water should include not only adequate treatment, but also measures to control cyanobacterial bloom development. See section 11.5 for more information on cyanobacteria, including further details on monitoring cyanobacterial blooms, the alert level framework, and prevention and management of cyanobacteria in source waters. Effectively minimizing the formation of blooms and locating the raw water intake away from blooms reduce the treatment steps required to remove cyanotoxins. Drinking-water treatment that removes particles—that is, soil, slow sand or riverbank filtration, conventional water treatment (coagulation, flocculation and filtration) or dissolved air flotation—can remove cell-bound STXs effectively. Soil, slow sand and riverbank filtration can also remove dissolved cyanotoxins. For all these processes, it important that they are optimized to target the removal of cells and dissolved toxins. Chlorination and ozonation at sufficiently high doses and contact times are effective for degrading dissolved STXs; however, elevated organic carbon in bloom situations will substantially increase the disinfectant demand. Chlorine dioxide and chloramine are ineffective for degrading STXs. Both for pre-oxidaton and conventional treatment, cell rupture and toxin release should be avoided. GAC and PAC can be effective for removing dissolved STXs, with efficacy dependent on several factors, including the type of activated carbon, contact times (PAC), flow rates (GAC) and water quality. As the challenges that blooms present for treatment are complex, periodic validation of efficacy during bloom situations and under the specific local conditions is particularly important. Avoiding bloom occurrence and intake is therefore the preferred option. Selenium Selenium is present in Earth’s crust, often in association with sulfur-containing minerals. Selenium is an essential trace element, and foodstuffs such as cereals, meat and fish are the principal source of selenium for the general population. Levels in food also vary greatly according to geographical area of production. However, even in high-selenium areas, the relative contribution of selenium from drinking-water is likely to be small in comparison with that from locally produced food. Provisional guideline value 0.04 mg/l (40 μg/l) The guideline value is designated as provisional because of the uncertainties inherent in the scientific database. Occurrence Most drinking‑water contains concentrations of selenium that are much lower than 10 μg/l, except in certain seleniferous areas Basis of guideline value An allocation of 20% of the upper tolerable intake of 400 μg/day to derivation drinking‑water provides a sensible balance that will assist regulators and suppliers in making decisions about whether further action is needed Limit of detection 0.5 μg/l by hydride generation AAS Treatment performance Selenium is not removed by conventional treatment processes; significant removals of selenium from water using activated alumina adsorption, ion exchange, reverse osmosis and nanofiltration have been reported. Guideline value derivation • allocation to water 20% of upper tolerable intake • consumption 2 litres/day Additional comments It is important that a proper balance be achieved between recommended intakes and undesirable intakes in determining an appropriate guideline value for selenium in drinking‑water. While for most parts of the world, the concentration of selenium in drinking‑water will not exceed 10 μg/l, there are circumstances in which selenium may be elevated significantly above normal concentrations, and guidance may be required. Where selenium intake from the diet is known, this should be used in determining a concentration that ensures that intake is safe and sufficient. Where selenium intake from the diet is not known, guidance may be required. For most Member States, a drinking‑water guideline for selenium is unnecessary. Where there are regions of high intake from a number of sources, of which drinking‑water may be one, then Member States should take into consideration exposure from all sources in determining actions to reduce exposure. For drinking‑water, this may include using alternative sources, blending low‑selenium sources with high‑selenium sources as well as considering selenium removal. Assessment date 2010 Principal references FAO/WHO (2004) Vitamin and mineral requirements in human nutrition WHO (2011) Selenium in drinking-water Selenium is an essential element for humans, and there are indications that selenium status may be marginal in many parts of the world, including western Europe. The potential for adverse effects from selenium deficiency appears to be dependent on a number of factors, including overall health and nutritional status. Very low selenium status in humans has been associated with a juvenile, multifocal myocarditis called Keshan disease and a chondrodystrophy called Kaschin-Beck disease. Several studies have also found blood selenium levels to be inversely associated with the prevalence of several types of cancer. High intakes of selenium are also associated with a number of specific diseases and the potential for adverse effects, but, again, this seems to be strongly influenced by other factors. Symptoms in people with high urinary selenium levels included gastrointestinal disturbances, discoloration of the skin, decayed teeth, hair or nail loss, nail abnormalities and changes in peripheral nerves. Slight biochemical changes have also been observed. One case of selenium toxicity directly attributable to a water source (well water containing selenium at a concentration of 9 mg/l) has been reported. The average dietary intake that is associated with selenosis has been found to be in excess of 900 μg/day. As selenium is an essential element, various national and international organizations have established recommended daily intakes of selenium. A joint FAO/WHO consultation recommended intakes of 6–21 μg of selenium per day for infants and children, according to age, 26 and 30 μg of selenium per day for adolescent females and males, respectively, and 26 and 35 μg of selenium per day for adult females and males, respectively. Because of concern about the adverse effects resulting from exposure to excessive levels of selenium, various national and international organizations have established upper limits of exposure for selenium. FAO/WHO established an upper tolerable limit for selenium of 400 μg/day. Silver Silver occurs naturally, mainly in the form of its very insoluble and immobile sulfide, oxides and some salts. Silver ions are primarily found in the +1 oxidation state, and the ionic compounds silver nitrate (soluble) and silver chloride (relatively insoluble) are the most important forms of silver for drinking-water. Silver can also occur as nanoparticles, which can be present in water bodies as a result of release into the environment from wastewater or industrial discharges. When silver is used in pointof-use water treatment devices, drinking-water is expected to be the major source of exposure to silver. Reason for not establishing Available data inadequate to permit derivation of health‑based a guideline value guideline value, and silver usually occurs in drinking‑water at concentrations well below those of health concern Provisional reference value* 0.1 mg/l Occurrence In surface water and groundwater, silver concentrations are usually below 5 μg/l; however, detections >100 μg/l, although rare, have been reported TDI A formal TDI could not be derived because of database limitations. However, 0.6 mg/kg bw of colloidal silver per day was considered a LOAEL based on a case report of argyria in a woman who ingested this dose of silver for 16 months. An uncertainty factor of 100 (10 for intraspecies variability and 10 for database limitations) was applied. The database limitations include the short duration of the study, uncertainty associated with a dose level derived from human recall and use of a LOAEL instead of a NOAEL. Limit of detection 2 ng/ by neutron activation analysis; 5 ng/l by ICP‑MS; 2 μg/l by graphite furnace AAS; 10 μg/l by a spectrographic and colorimetric method with dithizone for a 20 ml sample. In addition, asymmetric flow field‑flow fractionation in combination with single particle ICP‑MS can differentiate between silver nanoparticles and dissolved silver, although this method has not yet been standardized. Since ionic silver can be released from silver nanoparticles, it may be difficult to determine whether silver dispersed in water has originated from the ionic or the particulate fraction. Treatment performance Conventional treatment (coagulation, sedimentation, filtration) and lime softening are effective for removing ionic silver; conventional treatment is also expected to be effective for removing coated silver nanoparticles Guideline value derivation • allocation to water 80% • allocation to water 60 kg adult• consumption 2 litres/day Additional comments The provisional reference value is applicable particularly where silver is used in point‑of‑use water treatment devices. However, silver is not recommended for disinfection of drinking‑water including in point‑ofuse water treatment devices (see Part III of the supporting document Alternative drinking-water disinfectants: Bromine, iodine and silver; Annex 1). It is also noted that silver with copper may be necessary to control Legionella in the distribution systems of buildings, and the risks of legionellosis outweigh the risks from low levels of silver ions (usually in the low μg/l range) that may be detected in drinking‑water as a result of such use. See Silver in drinking-water, Annex 2, for further information, including on Legionella control measures. Assessment date 2021 Principal reference WHO (2021) Silver in drinking-water * Although a formal guideline value cannot be established, the provisional reference value was derived based on limited available studies, recognizing that a “bounding value” may be useful, to provide guidance to Member States in the event of need. Reference values are too uncertain to be used for developing regulations or standards. The only obvious sign of silver overload is argyria, a condition in which skin and hair are heavily discoloured by silver in the tissues. In argyria, silver is deposited in various organs (e.g. skin, kidney, liver) after oral ingestion of silver in its ionic form or as silver nanoparticles, oxidizing to insoluble silver sulfide, which is responsible for the discolouring effects. It is difficult to determine the lowest dose that may lead to development of argyria, and the toxicological database on silver is not adequate to support derivation of a formal guideline value. Furthermore, a formal guideline value is considered unnecessary since silver usually occurs naturally in drinking-water at concentrations well below those of health concern. Nevertheless, a bounding value (provisional reference value) may provide a useful benchmark where elevated concentrations of silver in drinking-water may be expected. The provisional reference value of 0.1 mg/l is supported by the prior assessments that concluded that 10 g of ingested silver can be considered a human NOAEL (WHO, 1984a, b, 1993; US EPA 1992). Over a 70-year period, assuming a drinking-water intake of 2 litres/day, 0.1 mg/l is a concentration in drinking-water that would give a total dose of half this NOAEL. Simazine Simazine (CAS No. 122-34-9) is a pre-emergence herbicide used on a number of crops as well as in non-crop areas. It is fairly resistant to physical and chemical dissipation processes in the soil. It is persistent and mobile in the environment. Guideline value 0.002 mg/l (2 μg/l) Occurrence Frequently detected in groundwater and surface water at concentrations of up to a few micrograms per litre TDI 0.52 μg/kg body weight, based on a NOAEL of 0.52 mg/kg body weight from a long‑term study in the rat (based on weight changes, effects on haematological parameters and an increase in mammary tumours) and an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for possible non‑genotoxic carcinogenicity) Limit of detection 0.01 μg/l by GC‑MS; 0.1–0.2 μg/l by GC with flame thermionic detection Treatment performance 0.1 μg/l should be achievable using GAC Guideline value derivation • allocation to water • weight • consumption 10% of TDI 60 kg adult 2 litres/day Assessment date 1993 Principal reference WHO (2003) Simazine in drinking-water Simazine does not appear to be genotoxic in mammalian systems. Recent studies have shown an increase in mammary tumours in the female rat but no effects in the mouse. IARC has classified simazine in Group 3 (not classifiable as to its carcinogenicity to humans). Sodium Sodium salts (e.g. sodium chloride) are found in virtually all food (the main source of daily exposure) and drinking-water. Although concentrations of sodium in potable water are typically less than 20 mg/l, they can greatly exceed this in some countries. The levels of sodium salts in air are normally low in relation to those in food or water. It should be noted that some water softeners can add significantly to the sodium content of drinking-water. Reason for not establishing Not of health concern at levels found in drinking‑water a guideline value Additional comments May affect acceptability of drinking‑water Assessment date 1993 Principal reference WHO (2003) Sodium in drinking-water No firm conclusions can be drawn concerning the possible association between sodium in drinking-water and the occurrence of hypertension. Therefore, no health-based guideline value is proposed. However, concentrations in excess of 200 mg/l may give rise to unacceptable taste (see chapter 10). Sodium dichloroisocyanurate Sodium dichloroisocyanurate is the sodium salt of a chlorinated hydroxytriazine and is used as a source of free available chlorine, in the form of hypochlorous acid, for the disinfection of water. It is widely used as a stable source of chlorine for the disinfection of swimming pools and in the food industry. It is also used as a means of disinfecting drinking-water, primarily in emergencies, when it provides an easy-to-use source of free chlorine, and, more recently, as the form of chlorine for household point-of-use water treatment. Guideline values Sodium dichloroisocyanurate: 50 mg/l (50 000 μg/l) Cyanuric acid: 40 mg/l (40 000 μg/l) Occurrence Where sodium dichloroisocyanurate is used for the disinfection of drinking‑water, exposure will be to both the chlorinated species and residual cyanuric acid. The concentrations will relate directly to the quantities added to achieve adequate disinfection. TDI 2.2 mg/kg body weight for anhydrous sodium dichloroisocyanurate and 1.54 mg/kg body weight for cyanuric acid, based on a NOEL of 154 mg/ kg body weight per day (equivalent to 220 mg/kg body weight per day as anhydrous sodium dichloroisocyanurate) for urinary tract and cardiac lesions from a 2‑year study of rats exposed to sodium cyanurate and using an uncertainty factor of 100 for interspecies and intraspecies variation Limit of detection 0.001 mg/l by GC with flame thermionic specific detection; 0.05 mg/l by reversed‑phase LC with UV detection; 0.09 mg/l by GC with MS selective ion monitoring Treatment performance At very high chlorine doses (up to 10 mg/l), the sodium cyanurate concentration would be below 11 mg/l. In emergency situations, “topping up” might be done in an attempt to maintain a free chlorine residual, but this practice should be discouraged. In this case, it would be possible for the sodium cyanurate concentration to build up to undesirable levels. In such cases, it would be very desirable to monitor the concentration of sodium cyanurate. Guideline value derivation • allocation to water 80% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments The controlling factors are the level of free chlorine and the residue of cyanuric acid, particularly if there is topping up of chlorine in a static system under emergency conditions. The concentration of free chlorine should normally be such that it should not give rise to unacceptable tastes and should not normally exceed the guideline value of 5 mg/l for free chlorine. Sodium dichloroisocyanurate used for disinfecting drinking‑water should be of adequate purity so that there is no increase in any inorganic or organic contaminants in the drinking‑water. The amounts of sodium dichloroisocyanurate used should be the lowest consistent with adequate disinfection, and the concentrations of cyanuric acid should be managed to be kept as low as is reasonably possible. Assessment date 2007 Principal references FAO/WHO (2004) Evaluation of certain food additives and contaminants WHO (2008) Sodium dichloroisocyanurate in drinking-water Studies of the toxicity of sodium cyanurate are appropriate for assessing the safety of sodium dichloroisocyanurate, because any residues of intact sodium dichloroisocyanurate in drinking-water would be rapidly converted to cyanuric acid on contact with saliva. Both sodium dichloroisocyanurate and sodium cyanurate have low acute oral toxicity. Sodium cyanurate does not induce any genotoxic, carcinogenic or teratogenic effects. The NOEL from which the guideline value was derived was based on multiple lesions of the urinary tract (calculi and hyperplasia, bleeding and inflammation of the bladder epithelium, dilated and inflamed ureters and renal tubular nephrosis) and cardiac lesions (acute myocarditis, necrosis and vascular mineralization) in male rats exposed at the next higher dose. Styrene Styrene, which is used primarily for the production of plastics and resins, is found in trace amounts in surface water, drinking-water and food. In industrial areas, exposure via air can result in intake of a few hundred micrograms per day. Smoking may increase daily exposure by up to 10-fold. Guideline value 0.02 mg/l (20 μg/l) Occurrence Has been detected in drinking‑water and surface water at concentrations below 1 μg/l TDI 7.7 μg/kg body weight, based on a NOAEL of 7.7 mg/kg body weight per day for decreased body weight observed in a 2‑year drinking‑water study in rats, and using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for the carcinogenicity and genotoxicity of the reactive intermediate styrene‑7,8‑oxide) Limit of detection 0.3 μg/l by GC with photoionization detection and confirmation by MS Treatment performance 0.02 mg/l may be achievable using GAC Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments May affect the acceptability of drinking‑water at the guideline value Assessment date 1993 Principal reference WHO (2003) Styrene in drinking-water Following oral or inhalation exposure, styrene is rapidly absorbed and widely distributed in the body, with a preference for lipid depots. It is metabolized to the active intermediate styrene-7,8-oxide, which is conjugated with glutathione or further metabolized. Metabolites are rapidly and almost completely excreted in urine. Styrene has a low acute toxicity. In short-term toxicity studies in rats, impairment of glutathione transferase activity and reduced glutathione concentrations were observed. In in vitro tests, styrene has been shown to be mutagenic in the presence of metabolic activation only. In in vitro as well as in vivo studies, chromosomal aberrations have been observed, mostly at high doses of styrene. The reactive intermediate styrene-7,8-oxide is a direct-acting mutagen. In long-term studies, orally administered styrene increased the incidence of lung tumours in mice at high dose levels but had no carcinogenic effect in rats. Styrene-7,8-oxide was carcinogenic in rats after oral administration. IARC has classified styrene in Group 2B (possibly carcinogenic to humans). The available data suggest that the carcinogenicity of styrene is due to overloading of the detoxification mechanism for styrene-7,8-oxide (e.g. glutathione depletion). Sulfate Sulfates occur naturally in numerous minerals and are used commercially, principally in the chemical industry. They are discharged into water in industrial wastes and through atmospheric deposition; however, the highest levels usually occur in groundwater and are from natural sources. In general, the average daily intake of sulfate from drinking-water, air and food is approximately 500 mg, food being the major source. However, in areas with drinking-water supplies containing high levels of sulfate, drinking-water may constitute the principal source of intake. Reason for not establishing Not of health concern at levels found in drinking‑water a guideline value Additional comments May affect acceptability of drinking‑water Assessment date 2003 Principal reference WHO (2004) Sulfate in drinking-water The existing data do not identify a level of sulfate in drinking-water that is likely to cause adverse human health effects. The data from a liquid diet study with piglets and from tap water studies with human volunteers indicate a laxative effect at concentrations of 1000–1200 mg/l, but no increase in diarrhoea, dehydration or weight loss. No health-based guideline is proposed for sulfate. However, because of the gastrointestinal effects resulting from ingestion of drinking-water containing high sulfate levels, it is recommended that health authorities be notified of sources of drinking-water that contain sulfate concentrations in excess of 500 mg/l. The presence of sulfate in drinking-water may also cause noticeable taste (see chapter 10) and may contribute to the corrosion of distribution systems. 2,4,5-T The half-lives for degradation of chlorophenoxy herbicides, including 2,4,5-T (CAS No. 93-76-5), also known as 2,4,5-trichlorophenoxyacetic acid, in the environment are in the order of several days. Chlorophenoxy herbicides are not often found in food. Guideline value 0.009 mg/l (9 μg/l) Occurrence Chlorophenoxy herbicides not frequently found in drinking‑water; when detected, concentrations usually no greater than a few micrograms per litre TDI 3 μg/kg body weight, based on a NOAEL of 3 mg/kg body weight for reduced body weight gain, increased liver and kidney weights and renal toxicity in a 2‑year study in rats, with an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 to take into consideration the suggested association between 2,4,5‑T and soft tissue sarcoma and non‑Hodgkin lymphoma in epidemiological studies) Limit of detection 0.02 μg/l by GC with ECD Treatment performance 1 μg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Assessment date 1993 Principal reference WHO (2003) Chlorophenoxy herbicides (excluding 2,4-D and MCPA) in drinking-water Chlorophenoxy herbicides, as a group, have been classified in Group 2B (possibly carcinogenic to humans) by IARC. However, the available data from studies in exposed populations and experimental animals do not permit assessment of the carcinogenic potential to humans of any specific chlorophenoxy herbicide. Therefore, drinking-water guidelines for these compounds are based on a threshold approach for other toxic effects. The NOAEL for reproductive effects (reduced neonatal survival, decreased fertility, reduced relative liver weights and thymus weights in litters) of dioxin-free (< 0.03 μg/kg) 2,4,5-T in a three-generation reproduction study in rats is the same as the NOAEL for reduced body weight gain, increased liver and kidney weights and renal toxicity in a toxicity study in which rats were fed 2,4,5-T (practically free from dioxin contamination) in the diet for 2 years. Terbuthylazine Terbuthylazine (CAS No. 5915-41-3), or TBA, a herbicide that belongs to the chlorotriazine family, is used in both pre-emergence and post-emergence treatment of a variety of agricultural crops and in forestry. Degradation of TBA in natural water depends on the presence of sediments and biological activity. Guideline value 0.007 mg/l (7 μg/l) Occurrence Concentrations in water seldom exceed 0.2 μg/l, although higher concentrations have been observed. TDI 2.2 μg/kg body weight, based on a NOAEL of 0.22 mg/kg body weight for decreased body weight gain at the next higher dose in a 2‑year toxicity/ carcinogenicity study in rats, with an uncertainty factor of 100 (for interspecies and intraspecies variation) Limit of detection 0.1 μg/l by HPLC with UV detection Treatment performance 0.1 μg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Assessment date 1998 Principal reference WHO (2003) Terbuthylazine in drinking-water There is no evidence that TBA is carcinogenic or mutagenic. In long-term dietary studies in rats, effects on red blood cell parameters in females, an increased incidence of non-neoplastic lesions in the liver, lung, thyroid and testis and a slight decrease in body weight gain were observed. Tetrachloroethene Tetrachloroethene (PCE) has been used primarily as a solvent in dry-cleaning industries, and to a lesser extent as a degreasing solvent. Since the 1980s, as a result of regulations in North America, Europe and elsewhere, its use has substantially decreased. PCE is widespread in the environment and is found in trace amounts in water, aquatic organisms, air, foodstuffs and human tissue. The most relevant routes of exposure are considered to be inhalation of contaminated air and ingestion of contaminated drinking-water, particularly from groundwater sources. Poor handling and improper disposal of PCE in landfills have been the main causes of water contamination. Higher levels of PCE are expected in groundwater than in surface water because of the lack of volatilization that occursfrom groundwater. However, drinking-water is not a major source of exposure, unless in the vicinity of a contaminated site. Guideline value 0.1 mg/l (100 μg/l) Occurrence Concentrations in drinking‑water are generally below 10 μg/l, although much higher concentrations have been detected in well water (23 mg/l) and in contaminated groundwater (1.5 mg/l) TDI 16 μg/kg bw, based on a BMDL10 of 4.7 mg/kg bw per day for neurological effects (decreased colour vision) observed in an occupational study and applying an uncertainty factor of 300 (10 each for inter‑ and intra‑species variability and 3 for extrapolation from an occupational study with intermittent exposure) Limit of detection 0.002–0.008 μg/l by GC with ECD after liquid–liquid extraction; 0.02–0.05 μg/l by purge‑and‑trap capillary GC with PD and ECD in series; 0.036 μg/l by volatile organic compound analysis with GC‑MS; and 0.05–0.14 μg/l by purge‑and‑trap capillary GC‑MS Prevention and treatment Source control, by improving handling and disposal practices, should be the priority action since PCE can persist in waters where volatilization cannot occur. Treatment of surface water sources is not needed because PCE volatilizes to the atmosphere. GAC, packed tower aeration and diffused aeration are effective central treatment technologies, although diffused aeration achieves lower removal efficiencies than packed tower aeration systems. Advanced oxidation processes may also be effective, but effectiveness depends on the physical and chemical properties of the water. Guideline value derivation • allocation to water 20% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments The guideline value is considered protective against both cancer and noncancer effects. In developing national standards, authorities may take into consideration the additional exposures through the dermal and inhalation routes from showering and bathing, especially in countries with low rates of ventilation in houses.Authorities may also consider overall exposure in developing national standards, noting the continued decline in human exposure to PCE by all probable exposure routes. Requirements for monitoring PCE in drinking‑water regulations and standards should be limited to groundwater sources, where a possibility of PCE contamination is indicated. Monitoring is not needed for surface water sources because PCE volatilizes to the atmosphere. Assessment date 2020 Principal reference WHO (2020) Tetrachloroethene in drinking-water IARC has classified PCE in Group 2A (probably carcinogenic to humans). PCE has been reported to produce liver tumours in male and female mice following inhalation, and there is some evidence of mononuclear cell leukaemia in male and female rats and kidney tumours in male rats. However, absorption, metabolic pathways, excretion and pattern of effects are similar for inhalation and oral exposure. The weight of evidence for PCE suggests that a nonlinear, nonmutagenic mode of action is predominant for liver tumours, and this effect is considered the most relevant end-point for cancer risk assessment of PCE exposure. Additionally, noncancer effects resulting from inhalation exposure, including evidence of neurotoxicity, were observed in human occupational studies and in laboratory animal studies. The nervous system is the most sensitive organ for noncancer effects. To protect against both cancer and noncancer effects, comparative points of departure were identified using benchmark dose modelling and physiologically based pharmacokinetic modelling (to account for the first-pass effects and inhalation-to-ingestion extrapolation). The most sensitive human-relevant effects were determined to be reductions in colour vision, and changes in cognitive function and reaction time in exposed workers; neurological effects were therefore the basis for the TDI derivation. Toluene Most toluene (in the form of benzene–toluene–ethylbenzene–xylene mixtures) is used in the blending of petrol. It is also used as a solvent and as a raw material in chemical production. The main exposure is via air. Exposure is increased by smoking and in traffic. Guideline value 0.7 mg/l (700 μg/l) Occurrence Concentrations of a few micrograms per litre have been found in surface water, groundwater and drinking‑water; point emissions can lead to higher concentrations in groundwater (up to 1 mg/l); it may also penetrate plastic pipes from contaminated soil TDI 223 μg/kg body weight, based on a LOAEL of 312 mg/kg body weight per day for marginal hepatotoxic effects observed in a 13‑week gavage study in mice, adjusting for daily dosing and using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for the short duration of the study and use of a LOAEL instead of a NOAEL) Limit of detection 0.13 μg/l by GC with FID; 6 μg/l by GC‑MS Treatment performance 0.001 mg/l should be achievable using air stripping Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments The guideline value exceeds the lowest reported odour threshold for toluene in water. Assessment date 2003 Principal reference WHO (2003) Toluene in drinking-water Toluene is absorbed completely from the gastrointestinal tract and rapidly distributed in the body, with a preference for adipose tissue. Toluene is rapidly metabolized and, following conjugation, excreted predominantly in urine. With occupational exposure to toluene by inhalation, impairment of the central nervous system and irritation of mucous membranes are observed. The acute oral toxicity is low. Toluene exerts embryotoxic and fetotoxic effects, but there is no clear evidence of teratogenic activity in laboratory animals and humans. In long-term inhalation studies in rats and mice, there is no evidence for carcinogenicity of toluene. Genotoxicity tests in vitro were negative, whereas in vivo assays showed conflicting results with respect to chromosomal aberrations. IARC has concluded that there is inadequate evidence for the carcinogenicity of toluene in both experimental animals and humans and classified it as Group 3 (not classifiable as to its carcinogenicity to humans). Total dissolved solids Total dissolved solids (TDS) comprise inorganic salts (principally calcium, magnesium, potassium, sodium, bicarbonates, chlorides and sulfates) and small amounts of organic matter that are dissolved in water. TDS in drinking-water originates from natural sources, sewage, urban runoff and industrial wastewater. Salts used for road de-icing in some countries may also contribute to the TDS content of drinking-water. Concentrations of TDS in water vary considerably in different geological regions owing to differences in the solubilities of minerals. Reason for not establishing Not of health concern at levels found in drinking‑water a guideline value Additional comments May affect acceptability of drinking‑water Assessment date 1993 Principal reference WHO (2003) Total dissolved solids in drinking-water Reliable data on possible health effects associated with the ingestion of TDS in drinking-water are not available, and no health-based guideline value is proposed. However, the presence of high levels of TDS in drinking-water may be objectionable to consumers (see chapter 10). Trichloroacetic acid Chlorinated acetic acids are formed from organic material during water chlorination. Guideline value 0.2 mg/l (200 μg/l) Occurrence Detected in groundwater and surface water distribution systems in the USA at mean concentrations of 5.3 μg/l (up to a maximum of 80 μg/l) and 16 μg/l (up to a maximum of 174 μg/l), respectively; maximum concentration (200 μg/l) measured in chlorinated water in Australia TDI 32.5 μg/kg body weight, based on a NOAEL of 32.5 mg/kg body weight per day from a study in which decreased body weight, increased liver serum enzyme activity and liver histopathology were seen in rats exposed to trichloroacetate in drinking‑water for 2 years, incorporating an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for database deficiencies, including the absence of a multigeneration reproductive study, the lack of a developmental study in a second species and the absence of full histopathological data in a second species) Limit of detection 1 μg/l by GC‑MS or GC‑ECD Treatment performance Concentrations may be reduced by installing or optimizing coagulation to remove precursors or by controlling the pH during chlorination. Guideline value derivation • allocation to water 20% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments A similar TDI for trichloroacetate was established by IPCS based on a NOAEL for hepatic toxicity in a long‑term study in mice. Assessment date 2003 Principal reference WHO (2003) Trichloroacetic acid in drinking-water Trichloroacetic acid has been shown to induce tumours in the liver of mice. It has given mixed results in in vitro assays for mutations and chromosomal aberrations and has been reported to cause chromosomal aberrations in in vivo studies. IARC has classified trichloroacetic acid in Group 3, not classifiable as to its carcinogenicity to humans. The weight of evidence indicates that trichloroacetic acid is not a genotoxic carcinogen. Trichlorobenzenes (total) Releases of trichlorobenzenes (TCBs) into the environment occur through their manufacture and use as industrial chemicals, chemical intermediates and solvents. TCBs are found in drinking-water, but rarely at levels above 1 μg/l. General population exposure will primarily result from air and food. Reason for not establishing Occur in drinking‑water at concentrations well below those of health a guideline value concern, and health‑based value would exceed lowest reported odour threshold Assessment date 2003 Principal reference WHO (2003) Trichlorobenzenes in drinking-water The TCBs are of moderate acute toxicity. After short-term oral exposure, all three isomers show similar toxic effects, predominantly on the liver. Long-term toxicity and carcinogenicity studies via the oral route have not been carried out, but the data available suggest that all three isomers are non-genotoxic. A health-based value of 20 μg/l can be calculated for total TCBs on the basis of a TDI of 7.7 μg/kg body weight, based on liver toxicity identified in a 13-week rat study, taking into consideration the short duration of the study. However, because TCBs occur at concentrations well below those of health concern, it is not considered necessary to derive a formal guideline value. It should be noted that the health-based value exceeds the lowest reported odour threshold in water. 1,1,1-Trichloroethane 1,1,1-Trichloroethane is widely used as a cleaning solvent for electrical equipment, as a solvent for adhesives, coatings and textile dyes and as a coolant and lubricant. It is found mainly in the atmosphere, although it is mobile in soils and readily migrates to groundwaters. 1,1,1-Trichloroethane has been found in only a small proportion of surface waters and groundwaters, usually at concentrations of less than 20 μg/l; higher concentrations (up to 150 μg/l) have been observed in a few instances. There appears to be increasing exposure to 1,1,1-trichloroethane from other sources. Reason for not establishing Occur in drinking‑water at concentrations well below those of health a guideline value concern Assessment date 2003 Principal reference WHO (2003) 1,1,1-Trichloroethane in drinking-water 1,1,1-Trichloroethane is rapidly absorbed from the lungs and gastrointestinal tract, but only small amounts—about 6% in humans and 3% in experimental animals—are metabolized. Exposure to high concentrations can lead to hepatic steatosis (fatty liver) in both humans and laboratory animals. In a well-conducted oral study in mice and rats, effects included reduced liver weight and changes in the kidney consistent with hyaline droplet neuropathy. IARC has placed 1,1,1-trichloroethane in Group 3. 1,1,1-Trichloroethane does not appear to be mutagenic. A health-based value of 2 mg/l can be calculated for 1,1,1-trichloroethane on the basis of a TDI of 0.6 mg/kg body weight, based on changes in the kidney that were consistent with hyaline droplet nephropathy observed in a 13-week oral study in male rats, and taking into account the short duration of the study. However, because 1,1,1trichloroethane occurs at concentrations well below those of health concern, it is not considered necessary to derive a formal guideline value. Trichloroethene Trichloroethene (TCE) is used primarily in metal degreasing. However, its use has been substantially declining since the 1990s as a result of increased environmental regulations on TCE emissions. It is emitted mainly to the atmosphere, but it may also be introduced into groundwater and, to a lesser extent, surface water in industrial effluents. Poor handling and improper disposal of TCE in landfills have been the main causes of water contamination. Higher levels of TCE are expected in groundwater than in surface water because of the lack of volatilization that occurs from groundwater. Therefore, the most relevant routes of exposure are considered inhalation of contaminated air and ingestion of contaminated drinking-water, particularly from groundwater sources. Guideline value 0.008 mg/l (8 μg/l) Occurrence Typically present at low or undetectable concentrations in surface water (≤1 μg/l) due to high volatility and continued decline in TCE production. Concentrations may be higher (usually below 100 μg/l) in groundwater systems where volatilization and biodegradation are limited. TDI 0.5 μg/kg bw, based on the TDI values derived from three key studies showing decreased thymus weight in female mice, increased incidence of developmental immunotoxicity in mice and increased incidence of fetal cardiac malformations in rats. The narrow range of TDIs among the three key studies was further supported by two other studies in rats evidencing renal effects. Where applicable, uncertainty factors were applied to the points of departure from each of the three studies to account for interspecies differences, intraspecies variation and use of a LOAEL instead of a NOAEL. Limit of detection 0.01 μg/l by GC with ECD after liquid–liquid extraction; 0.01–3.0 μg/l by purge‑and‑trap capillary GC with PD or with PD and ECD in series; and 0.5 μg/l by purge‑and‑trap capillary GC‑MS Prevention and treatment Source control is by improved handling and disposal practices. Treatment of surface water sources is not needed because TCE volatilizes to the atmosphere. For groundwater sources, aeration (packed tower aeration and air stripping) and GAC are effective central treatment techonologies. Ozone and advanced oxidation processes may also be effective. Guideline value derivation • allocation to water • weight • consumption 50% of TDI 60 kg adult 2 litres/day Additional comments The guideline value is considered protective against both cancer and noncancer effects. In developing national standards, authorities may take into consideration the additional exposures through the dermal and inhalation routes from showering and bathing, especially in countries with low rates of ventilation in houses. Requirements for monitoring TCE in drinking‑water regulations and standards should be limited to groundwater sources where a possibility of TCE contamination is indicated. Monitoring is not needed for surface water sources because TCE volatilizes to the atmosphere. Assessment date 2020 Principal reference WHO (2020) Trichloroethene in drinking-water Available human and animal data after repeated exposure to TCE identify the kidney, liver, immune system, male reproductive system and developing fetus as potential targets of TCE toxicity and/or carcinogenicity. IARC has classified TCE as carcinogenic to humans (Group 1), concluding that sufficient epidemiological data are available for an association between exposure to TCE and human kidney cancer. Associations reported for liver cancer and non-Hodgkin lymphoma are characterized as limited, and evidence for other tumours is classified as inadequate. Multiple points of departure from different studies, rather than a single key study, were included in the derivation of the TDI. The overall TDI is based on three key studies with TDIs ranging from 0.3 to 0.6 μg/kg bw per day, and supported by other two studies in rats. Critical effects include increased incidence of heart malformations in rats (TDI of 0.6 μg/kg bw per day), decreased thymus weights in mice (TDI of 0.6 μg/kg bw per day) and developmental immunotoxicity (TDI of 0.37 μg/kg bw per day). The further supporting data in the database include studies reporting toxic nephropathy in rats (TDI of 0.3 μg/kg bw per day) and increased kidney weight in rats (TDI of 0.8 μg/kg bw per day by using route-to-route extrapolation from the inhalation study). Whenever possible, benchmark dose modelling and physiologically based pharmacokinetic modelling (to account for first-pass effects and inhalation-to-ingestion extrapolation) were applied. Trifluralin Trifluralin (CAS No. 1582-09-8) is a pre-emergence herbicide used in a number of crops. It has low water solubility and a high affinity for soil. However, biodegradation and photodegradation processes may give rise to polar metabolites that may contaminate drinking water sources. Although this compound is used in many countries, relatively few data are available concerning contamination of drinking water. Guideline value 0.02 mg/l (20 μg/l) Occurrence Not detected in the small number of drinking‑water samples analysed; has been detected in surface water at concentrations above 0.5 μg/l and rarely in groundwater TDI 7.5 μg/kg body weight, based on a NOAEL of 0.75 mg/kg body weight for mild hepatic effects in a 1year feeding study in dogs, with an uncertainty factor of 100 (for interspecies and intraspecies variation) Limit of detection 0.05 μg/l by GC with nitrogen–phosphorus detection Treatment performance 1 μg/l should be achievable using GAC Guideline value derivation • allocation to water 10% of TDI • weight 60 kg adult• consumption 2 litres/day Additional comments Authorities should note that some impure technical grades of trifluralin could contain potent carcinogenic compounds and therefore should not be used. Assessment date 1993 Principal reference WHO (2003) Trifluralin in drinking-water Trifluralin of high purity does not possess mutagenic properties. Technical trifluralin of low purity may contain nitroso contaminants and has been found to be mutagenic. No evidence of carcinogenicity was demonstrated in a number of long-term toxicity/carcinogenicity studies with pure (99%) test material. IARC has assigned technical-grade trifluralin to Group 3 (not classifiable as to its carcinogenicity to humans). Trihalomethanes (bromoform, bromodichloromethane, chloroform, dibromochloromethane) THMs are formed in drinking-water primarily as a result of chlorination of organic matter present naturally in raw water supplies. The rate and degree of THM formation increase as a function of the chlorine and humic acid concentration, temperature, pH and bromide ion concentration. Chloroform is the most common THM and the principal disinfection by-product in chlorinated drinking-water. In the presence of bromides, brominated THMs are formed preferentially, and chloroform concentrations decrease proportionally. It is assumed that most THMs present in water are ultimately transferred to air as a result of their volatility. For chloroform, for example, individuals may be exposed during showering to elevated concentrations from chlorinated tap water. For the volatile THMs, approximately equal contributions to total exposure come from four areas: ingestion of drinking-water, inhalation of indoor air largely due to volatilization from drinking-water, inhalation and dermal exposure during showering or bathing and ingestion of food, with all but food exposure arising primarily from drinking-water. Indoor air exposure to the volatile THMs is particularly important in countries with low rates of ventilation in houses and high rates of showering and bathing. Guideline values Chloroform: 0.3 mg/l (300 μg/l) Bromoform: 0.1 mg/l (100 μg/l) Dibromochloromethane (DBCM): 0.1 mg/l (100 μg/l) Bromodichloromethane (BDCM): 0.06 mg/l (60 μg/l) Occurrence THMs are not expected to be found in raw water (unless near a pollution source), but are usually present in finished or chlorinated water; concentrations are generally below 100 μg/l; in most circumstances, chloroform is the dominant compound TDIs Chloroform: 15 μg/kg body weight, derived from the lower 95% confidence limit for 5% incidence of hepatic cysts, generated by physiologically based pharmacokinetic modelling, in dogs that ingested chloroform in toothpaste for 7.5 years, using an uncertainty factor of 25 (10 for intraspecies differences in toxicokinetics and toxicodynamics and 2.5 for differences in interspecies toxicodynamics) Bromoform: 17.9 μg/kg body weight, based on the absence of histopathological lesions in the liver in a well‑conducted and well‑documented 90‑day study in rats, using an uncertainty factor of 1000 (100 for intraspecies and interspecies variation and 10 for possible carcinogenicity and short duration of exposure) DBCM: 21.4 μg/kg body weight, based on the absence of histopathological effects in the liver in a well‑conducted and well‑documented 90‑day study in rats, using an uncertainty factor of 1000 (100 for intra‑species and interspecies variation and 10 for the short duration of the study); an additional uncertainty factor for potential carcinogenicity was not applied because of the questions regarding mouse liver tumours from corn oil vehicles and inconclusive evidence of genotoxicity Basis of guideline value BDCM: Application of the linearized multistage model for the observed derivation increases in incidence of kidney tumours in male mice observed in an NTP bioassay Limit of detection 0.1–0.2 μg/l (method detection limits) by purge‑and‑trap and liquid– liquid extraction and direct aqueous injection in combination with a chromatographic system; 0.1 μg/l by GC‑ECD; 2.2 μg/l by GC‑MS Treatment performance Concentrations can be reduced by changes to disinfection practice (e.g. reducing organic THM precursors) or using air stripping. Guideline value derivation • allocation to water 20% of TDI for bromoform and DBCM 75% of TDI for chloroform • weight 60 kg adult• consumption 2 litres/day Additional comments on THMs For authorities wishing to establish a total THM standard to account for additive toxicity, the following fractionation approach could be taken: Cbromoform CDBCM CBDCM Cchloroform + + +≤ 1 GVbromoform GVDBCM GVBDCM GVchloroform where C = concentration and GV = guideline value. Authorities wishing to use a guideline value for total THMs should not simply add up the guideline values for the individual compounds in order to arrive at a standard. It is emphasized that adequate disinfection should never be compromised in attempting to meet guidelines for THMs. Nevertheless, in view of the potential link between adverse reproductive outcomes and THMs, particularly brominated THMs, it is recommended that THM levels in drinking‑water be kept as low as practicable. Additional comments on In countries with low rates of ventilation in houses and high rates of chloroform showering and bathing, the guideline value could be lowered to account for the additional exposures from inhalation of indoor air largely due to volatilization from drinking‑water and inhalation and dermal exposure during showering or bathing. Additional comments on BDCM The guideline value is based on the same study as in the third edition; the increase in value is primarily a result of an increase in the allocation of exposure in drinking‑water from 50% to 75% to account for the fact that chloroform is used less now than it was in 1993 when the original guideline was developed. Although a health‑based value of 21 μg/l is derived, the previous guideline value of 60 μg/l has been retained for two reasons: 1) both calculations were based on the same study, the only differences being the model and model assumptions used to derive the guideline value; there is therefore no scientific basis on which to justify a change in the guideline value; and 2) BDCM concentrations below 50 μg/l may be difficult to achieve using currently available technology without compromising the effectiveness of disinfection. As with chloroform, countries with low rates of ventilation and high rates of showering and bathing may wish to lower the guideline value to account for dermal and inhalation exposures, although, as noted above, concentrations below 50 μg/l may be difficult to achieve using currently available technology without compromising the effectiveness of disinfection. As BDCM was negative for carcinogenicity in a recent NTP bioassay in which it was dosed in drinking‑water, exceedances of the guideline value are not likely to result in an increased risk of cancer. Assessment date 2004 Principal references IPCS (2000) Disinfectants and disinfectant by-products IPCS (2004) Chloroform USNTP (1987). Toxicology and carcinogenesis studies of bromodichloromethane in F344/N rats and B6C3F1 mice (gavage studies) WHO (2005) Trihalomethanes in drinking-water Chloroform The weight of evidence for genotoxicity of chloroform is considered negative. IARC has classified chloroform as possibly carcinogenic to humans (Group 2B) based on limited evidence of carcinogenicity in humans but sufficient evidence of carcinogenicity in experimental animals. The weight of evidence for liver tumours in mice is consistent with a threshold mechanism of induction. Although it is plausible that kidney tumours in rats may similarly be associated with a threshold mechanism, there are some limitations of the database in this regard. The most universally observed toxic effect of chloroform is damage to the centrilobular region of the liver. The severity of these effects per unit dose administered depends on the species, vehicle and method by which the chloroform is administered. Bromoform In an NTP bioassay, bromoform induced a small increase in relatively rare tumours of the large intestine in rats of both sexes but did not induce tumours in mice. Data from a variety of assays on the genotoxicity of bromoform are equivocal. IARC has classified bromoform in Group 3 (not classifiable as to its carcinogenicity to humans). Dibromochloromethane In an NTP bioassay, DBCM induced hepatic tumours in female mice and possibly in male mice but not in rats. The genotoxicity of DBCM has been studied in a number of assays, but the available data are considered inconclusive. IARC has classified DBCM in Group 3 (not classifiable as to its carcinogenicity to humans). Bromodichloromethane IARC has classified BDCM in Group 2B (possibly carcinogenic to humans). BDCM gave both positive and negative results in a variety of in vitro and in vivo genotoxicity assays. In an NTP bioassay, BDCM induced renal adenomas and adenocarcinomas in both sexes of rats and male mice, rare tumours of the large intestine (adenomatous polyps and adenocarcinomas) in both sexes of rats and hepatocellular adenomas and adenocarcinomas in female mice. However, BDCM was negative for carcinogenicity in a recent NTP bioassay in which it was dosed in drinking-water. Exposure to BDCM has also been linked to a possible increase in reproductive effects (increased risk for spontaneous abortion or stillbirth). Uranium Uranium is widespread in nature, occurring in granites and various other mineral deposits. It is used mainly as fuel in nuclear power stations. Uranium is present in the environment as a result of leaching from natural deposits, release in mill tailings, emissions from the nuclear industry, the combustion of coal and other fuels and the use of phosphate fertilizers that contain uranium. Intake of uranium through air is low, and it appears that intake through food is between 1 and 4 μg/day. Intake through drinking-water is normally extremely low; however, in circumstances in which uranium is present in a drinking-water source, the majority of intake can be through drinking-water. Provisional guideline value 0.03 mg/l (30 μg/l) The guideline value is designated as provisional because of scientific uncertainties surrounding uranium toxicity. Occurrence Levels in drinking‑water are generally less than 1 μg/l, although concentrations as high as 700 μg/l have been measured in private supplies. TDI 60 μg, derived from the lower 95% confidence limit on the 95th percentile uranium exposure distribution in a study from Finland, using an uncertainty factor of 10 for intraspecies variation Limit of detection 0.01 μg/l by ICP‑MS; 0.1 μg/l by solid fluorimetry with either laser excitation or UV light; 0.2 μg/l by ICP using adsorption with chelating resin Treatment performance 1 μg/l should be achievable using conventional treatment (e.g. coagulation or ion exchange) Guideline value derivation • consumption 2 litres/day Additional comments Where supplies exceed 30 μg/l, it is important that precipitate action be avoided. Consideration should first be given to exposure from all sources and the availability of alternative safe sources. Only chemical, not radiological, aspects of uranium toxicity have been addressed here. Assessment date 2003, revised in 2011 Principal reference WHO (2012) Uranium in drinking-water There are insufficient data regarding the carcinogenicity of uranium in humans and experimental animals. Nephritis is the primary chemically induced effect of uranium in humans. Little information is available on the chronic health effects of exposure to environmental uranium in humans. A number of epidemiological studies of populations exposed to uranium in drinking-water have shown a correlation with alkaline phosphatase and β-microglobulin in urine along with modest alterations in proximal tubular function. However, the actual measurements were still within the normal physiological range, and these findings are not consistent between studies. No clear no-effect concentration has emerged from the human studies to date. This is not surprising, as most of the study populations are quite small, and there is substantial normal variation in the measured parameters in the human population. However, the overall indications are that there is no clear evidence of effects below an exposure concentration of 30 μg/l. In fact, the evidence for effects on the kidney, which appears to be the most sensitive organ, is equivocal until much higher exposure concentrations. The provisional guideline value of 30 μg/l, which is derived from new epidemiological studies on populations exposed to high uranium concentrations, replaces the previous value derived from experimental animal studies and designated as provisional on the basis of uncertainties regarding the toxicology and epidemiology of uranium as well as difficulties concerning its technical achievability in smaller supplies. It is noted that studies on human populations, when available and of good quality, are the preferred source of health-related information to be used in deriving guideline values. Vinyl chloride Vinyl chloride is used primarily for the production of PVC. Owing to its high volatility, vinyl chloride has rarely been detected in surface waters, except in contaminated areas. Unplasticized PVC is increasingly being used in some countries for water mains supplies. Migration of vinyl chloride monomer from unplasticized PVC is a possible source of vinyl chloride in drinking-water. It appears that inhalation is the most important route of vinyl chloride intake, although drinking-water may contribute a substantial portion of daily intake where PVC piping with a high residual content of vinyl chloride monomer is used in the distribution network. Vinyl chloride has been reported in groundwater as a degradation product of the chlorinated solvents trichloroethene and tetrachloroethene. Guideline value 0.0003 mg/l (0.3 μg/l) Occurrence Rarely detected in surface waters, the concentrations measured generally not exceeding 10 μg/l; much higher concentrations found in groundwater and well water in contaminated areas; concentrations up to 10 μg/l detected in drinking‑water Basis of guideline value Application of a linear extrapolation by drawing a straight line between the derivation dose, determined using a pharmocokinetic model, resulting in tumours in 10% of animals in rat bioassays involving oral exposure and the origin (zero dose), determining the value associated with the upper‑bound risk of 10−5 and assuming a doubling of the risk for exposure from birth Limit of detection 0.01 μg/l by GC‑ECD or GC‑FID with MS for confirmation Treatment performance 0.001 mg/l should be achievable using air stripping Additional comments The results of the linear extrapolation are nearly identical to those derived using the linearized multistage model. As vinyl chloride is a known human carcinogen, exposure to this compound should be avoided as far as practicable, and levels should be kept as low as technically feasible. Vinyl chloride is primarily of concern as a potential contaminant from some grades of PVC pipe and is best controlled by specification of material quality. Assessment date 2003 Principal references IPCS (1999) Vinyl chloride WHO (2004) Vinyl chloride in drinking-water There is sufficient evidence of the carcinogenicity of vinyl chloride in humans from industrial populations exposed to high concentrations via the inhalation route, and IARC has classified vinyl chloride in Group 1 (carcinogenic to humans). Studies of workers employed in the vinyl chloride industry showed a marked exposure–response for all liver cancers, angiosarcomas and hepatocellular carcinoma, but no strong relationship between cumulative vinyl chloride exposure and other cancers. Experimental animal data show vinyl chloride to be a multisite carcinogen. When administered orally or by inhalation to mice, rats and hamsters, it produced tumours in the mammary gland, lungs, Zymbal gland and skin, as well as angiosarcomas of the liver and other sites. Evidence indicates that vinyl chloride metabolites are genotoxic, interacting directly with DNA. DNA adducts formed by the reaction of DNA with a vinyl chloride metabolite have also been identified. Occupational exposure has resulted in chromosomal aberrations, micronuclei and sister chromatid exchanges; response levels were correlated with exposure levels. Xylenes Xylenes are used in blending petrol, as a solvent and as a chemical intermediate. They are released to the environment largely via air. Exposure to xylenes is mainly from air, and exposure is increased by smoking. Guideline value 0.5 mg/l (500 μg/l) Occurrence Concentrations of up to 8 μg/l have been reported in surface water, groundwater and drinking‑water; levels of a few milligrams per litre were found in groundwater polluted by point emissions; xylenes can also penetrate plastic pipe from contaminated soil TDI 179 μg/kg body weight, based on a NOAEL of 250 mg/kg body weight per day for decreased body weight in a 103‑week gavage study in rats, adjusting for daily dosing and using an uncertainty factor of 1000 (100 for interspecies and intraspecies variation and 10 for the limited toxicological end‑points) Limit of detection 0.1 μg/l by GC‑MS; 1 μg/l by GC‑FID Treatment performance 0.005 mg/l should be achievable using GAC or air stripping Guideline value derivation • allocation to water • weight • consumption 10% of TDI 60 kg adult 2 litres/day Additional comments The guideline value exceeds the lowest reported odour threshold for xylenes in drinking‑water. Assessment date 1993 Principal reference WHO (2003) Xylenes in drinking-water Xylenes are rapidly absorbed by inhalation. Data on oral exposure are lacking. Xylenes are rapidly distributed in the body, predominantly in adipose tissue. They are almost completely metabolized and excreted in urine. The acute oral toxicity of xylenes is low. No convincing evidence for teratogenicity has been found. Long-term carcinogenicity studies have shown no evidence for carcinogenicity. In vitro as well as in vivo mutagenicity tests have proved negative. Zinc Zinc is an essential trace element found in virtually all food and potable water in the form of salts or organic complexes. The diet is normally the principal source of zinc. Although levels of zinc in surface water and groundwater normally do not exceed 0.01 and 0.05 mg/l, respectively, concentrations in tap water can be much higher as a result of dissolution of zinc from pipes. Reason for not establishing Not of health concern at levels found in drinking‑water a guideline value Additional comments May affect acceptability of drinking‑water Assessment date 1993 Principal reference WHO (2003) Zinc in drinking-water In 1982, JECFA proposed a PMTDI for zinc of 1 mg/kg body weight. The daily requirement for adult men is 15–20 mg/day. It was considered that, taking into account recent studies on humans, the derivation of a formal guideline value is not required at this time. However, drinking-water containing zinc at levels above 3 mg/l may not be acceptable to consumers (see chapter 10). 12.2 Pesticides used for vector control in drinking-water sources and containers In setting local guidelines or standards in the context of local storage practices and realistic insecticide application regimes, health authorities should take into consideration the potential for higher rates of water consumption in the area or region under consideration. However, exceeding the ADIs will not necessarily result in adverse effects. The diseases spread by vectors are significant causes of morbidity and mortality. It is therefore important to achieve an appropriate balance between the intake of the pesticides from drinking-water and the control of disease-carrying insects. Better than establishing guideline values are the formulation and implementation of a comprehensive management plan for household water storage and domestic waste management that does not rely exclusively on larviciding by insecticides, but also includes other environmental management measures and social behavioural changes. Formulations of pesticides used for vector control in drinking-water should strictly follow the label recommendations and should only be those approved for such a use by national authorities, taking into consideration the ingredients and formulants used in making the final product. National authorities should note that these assessments refer only to the active ingredients and do not consider the additives in different formulations. Bacillus thuringensis israelensis Two Bacillus thuringiensis israelensis (Bti) (strain AM65-52) products (water-dispersible granule and ready-to-use tablet) have been evaluated by WHOPES and recommended as mosquito larvicides, including their use against container-breeding mosquitoes. Quality control specifications and efficacy evaluations for Bti water-dispersible granule have been published. WHO recommendations on the use of pesticides in public health are valid only if linked to WHO specifications for their quality control. Reason for not establishing Not considered appropriate to set guideline values for pesticides used a guideline value for vector control in drinking‑water Assessment date 2009 Principal references IPCS (1999) Bacillus thuringiensis WHO (2004) Report of the seventh WHOPES working group meeting WHO (2006) Report of the ninth WHOPES working group meeting WHO (2007) WHO specifications and evaluations for public health pesticides WHO (2009) Bacillus thuringiensis israelensis (Bti) in drinking-water Preparations of Bti are widely used against mosquitoes, chironomids and black-flies, and this specific activity against disease vector species has resulted in the use of Bti in water. Bti is recommended under WHOPES for use in vector control, including against container-breeding mosquitoes, and can be used in drinking-water that will receive little or no further treatment for control of Aedes aegypti. It is essential that Bti for larvicidal use be prepared under carefully controlled conditions and properly assayed before use for evidence of potency, for excessive levels of expressed Bti constituents or metabolites that are toxic and for contamination by other undesirable microbes. Bti itself is not considered to pose a hazard to humans through drinking-water. Therefore, it is not considered necessary or appropriate to establish a health-based value for its use for controlling vector larvae in drinking-water. However, it is vital that authorities can be assured that Bti has been prepared to the highest quality and hygienic standards under appropriate conditions that will meet the WHOPES specifications. It is important that the possible risks are set against the risks from vector-borne diseases such as dengue fever. Application should be carried out by trained applicators and Bti used in conjunction with other approaches to vector control, including exclusion of mosquitoes from containers and other control options. Diflubenzuron Diflubenzuron is a direct-acting insecticide normally applied directly to plants or water. It is used in public health applications against mosquito and noxious fly larvae. WHO is considering diflubenzuron for use as a mosquito larvicide in drinking-water in containers, particularly to control dengue fever. The recommended dosage of diflubenzuron in potable water in containers should not exceed 0.25 mg/l under WHOPES. It is reported that public exposure to diflubenzuron through either food or drinking-water is negligible. However, there is a potential for direct exposure through drinking-water when diflubenzuron is directly applied to drinking-water storage containers. Reason for not establishing Not considered appropriate to set guideline values for pesticides used a guideline value for vector control in drinking‑water Assessment date 2007 Principal references FAO/WHO (2002) Pesticide residues in food—2001 evaluations WHO (2008) Diflubenzuron in drinking-water Diflubenzuron is considered to be of very low acute toxicity. The primary target for toxicity is the erythrocytes, although the mechanism of haematotoxicity is uncertain. There is no evidence that diflubenzuron is either genotoxic or carcinogenic. It also does not appear to be fetotoxic or teratogenic and does not show significant signs of reproductive toxicity. There is evidence that young animals are not significantly more sensitive than adults to the effects of diflubenzuron. It is not considered appropriate to set a formal guideline value for diflubenzuron used as a vector control agent in drinking-water. Where diflubenzuron is used for vector control in potable water, this will involve considerably less than lifetime exposure. The ADI determined by JMPR in 2001 was 0–0.02 mg/kg body weight. The maximum dosage in drinking-water of 0.25 mg/l would be equivalent to approximately 40% of the upper limit of the ADI allocated to drinking-water for a 60 kg adult drinking 2 litres of water per day. For a 10 kg child drinking 1 litre of water, the exposure would be 0.25 mg, compared with an exposure of 0.2 mg at the upper limit of the ADI. For a 5 kg bottle-fed infant drinking 0.75 litre per day, the exposure would be 0.19 mg, compared with an exposure of 0.1 mg at the upper limit of the ADI. Diflubenzuron is unlikely to remain in solution at the maximum recommended applied dose, and the actual levels of exposure are likely to be much lower than those calculated. Consideration should be given to using alternative sources of water for bottle-fed infants for a period after an application of diflubenzuron, where this is practical. However, exceeding the ADI will not necessarily result in adverse effects. Methoprene WHO has assessed methoprene for use as a mosquito larvicide in drinking-water in containers, particularly to control dengue fever. The recommended dosage of methoprene in potable water in containers should not exceed 1 mg/l under WHOPES. Reason for not establishing Not considered appropriate to set guideline values for pesticides used a guideline value for vector control in drinking‑water Assessment date 2007 Principal references FAO/WHO (2002) Pesticide residues in food—2001 evaluations WHO (2008) Methoprene in drinking-water In 2001, JMPR reaffirmed the basis of the ADI for racemic methoprene established in 1987, but lowered the value to 0–0.09 mg/kg body weight to correct for the purity of the racemate tested. The basis for the ADI was the NOAEL of 500 mg/kg diet, equivalent to 8.6 mg/kg body weight per day (corrected for purity), in a 90-day study in dogs (the main effect was increased relative liver weight) and a safety factor of 100. Young animals do not appear to be significantly more sensitive than adults. As no bridging studies with repeated doses were available for (S)-methoprene, JMPR made the conservative assumption that, in the absence of any information to the contrary, all the toxicity of the racemate was due to the S enantiomer. On this basis, JMPR established an ADI for (S)-methoprene of 0–0.05 mg/kg body weight, equal to one half the ADI for the racemate (which is a 1:1 mixture of the R and S enantiomers). It is not considered appropriate to set a formal guideline value for methoprene used as a vector control agent in drinking-water. Where methoprene is used for vector control in potable water, this will involve less than lifetime exposure. The maximum dosage in drinking-water of 1 mg/l would be equivalent to approximately 66% of the upper limit of the ADI (0.033 mg/kg body weight) for a 60 kg adult drinking 2 litres of water per day. The exposure for a 10 kg child drinking 1 litre of water would be approximately 0.1 mg/kg body weight, and for a 5 kg bottle-fed infant, the exposure would be approximately 0.15 mg/kg body weight, compared with the upper limit of the ADI of 0.05 mg/kg body weight. However, the low solubility and the high log octanol–water partition coefficient of methoprene indicate that it is unlikely to remain in solution at the maximum recommended applied dose, and the actual levels of exposure are likely to be much lower than those calculated. Exposure from food is considered to be low. Consideration should be given to using alternative sources of water for small children and bottle-fed infants for a period after an application of methoprene, where this is practical. However, exceeding the ADI will not necessarily result in adverse effects. Novaluron Novaluron has been registered as an insecticide for food crops and ornamentals in a number of countries. WHO has assessed novaluron for use as a mosquito larvicide in drinking-water in containers, particularly to control dengue fever. The recommended dosage of novaluron in potable water in containers should not exceed 0.05 mg/l under WHOPES. Reason for not establishing Not considered appropriate to set guideline values for pesticides used a guideline value for vector control in drinking‑water Assessment date 2007 Principal references FAO/WHO (2006) Pesticide residues in food—2005 evaluations WHO (2008) Novaluron in drinking-water. In view of the absence of a carcinogenic potential in rodents and the lack of genotoxic potential in vitro and in vivo, JMPR concluded that novaluron is unlikely to pose a carcinogenic risk to humans. JMPR also concluded that novaluron is not a developmental toxicant. JMPR established an ADI of 0–0.01 mg/kg body weight on the basis of the NOAEL of 1.1 mg/kg body weight per day for erythrocyte damage and secondary splenic and liver changes in a 2-year dietary study in rats, using a safety factor of 100. It is not considered appropriate to set a formal guideline value for novaluron as a vector control agent in drinking-water. At the maximum recommended dosage for drinking-water of 0.05 mg/l, the intake of a 60 kg adult drinking 2 litres of water would represent only 17% of the upper limit of the ADI. Similarly, the intake for a 10 kg child drinking 1 litre of water would be 50% of the upper limit of the ADI, whereas a 5 kg bottle-fed infant drinking 0.75 litre of water would receive an intake of 75% of the upper limit of the ADI. The high log octanol–water partition coefficient of 4.3 indicates that novaluron is likely to adsorb to the sides of containers, and so the actual concentration is likely to be less than the recommended dose. Exposure to novaluron through food is not expected to be significant. Permethrin Permethrin (CAS No. 52645-53-1) is a contact insecticide effective against a broad range of pests in agriculture, forestry and public health. It has been used as a larvicide to control aquatic invertebrates in water mains. Permethrin is photodegraded both in water and on soil surfaces. In soil, permethrin is rapidly degraded by hydrolysis and microbial action under aerobic conditions. Exposure of the general population to permethrin is mainly via the diet. Reason for not establishing Not recommended for direct addition to drinking‑water as part of a guideline value WHO’s policy to exclude the use of any pyrethroids for larviciding of mosquito vectors of human disease Assessment date 2011 Principal references FAO/WHO (2000) Pesticide residues in food—1999 evaluations WHO (2011) Permethrin in drinking-water Technical-grade permethrin is of low acute toxicity. The cis isomer is considerably more toxic than the trans isomer. IARC has classified permethrin in Group 3 (not classifiable as to its carcinogenicity to humans), as there are no human data and only limited data from experimental animal studies. Permethrin is not genotoxic. JMPR concluded that technical-grade permethrin is not a reproductive or developmental toxin. For guidance purposes, a health-based value can be derived from an ADI of 0–0.05 mg/kg body weight, established for technical-grade permethrin with cis:trans ratios of 25:75 to 40:60 on the basis of a NOAEL of 5 mg/kg body weight per day in a 2-year dietary study in rats, which was based on clinical signs and changes in body and organ weights and blood chemistry at the next higher dose, and a NOAEL of 5 mg/kg body weight per day in a 1-year study in dogs, based on reduced body weight at 100 mg/kg body weight per day, and applying an uncertainty factor of 100 for interspecies and intraspecies variation. Assuming a 60 kg adult drinking 2 litres of water per day and allocating 20% of the upper limit of the ADI to drinking-water, a health-based value of 0.3 mg/l can be derived. Adding permethrin directly to drinking-water for public health purposes is not recommended by WHO, as part of its policy to exclude the use of any pyrethroids for larviciding of mosquito vectors of human disease. This policy is based on concern over the possible accelerated development of vector resistance to synthetic pyrethroids, which, in their application to insecticide-treated mosquito nets, are crucial in the current global anti-malaria strategy. Pirimiphos-methyl Pirimiphos-methyl is an organophosphorus compound that is used in a wide range of pesticidal applications. Pirimiphos-methyl is being considered by WHO for addition to potable water in containers as a mosquito larvicide treatment, particularly to control dengue fever. The manufacturer recommends the direct addition of 1 mg/l to water. Reason for not establishing Not recommended for direct application to drinking‑water unless no a guideline value other effective and safe treatments are available Assessment date 2007 Principal references FAO/WHO (1993) Pesticide residues in food—1992 evaluations FAO/WHO (2008) Pesticide residues in food—2006 evaluations WHO (2008) Pirimiphos-methyl in drinking-water The only biochemical effect consistently observed with pirimiphos-methyl in acute, short-term or long-term studies is cholinesterase inhibition. Studies with mice, rats and dogs showed NOAELs of 0.5 mg/kg body weight per day and above. Young animals do not appear to be significantly more sensitive than adults. In human studies, no cholinesterase inhibition was seen at 0.25 mg/kg body weight per day (the highest dose tested). On this basis, JMPR revised the ADI to 0–0.03 mg/kg body weight by applying a 10-fold safety factor to the NOAEL in the human studies. At the maximum recommended dosage for drinking-water of 1 mg/l, a 60 kg adult drinking 2 litres of water would have an intake of 0.033 mg/kg body weight, compared with the upper limit of the ADI of 0.03 mg/kg body weight. The intake for a 10 kg child drinking 1 litre of water would be 0.1 mg/kg body weight; for a 5 kg bottle-fed infant drinking 0.75 litre, it would be 0.15 mg/kg body weight. There is uncertainty regarding the level that would cause effects in humans, as the NOAEL on which the ADI is based was the highest dose tested, and so the ADI may be more conservative than is at first apparent. These intake figures are all below the acute reference dose of 0.2 mg/kg body weight and would not result in an acute exposure risk from the initial application of pirimiphos-methyl to drinking-water containers at the recommended dose. In addition, the low solubility and the high log octanol– water partition coefficient of pirimiphos-methyl indicate that the larvicide is very unlikely to remain in solution at the maximum recommended applied dose, so the actual levels of exposure are expected to be lower than those calculated. Exposure from food is generally considered to be low, but occasional high exposures can be experienced. Based on the above calculations, pirimiphos-methyl is not recommended for direct application to drinking-water unless no other effective and safe treatments are available. If pirimiphos-methyl is applied directly to drinking-water, consideration should be given to using alternative sources of water for bottle-fed infants and small children for a period after its application, where this is practical. However, it is noted that exceeding the ADI will not necessarily result in adverse effects. Pyriproxyfen Pyriproxyfen is a broad-spectrum insect growth regulator with insecticidal activity against public health insect pests, including mosquitoes. WHO has assessed pyriproxyfen for use as a mosquito larvicide in drinking-water in containers, particularly to control dengue fever. The recommended dosage of pyriproxyfen in potable water in containers should not exceed 0.01 mg/l under WHOPES. Reason for not establishing Not considered appropriate to set guideline values for pesticides used a guideline value for vector control in drinking‑water Assessment date 2007 Principal references FAO/WHO (2000) Pesticide residues in food—1999 evaluations WHO (2008) Pyriproxyfen in drinking-water JMPR evaluated pyriproxyfen and concluded that it was not genotoxic and does not pose a carcinogenic risk to humans. Young animals do not appear to be significantly more sensitive than adults. JMPR established an ADI of 0–0.1 mg/kg body weight on the basis of an overall NOAEL of 10 mg/kg body weight per day, based on increased relative liver weight and increased total plasma cholesterol concentration in male dogs in two 1-year studies of toxicity and using a safety factor of 100. It is not considered appropriate to set a formal guideline value for pyriproxyfen used for vector control in drinking-water. The maximum recommended dosage in drinking-water of 0.01 mg/l would be equivalent to less than 1% of the upper limit of the ADI allocated to drinking-water for a 60 kg adult drinking 2 litres of water per day. For a 10 kg child drinking 1 litre of water, the exposure would be 0.01 mg, compared with an exposure of 1 mg at the upper limit of the ADI. For a 5 kg bottle-fed infant drinking 0.75 litre per day, the exposure would be 0.0075 mg, compared with an exposure of 0.5 mg at the upper limit of the ADI. The low solubility and the high log octanol–water partition coefficient of pyriproxyfen indicate that it is unlikely to remain in solution at the maximum recommended applied dose, and the actual levels of exposure are likely to be even lower than those calculated. Spinosad Spinosad is a natural product derived from the bacterium Saccharopolyspora spinosa. Spinosad DT is a mixture of spinosyn A and spinosyn D. It is used for mosquito control in potable water in containers. Spinosad DT 7.48% is specified for use as a vector control agent in drinking-water sources against Aedes aegypti by WHO under WHOPES. Formulations for control of vectors are specified by WHO at a dose of 0.25–0.5 mg/l. The expected duration of efficacy under field conditions is 4–6 weeks. Three formulations of spinosad have been evaluated by WHOPES for mosquito larviciding. WHO specifications for quality control and international trade have been published for the three formulations: i.e. spinosad granules (636/GR), aqueous suspension concentrate (636/SC) and tablets for direct application (636/DT). Only the tablet formulation is used for mosquito larviciding in potable water at the dosage of 0.25–0.5 mg/l of the active ingredient. In a 14-day study conducted by the manufacturer, a single tablet was added to a 200-litre container of water, and 10% of the water in this container was replenished each day of the study. The concentration of spinosad was found to be in the range 26.5–51.7 μg/l. Reason for not establishing Not considered appropriate to set guideline values for pesticides used a guideline value for vector control in drinking‑water Assessment date 2009 Principal references FAO/WHO (2002) Pesticide residues in food—2001 evaluations WHO (2010) Spinosad in drinking-water It is not appropriate to set a formal guideline value for spinosad DT for use to control vectors breeding in drinking-water containers; however, it is appropriate to compare the probable intakes with the ADI of 0–0.02 mg/kg body weight, with no acute reference dose set because of its low acute toxicity. The maximum concentration actually achieved with the slow-release formulation was approximately 52 μg/l. The intake would therefore be: • 39 μg for a 5 kg bottle-fed infant assuming consumption of 0.75 litre = 7.8 μg/kg body weight • 52 μg for a 10 kg child assuming consumption of 1 litre = 5.2 μg/kg body weight • 104 μg for a 60 kg adult assuming consumption of 2 litres = 1.7 μg/kg body weight. However, this could be higher if drinking-water consumption is also higher. This means that the exposure is well below the upper limit of the ADI for all sectors of the population. Even the application of a double dose would result in exposure below the upper limit of the ADI. The ADI is, of course, set for lifetime exposure, and the average exposure over time will be lower than the exposures indicated above. Temephos Temephos is an organophosphorus insecticide that is used mainly as a larvicide to control mosquitoes on ponds, marshes and swamps and midges, black flies and other insects in public health. It is also used for mosquito control in potable water in containers. It is specified for use as a vector control agent in drinking-water sources by WHO under WHOPES. Formulations for control of vectors are specified by WHO, and only those approved by WHOPES should be used for this purpose. The recommendation for the use of temephos in potable water is that the dosage should not exceed 1 mg/l. Reason for not establishing Not considered appropriate to set guideline values for pesticides used a guideline value for vector control in drinking‑water Assessment date 2009 Principal references FAO/WHO (2008) Pesticide residues in food—2006 evaluations WHO (2009) Temephos in drinking-water The NOAEL for human risk assessment for temephos is 2.3 mg/kg body weight per day on the basis of inhibition of brain acetylcholinesterase activity in rats, as determined by JMPR in 2006. Although JMPR considered that the database was insufficiently robust to serve as the basis for establishing an ADI, for the purposes of these Guidelines, a TDI of 0.023 mg/kg body weight can be calculated from this NOAEL, using an uncertainty factor of 100. Young animals do not appear to be significantly more sensitive than adults, and exposure from food is considered to be low. It is not appropriate to set a formal guideline value for temephos used as a vector control agent in drinking-water. Where temephos is used for vector control in potable water, this will involve less than lifetime exposure. The maximum dosage in drinking-water of 1 mg/l for a 60 kg adult drinking 2 litres of water per day would be equivalent to approximately 0.033 mg/kg body weight, compared with the TDI of 0.023 mg/kg body weight. The exposure for a 10 kg child drinking 1 litre of water would be approximately 0.1 mg/kg body weight; for a 5 kg bottle-fed infant, the exposure would be approximately 0.15 mg/kg body weight, compared with the TDI of 0.023 mg/kg body weight. Consideration should be given to using alternative sources of water for small children and bottle-fed infants for a period after an application of temephos, where this is practical. However, exceeding the TDI does not necessarily mean that this will result in adverse effects. Indeed, the low solubility and the high log octanol–water partition coefficient of temephos indicate that it is unlikely to remain in solution at the maximum recommended applied dose, and the use of the slow-release formulation should result in very much lower concentrations than the approved dose of 1 mg/l and actual exposures much lower than the theoretical exposures calculated above. ANNEX 1 Supporting documentation to the Guidelines TThe Guidelines for drinking-water quality are accompanied by separate texts that provide background information substantiating the derivation of the Guidelines and providing guidance on good practice towards effective implementation. These are available as published texts, through the internet (https://www.who.int/teams/ environment-climate-change-and-health/water-sanitation-and-health/water-safetyand-quality/publications). Some of these may also be ordered at https://extranet.who. int/dataformv3/index.php/415855. Published supporting documents A guide to equitable water safety planning: Ensuring no one is left behind Published in 2019 by the World Health Organization Provides step-by-step guidance and good practice examples on how the WSP approach can bring tangible improvements in water quality and availability for all users. www.who.int/publications/i/item/9789241515313 A practical guide to auditing water safety plans Published in 2015 by the World Health Organization Provides guidance on developing and implementing a WSP auditing scheme, including examples, case studies and tools from more than a dozen low-, middle- and high-income countries with WSP auditing experience https://www.who.int/publications/i/item/9789241509527 Alternative drinking-water disinfectants: Bromine, iodine and silver Published iin 2018 by the World Health Organization A review of the evidence on the efficacy and toxicity of bromine, iodine and silver as drinking-water disinfectants. www.who.int/publications/i/item/9789241513692 Arsenic primer: Guidance on the investigation and mitigation of arsenic contamination Published in 2018 by United Nations Children’s Fund and the World Health Organization Provides practical advice for UN agencies, government counterparts and development workers responding to the challenge of arsenic contamination of drinking-water. https://www.who.int/publications/m/item/arsenic-primer 491 Assessing microbial safety of drinking water: Improving approaches and methods Edited by A. Dufour et al. Published in 2003 by IWA Publishing on behalf of the World Health Organization and the Organisation for Economic Co-operation and Development A state-of-the-art review of approaches and methods used in assessing the microbial safety of drinking-water. https://www.who.int/publications/i/item/9241546301 Boil water Published in 2015 by the World Health Organization Provides the scientific basis for the efficacy of boiling water https://www.who.int/publications/i/item/WHO-FWC-WSH-15.02 Calcium and magnesium in drinking-water: Public health significance Edited by J. Cotruvo and J. Bartram Published in 2009 by the World Health Organization A review of the contribution of drinking-water to total daily intake of calcium and magnesium, and an assessment of possible health benefits, including reducing cardiovascular disease mortality and osteoporosis. https://www.who.int/publications/i/item/9789241563550 Chemical mixtures in source water and drinking-water Published in 2017 by the World Health Organization Provides an overview of available tools and practical recommendations to support the assessment and management of risks to human health associated with chemical mixtures in drinking-water and its sources. https://www.who.int/publications/i/item/9789241512374 Chemical safety of drinking-water: Assessing priorities for risk management T. Thompson et al. Published in 2007 by the World Health Organization A tool to assist in undertaking a systematic assessment of water supply systems to prioritize, control or eliminate chemicals in drinking-water. https://www.who.int/publications-detail-redirect/924154676X Climate-resilient water safety plans: Managing health risks associated with climate variability and change Published in 2017 by the World Health Organization Provides guidance on how climate considerations can be integrated into water safety planning to provide greater resilience to the current and predicted impacts of climate change and variability on water supplies. www.who.int/publications/i/item/9789241512794 Developing drinking-water quality regulations and standards Published in 2018 by the World Health Organization Provides practical guidance to support the development or revision of customized national or subnational drinking-water quality regulations and standards. www.who.int/publications/i/item/9789241513944 Domestic water quantity, service level and health, 2nd edition G. Howard et al. Published in 2020 by the World Health Organization Reviews the evidence about the relationships between water quantity, water accessibility and health, providing updated guidance, including recommended targets on domestic water supply to ensure beneficial health outcomes. www.who.int/publications/i/item/9789240015241 Evaluating household water treatment options: Health-based targets and microbiological performance specifications J. Brown and M. Sobsey Published in 2011 by the World Health Organization Establishes health-based targets and testing protocols for point-of-use water treatment approaches, including to inform development of country certification programmes. https://www.who.int/publications/i/item/9789241548229 Evaluation of the H2S method for detection of fecal contamination of drinking water M. Sobsey and F. Pfaender Published in 2002 by the World Health Organization Provides the scientific basis, validity, available data and other information concerning the use of “H2S tests” as measures or indicators of faecal contamination in drinking-water. https://www.who.int/publications/i/item/WHO-SDE-WSH-02.08 Fluoride in drinking-water J.K. Fawell et al. Published in 2006 by IWA Publishing on behalf of the World Health Organization Provides information on the occurrence of fluoride in drinking-water, its health effects, ways of reducing excess levels and methods for analysis of fluoride in water. https://www.who.int/publications/i/item/9241563192 Guide to hygiene and sanitation in aviation, 3rd edition. Module 1: Water; Module 2: Cleaning and disinfection of facilities Published in 2009 by the World Health Organization Addresses water and cleaning and disinfection of facilities with the ultimate goal of assisting all types of airport and aircraft operators and other responsible bodies in achieving high standards of hygiene and sanitation, to protect travellers. https://www.who.int/publications/i/item/9789241547772 Guide to ship sanitation, 3rd edition Published in 2011 by the World Health Organization Presents the public health significance of ships in terms of disease and highlights the importance of applying appropriate control measures. https://www.who.int/publications/i/item/9789241546690 Hazard characterization for pathogens in food and water: Guidelines Published in 2003 by the Food and Agriculture Organization of the United Nations and the World Health Organization A practical framework and structured approach for the characterization of microbial hazards in food and water, to assist governmental and research scientists. https://www.who.int/publications-detail-redirect/9241562374 Health aspects of plumbing Published in 2006 by the World Health Organization and the World Plumbing Council A description of the processes involved in the design, installation and maintenance of effective plumbing systems and consideration of the microbial, chemical, physical and financial concerns associated with plumbing. https://www.who.int/publications/i/item/9241563184 Heterotrophic plate counts and drinking-water safety: The significance of HPCs for water quality and human health Edited by J. Bartram et al. Published in 2003 by IWA Publishing on behalf of the World Health Organization Assessment of the role of the heterotrophic plate count measurement in drinking-water safety management. https://www.who.int/publications/i/item/9241562269 Legionella and the prevention of legionellosis Edited by J. Bartram et al. Published in 2007 by the World Health Organization An overview of the sources, ecology and laboratory detection of Legionella bacteria, risk assessment and risk management of susceptible environments, the necessary measures to prevent or adequately control the risks and the policies and practices for outbreak management. https://www.who.int/publications/i/item/9241562978 Management of radioactivity in drinking-water Published in 2018 by the World Health Organization Guidance to support radionuclide management in drinking-water in emergency and nonemergency situations. https://www.who.int/publications/i/item/9789241513746 Managing water in the home: Accelerated health gains from improved water supply M. Sobsey Published in 2002 by the World Health Organization A review of the various methods and systems for household water collection, treatment and storage. https://www.who.int/publications/i/item/WHO-SDE-WSH-02.07 Microplastics in drinking-water Published in 2019 by the World Health Organization Critically examines the evidence related to the occurrence of microplastics in the water cycle (including both tap and bottled drinking-water and its sources), the potential health impacts from microplastic exposure and the removal of microplastics during wastewater and drinking-water treatment. https://www.who.int/publications/i/item/9789241516198 Pathogenic mycobacteria in water: A guide to public health consequences, monitoring and management Edited by J. Bartram et al. Published in 2004 by IWA Publishing on behalf of the World Health Organization A description of the distribution, routes of transmission and infection, and guidance on the control of pathogenic environmental mycobacteria in water and other parts of the environment. https://www.who.int/publications/i/item/9241562595 Pharmaceuticals in drinking-water Published in 2012 by the World Health Organization Provides evidence-based, practical guidance and recommendations for managing pharmaceuticals in drinking-water. https://www.who.int/publications/i/item/9789241502085 Potable reuse: Guidance for producing safe drinking-water Published in 2017 by the World Health Organization Highlights specific aspects of potable reuse including the quality of source wastewaters and public acceptance and provides guidance on appropriate risk assessment and risk management procedures in order to ensure the safety of drinking-water from municipal wastewater. https://www.who.int/publications/i/item/9789241512770 Protecting groundwater for health: Managing the quality of drinking-water sources Edited by O. Schmoll et al. Published in 2006 by the World Health Organization An analysis of the hazards to groundwater quality, and the risk they may present to a specific supply. This is a tool for developing strategies to protect groundwater for health by managing the quality of drinking-water sources. https://www.who.int/publications/i/item/9241546689 Protecting surface water for health: Identifying, assessing and managing drinking-water quality risks in surface water catchments Published in 2016 by the World Health Organization Provides a structured approach to understanding surface waters and their catchments to support the identification, assessment and prioritization of the risks, and the development of management strategies for their control, as a basis for providing safe drinking-water https://www.who.int/publications/i/item/9789241510554 Quantifying public health risk in the WHO guidelines for drinking-water quality: A burden of disease approach A.H. Havelaar and J.M. Melse Published in 2003 by the National Institute for Public Health and the Environment of the Netherlands A discussion paper on the concepts and methodology of disability-adjusted life years (DALYs) as a common public health metric and its usefulness for drinking-water quality. https://www.who.int/publications/m/item/quantifying-public-health-risks-in-the-whoguidelines-for-drinking-water-quality Quantitative microbial risk assessment: application for water safety management Published in 2016 by the World Health Organization Synthesizes the current knowledge on quantitative microbial risk assessment (QMRA) to facilitate its application in the practice of water supply, water reuse and water recreation to support the management of risks associated with faecal pathogens in the water-related context. https://www.who.int/publications/i/item/9789241565370 Rapid assessment of drinking-water quality: A handbook for implementation Published in 2011 by the World Health Organization and the United Nations Children’s Fund A practical guide to rapidly monitor water quality and safety, incorporating statistical methods, sanitary survey, and field approaches. https://www.who.int/publications/i/item/789241504683 Review of latest available evidence on potential transmission of avian influenza (H5N1) through water and sewage and ways to reduce the risks to human health Published in 2006 by the World Health Organization A summary of the latest available studies and findings on avian influenza (H5N1) pertaining to water resources, water supplies, sanitation (human excreta, sewerage systems and health-care waste) and hygiene. https://www.who.int/publications/i/item/WHO-SDE-WSH-06.1 Risk assessment of Cryptosporidium in drinking water G. Medema et al. Published in 2009 by the World Health Organization A text supporting the Guidelines for drinking-water quality by providing further data on Cryptosporidium to assist country authorities in setting health-based targets and water suppliers in determining required performance of water treatment processes as part of a system-specific water safety plan. https://www.who.int/publications/i/item/WHO-HSE-WSH-09.04 Safe drinking-water from desalination Published in 2011 by the World Health Organization Highlights the principal health risks related to different desalination processes and provides guidance on appropriate risk assessment and risk management procedures in order to ensure the safety of desalinated drinking-water. https://www.who.int/publications/i/item/WHO-HSE-WSH-11.03 Safe piped water: Managing microbial water quality in piped distribution systems Edited by R. Ainsworth Published in 2004 by IWA Publishing on behalf of the World Health Organization A report on microbial contaminants and growth of microorganisms in distribution networks and the practices that contribute to ensuring drinking-water safety in piped distribution systems. https://www.who.int/publications/i/item/924156251X Scaling up household water treatment among low-income populations T. Clasen Published in 2009 by the World Health Organization Examines the evidence to date regarding the scalability of household water treatment systems. Its primary aims are to review the development and evolution of leading household water treatment technologies in their efforts to achieve scale, identify the main constraints that they have encountered and recommend ways forward. https://www.who.int/publications/i/item/WHO-HSE-WSH-09.02 Toxic cyanobacteria in water, 2nd edition Edited by I. Chorus and M. Welker Published in 2021 by CRC Press on behalf of the World Health Organization Provides background information and practical guidance to support the assessment and management of the risks from cyanobacteria and their toxins in drinking-water and recreational water bodies. https://www.who.int/publications/m/item/toxic-cyanobacteria-in-water---second-edition Turbidity: Information for regulators and operators of water supplies Published in 2017 by the World Health Organization Provides information on the uses and significance of turbidity, and is intended for regulators and operators of drinking-water supplies. https://www.who.int/publications/i/item/WHO-FWC-WSH-17.01 Upgrading water treatment plants E.G. Wagner and R.G. Pinheiro Published in 2001 by Spon Press on behalf of the World Health Organization A practical guide to improving the performance of water treatment plants. https://www.routledge.com/Upgrading-Water-Treatment-Plants/Pinheiro-Wagner/p/book/ 9780419260509 Water quality—Guidelines, standards and health: Assessment of risk and risk management for water-related infectious disease Edited by L. Fewtrell and J. Bartram Published in 2001 by IWA Publishing on behalf of the World Health Organization Guidance on issues relating to microbial water quality and health, including environmental and public health scientists, water scientists, policy-makers and those responsible for developing standards and regulations. https://www.who.int/publications/i/item/924154533X Water safety in buildings Edited by D. Cunliffe et al. Published in 2011 by the World Health Organization Provides guidance for managing water supplies in buildings (e.g. hospitals, schools, care facilities, hotels) where people may drink water; use water for food preparation; wash, shower, swim or use water for other recreational activities; or be exposed to aerosols produced by water-using devices, such as cooling towers. https://www.who.int/publications/i/item/9789241548106 Water safety in distribution systems Published in 2014 by the World Health Organization A reference tool to help water suppliers and regulators who are familiar with the water safety plan approach to enhance risk assessment and management and investment planning for their water distribution systems. https://www.who.int/publications/i/item/9789241548892 Water safety plan: A field guide to improving drinking-water safety in small communities Published in 2014 by the World Health Organization Regional Office for Europe Contains short explanations of the water safety planning process (including practical templates and tips) that support WSP development and implementation in small communities http://www.euro.who.int/en/health-topics/environment-and-health/water-and-sanitation/ publications/2014/water-safety-plan-a-field-guide-to-improving-drinking-water-safety-insmall-communities Water safety plan manual: Step-by-step risk management for drinking-water suppliers J. Bartram et al. Published in 2009 by the World Health Organization Guidance on developing and implementing a water safety plan through 11 learning modules, each representing a key step in the water safety plan development and implementation process. https://www.who.int/publications/i/item/9789241562638 Water safety planning for small community water supplies: Step-by-step risk management guidance for drinking-water supplies in small communities Published in 2012 by the World Health Organization Step-by-step guidance for the planning, design and implementation of water safety plans by and for rural and remote communities, including communities with piped schemes, those served by point sources and community-wide water supply services using various technical options. https://www.who.int/publications/i/item/9789241548427 Water safety plans: Managing drinking-water quality from catchment to consumer A. Davison et al. Published in 2005 by the World Health Organization Guidance on improved strategies for the preventive management, control and monitoring of drinking-water quality. https://www.who.int/publications/i/item/WHO-SDE-WSH-05.06 Water treatment and pathogen control: Process efficiency in achieving safe drinking-water M.W. LeChevallier and KK Au Published in 2004 by IWA Publishing on behalf of the World Health Organization A critical analysis of the removal and inactivation of pathogenic microbes in water to aid the water quality specialist and design engineer in making decisions regarding microbial water quality. https://www.who.int/publications/i/item/9241562552 Waterborne zoonoses: Identification, causes and control Edited by J.A. Cotruvo et al. Published in 2004 by IWA Publishing on behalf of the World Health Organization An invaluable tool for all professionals concerned with assessing and managing waterborne zoonoses, which are diseases caused by microorganisms of animal origin that also infect humans. https://www.who.int/publications/i/item/9241562730 ANNEX 2 References cited1,2 Chapter 1 Brikké F (2000) Operation and maintenance of rural water supply and sanitation systems: A training package for managers and planners. Delft, IRC International Water and Sanitation Centre; and Geneva, World Health Organization (https://www.who.int/publications/i/item/ WHO-SDE-WSH-00.2). Sawyer R, Simpson-Hébert M, Wood S (1998) PHAST step-by-step guide: A participatory approach for the control of diarrhoeal disease. Geneva, World Health Organization (WHO/ EOS/98.3; https://www.who.int/publications/i/item/WHO-EOS-98.3). Simpson-Hébert M, Sawyer R, Clarke L (1996) The PHAST initiative: Participatory Hygiene and Sanitation Transformation—A new approach to working with communities. Geneva, World Health Organization, United Nations Development Programme/World Bank Water and Sanitation Program (WHO/EOS/96.11; http://apps.who.int/iris/handle/10665/63260). WHO (1976) Surveillance of drinking-water quality. Geneva, World Health Organization (https://www.who.int/publications/i/item/9241400633). WHO (1997) Guidelines for drinking-water quality, 2nd ed. Vol. 3. Surveillance and control of community supplies. Geneva, World Health Organization (https://www.who.int/ publications/i/item/9241545038). WHO (2006) Guidelines for the safe use of wastewater, excreta and greywater in agriculture and aquaculture, Vol. 1–4. Geneva, World Health Organization (https://www.who.int/teams/ environment-climate-change-and-health/water-sanitation-and-health/sanitation-safety/ guidelines-for-safe-use-of-wastewater-greywater-and-excreta). WHO (2018) Guidelines on sanitation and health. Geneva, World Health Organization (https:// www.who.int/publications/i/item/9789241514705). WHO (2021) Guidelines on recreational water quality. Vol. 1: Coastal and fresh waters. Geneva, World Health Organization (https://www.who.int/publications/i/item/9789240031302). 1 This list includes all references cited in the text, except for the supporting documents to the Guidelines, which are listed separately in Annex 1, and the selected bibliographic references in chapter 11, which are cited following each microbial fact sheet in that chapter. 2 The web links given in this annex were current as of December 2021. 499 Chapter 2 UNGA (United Nations General Assembly) (2010a) Resolution 64/292: The human right to water and sanitation. In: 108th Plenary Meeting, 28 July 2010. UNGA (United Nations General Assembly) (2010b) Human Rights Council Resolution 15/9: Human rights and access to safe drinking water and sanitation. Chapter 3 Howard G et al. (2002) Healthy villages: A guide for communities and community health workers. Geneva, World Health Organization (https://www.who.int/publications/i/item/ 9241545534).Prüss-Üstün A et al. (2016) Preventing disease through healthy environments: A global assessment of the burden of disease from environmental risks. Geneva, World Health Organization (https://www.who.int/publications/i/item/9789241565196). WHO (2019) Safer water, better health. Geneva, World Health Organization (https://www.who. int/publications/i/item/9789241516891). Chapter 4 Bartram J, Ballance R, eds (1996) Water quality monitoring: A practical guide to the design and implementation of freshwater quality studies and monitoring programmes. Published by E & FN Spon, London, on behalf of the United Nations Educational, Scientific and Cultural Organization, the World Health Organization and the United Nations Environment Pro-gramme (https://apps.who.int/iris/handle/10665/41851). WHO (1997) Guidelines for drinking-water quality, 2nd ed. Vol. 3. Surveillance and control of community supplies. Geneva, World Health Organization (https://www.who.int/ publications/i/item/9241545038). Chapter 5 Lloyd B, Bartram J (1991) Surveillance solutions to microbiological problems in water quality control in developing countries. Water Science and Technology, 24(2):61–75. UNICEF, WHO (2021) The measurement and monitoring of water supply, sanitation and hygiene (WASH) affordability: A missing element of monitoring of Sustainable Development Goal (SDG) targets 6.1 and 6.2. New York, United Nations Children’s Fund and World Health Organization (https://washdata.org/sites/default/files/2021-05/unicef-who-2021-affordability-of-wash-services-full.pdf). WHO (1976) Surveillance of drinking-water quality. Geneva, World Health Organization. (https://www.who.int/publications/i/item/9241400633). WHO (1997) Guidelines for drinking-water quality, 2nd ed. Vol. 3: Surveillance and control of community supplies. Geneva, World Health Organization (https://www.who.int/ publications/i/item/9241545038). WHO, UNICEF (2017) Progress on drinking water, sanitation and hygiene: 2017 update and SDG baselines. Geneva, World Health Organization and United Nations Children’s Fund (https://www.who.int/publications/i/item/9789241512893). Chapter 6 APHA, AWWA, WEF (2005) Standard methods for the examination of water and wastewater, 21st ed. Washington, DC, American Public Health Association, American Water Works Association and Water Environment Federation, pp. 7–15. Bartram J, Ballance R, eds (1996) Water quality monitoring: A practical guide to the design and implementation of freshwater quality studies and monitoring programmes. Published by E & FN Spon, London, on behalf of the United Nations Educational, Scientific and Cultural Organization, the World Health Organization and the United Nations Environment Programme (https://apps.who.int/iris/handle/10665/41851). Cotruvo JA et al. (2010) Desalination technology: Health and environmental impacts. Boca Raton, FL, CRC Press. FAO/WHO (2009) Benefits and risks of the use of chlorine-containing disinfectants in food production and food processing: Report of a Joint FAO/WHO Expert Meeting. Geneva, Food and Agriculture Organization of the United Nations and World Health Organization (https://apps.who. int/iris/handle/10665/44250). FAO/WHO (2019) Safety and quality of water used in food production and processing: Meeting report. Rome, Food and Agriculture Organization of the United Nations and World Health Organization (Microbiological Risk Assessment Series No. 33; https://www.fao.org/documents/card/en/c/ca6062en). FAO/WHO (2021) Safety and quality of water used with fresh fruits and vegetables. Rome, Food and Agriculture Organization of the United Nations and World Health Organization (Microbiological Risk Assessment Series No. 37; https://www.who.int/publications/i/ item/9789240030220). Hutin Y, Luby S, Paquet C (2003) A large cholera outbreak in Kano City, Nigeria: The importance of hand washing with soap and the danger of street-vended water. Journal of Water and Health, 1:45–52. Sphere Association (2018) The Sphere handbook: Humanitarian charter and minimum standards in humanitarian response, 4th ed. Geneva, Sphere Association. WHO (1997) Guidelines for drinking-water quality, 2nd ed. Vol. 3: Surveillance and control of community supplies. Geneva, World Health Organization (https://www.who.int/ publications/i/item/9241545038). WHO (2005) Nutrients in drinking water. Geneva, World Health Organization (https://www. who.int/publications/i/item/9241593989). WHO (2016) International health regulations (2005), 3rd ed. Geneva, World Health Organization (https://www.who.int/publications/i/item/9789241580496). WHO (2020) Water, sanitation, hygiene, and waste management for SARS-CoV-2, the virus that causes COVID-19. Geneva, World Health Organization (https://www.who.int/ publications/i/item/WHO-2019-nCoV-IPC-WASH-2020.4). WHO (2021) Ebola virus disease: Key questions and answers concerning water, sanitation and hygiene. Geneva, World Health Organization (https://www.who.int/publications/i/item/ ebola-virus-disease-(evd)-key-questions-and-answers-concerning-water-sanitation-andhygiene). WHO/WEDC (2013) Technical notes on WASH in emergencies. Geneva, World Health Organization, and Water, Engineering and Develoment Centre, Loughborough University (https://www.who.int/teams/environment-climate-change-and-health/water-sanitationand-health/environmental-health-in-emergencies/technical-notes-on-wash-in-emergencies). Wisner B, Adams J (2002) Environmental health in emergencies and disasters: a practical guide. Geneva, World Health Organization (https://www.who.int/publications/i/item/ 9241545410). Chapter 7 AWWA (1999) Waterborne pathogens: AWWA manual M48. Denver, CO, American Water Works Association. Bitton G (2005) Wastewater microbiology, 3rd ed. New York, NY, John Wiley & Sons. Chevrefils G et al. (2006) UV dose required to achieve incremental log inactivation of bacteria, protozoa and viruses. IUVA News, 8(1):38–45. Clasen T et al. (2006) Interventions to improve water quality for preventing diarrhoea (Cochrane Review). In: The Cochrane Library, Issue 3. Oxford, Update Software (CD004794). Cotruvo JA, Sobsey M (2006) Point-of-use water treatment for home and travel. In: Grabow W, ed. UNESCO encyclopedia of life support systems. Paris, United Nations Educational, Scientific and Cultural Organization (http://www.eolss.net). Dullemont YJ et al. (2006) Removal of microorganisms by slow sand filtration. In: Gimbel R, Graham NJD, Collins MR, eds. Recent progress in slow sand and alternative biofiltration processes. London, IWA Publishing, pp. 12–20. Feachem RG et al. (1983) Sanitation and disease: Health aspects of excreta and wastewater management. Chichester, John Wiley. Fewtrell L, Colford J (2004) Water, sanitation and hygiene: Interventions and diarrhoea—A systematic review and meta-analysis. Health, Nutrition, and Population Family of the World Bank Human Development Network (https://openknowledge.worldbank.org/ handle/10986/13742). Gerba CP et al. (1996) Waterborne rotavirus: A risk assessment. Water Research, 30(12):2929– 2940. Haas CN, Rose JB, Gerba CP (1999) Quantitative microbial risk assessment. New York, NY, Wiley. Hijnen WAM, Beerendonk EF, Medema GJ (2006) Inactivation credit of UV radiation for viruses, bacteria and protozoan (oo)cysts in water: A review. Water Research, 40:3–22. Jones K, Betaieb M, Telford DR (1990) Seasonal variation of thermophilic campylobacters in sewage sludge. Journal of Applied Bacteriology, 69:185–189. Koenraad PMFJ et al. (1994) Survey of Campylobacter in sewage plants in the Netherlands. Food Microbiology, 11:65–73. Lodder WJ, de Roda Husman AM (2005) Presence of noroviruses and other enteric viruses in sewage and surface waters in the Netherlands. Applied and Environmental Microbiology, 71(3):1453–1461. Lodder WJ et al. (2010) Presence of enteric viruses in source waters for drinking water production in the Netherlands. Applied and Environmental Microbiology, 76(17):5965–5971. Maier RM, Pepper IL, Gerba CP (2000) Environmental microbiology. New York, NY, Academic Press. Masini L et al. (2007) Research and characterization of pathogenic vibrios from bathing water along the Conero Riviera (central Italy). Water Research, 41(18):4031–4040. Metcalf & Eddy, Inc. (2003) Wastewater engineering: Treatment and reuse. New York, NY, McGraw Hill. Nath KJ, Bloomfield S, Jones M (2006) Household water storage, handling and point-of-use treatment. A review commissioned by the International Scientific Forum on Home Hygiene (http://www.ifh-homehygiene.org/2003/2library/low_res_water_paper.pdf). Rutjes SA et al. (2009) Detection of infectious rotavirus in naturally contaminated source waters for drinking water production. Journal of Applied Microbiology, 107(1):97–105. Schijven JF, de Roda Husman AM (2006) A survey of diving behaviour and accidental water ingestion among Dutch occupational and sport divers to assess the risk of infection with waterborne pathogenic microorganisms. Environmental Health Perspectives, 114:712–717. Stampi S et al. (1992) Occurrence, removal, and seasonal variation of “thermophilic” campylobacters in a sewage treatment plant in Italy. Zentralblatt für Hygiene und Umweltmedizin, 193:199–210. Stelzer W (1988) [Detection of Campylobacter jejuni and C. coli in waste water.] Zentralblatt für Mikrobiologie, 143(1):47–54 (in German). WHO (2003) Emerging issues in water and infectious disease. Geneva, World Health Organization (https://www.who.int/publications/i/item/9241590823). WHO (2005) Preventing travellers’ diarrhoea: How to make drinking water safe. Geneva, World Health Organization (https://www.who.int/water_sanitation_health/water-quality/ sdwtravel.pdf). World Health Assembly (1991) Elimination of dracunculiasis: Resolution of the 44th World Health Assembly. Geneva, World Health Organization (Resolution No. WHA 44.5). Wright J, Gundry S, Conroy R (2003) Household drinking water in developing countries: A systematic review of microbiological contamination between source and point-of-use. Tropical Medicine & International Health, 9(1):106–117. Chapter 8 Bhat VS et al. (2017) Evolution of chemical-specific adjustment factors (CSAF) based on recent international experience: Increasing utility and facilitating regulatory acceptance. Critical Reviews in Toxicology, 47(9):733–753. FAO/WHO (2009) Principles and methods for the risk assessment of chemicals in food. Geneva, Food and Agriculture Organization of the United Nations and World Health Organization (Environmental Health Criteria 240; https://www.who.int/publications/i/ item/9789241572408). IPCS (1994) Assessing human health risks of chemicals: Derivation of guidance values for health-based exposure limits. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 170; http://www.inchem.org/documents/ ehc/ehc/ehc170.htm). IPCS (2000) Disinfectants and disinfectant by-products. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 216; http:// www.inchem.org/documents/ehc/ehc/ehc216.htm). IPCS (2005) Chemical-specific adjustment factors for interspecies differences and human variability: Guidance document for use of data in dose/concentration–response assessment. Geneva, World Health Organization, International Programme on Chemical Safety (Harmonization Project Document No. 2; http://www.inchem.org/documents/harmproj/harmproj/ harmproj2.pdf). IPCS (2009) Principles for modelling dose–response for the risk assessment of chemicals. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 239; http://whqlibdoc.who.int/publications/2009/9789241572392_eng. pdf). Krishnan K, Carrier R (2013). The use of exposure source allocation factor in the risk assessment of drinking-water contaminants. Journal of Toxicology and Environmental Health. Part B. Critical Reviews, 16(1):39–51. Solecki R et al. (2005) Guidance on setting of acute reference dose (ARfD) for pesticides. Food and Chemical Toxicology, 43:1569–1593. WHO (2006) Guidelines for safe recreational water environments. Vol. 2. Swimming pools and similar environments. Geneva, World Health Organization (https://www.who.int/ publications/i/item/9241546808). WHO/TDR (2009) Dengue: Guidelines for diagnosis, treatment, prevention and control. Geneva, World Health Organization and the Special Programme for Research and Training in Tropical Diseases (TDR) (https://apps.who.int/iris/handle/10665/44188). Chapter 9 APHA, AWWA, WEF (2017) 7110C: Coprecipitation method for gross alpha radioactivity in drinking water. In: Baird RB, Eaton AD, Rice EW, eds. Standard methods for the examination of water and wastewater, 23rd ed. Denver, American Public Health Association, American Water Works Association and Water Environment Federation, p. 7–15. Auvinen A et al. (2005) Radon and other natural radionuclides in drinking water and risks of stomach cancer: A case–cohort study in Finland. International Journal of Cancer, 10:109– 113. Brenner D et al. (2003) Cancer risks attributable to low doses of ionizing radiation: Assessing what we really know. Proceedings of the National Academy of Sciences of the United States of America, 100(24):13761–13766. Brown J, Hammond B, Wilkins DT (2008) Handbook for assessing the impact of a radiological incident on levels of radioactivity in drinking water and risks to operatives at water treatment works: Supporting scientific report. Chilton, Oxfordshire, Health Protection Agency (HPARPD-041; https://assets.publishing.service.gov.uk/government/uploads/system/uploads/ attachment_data/file/419023/HPA-RPD-041_for_website.pdf). Euratom (2013). Council Directive 2013/51/Euratom of 22 October 2013 laying down requirements for the protection of the health of the general public with regard to radioactive substances in water intended for human consumption. Official Journal of the European Union, L 296/12 (https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX %3A32013L0051). Health Canada (2009) Guidelines for Canadian drinking water quality: Guideline technical document—Radiological parameters. Ottawa, Ontario, Health Canada, Healthy Environments and Consumer Safety Branch, Radiation Protection Bureau (Catalogue No. H128-1/10614E-PDF; http://www.hc-sc.gc.ca/ewh-semt/alt_formats/hecs-sesc/pdf/pubs/water-eau/ radiological_para-radiologiques/radiological_para-radiologiques-eng.pdf). IAEA (2011). Criteria for use in preparedness and response for a nuclear or radiological emergency. Vienna, International Atomic Energy Agency (IAEA Safety Standards Series No. GSG-2. (http://www-pub.iaea.org/MTCD/publications/PDF/Pub1467_web.pdf). IAEA (2014) Radiation protection and safety of radiation sources: international basic safety standards. Vienna, International Atomic Energy Agency (IAEA Safety Standards Series No. GSR Part 3; https://www.iaea.org/publications/8930/radiation-protection-and-safety-of-radiation-sources-international-basic-safety-standards). IAEA (2015) Preparedness and response for a nuclear or radiological emergency. Vienna, International Atomic Energy Agency (IAEA Safety Standards Series No. GSR Part 7; http://wwwpub.iaea.org/MTCD/publications/PDF/P_1708_web.pdf). IAEA (2018) Arrangements for the termination of a nuclear or radiological emergency. Vienna, International Atomic Energy Agency (IAEA Safety Standards Series No. GSG-11; https:// www-pub.iaea.org/MTCD/Publications/PDF/PUB1796_web.pdf). ICRP (1996) Age-dependent doses to the members of the public from intake of radionuclides. Part 5. Compilation of ingestion and inhalation coefficients. ICRP Publication 72. Annals of the ICRP, 26(1). ICRP (2000) Protection of the public in situations of prolonged radiation exposure. Recommendations of the International Commission on Radiological Protection. ICRP Publication 82. Annals of the ICRP, 29(1–2). ICRP (2007) The 2007 recommendations of the International Commission on Radiological Protection. ICRP Publication 103. Annals of the ICRP, 37(2–4). ICRP (2009) International Commission on Radiological Protection statement on radon. Ottawa, International Commission on Radiological Protection (ICRP Ref 00/902/09; http://www. icrp.org/docs/ICRP_Statement_on_Radon%28November_2009%29.pdf). ICRP (2020) Radiological protection of people and the environment in the event of a large nuclear accident: update of ICRP Publications 109 and 111. ICRP Publication 146. Annals of the ICRP, 49(4). ISO (2003) Standard ISO 5667 3: Water quality—Sampling—Part 3: Guidance on the preservation and handling of water samples. Geneva, International Organization for Standardization. ISO (2006a) Standard ISO 5667 1: Water quality—Sampling—Part 1: Guidance on the design of sampling programmes and sampling techniques. Geneva, International Organization for Standardization. ISO (2006b) Standard ISO 5667-5: Water quality—Sampling—Part 5: Guidance on sampling of drinking water from treatment works and piped distribution systems. Geneva, International Organization for Standardization. ISO (2009) Standard ISO 5667-11: Water quality—Sampling—Part 11: Guidance on sampling of groundwaters. Geneva, International Organization for Standardization. ISO (2017) Standard ISO 9696: Water quality—Measurement of gross alpha activity in non-saline water—Thick source method. Geneva, International Organization for Standardization. ISO (2018a) Standard ISO 9697: Water quality—Measurement of gross beta activity in non-saline water—Thick source method. Geneva, International Organization for Standardization. ISO (2018b) Standard ISO 11704: Water quality—Measurement of gross alpha and gross beta activity—Test method using liquid scintillation counting. Geneva, International Organization for Standardization. ISO (2019) Standard ISO 10704: Water quality—Measurement of gross alpha and gross beta activity in non-saline water—Thin source deposit method. Geneva, International Organization for Standardization. Jobbágy V et al. (2016) Evaluation of the 2012 EC interlaboratory comparison on gross alpha/beta activity concentration in drinking water. EUR 28351 EN. Luxembourg, Publications Office of the European Union. NAS (1999) Report on risk assessment of radon in drinking water. Washington, DC, National Research Council, National Academy Press. Picano E (2008) Informed consent and communication of risk from radiological and nuclear medicine examinations: How to escape from a communication inferno. British Medical Journal, 329:849–851. Rühm W, Laurier D, Wakeford R (2022) Cancer risk following low doses of ionising radiation—Current epidemiological evidence and implications for radiological protection. Mutation Research/Genetic Toxicology and Environmental Mutagenesis, 873:503436. Standards Australia, Standards New Zealand (1998) Water quality—Sampling—Guidance on the design of sampling programs, sampling techniques and the preservation and handling of samples. Homebush, Australia, and Wellington, New Zealand, Joint Australian/New Zealand Standards (AS/NZS 5667.1.1998). UNSCEAR (2000) Report: Sources, effects and risks of ionizing radiation. New York, NY, United Nations, United Nations Scientific Committee on the Effects of Atomic Radiation (http://www.unscear.org/unscear/en/publications/2000_1.html). UNSCEAR (2008) Report: Sources and effects of ionizing radiation. Vol. I. Sources. New York, NY, United Nations, United Nations Scientific Committee on the Effects of Atomic Radiation (http://www.unscear.org/unscear/en/publications/2008_1.html). UNSCEAR (2010) Low-dose radiation effects on health. New York, NY, United Nations, United Nations Scientific Committee on the Effects of Atomic Radiation (https://www.unscear. org/unscear/en/publications/2010.html). UNSCEAR (2012) Sources, effects and risks of ionizing radiation. Annex B: Uncertainties in risk estimates for radiation-induced cancer. New York, NY, United Nations, United Nations Scientific Committee on the Effects of Atomic Radiation (https://www.unscear.org/docs/ reports/2012/UNSCEAR2012Report_AnnexB_Uncertainty_AdvanceCopy.pdf). USEPA (2007) Communicating radiation risks. Washington, DC, Environmental Protection Agency (EPA Publication 402-F-07-008). WHO (2002) Establishing a dialogue on risks from electromagnetic fields. Geneva, World Health Organization (https://www.who.int/peh-emf/publications/en/EMF_Risk_ALL.pdf). WHO (2009) WHO handbook on indoor radon: A public health perspective. Geneva, World Health Organization (https://www.who.int/publications/i/item/9789241547673). Ye W et al. (1998) Mortality and cancer incidence in Misasa, Japan, a spa area with elevated radon levels. Japanese Journal of Cancer Research, 89(8):789–796. Chapter 111 WHO (2003) Emerging issues in water and infectious disease. Geneva, World Health Organization (https://www.who.int/publications/i/item/9241590823). Chapter 122 Background documents for preparation of WHO Guidelines for drinking-water quality3 WHO (2003) 1,1-Dichloroethane in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/19). WHO (2003) 1,1,1-Trichloroethane in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/65). WHO (2003) 1,2-Dibromo-3-chloropropane in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/03.04/34). WHO (2003) 1,2-Dibromoethane in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/66). WHO (2003) 1,2-Dichloroethane in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/67). WHO (2003) 1,2-Dichloroethene in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/72). WHO (2003) 1,2-Dichloropropane (1,2-DCP) in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/03.04/61). 1 Selected bibliographical references are included at the end of each microbial fact sheet in chapter 11. 2 References cited using the same author and date in chapter 12 are alphabetized by title here. 3 All background documents may be found at https://www.who.int/teams/environment-climate-changeand-health/water-sanitation-and-health/chemical-hazards-in-drinking-water. WHO (2003) 1,3-Dichloropropane in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/35). WHO (2003) 1,3-Dichloropropene in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/36). WHO (2003) 2,4-D in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/70). WHO (2003) 2-Phenylphenol and its sodium salt in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/03.04/69). WHO (2003) Alachlor in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/31). WHO (2003) Aldicarb in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/72). WHO (2003) Aldrin and dieldrin in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/73). WHO (2003) Ammonia in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/1). WHO (2003) Antimony in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/74). WHO (2003) Benzene in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/24). WHO (2003) Chlordane in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/84). WHO (2003) Chloride in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/3). WHO (2003) Chlorine in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/45). WHO (2003) Chloroacetones in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/50). WHO (2003) Chlorophenols in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/47). WHO (2003) Chlorophenoxy herbicides (excluding 2,4-D and MCPA) in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/03.04/44). WHO (2003) Chloropicrin in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/52). WHO (2003) Chlorotoluron in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/33). WHO (2003) Chlorpyrifos in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/87). WHO (2003) Cyanazine in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/60). WHO (2003) Dichlorobenzenes in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/28). WHO (2003) Dichloromethane in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/18). WHO (2003) Di(2-ethylhexyl)adipate in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/68). WHO (2003) Di(2-ethylhexyl)phthalate in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/03.04/29). WHO (2003) Edetic acid (EDTA) in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/58). WHO (2003) Ethylbenzene in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/26). WHO (2003) Heptachlor and heptachlor epoxide in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/03.04/99). WHO (2003) Hexachlorobutadiene in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/101). WHO (2003) Hydrogen sulfide in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/7). WHO (2003) Iron in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/03.04/08). WHO (2003) Isoproturon in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/37). WHO (2003) Lindane in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/102). WHO (2003) Malathion in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/103). WHO (2003) Metolachlor in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/39). WHO (2003) Molinate in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/40). WHO (2003) MX in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/03.04/108). WHO (2003) Nitrilotriacetic acid in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/30). WHO (2003) Pendimethalin in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/41). WHO (2003) Pentachlorophenol in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/62). WHO (2003) Polynuclear aromatic hydrocarbons in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/03.04/59). WHO (2003) Propanil in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/112). NickelWHO (2003) Simazine in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/42). WHO (2003) Sodium in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/15). WHO (2003) Styrene in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/27). WHO (2003) Terbuthylazine in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/63). WHO (2003) Toluene in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/116). WHO (2003) Total dissolved solids in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/16). WHO (2003) Trichloroacetic acid in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/120). WHO (2003) Trichlorobenzenes in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/117). WHO (2003) Trifluralin in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/43). WHO (2003) Xylenes in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/25). WHO (2003) Zinc in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/03.04/17). WHO (2004) Brominated acetic acids in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/79). WHO (2004) Carbofuran in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/81). WHO (2004) Carbon tetrachloride in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/82). WHO (2004) Copper in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/88). WHO (2004) DDT and its derivatives in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/89). WHO (2004) Dimethoate in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04). WHO (2004) Endosulfan in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/92). WHO (2004) Endrin in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/93). WHO (2004) Epichlorohydrin in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/94). WHO (2004) Fenitrothion in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/95). WHO (2004) Fluoride in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/96). WHO (2004) Halogenated acetonitriles in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/03.04/98). WHO (2004) Hexachlorobenzene in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/100). WHO (2004) Inorganic tin in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/115). WHO (2004) Methoxychlor in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/105). WHO (2004) Methyl parathion in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/106). WHO (2004) Monochloramine in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/83). WHO (2004) Monochloroacetic acid in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/85). WHO (2004) Monochlorobenzene in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/107). WHO (2004) Parathion in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/110). WHO (2004) Sulfate in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/114). WHO (2004) Vinyl chloride in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/119). WHO (2005) 1,1-Dichloroethene in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/05.08/20). WHO (2005) 1,4-Dioxane in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/05.08/120). WHO (2005) Bromate in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/05.08/78). WHO (2005) Chloral hydrate in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/05.08/49). WHO (2005) Dichloroacetic acid in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/05.08/121). WHO (2005) Formaldehyde in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/05.08/48). WHO (2005) Glyphosate and AMPA in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/03.04/97). WHO (2005) Mercury in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/05.08/10). WHO (2005) Methyl tertiary-butyl ether (MTBE) in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/05.08/122). WHO (2005) Trihalomethanes in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/05.08/64). WHO (2007) Nickel in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/07.08/55). WHO (2007) pH in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/WSH/07.01/1). WHO (2008) Carbaryl in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/ AMR/08.03/5). WHO (2008) Diflubenzuron in drinking-water: Use for vector control in drinking-water sources and containers. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/AMR/08.03/6). WHO (2008) Methoprene in drinking-water: Use for vector control in drinking-water sources and containers. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/AMR/08.03/14). WHO (2008) N-Nitrosodimethylamine in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/AMR/08.03/8). WHO (2008) Novaluron in drinking-water: Use for vector control in drinking-water sources and containers. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/AMR/08.03/11). WHO (2008) Petroleum products in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/WSH/05.08/123). WHO (2008) Pirimiphos-methyl in drinking-water: Use for vector control in drinking-water sources and containers. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/AMR/08.03/15). WHO (2008). Pyriproxyfen in drinking-water: Use for vector control in drinking-water sources and containers. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/AMR/08.03/9). WHO (2008) Sodium dichloroisocyanurate in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/AMR/08.03/3). WHO (2009) Bacillus thuringiensis israelensis (Bti) in drinking-water: Use for vector control in drinking-water sources and containers. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/ WSH/09.01/8). WHO (2009) Beryllium in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/ WSH/09.01/5). WHO (2009) Boron in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/ WSH/09.01/2). WHO (2009) Bromide in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/ WSH/09.01/6). WHO (2009) Cyanide in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/ WSH/09.01/3). WHO (2009) Cyanogen chloride in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ HSE/WSH/09.01/9). WHO (2009) Nitrobenzene in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/ WSH/09.01/4). WHO (2009) Potassium in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/ WSH/09.01/7). WHO (2009) Temephos in drinking-water: Use for vector control in drinking-water sources and containers. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/AMR/09.01/1). WHO (2010) Aluminium in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/53). WHO (2010) Spinosad in drinking-water: Use for vector control in drinking-water sources and containers. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/WSH/10.01.12). WHO (2011) Acrylamide in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/71/Rev/1). WHO (2011) Arsenic in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/75/Rev/1). WHO (2011) Atrazine and its metabolites in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/WSH/10.01/11/Rev/1). WHO (2011) Cadmium in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/80/Rev/1). WHO (2011) Hardness in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HSE/ WSH/10.01/10/Rev/1). WHO (2011) Molybdenum in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/11/Rev/1). WHO (2011) Permethrin in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/05.08/111/Rev/1). WHO (2011) Selenium in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/13/Rev/1). WHO (2012) Uranium in drinking-water. Background document for preparation of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ WSH/03.04/118/Rev/1). WHO (2016) Barium in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/FWC/ WSH/16.48). WHO (2016) Chlorine dioxide, chlorate and chlorite in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/FWC/WSH/16.49). WHO (2016) Dichlorvos in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/FWC/ WSH/16.44). WHO (2016) Dicofol in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/FWC/ WSH/16.45). WHO (2016) Diquat in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/FWC/ WSH/16.50). WHO (2016) Lead in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO WHO/FWC/ WSH/16.53) WHO (2016) MCPA in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ FWC/16.51). WHO (2016) Nitrate and nitrite in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ SDE/FWC/16.52). WHO (2016) Perchlorate in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/SDE/ FWC/16.46). WHO (2020) Bentazone in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HEP/ ECH/WSH/2020.2). WHO (2020) Chromium in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HEP/ ECH/WSH/2020.3). WHO (2020) Cyanobacterial toxins: anatoxin-a and analogues. Background document for development of WHO Guidelines for drinking-water quality and Guidelines for safe recreational water environments. Geneva, World Health Organization (WHO/HEP/ECH/WSH/2020.1). WHO (2020) Cyanobacterial toxins: cylindrospermopsins. Background document for development of WHO Guidelines for drinking-water quality and Guidelines for safe recreational water environments. Geneva, World Health Organization (WHO/HEP/ECH/WSH/2020.4). WHO (2020) Cyanobacterial toxins: microcystins. Background document for development of WHO Guidelines for drinking-water quality and Guidelines for safe recreational water environments. Geneva, World Health Organization (WHO/HEP/ECH/WSH/2020.6). WHO (2020) Cyanobacterial toxins: saxitoxins. Background document for development of WHO Guidelines for drinking-water quality and Guidelines for safe recreational water environments. Geneva, World Health Organization (WHO/HEP/ECH/WSH/2020.8). WHO (2020) Iodine in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HEP/ECH/ WSH/2020.5). WHO (2020) Organotins in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HEP/ ECH/WSH/2020.7). WHO (2020) Tetrachloroethene in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/ HEP/ECH/WSH/2020.9). WHO (2020) Trichloroethene in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HEP/ ECH/WSH/2020.10). WHO (2021) Asbestos in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HEP/ECH/ WSH/2021.4). WHO (2021) Manganese in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HEP/ ECH/WSH/2021.5). WHO (2021) Nickel in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HEP/ECH/ WSH/2021.6). WHO (2021) Silver in drinking-water. Background document for development of WHO Guidelines for drinking-water quality. Geneva, World Health Organization (WHO/HEP/ECH/ WSH/2021.7). Other references cited EFSA (2009) Marine biotoxins in shellfish: saxitoxin group. EFSA Journal, 7(4):1019. EFSA (2014) Scientific opinion on the risks to public health related to the presence of perchlorate in food, in particular fruits and vegetables. EFSA Journal, 12(10):3869. EFSA (2020) Update of the risk assessment of nickel in food. EFSA Journal. 18(11):6268. FAO (2004) Marine biotoxins. Rome, FAO (Nutrition Paper 80). FAO/WHO (1992) Pesticide residues in food—1991 evaluations. Part II—Toxicology. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues (WHO/ PCS/92.52). FAO/WHO (1993) Pesticide residues in food—1992 evaluations. Part II—Toxicology. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues (WHO/ PCS/93.34). FAO/WHO (1995) Pesticide residues in food—1994 evaluations. Part II—Toxicology. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues (WHO/ PCS/95.2). FAO/WHO (1996) Pesticide residues in food—1995 evaluations. Part II—Toxicological and environmental. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues (WHO/PCS/96.48). FAO/WHO (1997) Pesticide residues in food—1996 evaluations. Part II—Toxicological. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues (WHO/ PCS/97.1). FAO/WHO (1998) Pesticide residues in food—1997 evaluations. Part II—Toxicological and environmental. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues (WHO/PCS/98.6). FAO/WHO (1999) Pesticide residues in food—1998 evaluations. Part II—Toxicological. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues (WHO/ PCS/99.18). FAO/WHO (2000) Pesticide residues in food—1999 evaluations. Part II—Toxicological. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues (WHO/ PCS/00.4). FAO/WHO (2001) Evaluation of certain food additives and contaminants. Fifty-fifth report of the Joint FAO/WHO Expert Committee on Food Additives. Geneva, World Health Organization (WHO Technical Report Series, No. 901). FAO/WHO (2001) Pesticide residues in food—2000 evaluations. Part II—Toxicological. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues (WHO/ PCS/01.3). FAO/WHO (2002) Pesticide residues in food—2001 evaluations. Part II—Toxicological. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues (WHO/ PCS/02.1). FAO/WHO (2003) Pesticide residues in food—2002 evaluations. Part II—Toxicological. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues (WHO/ PCS/03.1). FAO/WHO (2004) Evaluation of certain food additives and contaminants. Sixty-first report of the Joint FAO/WHO Committee on Food Additives. Geneva, World Health Organization (WHO Technical Report Series No. 922). FAO/WHO (2004) Vitamin and mineral requirements in human nutrition, 2nd ed. Report of a Joint FAO/WHO Expert Consultation, Bangkok, Thailand, 21–30 September 1998. Geneva, World Health Organization (http://whqlibdoc.who.int/publications/2004/9241546123.pdf). FAO/WHO (2006) Pesticide residues in food—2005 evaluations. Part II—Toxicological. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues. FAO/WHO (2007) Aluminium (from all sources, including food additives). In: Evaluation of certain food additives and contaminants. Sixty-seventh report of the Joint FAO/WHO Expert Committee on Food Additives. Geneva, World Health Organization, pp. 33–44 (WHO Technical Report Series, No. 940). FAO/WHO (2008) Pesticide residues in food—2006 evaluations. Part II—Toxicological. Geneva, World Health Organization, Joint FAO/WHO Meeting on Pesticide Residues. FAO/WHO (2009) Pesticide residues in food—2007 evaluations. Part II—Toxicological. Geneva, World Health Organization. FAO/WHO (2011) Evaluation of certain contaminants in food. Seventy-second report of the Joint FAO/WHO Expert Committee on Food Additives. Geneva, World Health Organization (WHO Technical Report Series, No. 959). FAO/WHO (2011) Evaluation of certain food additives and contaminants. Seventy-third report of the Joint FAO/WHO Expert Committee on Food Additives. Geneva, World Health Organization (WHO Technical Report Series, No. 960). FAO/WHO (2011). Safety evaluation of certain contaminants in food. Prepared by the seventy-second meeting of the Joint FAO/WHO Expert Committee on Food Additives. Geneva, World Health Organization, pp. 685–762 (WHO Food Additives Series 63; FAO JECFA Monographs 8). Fawell J et al. (2006) Fluoride in drinking-water. London, IWA Publishing on behalf of the World Health Organization (WHO Drinking-water Quality Series; https://www.who.int/ publications/i/item/9241563192). Health Canada (2013) Guidelines for Canadian drinking water quality: Guideline technical document—Nitrate and nitrite. Ottawa, Health Canada (http://hc-sc.gc.ca/ewh-semt/pubs/ water-eau/nitrate_nitrite/index-eng.php). IARC (1987) Overall evaluations of carcinogenicity: An updating of IARC Monographs volumes 1–42. Lyon, International Agency for Research on Cancer, pp. 100–106 (IARC Monographs on the Evaluation of Carcinogenic Risks to Humans, Supplement 7). IPCS (1992) Endrin. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 130; http://www.inchem.org/documents/ehc/ ehc/ehc130.htm). IPCS (1992) Methyl parathion. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 145; http://www.inchem.org/ documents/ehc/ehc/ehc145.htm). IPCS (1994) Glyphosate. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 159; http://www.inchem.org/documents/ ehc/ehc/ehc159.htm). IPCS (1994) Hexachlorobutadiene. Geneva, World Health Organization, International Pro-gramme on Chemical Safety (Environmental Health Criteria 156; http://www.inchem.org/ documents/ehc/ehc/ehc156.htm). IPCS (1995) 1,2-Dichloroethane, 2nd ed. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 176; http://www.inchem. org/documents/ehc/ehc/ehc176.htm). IPCS (1995) Report of the 1994 meeting of the Core Assessment Group. Geneva, World Health Organization, International Programme on Chemical Safety, Joint Meeting on Pesticides (WHO/PCS/95.7). IPCS (1996) 1,2-Dibromoethane. Geneva, World Health Organization, International Pro-gramme on Chemical Safety (Environmental Health Criteria 177; http://www.inchem.org/ documents/ehc/ehc/ehc177.htm). IPCS (1997) Aluminium. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 194; http://www.inchem.org/documents/ ehc/ehc/ehc194.htm). IPCS (1997) Hexachlorobenzene. Geneva, World Health Organization, International Pro-gramme on Chemical Safety (Environmental Health Criteria 195; http://www.inchem. org/documents/ehc/ehc/ehc195.htm). IPCS (1998) 1,2-Dichloroethane. Geneva, World Health Organization, International Programme on Chemical Safety (Concise International Chemical Assessment Document 1; http://www. who.int/entity/ipcs/publications/cicad/en/cicad01.pdf). IPCS (1998) Copper. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 200; http://www.inchem.org/documents/ ehc/ehc/ehc200.htm). IPCS (1998) Methyl tertiary-butyl ether. Geneva, World Health Organization, International Pro-gramme on Chemical Safety (Environmental Health Criteria 206; http://www.inchem.org/ documents/ehc/ehc/ehc206.htm). IPCS (1999) Bacillus thuringiensis. Geneva, World Health Organization, International Pro-gramme on Chemical Safety (Environmental Health Criteria 217; http://www.inchem.org/ documents/ehc/ehc/ehc217.htm). IPCS (1999) Carbon tetrachloride. Geneva, World Health Organization, International Pro-gramme on Chemical Safety (Environmental Health Criteria 208; http://www.who.int/ entity/ipcs/publications/ehc/who_ehc_208.pdf). IPCS (1999) Vinyl chloride. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 215; http://www.inchem.org/documents/ ehc/ehc/ehc215.htm). IPCS (2000) Chloral hydrate. Geneva, World Health Organization, International Programme on Chemical Safety (Concise International Chemical Assessment Document No. 25; http:// www.who.int/entity/ipcs/publications/cicad/en/cicad25.pdf). IPCS (2000) Disinfectants and disinfectant by-products. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 216; http:// www.inchem.org/documents/ehc/ehc/ehc216.htm). IPCS (2001) Arsenic and arsenic compounds. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 224; http://www.inchem. org/documents/ehc/ehc/ehc224.htm). IPCS (2001) Barium and barium compounds. Geneva, World Health Organization, International Programme on Chemical Safety (Concise International Chemical Assessment Document 33; http://www.who.int/entity/ipcs/publications/cicad/en/cicad33.pdf). IPCS (2001) Beryllium and beryllium compounds. Geneva, World Health Organization, International Programme on Chemical Safety (Concise International Chemical Assessment Document 32; http://www.inchem.org/documents/cicads/cicads/cicad32.htm). IPCS (2002) Fluorides. Geneva, World Health Organization, International Programme on Chemical Safety (Environmental Health Criteria 227; http://www.inchem.org/documents/ ehc/ehc/ehc227.htm). IPCS (2002) Formaldehyde. Geneva, World Health Organization, International Programme on Chemical Safety (Concise International Chemical Assessment Document 40; http://www. who.int/entity/ipcs/publications/cicad/en/cicad40.pdf). IPCS (2002) N-Nitrosodimethylamine. Geneva, World Health Organization, International Pro-gramme on Chemical Safety (Concise International Chemical Assessment Document No. 38; http://www.who.int/entity/ipcs/publications/cicad/en/cicad38.pdf). IPCS (2003) 1,1-Dichloroethene (vinylidene chloride). Geneva, World Health Organization, International Programme on Chemical Safety (Concise International Chemical Assessment Document 51; http://www.who.int/entity/ipcs/publications/cicad/en/cicad51.pdf). IPCS (2003) Elemental mercury and inorganic mercury compounds: Human health aspects. Geneva, World Health Organization, International Programme on Chemical Safety (Concise International Chemical Assessment Document 50; http://www.who.int/entity/ipcs/ publications/cicad/en/cicad50.pdf). IPCS (2004) Chloroform. Geneva, World Health Organization, International Programme on Chemical Safety (Concise International Chemical Assessment Document 58; http://www. who.int/entity/ipcs/publications/cicad/en/cicad58.pdf). IPCS (2004) Hydrogen cyanide and cyanides: Human health aspects. Geneva, World Health Organization, International Programme on Chemical Safety (Concise International Chemical Assessment Document No. 61; http://www.who.int/entity/ipcs/publications/cicad/en/ cicad61.pdf). ISO (1982) Water quality—determination of total arsenic. Geneva, International Organization for Standardization (ISO 6595-1982). USEPA (1992) Silver drinking water health advisory. Washington, DC, United States Environmental Protection Agency. USEPA (2005) Toxicological review of barium and compounds (CAS No. 7440-39-3): in support of summary information on the Integrated Risk Information System (IRIS). Washington, DC, United States Environmental Protection Agency (https://cfpub.epa.gov/ncea/iris/iris_ documents/documents/toxreviews/0010tr.pdf). USNRC (2001) Arsenic in drinking water, 2001 update. Washington, DC, United States National Research Council, National Academies Press. USNRC (2006) Fluoride in drinking water: A scientific review of EPA’s standards. Washington, DC, United States National Research Council, National Academies Press. USNTP (1987) Toxicology and carcinogenesis studies of bromodichloromethane (CAS No. 75-274) in F344/N rats and B6C3F1 mice (gavage studies). Research Triangle Park, United States Department of Health and Human Services, Public Health Service, National Toxicology Program (NTP TR 321). WHO (1984a) Guidelines for drinking-water quality: Volume 1: Recommendations. Geneva, World Health Organization (https://www.who.int/publications/i/item/9241541687). WHO (1984b) Guidelines for drinking-water quality: Volume 2: Health criteria and other supporting information. Geneva, World Health Organization (https://www.who.int/publications/i/ item/9241541695). WHO (1993) Guidelines for drinking-water quality, 2nd ed. Volume 1: Recommendations. Geneva, World Health Organization (https://www.who.int/publications/i/item/924154460O). WHO (2004) Report of the seventh WHOPES working group meeting, 2–4 December 2003, Geneva. Geneva, World Health Organization, WHO Pesticide Evaluation Scheme (WHO/CDS/ WHOPES/2004.8; http://whqlibdoc.who.int/hq/2004/WHO_CDS_WHOPES_2004.8.pdf). WHO (2006) Report of the ninth WHOPES working group meeting, 5–9 December 2005, Geneva. Geneva, World Health Organization, Control of Neglected Tropical Diseases, WHO Pesticide Evaluation Scheme (WHO/CDS/NTD/WHOPES/2006.2; http://whqlibdoc.who.int/ hq/2006/WHO_CDS_NTD_WHOPES_2006.2_eng.pdf). WHO (2007) WHO specifications and evaluations for public health pesticides: Bacillus thuringiensis subspecies israelensis strain AM65-52. Geneva, World Health Organization (http:// www.who.int/whopes/quality/Bti_eval_spec_Jun_07.pdf). WHO (2008) Acidified sodium chlorite. In: Safety evaluation of certain food additives and contaminants. Prepared by the sixty-eighth meeting of the Joint FAO/WHO Expert Committee on Food Additives (JECFA). Geneva, World Health Organization; 3–54 (Food Additives Series, No. 59; http://whqlibdoc.who.int/publications/2008/9789241660594_eng.pdf?ua=1). WHO (2012) Pesticide residues in food—2011 evaluations. Part II—Toxicological. Geneva, World Health Organization. WHO (2013) Pesticide residues in food—2012 evaluations. Part II—Toxicological. Geneva, World Health Organization. WHO (2014) Pesticide residues in food—2013 evaluations.Part II—Toxicological. Geneva, World Health Organization. WHO (2016) Pesticide residues in food—2016 evaluations. Part II—Toxicological. Geneva, World Health Organization. WHO (2021) Guidelines for safe recreational water environments. Geneva, World Health Organization (https://www.who.int/publications/i/item/9789240031302). ANNEX 3 Chemical summary tables Table A3.1 Chemicals excluded from guideline value derivation Chemical Reason for exclusion Amitraz Degrades rapidly in the environment and is not expected to occur at measurable concentrations in drinking‑water supplies Chlorobenzilate Unlikely to occur in drinking‑water Chlorothalonil Unlikely to occur in drinking‑water Cypermethrin Unlikely to occur in drinking‑water Deltamethrin Unlikely to occur in drinking‑water Diazinon Unlikely to occur in drinking‑water Dinoseb Unlikely to occur in drinking‑water Ethylene thiourea Unlikely to occur in drinking‑water Fenamiphos Unlikely to occur in drinking‑water Formothion Unlikely to occur in drinking‑water Hexachlorocyclohexanes Unlikely to occur in drinking‑water (mixed isomers) MCPBa Unlikely to occur in drinking‑water Methamidophos Unlikely to occur in drinking‑water Methomyl Unlikely to occur in drinking‑water Mirex Unlikely to occur in drinking‑water Monocrotophos Has been withdrawn from use in many countries and is unlikely to occur in drinking‑water Oxamyl Unlikely to occur in drinking‑water Phorate Unlikely to occur in drinking‑water Propoxur Unlikely to occur in drinking‑water Pyridate Not persistent and only rarely found in drinking‑water Quintozene Unlikely to occur in drinking‑water Toxaphene Unlikely to occur in drinking‑water Triazophos Unlikely to occur in drinking‑water Trichlorfon Unlikely to occur in drinking‑water a 4‑(4‑chloro‑o‑tolyloxy)butyric acid. Table A3.2 Chemicals for which guideline values have not been establisheda Chemical Reason for not establishing a guideline value Aluminium The health‑based value exceeds practicable levels based on optimization of the coagulation process in drinking‑water plants using aluminium‑based coagulants: 0.1 mg/l or less in large water treatment facilities and 0.2 mg/l or less in small facilities Ammonia Occurs in drinking‑water at concentrations well below those of health concern Anatoxins (cyanobacterial toxins) Available data inadequate to permit derivation of a health‑based guideline value Asbestos No consistent evidence that ingested asbestos is hazardous to health and available data inadequate to permit derivation of health‑based guideline value Bentazone Occurs in drinking‑water or drinking‑water sources at concentrations well below those of health concern Beryllium Rarely found in drinking‑water at concentrations of health concern Bromide Occurs in drinking‑water at concentrations well below those of health concern Bromochloroacetate Available data inadequate to permit derivation of health‑based guideline value Bromochloroacetonitrile Available data inadequate to permit derivation of health‑based guideline value Bacillus thuringiensis israelensis Not considered appropriate to set guideline values for pesticides (Bti) used for vector control in drinking‑water Carbaryl Occurs in drinking‑water at concentrations well below those of health concern Chloral hydrate Occurs in drinking‑water at concentrations well below those of health concern Chloride Not of health concern at levels found in drinking‑waterb Chlorine dioxide Reduced primarily to chlorite, chlorate and chloride in drinking‑water, and to chlorite and chloride upon ingestion; the provisional guideline values for chlorite and chlorate are protective for potential toxicity from chlorine dioxide Chloroacetones Available data inadequate to permit derivation of health‑based guideline values for any of the chloroacetones 2‑Chlorophenol Available data inadequate to permit derivation of health‑based guideline value Chloropicrin Available data inadequate to permit derivation of health‑based guideline value Cyanide Occurs in drinking‑water at concentrations well below those of health concern, except in emergency situations following a spill to a water source Cyanogen chloride Occurs in drinking‑water at concentrations well below those of health concern Dibromoacetate Available data inadequate to permit derivation of health‑based guideline value Table A3.2 (continued) Chemical Reason for not establishing a guideline value Dichloramine 1,3‑Dichlorobenzene 1,1‑Dichloroethane 1,1‑Dichloroethene 2,4‑Dichlorophenol 1,3‑Dichloropropane Dichlorvos Dicofol Di(2‑ethylhexyl)adipate Diflubenzuron Diquat Endosulfan Fenitrothion Fluoranthened Formaldehyde Glyphosate and AMPAe Hardness Heptachlor and heptachlor epoxide Hexachlorobenzene Hydrogen sulfide Inorganic tin Iodine Available data inadequate to permit derivation of health‑based guideline value Available data inadequate to permit derivation of health‑based guideline value Available data inadequate to permit derivation of health‑based guideline value Occurs in drinking‑water at concentrations well below those of health concern Available data inadequate to permit derivation of health‑based guideline value Available data inadequate to permit derivation of health‑based guideline value Occurs in drinking‑water or drinking‑water sources at concentrations well below those of health concern Unlikely to occur in drinking‑water or drinking‑water sourcesc Occurs in drinking‑water at concentrations well below those of health concern Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water Occurs in drinking‑water or drinking‑water sources at concentrations well below those of health concern Occurs in drinking‑water at concentrations well below those of health concern Occurs in drinking‑water at concentrations well below those of health concern Occurs in drinking‑water at concentrations well below those of health concern Occurs in drinking‑water at concentrations well below those of health concern Occur in drinking‑water at concentrations well below those of health concern Not of health concern at levels found in drinking‑waterb Occur in drinking‑water at concentrations well below those of health concern Occurs in drinking‑water at concentrations well below those of health concern Not of health concern at levels found in drinking‑waterb Occurs in drinking‑water at concentrations well below those of health concern Available data inadequate to permit derivation of health‑based guideline value. Additionally, occurrence in drinking‑water is usually low. Although higher levels of exposure may occur when iodine is used as a drinking‑water disinfectant at the point of use, extended periods of exposure to iodine through water disinfection are unlikely. Table A3.2 (continued) Chemical Reason for not establishing a guideline value Iron Not of health concern at levels causing acceptability problems in drinking‑watera Malathion Occurs in drinking‑water at concentrations well below those of health concern MCPAf Occurs in drinking‑water or drinking‑water sources at concentrations well below those of health concern Methoprene Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water Methyl parathion Occurs in drinking‑water at concentrations well below those of health concern Methyl tertiary‑butyl ether (MTBE) Any guideline that would be derived would be significantly higher than concentrations at which MTBE would be detected by odour Molybdenum Occurs in drinking‑water at concentrations well below those of health concern Monobromoacetate Available data inadequate to permit derivation of health‑based guideline value Monochlorobenzene Occurs in drinking‑water at concentrations well below those of health concern, and health‑based value would far exceed lowest reported taste and odour threshold MXg Occurs in drinking‑water at concentrations well below those of health concern Nitrobenzene Rarely found in drinking‑water at concentrations of health concern Novaluron Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water Organotins TBT, TPT, DBT, DOTh Occurs in drinking‑water at concentrations well below those of health concern MMT, DMT, DMTCi Unnecessary since their use as stabilizers in polyvinyl chloride and chlorinated polyvinyl chloride are normally controlled by product specification Other organotins Available data inadequate to permit derivation of health‑based guideline value Parathion Occurs in drinking‑water at concentrations well below those of health concern Permethrin Not recommended for direct addition to drinking‑water as part of WHO’s policy to exclude the use of any pyrethroids for larviciding of mosquito vectors of human disease Petroleum products Taste and odour will in most cases be detectable at concentrations below those of health concern, particularly with short‑term exposure pHi Not of health concern at levels found in drinking‑waterj 2‑Phenylphenol and its sodium Occurs in drinking‑water at concentrations well below those of salt health concern Table A3.2 (continued) Chemical Reason for not establishing a guideline value Pirimiphos‑methyl Not recommended for direct application to drinking‑water unless no other effective and safe treatments are available Potassium Occurs in drinking‑water at concentrations well below those of health concern Propanil Readily transformed into metabolites that are more toxic; a guideline value for the parent compound is considered inappropriate, and there are inadequate data to enable the derivation of guideline values for the metabolites Pyriproxyfen Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water Silver Available data inadequate to permit derivation of health‑based guideline value and usually occurs in drinking water at concentrations well below those of health concern Sodium Not of health concern at levels found in drinking‑waterb Spinosad Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water Sulfate Not of health concern at levels found in drinking‑waterb Temephos Not considered appropriate to set guideline values for pesticides used for vector control in drinking‑water Total dissolved solids Not of health concern at levels found in drinking‑waterb Trichloramine Available data inadequate to permit derivation of health‑based guideline value Trichloroacetonitrile Available data inadequate to permit derivation of health‑based guideline value Trichlorobenzenes (total) Occurs in drinking‑water at concentrations well below those of health concern, and health‑based value would exceed lowest reported odour threshold 1,1,1‑Trichloroethane Occurs in drinking‑water at concentrations well below those of health concern Zinc Not of health concern at levels found in drinking‑waterb a Health‑based values have been developed for a number of these chemicals, whereas, for anatoxins and silver, provisional reference values have been established. These values may be useful to guide actions by Member States when there is a reason for local concern. For further information on guideline values, health‑based values and provisional reference values, see section 8.2. Where health‑based values or provisional reference values have been established, this information can be found in the chemical fact sheets in chapter 12. b May affect acceptability of drinking‑water (see chapter 10). c Although dicofol does not fulfil one of the three criteria for evaluation in the Guidelines, a background document has been prepared and a health‑based value has been established, in response to a request from Member States for guidance. d See fact sheet on polynuclear aromatic hydrocarbons. e Aminomethylphosphonic acid. f (2‑Methyl‑4‑chlorophenoxy)acetic acid. g 3‑Chloro‑4‑dichloromethyl‑5‑hydroxy‑2(5H)‑furanone. h Tributyltin, triphenyltin, dibutyltin, and di‑n‑octyltin respectively. i Monomethyltin dimethyltin and dimethyltin dichloride, respectively. j An important operational water quality parameter. Table A3.3 Guideline values for chemicals that are of health significance in drinking-water Guideline value Chemical mg/l μg/l Remarks Acrylamide 0.0005a 0.5a Alachlor 0.02a 20a Aldicarb 0.01 10 Applies to aldicarb sulfoxide and aldicarb sulfone Aldrin and dieldrin 0.000 03 0.03 For combined aldrin plus dieldrin Antimony 0.02 20 Arsenic 0.01 (A, T) 10 (A, T) Atrazine and its chloro‑s‑0.1 100 triazine metabolites Barium 1.3 1300 Benzene 0.01a 10a Benzo[a]pyrene 0.0007a 0.7a Boron 2.4 2 400 Bromate 0.01a (A, T) 10a (A, T) Bromodichloromethane 0.06a 60a Bromoform 0.1 100 Cadmium 0.003 3 Carbofuran 0.007 7 Carbon tetrachloride 0.004 4 Chlorate 0.7 (D) 700 (D) Chlordane 0.0002 0.2 Chlorine 5 (C) 5 000 (C) For free chlorine. For effective disinfection, there should be a residual concentration of free chlorine of ≥0.5 mg/l after at least 30 min contact time at pH <8.0. A chlorine residual should be maintained throughout the distribution system. At the point of delivery, the minimum residual concentration of free chlorine should be 0.2 mg/l. Chlorite 0.7 (D) 700 (D) Chloroform 0.3 300 Chlorotoluron 0.03 30 Chlorpyrifos 0.03 30 Chromium 0.05 50 For total chromium Copper 2 2 000 Staining of laundry and sanitary ware may occur below guideline value Cyanazine 0.0006 0.6 Table A3.3 (continued) Guideline value Chemical mg/l μg/l Remarks Cylindrospermopsins 0.0007 (P) 0.7 (P) (cyanobacterial toxin) 0.003 (P) 3 (P) For short‑term exposureb Values are for total cylindrospermopsins (sum of all congeners, free plus‑cell bound) 2,4‑Dc 0.03 30 Applies to free acid 2,4‑DBd 0.09 90 DDTe and metabolites 0.001 1 Dibromoacetonitrile 0.07 70 Dibromochloromethane 0.1 100 1,2‑Dibromo‑3‑0.001a 1a chloropropane 1,2‑Dibromoethane 0.0004a (P) 0.4a (P) Dichloroacetate 0.05a (D) 50a (D) Dichloroacetonitrile 0.02 (P) 20 (P) 1,2‑Dichlorobenzene 1 (C) 1 000 (C) 1,4‑Dichlorobenzene 0.3 (C) 300 (C) 1,2‑Dichloroethane 0.03a 30a 1,2‑Dichloroethene 0.05 50 Dichloromethane 0.02 20 1,2‑Dichloropropane 0.04 (P) 40 (P) 1,3‑Dichloropropene 0.02a 20a Dichlorprop 0.1 100 Di(2‑ethylhexyl)phthalate 0.008 8 Dimethoate 0.006 6 1,4‑Dioxane 0.05a 50a Derived using tolerable daily intake approach as well as linearized multistage modelling Edetic acid 0.6 600 Applies to the free acid Endrin 0.0006 0.6 Epichlorohydrin 0.0004 (P) 0.4 (P) Ethylbenzene 0.3 (C) 300 (C) Fenoprop 0.009 9 Fluoride 1.5 1 500 Volume of water consumed and intake from other sources should be considered when setting national standards Hexachlorobutadiene 0.0006 0.6 Hydroxyatrazine 0.2 200 Atrazine metabolite Isoproturon 0.009 9 Table A3.3 (continued) Guideline value Chemical mg/l μg/l Remarks Lead 0.01 (A, T) 10 (A, T) Lindane 0.002 2 Manganese 80 (P) 0.08 (P) For total manganese. Aesthetic as well as health aspects should be considered when setting national standards Mecoprop 0.01 10 Mercury 0.006 6 For inorganic mercury Methoxychlor 0.02 20 Metolachlor 0.01 10 Microcystins 0.001 (P) 1 (P) (cyanobacterial toxin) 0.012 (P) 12 (P) For short‑term exposureb Values are for total microcystins (sum of all congeners, free plus‑cell bound) Molinate 0.006 6 Monochloramine 3 3 000 Monochloroacetate 0.02 20 Nickel 0.07 70 Based on long‑term effects, but protective for short‑term effects Nitrate (as NO3−) 50 50 000 Based on short‑term effects, but protective for long‑term effects Nitrilotriacetic acid 0.2 200 Nitrite (as NO2−) 3 3 000 Based on short‑term effects, but protective for long‑term effects N‑Nitrosodimethylamine 0.0001 0.1 Pendimethalin 0.02 20 Pentachlorophenol 0.009a (P) 9a (P) Saxitoxins 0.003 3 For acute exposure (cyanobacterial toxin) For total saxitoxins (sum of all congeners, free plus‑cell bound) Selenium 0.04 (P) 40 (P) Simazine 0.002 2 Sodium 50 50 000 As sodium dichloroisocyanurate dichloroisocyanurate 40 40 000 As cyanuric acid Styrene 0.02 (C) 20 (C) 2,4,5‑Tf 0.009 9 Terbuthylazine 0.007 7 Tetrachloroethene 0.1 100 Toluene 0.7 (C) 700 (C) Trichloroacetate 0.2 200 Trichloroethene 0.008 8 Table A3.3 (continued) Guideline value Chemical mg/l μg/l Remarks 2,4,6‑Trichlorophenol 0.2a (C) 200a (C) Trifluralin 0.02 20 Trihalomethanes The sum of the ratio of the concentration of each to its respective guideline value should not exceed 1 Uranium 0.03 (P) 30 (P) Only chemical, not radiological, aspects of uranium addressed Vinyl chloride 0.0003a 0.3a Xylenes 0.5 (C) 500 (C) A, provisional guideline value because calculated guideline value is below the achievable quantification level; C, concentrations of the substance at or below the health‑based guideline value may affect the appearance, taste or odour of the water, leading to consumer complaints; D, provisional guideline value because effective disinfection may result in the guideline value being exceeded; P, provisional guideline value because of uncertainties in the health database; T, provisional guideline value because calculated guideline value is below the level that can be achieved through practical treatment methods, source protection, etc. a For substances that are considered to be carcinogenic, the guideline value is the concentration in drinking‑water associated with an upper‑bound excess lifetime cancer risk of 10−5 (one additional case of cancer per 100 000 of the population ingesting drinking‑water containing the substance at the guideline value for 70 years). Concentrations associated with upper‑bound estimated excess lifetime cancer risks of 10−4 and 10−6 can be calculated by multiplying and dividing, respectively, the guideline value by 10. b See the respective chemical fact sheet in chapter 12 for considerations for bottle‑fed infants. 2,4‑Dichlorophenoxyacetic acid. d 2,4‑Dichlorophenoxybutyric acid. e Dichlorodiphenyltrichlorethane. f 2,4,5‑Trichlorophenoxyacetic acid. ANNEX 4 Analytical methods and achievability A4.1 Analytical methods In volumetric titration, chemicals are analysed by titration with a standardized titrant. The titration end-point is identified by the development of colour resulting from the reaction with an indicator, by the change of electrical potential or by the change of pH value. Colorimetric methods are based on measuring the intensity of colour of a coloured target chemical or reaction product. The optical absorbance is measured using light of a suitable wavelength. The concentration is determined by means of a calibration curve obtained using known concentrations of the determinant. The ultraviolet (UV) method is similar to this method except that UV light is used. For ionic materials, the ion concentration can be measured using an ion selective electrode. The measured potential is proportional to the logarithm of the ion concentration. Some organic compounds absorb UV light (wavelength 190–380 nm) in proportion to their concentration. UV absorption is useful for qualitative estimation of organic substances, because a strong correlation may exist between UV absorption and organic carbon content. Atomic absorption spectrometry (AAS) is used for the determination of metals. It is based on the phenomenon that the atom in the ground state absorbs the light of wavelengths that are characteristic to each element when light is passed through the atoms in the vapour state. Because this absorption of light depends on the concentration of atoms in the vapour, the concentration of the target element in the water sample is determined from the measured absorbance. The Beer-Lambert law describes the relationship between concentration and absorbance. In flame atomic absorption spectrometry (FAAS), a sample is aspirated into a flame and atomized. A light beam from a hollow cathode lamp of the same element as the target metal is radiated through the flame, and the amount of absorbed light is measured by the detector. This method is much more sensitive than other methods and free from spectral or radiation interference by co-existing elements. Pretreatment is either unnecessary or straightforward. However, it is not suitable for simultaneous analysis of many elements, because the light source is different for each target element. 529 Electrothermal atomic absorption spectrometry (EAAS) is based on the same principle as FAAS, but an electrically heated atomizer or graphite furnace replaces the standard burner head for determination of metals. In comparison with FAAS, EAAS gives higher sensitivities and lower detection limits, and a smaller sample volume is required. EAAS suffers from more interference through light scattering by co-existing elements and requires a longer analysis time than FAAS. The principle of inductively coupled plasma atomic emission spectrometry (ICPAES) for determination of metals is as follows. An ICP source consists of a flowing stream of argon gas ionized by an applied radio frequency. A sample aerosol is generated in a nebulizer and spray chamber and then carried into the plasma through an injector tube. A sample is heated and excited in the high-temperature plasma. The high temperature of the plasma causes the atoms to become excited. On returning to the ground state, the excited atoms produce ionic emission spectra. A monochromator is used to separate specific wavelengths corresponding to different elements, and a detector measures the intensity of radiation of each wavelength. A significant reduction in chemical interference is achieved. In the case of water with low pollution, simultaneous or sequential analysis is possible without special pretreatment to achieve low detection limits for many elements. This, coupled with the extended dynamic range from three digits to five digits, means that multielement determination of metals can be achieved. ICP-AES has similar sensitivity to FAAS or EAAS. In inductively coupled plasma mass spectrometry (ICP-MS), elements are atomized and excited as in ICP-AES, then passed to a mass spectrometer. Once inside the mass spectrometer, the ions are accelerated by high voltage and passed through a series of ion optics, an electrostatic analyser and, finally, a magnet. By varying the strength of the magnet, ions are separated according to mass/charge ratio and passed through a slit into the detector, which records only a very small atomic mass range at a given time. By varying the magnet and electrostatic analyser settings, the entire mass range can be scanned within a relatively short period of time. In the case of water with low pollution, simultaneous or sequential analysis is possible without special pretreatment to achieve low detection limits for many elements. This, coupled with the extended dynamic range from three digits to five digits, means that multielement determination of metals can be achieved. Chromatography is a separation method based on the affinity difference between two phases, the stationary and mobile phases. A sample is injected into a column, either packed or coated with the stationary phase, and separated by the mobile phase based on the difference in interaction (distribution or adsorption) between compounds and the stationary phase. Compounds with a low affinity for the stationary phase move more quickly through the column and elute earlier. The compounds that elute from the end of the column are determined by a suitable detector. In ion chromatography, an ion exchanger is used as the stationary phase, and the eluant for determination of anions is typically a dilute solution of sodium hydrogen carbonate and sodium carbonate. Colorimetric, electrometric or titrimetric detectors can be used for determining individual anions. In suppressed ion chromatography, anions are converted to their highly conductive acid forms; in the carbonate–bicarbonate eluant, anions are converted to weakly conductive carbonic acid. The separated acid forms are measured by conductivity and identified on the basis of retention time as compared with their standards. High-performance liquid chromatography (HPLC) is an analytical technique using a liquid mobile phase and a column containing a liquid stationary phase. Detection of the separated compounds is achieved through the use of absorbance detectors for organic compounds and through conductivity or electrochemical detectors for metallic and inorganic compounds. Gas chromatography (GC) permits the identification and quantification of trace organic compounds. In GC, gas is used as the mobile phase, and the stationary phase is a liquid that is coated either on an inert granular solid or on the walls of a capillary column. When the sample is injected into the column, the organic compounds are vaporized and moved through the column by the carrier gas at different rates depending on differences in partition coefficients between the mobile and stationary phases. The gas exiting the column is passed to a suitable detector. A variety of detectors can be used, including flame ionization (FID), electron capture (ECD) and nitrogen–phosphorus. As separation ability is good in this method, mixtures of substances with similar structure are systematically separated, identified and determined quantitatively in a single operation. The gas chromatography/mass spectrometry (GC-MS) method is based on the same principle as the GC method, using a mass spectrometer as the detector. As the gas emerges from the end of the GC column opening, it flows through a capillary column interface into the MS. The sample then enters the ionization chamber, where a collimated beam of electrons impacts the sample molecules, causing ionization and fragmentation. The next component is a mass analyser, which uses a magnetic field to separate the positively charged particles according to their mass. Several types of separating techniques exist; the most common are quadrupoles and ion traps. After the ions are separated according to their masses, they enter a detector. The purge-and-trap packed column GC-MS method or purge-and-trap packed column GC method is applicable to the determination of various purgeable organic compounds that are transferred from the aqueous to the vapour phase by bubbling purge gas through a water sample at ambient temperature. The vapour is trapped with a cooled trap. The trap is heated and backflushed with the same purge gas to desorb the compounds onto a GC column. The principles of GC or GC-MS are as referred to above. The principle of enzyme-linked immunosorbent assay (ELISA) is as follows. The protein (antibody) against the chemical of interest (antigen) is coated onto the solid material. The target chemical in the water sample binds to the antibody, and a second antibody with an enzyme attached is also added that will attach to the chemical of interest. After washing to remove any of the free reagents, a chromogen is added that will give a colour reaction due to cleavage by the enzyme that is proportional to the quantity of the chemical of interest. The ELISA method can be used to determine microcystin and synthetic surfactants. A4.2 Analytical achievability for chemicals for which guideline values have been established Analytical achievability for chemicals for which guideline values have been established is given in Tables A4.1–A4.6. Table A4.1 Analytical achievability for inorganic chemicals for which guideline values have been established, by source categorya Field methods Laboratory methods Col Absor IC FAAS EAAS ICP ICP-MS Naturally occurring chemicals Arsenic +++ # ++(H) + ++(H) +++ Barium ++ +++ +++ +++ Boron ++ +++ +++ Chromium # ++ ++ +++ Fluoride # + +++ Manganese # ++ +++ ++b +++ Selenium # ++(H) ++ ++(H) +++ Uranium +++ Chemicals from industrial sources and human dwellings Cadmium # ++ ++ +++ Mercury +++ Chemicals from agricultural activities Nitrate/nitrite +++ +++ +++ Chemicals used in water treatment or materials in contact with drinking-water Antimony +++(H) ++(H) +++ Copper # +++ +++ +++ +++ +++ Lead # + + +++ Nickel + + ++ ++ +++ a For definitions and notes to Table A4.1, see below Table A4.6. b For ICP‑AES Table A4.2 Analytical achievability for organic chemicals from industrial sources and human dwellings for which guideline values have been establisheda (PT-) (PT-) PT-GC-HPLC HPLC-Col GC GC-PD GC-ECD GC-FID GC-FPD GC-TID GC-MS MS HPLC -FD UVPAD EAAS IC-FD IC-SCD LC-MS Benzene +++ +++ Carbon tetrachloride +++ +++ 1,2‑Dichlorobenzene +++ +++ +++ +++ 1,4‑Dichlorobenzene +++ +++ +++ +++ 1,2‑Dichloroethane +++ +++ 1,2‑Dichloroethene +++ +++ +++ Dichloromethane +++ +++ Di(2‑ethylhexyl)phthalate ++ 1,4‑Dioxane +++ Edetic acid +++ Ethylbenzene +++ +++ Hexachlorobutadiene ++ ++ ++ Nitrilotriacetic acid +++ +++ Pentachlorophenol +++ + + Perchlorate ++ +++ Styrene +++ +++ Tetrachloroethene +++ +++ +++ +++ Toluene +++ +++ Trichloroethene +++ +++ +++ +++ Xylenes +++ +++ a For definitions and notes to Table A4.2, see below Table A4.6. Table A4.3 Analytical achievability for organic chemicals from agricultural activities for which guideline values have been establisheda,b (PT-) (PT-) Col GC GC-PD GC-ECD GC-FID GC-FPD GC-TID GC-MS PT-GC-MS HPLC HPLC-FD HPLC-UVPAD EAAS IC-FD Alachlor +++ +++ Aldicarb +++ Aldrin and dieldrin ++ ++ Atrazine and its chloro+++ +++ +++ s‑triazine metabolites Carbofuran ++ Chlordane +++ +++ Chlorotoluron +++ +++ Cyanazine +++ +++ + 2,4‑D +++ +++ ++ 2,4‑DB +++ ++ ++ 1,2‑Dibromo‑3‑chloro+++ +++ +++ propane 1,2‑Dibromoethane ++ ++ +++ 1,2‑Dichloropropane +++ +++ 1,3‑Dichloropropene +++ +++ Dichlorprop +++ +++ Dimethoate +++ Endrin +++ +++ Fenoprop +++ + Hydroxyatrazine +++ +++ Isoproturon +++ +++ Lindane +++ +++ Table A4.3 (continued) (PT-) (PT-) Col GC GC-PD GC-ECD GC-FID GC-FPD GC-TID GC-MS PT-GC-MS HPLC HPLC-FD HPLC-UVPAD EAAS IC-FD Mecoprop +++ +++ Methoxychlor +++ Metolachlor +++ +++ Molinate +++ +++ Pendimethalin +++ Simazine +++ +++ 2,4,5‑T +++ + Terbuthylazine +++ ++ Trifluralin +++ +++ +++ a For definitions and notes to Table A4.3, see below Table A4.6. b LC‑MS is also applicable for many of these agricultural chemicals. Table A4.4 Analytical achievability for chemicals used in water treatment or from materials in contact with water for which guideline values have been establisheda (PT-) (PT-) Col GC GC-PD GC-ECD GC-FID GC-FPD GC-TID GC-MS PT- GC-MS HPLC HPLC-FD HPLC-UVPAD EAAS IC Disinfectants Monochloramine +++ Chlorine +++ Sodium +++ +++ +++ dichloroisocyanurate Disinfection by-products Bromate ++ Bromodichloromethane +++ +++ +++ Bromoform +++ +++ +++ Chlorate +++ Table A4.4 (continued) (PT-) (PT-) Col GC GC-PD GC-ECD GC-FID GC-FPD GC-TID GC-MS PT- GC-MS HPLC HPLC-FD HPLC-UVPAD EAAS IC Chlorite +++ Chloroform +++ +++ +++ Dibromoacetonitrile +++ +++ Dibromochloromethane +++ +++ +++ Dichloroacetic acid +++ +++ Dichloroacetonitrile +++ +++ Monochloroacetic acid +++ ++ N‑Nitrosodimethylamine +++ Trichloroacetic acid +++ +++ 2,4,6‑Trichlorophenol +++ +++ Trihalomethanesb +++ +++ +++ Organic contaminants from treatment chemicals Acrylamide + + Epichlorohydrin +++ +++ + Organic contaminants from pipes and fittings Benzo[a]pyrene ++ ++ Vinyl chloride ++ ++ + a For definitions and notes to Table A4.4, see below Table A4.6. b See also individual trihalomethanes. Table A4.5 Analytical achievability for pesticides used in water for public health purposes for which guideline values have been establisheda Col GC GC-PD GC-EC GC-FID GC-FPD GC-TID GC-MS PT-GC-MS HPLC HPLC-FD HPLC-UVPAD EAAS IC/FD Chlorpyrifos +++ ++ ++ +++ DDT (and metabolites) ++ ++ a For definitions and notes to Table A4.5, see below Table A4.6. Table A4.6 Analytical achievability for cyanobacterial toxins for which guideline values have been establisheda,b,c Cyanotoxin group PPA RBA ELISA HPLC-UVPAD LC-MS/PAD HPLC-FD with pre- or post-LC-MS/MSd column derivatization Anatoxin‑a variants + ++ +++ +++ Cylindrospermopsins ++ ++ ++ +++ Microcystins + ++ ++ ++ +++ Saxitoxins ++ + +++ +++ a For definitions and notes to Table A4.6, see below this table. b Note that most cyanotoxins are contained within the cell material and prior extraction is crucial. See the chapter 12 fact sheets for further details. Note that for anatoxin‑a variants, although the data were inadequate to establish a guideline value, a provisional reference value was derived. d Quantitative reference standards are needed for congeners. See the chapter 12 fact sheets for further details. Definitions to Tables A4.1–A4.6 Absor Absorptiometry HPLC‑UVPAD High‑performance liquid chromatography–ultraviolet photodiode array detector Col Colorimetry IC Ion chromatography EAAS Electrothermal atomic absorption spectrometry IC‑FAAS Ion chromatography–flame atomic absorption spectrometry ELISA Enzyme‑linked immunosorbent assay IC‑FD Ion chromatography–fluorescence detector FAAS Flame atomic absorption spectrometry IC‑SCD Ion chromatography‑suppressed conductivity detection GC Gas chromatography ICP Inductively coupled plasma GC‑ECD Gas chromatography–electron capture detector ICP‑AES Inductively coupled plasma atomic emission spectrometry GC‑FID Gas chromatography–flame ionization detector ICP‑MS Inductively coupled plasma mass spectrometry GC‑FPD Gas chromatography–flame photodiode detector LC‑MS Liquid chromatography–mass spectrometry GC‑MS Gas chromatography–mass spectrometry LC‑MS/MS Liquid chromatography–tandem mass spectrometry GC‑PD Gas chromatography–photoionization detector LC‑MS/PAD Liquid chromatography–mass spectrometry‑photodiode array detector GC‑TID Gas chromatography–thermal ionization detector PPA Protein phosphatase assay HPLC High‑performance liquid chromatography PT‑GC‑MS Purge‑and‑trap gas chromatography–mass spectrometry HPLC‑FD High‑performance liquid chromatography– RBA Receptor‑binding assay fluorescence detector Notes to Tables A4.1–A4.6 + The detection limit is between the guideline value and 1/10th of its value. ++ The detection limit is between 1/10th and 1/50th of the guideline value. +++ The detection limit is under 1/100th of the guideline value. # The analytical method is available for detection of the guideline value concentration, but it is difficult to detect the concentration of 1/10 of the guideline value. (H) This method is applicable to the determination by conversion to their hydrides by hydride generator. ANNEX 5 Treatment methods and performance A5.1 Treatment methods A5.1.1 Chlorination Chlorination can be achieved by using liquefied chlorine gas, sodium hypochlorite solution or calcium hypochlorite granules and on-site chlorine generators. Liquefied chlorine gas is supplied in pressurized containers. The gas is withdrawn from the cylinder and dosed into water by a chlorinator, which both controls and measures the gas flow rate. Sodium hypochlorite solution is dosed using a positive-displacement electric dosing pump or gravity feed system. Calcium hypochlorite has to be dissolved in water, then mixed with the main supply. Chlorine, whether in the form of chlorine gas from a cylinder, sodium hypochlorite or calcium hypochlorite, dissolves in water to form hypochlorous acid (HOCl) and hypochlorite ion (OCl−). Different techniques of chlorination can be used, including breakpoint chlorination, marginal chlorination and superchlorination/dechlorination. Breakpoint chlorination is a method in which the chlorine dose is sufficient to rapidly oxidize all the ammonia nitrogen in the water and to leave a suitable free residual chlorine available to protect the water against reinfection from the point of chlorination to the point of use. Superchlorination/dechlorination is the addition of a large dose of chlorine to effect rapid disinfection and chemical reaction, followed by reduction of excess free chlorine residual. Removing excess chlorine is important to prevent taste problems. It is used mainly when the bacterial load is variable or the detention time in a tank is not enough. Marginal chlorination is used where water supplies are of high quality and is the simple dosing of chlorine to produce a desired level of free residual chlorine. The chlorine demand in these supplies is very low, and a breakpoint might not even occur. Chlorination is employed primarily for microbial disinfection. However, chlorine also acts as an oxidant and can remove or assist in the removal or chemical conversion of some chemicals—for example, decomposition of easily oxidized pesticides, such as aldicarb; oxidation of dissolved species (e.g. manganese(II)) to form insoluble products that can be removed by subsequent filtration; and oxidation of dissolved species to more easily removable forms (e.g. arsenite to arsenate). 539 A disadvantage of chlorine is its ability to react with natural organic matter to produce trihalomethanes and other halogenated disinfection by-products. However, by-product formation may be controlled by optimization of the treatment system. A5.1.2 Ozonation Ozone is a powerful oxidant and has many uses in water treatment, including oxidation of organic chemicals. Ozone can be used as a primary disinfectant. Ozone gas (O3) is formed by passing dry air or oxygen through a high-voltage electric field. The resultant ozone-enriched air is dosed directly into the water by means of porous diffusers at the base of baffled contactor tanks. The contactor tanks, typically about 5 m deep, provide 10–20 minutes of contact time. Dissolution of at least 80% of the applied ozone should be possible, with the remainder contained in the off-gas, which is passed through an ozone destructor and vented to the atmosphere. The performance of ozonation relies on achieving the desired concentration after a given contact period. For oxidation of organic chemicals, such as some oxidizable pesticides, a residual of about 0.5 mg/l after a contact time of up to 20 minutes is typically used. The doses required to achieve this vary with the type of water but are typically in the range 2–5 mg/l. Higher doses are needed for untreated waters, because of the ozone demand of the natural background organics. Ozone reacts with natural organics to increase their biodegradability, measured as assimilable organic carbon. To avoid undesirable bacterial growth in distribution, ozonation is normally used with subsequent treatment, such as biological filtration or granular activated carbon (GAC), to remove biodegradable organics, followed by a chlorine residual, as ozone does not provide a disinfectant residual. Ozone is effective for the degradation of a wide range of pesticides and other organic chemicals. A5.1.3 Other disinfection processes Other disinfection methods include chloramination, the use of chlorine dioxide and UV radiation, as well as alternative disinfection techniques that may be used in smaller-scale applications, such as for household water. Chloramines (monochloramine, dichloramine and trichloramine, or nitrogen trichloride) are produced by the reaction of aqueous chlorine with ammonia. Monochloramine is the only useful chloramine disinfectant, and conditions employed for chloramination are designed to produce only monochloramine. Monochloramine is a less effective disinfectant than free chlorine, but it is persistent, and it is therefore an attractive secondary disinfectant for the maintenance of a stable distribution system residual. Chlorine dioxide has been used in recent years because of concerns about disinfection by-product production associated with chlorine disinfection. Typically, chlorine dioxide is generated immediately prior to application by the addition of chlorine gas or an aqueous chlorine solution to aqueous sodium chlorite. Chlorine dioxide decomposes in water to form chlorite and chlorate. UV radiation, emitted by a low-pressure or medium-pressure mercury arc lamp, is biocidal between wavelengths of 180 and 320 nm. It can be used to inactivate protozoa, bacteria, bacteriophage, yeast, viruses, fungi and algae. Turbidity can inhibit UV disinfection. UV radiation can act as a catalyst in oxidation reactions when used in conjunction with ozone or hydrogen peroxide. Numerous possible disinfection techniques are being developed and are typically used in smaller-scale applications, such as household point-of-use and point-of-entry water treatment systems. Some of these, including bromine and iodine, show promise for expanded use. Bromine and iodine are halogens, like chlorine, and they are well-known biocides. Iodine is commonly used for short-term applications, such as by travellers in areas where water quality is questionable. Some forms of silver may have applications as bacteriostats or possibly as slow-acting disinfectants for some microorganisms; however, there are not good peer-reviewed published data to quantify the latter. It will be necessary to develop a more thorough analysis of the biocidal efficacy, potential disinfection by-products and risks from long-term exposures and application conditions for these lesser-used treatment chemicals to provide appropriate guidance as to their potential for wider applications. A5.1.4 Filtration Particulate matter can be removed from raw waters by rapid gravity, horizontal, pressure or slow sand filters. Slow sand filtration is essentially a biological process, whereas the others are physical treatment processes. Rapid gravity, horizontal and pressure filters can be used for filtration of raw water, without pretreatment. Rapid gravity and pressure filters are commonly used to filter water that has been pretreated by coagulation and sedimentation. An alternative process is direct filtration, in which coagulation is added to the water, which then passes directly onto the filter where the precipitated floc (with contaminants) is removed; the application of direct filtration is limited by the available storage within the filter to accommodate solids. Rapid gravity filters Rapid gravity sand filters usually consist of open rectangular tanks (usually < 100 m2) containing silica sand (size range 0.5–1.0 mm) to a depth of between 0.6 and 2.0 m. The water flows downwards, and solids become concentrated in the upper layers of the bed. The flow rate is generally in the range 4–20 m3/m2·h. Treated water is collected via nozzles in the floor of the filter. The accumulated solids are removed periodically by backwashing with treated water, sometimes preceded by scouring of the sand with air. A dilute sludge that requires disposal is produced. In addition to single-medium sand filters, dual-media or multimedia filters are used. Such filters incorporate different materials, such that the structure is from coarse to fine as the water passes through the filter. Materials of suitable density are used in order to maintain the segregation of the different layers following backwashing. A common example of a dual-media filter is the anthracite–sand filter, which typically consists of a 0.2 m deep layer of 1.5 mm anthracite over a 0.6 m deep layer of silica sand. Anthracite, sand and garnet can be used in multimedia filters. The advantage of dual-media and multimedia filters is that there is more efficient use of the whole bed depth for particle retention—the rate of headloss development can be half that of single-medium filters, which can allow higher flow rates without increasing headloss development. Rapid gravity filters are most commonly used to remove floc from coagulated waters (see section A5.1.6). They may also be used to reduce turbidity (including adsorbed chemicals) and oxidized iron and manganese from raw waters. Roughing filters Roughing filters can be applied as pre-filters prior to other processes such as slow sand filters. Roughing filters with coarse gravel or crushed stones as the filter medium can successfully treat water of high turbidity (> 50 nephelometric turbidity units). The main advantage of roughing filtration is that as the water passes through the filter, particles are removed by both filtration and gravity settling. Horizontal filters can be up to 10 m long and are operated at filtration rates of 0.3–1.0 m3/m2·h. Pressure filters Pressure filters are sometimes used where it is necessary to maintain head in order to eliminate the need for pumping into supply. The filter bed is enclosed in a cylindrical shell. Small pressure filters, capable of treating up to about 15 m3/h, can be manufactured in glass-reinforced plastics. Larger pressure filters, up to 4 m in diameter, are manufactured in specially coated steel. Operation and performance are generally as described for the rapid gravity filter, and similar facilities are required for backwashing and disposal of the dilute sludge. Slow sand filters Slow sand filters usually consist of tanks containing sand (effective size range 0.15–0.3 mm) to a depth of between 0.5 and 1.5 m. The raw water flows downwards, and turbidity and microorganisms are removed primarily in the top few centimetres of the sand. A biological layer, known as the “schmutzdecke”, develops on the surface of the filter and can be effective in removing microorganisms. Treated water is collected in underdrains or pipework at the bottom of the filter. The top few centimetres of sand containing the accumulated solids are removed and replaced periodically. Slow sand filters are operated at a water flow rate of between 0.1 and 0.3 m3/m2·h. Slow sand filters are more suitable for low-turbidity water or water that has been pre-filtered. They are used to remove algae and microorganisms, including protozoa, and, if preceded by microstraining or coarse filtration, to reduce turbidity (including adsorbed chemicals). Slow sand filtration is effective for the removal of some organics, including certain pesticides and also ammonia. Bank filtration Bank filtration is a process that produces an influx of surface water through the groundwater, via the bed and banks of the surface water body. This is commonly achieved through abstraction from boreholes adjacent to the surface water source. It is a relatively simple and low-cost means for removing particulates and microorganisms from surface water by placing pumping wells in alluvial sediments of the river or stream banks. The sediments act as both a filter and biofilter, trapping and reducing the concentrations of microorganisms and many organic pollutants. Bank filtration wells can be either horizontal or vertical, depending upon the hydrogeological circumstances and required production rate. Horizontal wells are often used where alluvial deposits are shallow or where high pumping rates are required. Bank filtration can remove particles, bacteria, viruses, parasites, heavy metals and easily biodegradable compounds. Bank filtration attenuates concentration peaks, providing uniform quality of raw water feed to downstream treatment. The performance of bank filtration can be highly dependent upon several factors, including soil and geological conditions as well as the quality of the source water. Bank filters can become clogged, resulting in pressure drops. Site-specific testing is needed to determine whether the appropriate geology is present as well as the effectiveness and operational parameters. A5.1.5 Aeration Aeration processes are designed to achieve removal of gases and volatile compounds by air stripping. Transfer can usually be achieved using a simple cascade or diffusion of air into water, without the need for elaborate equipment. Stripping of gases or volatile compounds, however, may require a specialized plant that provides a high degree of mass transfer from the liquid phase to the gas phase. Cascade or step aerators are designed so that water flows in a thin film to achieve efficient mass transfer. Cascade aeration may introduce a significant headloss; design requirements are between 1 and 3 m to provide a loading of 10–30 m3/m2·h. Alternatively, compressed air can be diffused through a system of submerged perforated pipes. These types of aerator are used for oxidation and precipitation of iron and manganese. Air stripping can be used for removal of volatile organics (e.g. solvents), some taste- and odour-causing compounds and radon. Aeration processes to achieve air stripping need to be much more elaborate to provide the necessary contact between the air and water. The most common technique is cascade aeration, usually in packed towers in which water is allowed to flow in thin films over plastic media with air blown counter-current. The required tower height and diameter are functions of the volatility and concentration of the compounds to be removed and the flow rate. Increasing the dissolved oxygen content of a water can increase its corrosivity towards some metallic materials used in distribution pipes and plumbing, and this should be taken into account when considering aeration as a treatment process. A5.1.6 Chemical coagulation Chemical coagulation-based treatment is the most common approach for treatment of surface waters and is almost always based on the following unit processes. Chemical coagulants, usually salts of aluminium or iron, are dosed to the raw water under controlled conditions to form a solid flocculent metal hydroxide. Typical coagulant doses are 2–5 mg/l as aluminium or 4–10 mg/l as iron. The precipitated floc removes suspended and dissolved contaminants by mechanisms of charge neutralization, adsorption and entrapment. The efficiency of the coagulation process depends on raw water quality, the coagulant or coagulant aids used and operational factors, including mixing conditions, coagulation dose and pH. The floc is removed from the treated water by subsequent solid–liquid separation processes such as sedimentation or flotation and/or rapid or pressure gravity filtration. Effective operation of the coagulation process depends on selection of the optimum coagulant dose and also the pH value. The required dose and pH can be determined by using small-scale batch coagulation tests, often termed “jar tests”. Increasing doses of coagulant are applied to raw water samples that are stirred and allowed to settle. The optimum dose is selected as that which achieves adequate removal of col-our and turbidity; the optimum pH can be selected in a similar manner. These tests have to be conducted at a sufficient frequency to keep pace with changes in raw water quality and hence coagulant demand. Powdered activated carbon (PAC) may be dosed during coagulation to adsorb organic chemicals, such as some hydrophobic pesticides. The PAC will be removed as an integral fraction of the floc and disposed of with the waterworks sludge. The floc may be removed by sedimentation to reduce the solids loading to the subsequent rapid gravity filters. Sedimentation is most commonly achieved in horizontal flow or floc blanket clarifiers. Alternatively, floc may be removed by dissolved air flotation, in which solids are contacted with fine bubbles of air that attach to the floc, causing them to float to the surface of the tank, where they are removed periodically as a layer of sludge. The treated water from either process is passed to rapid gravity filters (see section A5.1.4), where remaining solids are removed. Filtered water may be passed to a further stage of treatment, such as additional oxidation and filtration (for removal of manganese), ozonation and/or GAC adsorption (for removal of pesticides and other trace organics), prior to final disinfection before the treated water enters the supply. Coagulation is suitable for removal of particulates and bound microorganisms, certain heavy metals and low-solubility organic chemicals, such as certain organochlorine pesticides. For other organic chemicals, coagulation is generally ineffective, except where the chemical is bound to humic material or adsorbed onto particulates. A5.1.7 Activated carbon adsorption Activated carbon is produced by the controlled thermalization of carbonaceous material, normally wood, coal, coconut shells or peat. This activation produces a porous material with a large surface area (500–1500 m2/g) and a high affinity for organic compounds. It is normally used in either powdered (PAC) or granular (GAC) form. When the adsorption capacity of the carbon is exhausted, it can be reactivated by burning off the organics in a controlled manner. However, PAC (and some GAC) is normally used only once before disposal. Different types of activated carbon have different affinities for types of contaminants. The choice between PAC and GAC will depend upon the relative cost-effectiveness, frequency and dose required. PAC would generally be preferred in the case of seasonal or intermittent contamination or where low dosage rates are required. PAC is dosed as a slurry into the water and removed by subsequent treatment processes, together with the waterworks sludge. Its use is therefore restricted to surface water treatment works with existing filters. GAC in fixed-bed adsorbers is used much more efficiently than PAC dosed into the water, and the effective carbon use per water volume treated would be much lower than the dose of PAC required to achieve the same removal. GAC is used for taste and odour control. It is normally used in fixed beds, either in purpose-built adsorbers for chemicals or in existing filter shells by replacement of sand with GAC of a similar particle size. Although at most treatment works it would be cheaper to convert existing filters rather than build separate adsorbers, use of existing filters usually allows only short contact times, and they are not capable of facile reactivation. It is therefore common practice to install additional GAC adsorbers (in some cases preceded by ozonation) between the rapid gravity filters and final disinfection. Most groundwater sources do not have existing filters, and separate adsorbers would need to be installed. The service life of a GAC bed is dependent on the capacity of the carbon used and the contact time between the water and the carbon, the empty bed contact time, controlled by the flow rate of the water. Empty bed contact times are usually in the range 5–30 minutes. GACs vary considerably in their capacity for specific organic compounds, which can have a significant effect upon their service life. A guide to capacity can be obtained from published isotherm data. Carbon capacity is strongly dependent on the water source and is greatly reduced by the presence of background organic compounds. The properties of a chemical that influence its adsorption onto activated carbon include the water solubility and octanol–water partition coefficient. As a general rule, chemicals with low solubilities and high log octanol–water partition coefficients are well adsorbed. Activated carbon is used for the removal of pesticides and other organic chemicals, taste and odour compounds, cyanobacterial toxins and total organic carbon. A5.1.8 Ion exchange Ion exchange is a process in which ions of like charge are exchanged between the water phase and the solid resin phase. Water softening is achieved by cation exchange. Water is passed through a bed of cationic resin, and the calcium ions and magnesium ions in the water are replaced by sodium ions. When the ion exchange resin is exhausted (i.e. the sodium ions are depleted), it is regenerated using a solution of sodium chloride. The process of “dealkalization” can also soften water. Water is passed through a bed of weakly acidic resin, and the calcium and magnesium ions are replaced by hydrogen ions. The hydrogen ions react with the carbonate and bicarbonate ions to produce carbon dioxide. The hardness of the water is thus reduced without any increase in sodium levels. Anion exchange can be used to remove contaminants such as nitrate, fluoride, arsenate and uranium (as the uranyl anion), which are exchanged for chloride. Several appropriate resins are available for this purpose. An ion exchange plant normally consists of two or more resin beds contained in pressure shells with appropriate pumps, pipework and ancillary equipment for regeneration. The pressure shells are typically up to 4 m in diameter, containing 0.6–1.5 m depth of resin. Cation exchange can be used for removal of certain heavy metals. Potential applications of anionic resins, in addition to nitrate removal, are for removal of arsenic and selenium species. A5.1.9 Membrane processes The membrane processes of most significance in water treatment are reverse osmosis, ultrafiltration, microfiltration and nanofiltration. These processes have traditionally been applied to the production of water for industrial or pharmaceutical applications, but are now being applied to the treatment of drinking-water. High‑pressure processes If two solutions are separated by a semipermeable membrane (i.e. a membrane that allows the passage of the solvent but not of the solute), the solvent will naturally pass from the lower-concentration solution to the higher-concentration solution. This process is known as osmosis. It is possible, however, to force the flow of solvent in the opposite direction, from the higher to the lower concentration, by increasing the pressure on the higher-concentration solution. The required pressure differential is known as the osmotic pressure, and the process is known as reverse osmosis. Reverse osmosis results in the production of a treated water stream and a relatively concentrated waste stream. Typical operating pressures are in the range 15–50 bar, depending on the application. Reverse osmosis rejects monovalent ions and organics of molecular weight greater than about 50 daltons (membrane pore sizes are less than 0.002 μm). The most common application of reverse osmosis is desalination of brackish water and seawater. Nanofiltration uses a membrane with properties between those of reverse osmosis and ultrafiltration membranes; pore sizes are typically 0.001–0.01 μm. Nanofiltration membranes allow monovalent ions such as sodium or potassium to pass but reject a high proportion of divalent ions such as calcium and magnesium and some higher molecular weight organics. Operating pressures are typically about 5 bar. Nanofiltration may be effective for the removal of colour-forming organic compounds. Lower‑pressure processes Ultrafiltration is similar in principle to reverse osmosis, but the membranes have much larger pore sizes (typically 0.002–0.03 μm) and operate at lower pressures. Ultrafiltration membranes reject organic molecules of molecular weight above about 800 daltons and usually operate at pressures less than 5 bar. Microfiltration is a direct extension of conventional filtration into the submicrometre range. Microfiltration membranes have pore sizes typically in the range 0.01–12 μm and do not separate molecules but reject colloidal and suspended material at operating pressures of 1–2 bar. Microfiltration is capable of sieving out particles greater than 0.05 μm. It has been used for water treatment in combination with coagulation or PAC to remove particulates and some dissolved organic carbon prior to reverse osmosis membranes and to improve permeate flux. A5.1.10 Other treatment processes Processes aimed at generating hydroxyl radicals are known collectively as advanced oxidation processes and can be effective for the destruction of chemicals that are difficult to treat using other methods, such as ozone alone. Hydrogen peroxide with UV is also a source of hydroxyl radicals. Chemicals can react either directly with molecular ozone or with the hydroxyl radical (HO·), which is a product of the decomposition of ozone in water and is an exceedingly powerful indiscriminate oxidant that reacts readily with a wide range of organic chemicals. The formation of hydroxyl radicals can be encouraged by using ozone at high pH. One advanced oxidation process using ozone or UV plus hydrogen peroxide involves dosing hydrogen peroxide simultaneously with ozone at a rate of approximately 0.4 mg of hydrogen peroxide per litre per milligram of ozone dosed per litre (the theoretical optimum ratio for hydroxyl radical production) and bicarbonate. Other treatment processes that can be used in certain applications include: • precipitation softening (addition of lime, lime plus sodium carbonate or sodium hydroxide to precipitate hardness at high pH); • ion exchange softening; • biological denitrification for removal of nitrate from surface waters; • biological nitrification for removal of ammonia from surface waters; • activated alumina (or other adsorbents) for specialized applications, such as removal of fluoride and arsenic. A5.2 Treatment performance for chemicals for which guideline values have been established Treatment performance for chemicals for which guideline values have been established is given in Tables A5.1–A5.5. A5.3 Corrosion of metals used in water treatment and distribution A5.3.1 Brass The main corrosion problem with brasses is dezincification, which is the selective dissolution of zinc from duplex brass, leaving behind copper as a porous mass of low mechanical strength. Meringue dezincification, in which a voluminous corrosion product of basic zinc carbonate forms on the brass surface, largely depends on the ratio of chloride to alkalinity. Meringue dezincification can be controlled by maintaining a low zinc to copper ratio (1:3 or lower) and by keeping pH below 8.3. General dissolution of brass can also occur, releasing metals, including lead, into the water. Impingement attack can occur under conditions of high water velocity with waters that form poorly protective corrosion product layers and that contain large amounts of dissolved or entrained air. A5.3.2 Concrete and cement Concrete is a composite material consisting of a cement binder in which an inert aggregate is embedded. Cement is primarily a mixture of calcium silicates and aluminates together with some free lime. Cement mortar, in which the aggregate is fine sand, is used as a protective lining in iron and steel water pipes. In asbestos–cement pipe, the aggregate is asbestos fibres, which are not of concern in drinking-water (see also asbestos fact sheet in chapter 12). Cement is subject to deterioration on prolonged exposure to aggressive water, due either to the dissolution of lime and other soluble compounds Table A5.1 Treatment performance for naturally occurring chemicals for which guideline values have been establisheda,b Coagulation Ion exchange Precipitation Activated Activated Ozonation Adsorption/ Membranes softening alumina carbon oxidation Arsenicc ++ +++ ++ +++ +++d <0.005 <0.005 <0.005 <0.005 <0.005 Fluoride ++ +++ +++ <1 <1 Manganese ++e +++ ++f +++g +++h <0.05 <0.02 <0.05 <0.02 <0.01 Selenium ++ +++ +++ +++ <0.01 <0.01 <0.01 Uranium ++ +++ ++ +++ <0.001 <0.001 a Symbols are as follows: ++ Approximately 50% or more removal +++ Approximately 80% or more removal b The table includes chemicals for which some treatment data are available. A blank entry in the table indicates either that the process is completely ineffective or that there are no data on the effectiveness of the process. For the most effective processes, the table estimates the concentration of the chemical (in mg/l) that could be achievable in an ideal water. Iron oxide–based and iron hydroxide–based media have been shown to be very effective for both arsenate and arsenite forms. d Reverse osmosis membranes are more effective for removal of arsenate than arsenite. However, arsenite is readily oxidized to arsenate by disinfectants (e.g. chlorine). e Dissolved manganese(II) can be removed through cation exchange in zeolite softening processes. f Oxidation of manganese using ozone must be followed by a filtration step. g Includes manganese greensand and other filter media coated with manganese oxides (such as conventional filtration media with free chlorine applied across the filter bed). h Low pressure membrane filtration preceded by oxidation using potassium permanganate. Table A5.2 Treatment performance for chemicals from industrial sources and human dwellings for which guideline values have been establisheda,b UV Air Ion Precipitation Activated Advanced Biological irradiastripping Coagulation exchange softening carbon Ozonation oxidation Membranes treatmentc tion Cadmium +++ +++ +++ +++ <0.002 <0.002 <0.002 <0.002 Mercury +++ +++ +++ +++ <0.0001 <0.0001 <0.0001 <0.0001 Benzene +++ +++ <0.01 <0.01 Carbon tetrachloride +++ +++ <0.001 <0.001 1,2‑Dichlorobenzene +++ +++ <0.01 <0.01 1,4‑Dichlorobenzene +++ +++ <0.01 <0.01 1,2‑Dichloroethane +++ +++ <0.01 1,2‑Dichloroethene +++ +++ <0.01 <0.01 1,4‑Dioxane + Edetic acid +++ <0.01 Ethylbenzene ++ + +++ <0.001 <0.001 Hexachlorobutadiene +++ <0.001 +++ <0.01 Yesd + +++ Yesd <0.01 +++ Yesd <0.01 + +++ <0.01 +++ 0.05 +++++ + ++ <0.001 + Table A5.2 (continued) UV Air Ion Precipitation Activated Advanced Biological irradiastripping Coagulation exchange softening carbon Ozonation oxidation Membranes treatmentc tion Nitrilotriacetic acid ++ ++ N‑Nitrosodimethylamine + ++ + Pentachlorophenol +++ <0.0004 ++ Perchlorate yesd yesd yesd Styrene +++ <0.02 +++ <0.002 ++ + + Tetrachloroethene +++ <0.001 +++ <0.001 + Toluene +++ <0.001 +++ <0.001 +++ <0.001 +++e <0.001 ++ <0.001 Trichloroethene +++ <0.02 +++ <0.02 +++ <0.02 +++e <0.02 Xylenes +++ <0.005 +++ <0.005 +++e <0.005 ++ a Symbols are as follows: + Limited removal ++ Approximately 50% or more removal +++ Approximately 80% or more removal b The table includes only those chemicals for which some treatment data are available. A blank entry in the table indicates either that the process is completely ineffective or that there are no data on the effectiveness of the process. For the most effective processes, where data are available, the table indicates the concentration of the chemical (in mg/l) that should be achievable. Biological treatment includes slow sand filtration and bank filtration. d Yes means known or likely to be effective, but performance was not quantified. e Might be effective, but other techniques would be more likely to be applied due to cost. Table A5.3 Treatment performance for chemicals from agricultural activities for which guideline values have been establisheda,b Ion Activated Advanced Biological Chlorination Air stripping Coagulation exchange carbon Ozonation oxidation Membranes treatmentc Nitrate +++ +++ +++ <5 <5 <5 Nitrite +++ + +++ <0.1 Alachlor +++ <0.001 Aldicarb +++ <0.001 Aldrin/dieldrin + +++ <0.000 02 Atrazine and its chloro‑striazine metabolites + +++ <0.0001 Carbofuran + +++ <0.001 Chlordane +++ <0.0001 Chlorotoluron +++ <0.0001 Cyanazine +++ <0.0001 2,4‑D +++ <0.001 1,2‑Dibromo‑3‑chloropropane ++ <0.001 +++ <0.0001 ++ +++ <0.001 +++ <0.001 +++ <0.001 +++ <0.001 ++ <0.000 02 +++ <0.00002 Yesd +++ <0.0001 +++ <0.0001 +++e <0.0001 Yesd +++ <0.001 ++ <0.0001 Yesd +++ <0.0001 + +++ <0.0001 +++ <0.001 Yesd Table A5.3 (continued) Ion Activated Advanced Biological Chlorination Air stripping Coagulation exchange carbon Ozonation oxidation Membranes treatmentc 1,2‑Dibromoethane +++ +++ <0.0001 <0.0001 1,2‑Dichloropropane Yes +++ <0.001 Dimethoate +++ ++ <0.001 Endrin + +++ <0.0002 Hydroxyatrazine Isoproturon ++ +++ <0.0001 Lindane +++ <0.0001 Mecoprop +++ <0.0001 Methoxychlor ++ +++ <0.0001 Metalochlor +++ <0.0001 Simazine +++ <0.0001 2,4,5‑T +++ <0.001 + ++ Yesd +++ <0.0001 ++ +++ <0.001 +++ <0.0001 Yesd +++ <0.0001 Yesd + ++ +++ <0.0001 +++ <0.0001 ++ Yesd Yesd +++ <0.0001 ++ ++ +++ <0.0001 +++ <0.0001 Yesd Table A5.3 (continued) Ion Activated Advanced Biological Chlorination Air stripping Coagulation exchange carbon Ozonation oxidation Membranes treatmentc Terbuthylazine + +++ <0.0001 ++ Trifluralin +++ <0.0001 +++f <0.0001 a Symbols are as follows: + Limited removal ++ Approximately 50% or more removal +++ Approximately 80% or more removal b The table includes only those chemicals for which some treatment data are available. A blank entry in the table indicates either that the process is completely ineffective or that there are no data on the effectiveness of the process. For the most effective processes, the table indicates the concentration of the chemical (in mg/l) that should be achievable. Biological treatment includes slow sand filtration, bank filtration and biological denitrification (for nitrate removal). d Yes means known or likely to be effective, but performance was not quantified. e For bank filtration; slow sand filtration is not effective. f Might be effective, but other techniques would be more likely to be applied due to cost. Table A5.4Treatment performance for pesticides used in water for public health for which guideline values have been establisheda,b Chlorination Coagulation Activated carbon Ozonation Advanced oxidation Membranes DDT and metabolites + +++ + +++c +++c <0.0001 <0.0001 <0.0001 a Symbols are as follows: + Limited removal +++ Approximately 80% or more removal b For the most effective processes, the table indicates the concentration of the chemical (in mg/l) that should be achievable. Might be effective, but other techniques would be more likely to be applied due to cost. Table A5.5 Treatment performance for cyanobacterial cells and cyanotoxins for which guideline values or a reference value have been establisheda,b,c Chlorination Coagulation Activated carbon Ozonation Advanced oxidation Membranes Sediment passaged Cyanobacterial cells +++ +++ +++ Cyanotoxins +++e +++ +++ +++ +++f +++ a Chlorination, ozonation or advanced oxidation may release cyanotoxins. b +++ = 80% or more removal. The table includes only those chemicals for which some treatment data are available. A blank entry in the table indicates either that the process is completely ineffective or that there are no data on the effectiveness of the process. d Sediment passage includes slow sand filtration and bank filtration. Note that percolation through soils, such as during artificial groundwater recharge, can also be effective in removing cyanobacteria and cyanotoxins. e Not effective for anatoxins. f May be effective, depending on pore size and toxin. or to chemical attack by aggressive ions such as chloride or sulfate, and this may result in structural failure. Newly installed cement materials will leach lime, with consequent increases in pH, alkalinity and hardness. Cement contains a variety of metals that can be leached into the water. Aggressiveness to cement is related to the “aggressivity index”, which has been used specifically to assess the potential for the dissolution of concrete. A pH of 8.5 or higher may be necessary to control cement corrosion. A5.3.3 Copper The corrosion of copper pipework and hot water cylinders can cause blue water, blue or green staining of bathroom fittings and, occasionally, taste problems. Copper tubing may be subject to general corrosion, impingement attack and pitting corrosion. General corrosion is most often associated with soft, acidic waters; waters with pH below 6.5 and hardness of less than 60 mg of calcium carbonate per litre are very aggressive to copper. Copper, like lead, can enter water by dissolution of the corrosion product, basic copper carbonate. The solubility is mainly a function of pH and total inorganic carbon. Solubility decreases with increase in pH, but increases with increase in concentrations of carbonate species. Raising the pH to between 8 and 8.5 is the usual procedure to overcome these difficulties. Impingement attack is the result of excessive flow velocities and is aggravated in soft water at high temperature and low pH. The pitting of copper is commonly associated with hard groundwaters having a carbon dioxide concentration above 5 mg/l and high dissolved oxygen. Phosphates have been used to suppress copper corrosion in those cases. Surface waters with organic colour may also be associated with pitting corrosion. Copper pipes can fail by pitting corrosion, which involves highly localized attacks leading to perforations with negligible loss of metal. Two main types of attack are recognized. Type I pitting affects cold water systems (below 40 °C) and is associated, particularly, with hard borehole waters and the presence of a carbon film in the bore of the pipe, derived from the manufacturing process. Tubes that have had the carbon removed by cleaning are immune from Type I pitting. Type II pitting occurs in hot water systems (above 60 °C) and is associated with soft waters. A high proportion of general and pitting corrosion problems are associated with new pipe in which a protective oxide layer has not yet formed. Calcium carbonate precipitation indices such as Langelier and Ryznar are not good predictors of corrosion for copper systems. A5.3.4 Iron Iron (either cast or ductile) is frequently used in water distribution systems, and its corrosion is of concern. While structural failure as a result of iron corrosion is rare, water quality problems (e.g. “red water”) can arise as a result of excessive corrosion of iron pipes. The corrosion of iron is a complex process that involves the oxidation of the metal, normally by dissolved oxygen, ultimately to form a precipitate of iron(III). This leads to the formation of tubercules on the pipe surface. The major water quality factors that determine whether the precipitate forms a protective scale are pH and alkalinity. The concentrations of calcium, chloride and sulfate also influence iron corrosion. Successful control of iron corrosion has been achieved by adjusting the pH to the range 6.8–7.3, hardness and alkalinity to at least 40 mg/l (as calcium carbonate), oversaturation with calcium carbonate of 4–10 mg/l and a ratio of alkalinity to chloride plus sulfate of at least 5 (when both are expressed as calcium carbonate). Silicates and polyphosphates are often described as “corrosion inhibitors”, but there is no guarantee that they will inhibit corrosion in water distribution systems. However, they can complex dissolved iron (in the iron(II) state) and prevent its precipitation as visibly obvious red “rust”. These compounds may act by masking the effects of corrosion rather than by preventing it. Orthophosphate is a possible corrosion inhibitor and, like polyphosphates, is used to prevent “red water”. A5.3.5 Lead Lead corrosion (plumbosolvency) is of particular concern. Lead piping is still common in old houses in some countries, lead solders have been used widely for jointing copper tubing and brass fittings can contain substantial amounts of lead. Galvanized iron pipe plumbing can accumulate incoming lead and release it at a later time as particulates. The solubility of lead is governed by the formation of lead carbonates as pipe deposits. Wherever practicable, lead pipework should be replaced. Lead can also leach from lead-based solders and brass and bronze fittings. The solubility of corrosion-related lead salts increases markedly as the pH increases above or decreases below 8.3 because of the substantial decrease in the equilibrium carbonate concentration. Thus, plumbosolvency tends to be at a maximum in waters with a low pH and low alkalinity, and a useful interim control procedure, pending pipe replacement, is to increase the pH to 8.0–8.5 after chlorination prior to distribution. Orthophosphate and other phosphates are effective in suppressing dissolution of lead. Lead concentrations increase with increasing standing time of water in lead pipe. Flushing the pipework before drawing water for consumption can be used as an interim measure to reduce exposure to lead. Showering, bathing and flushing the toilet can be used to flush out the system. Lead can corrode more rapidly when it is coupled to copper. The rate of such galvanic corrosion is faster than that of simple oxidative corrosion, and lead concentrations are not limited by the solubility of the corrosion products. The rate of galvanic corrosion is affected principally by chloride concentration. Galvanic corrosion is less easily controlled but can be reduced by dosing zinc in conjunction with orthophosphate and by adjustment of pH. Treatment to reduce plumbosolvency usually involves pH adjustment. When the water is very soft (calcium carbonate concentration less than 50 mg/l), the optimum pH is about 8.0–8.5. Alternatively, dosing with orthophosphoric acid or sodium orthophosphate might be more effective, particularly when plumbosolvency occurs in non-acidic waters. Calcium carbonate precipitation indices such as Langelier and Ryznar are not considered to be necessarily good predictors of corrosion for lead. A5.3.6 Nickel Nickel in water may arise due to leaching from nickel- or chromium-plated taps as well as from stainless steel pipes and fittings. Nickel leaching is particularly influenced by stagnation time. There is some evidence that corrosion control measures (e.g. pH control) may reduce leaching of nickel from stainless steel materials. Product certification is the most important means of control. Consumers, particularly nickel-sensitive people, should flush nickel- or chromium-plated taps before using the water, particularly after periods of stagnation. A5.3.7 Zinc Galvanized pipes will release zinc (from the galvanizing layer) and can also leach cadmium and lead. Corrosion can be a particular problem where galvanized steel or iron piping is connected to dissimilar materials, such as brass, in taps and fittings. The solubility of zinc in water is a function of pH and total inorganic carbon concentrations; the solubility of basic zinc carbonate decreases with increase in pH and concentrations of carbonate species. For low-alkalinity waters, an increase of pH to 8.5 should be sufficient to control the dissolution of zinc. With galvanized iron, the zinc layer initially protects the steel by corroding preferentially. In the long term, a protective deposit of basic zinc carbonate forms; however, galvanized pipe is also prone to uncontrolled deposition and clogging. Recent findings have shown that lead can accumulate on galvanized pipe particulates and become resuspended by physical disruption, such as water hammer. Protective deposits do not form in soft waters where the alkalinity is less than 50 mg/l as calcium carbonate or waters containing high carbon dioxide concentrations (> 25 mg/l), and galvanized steel is unsuitable for these waters. Electrolytic corrosion can occur where galvanized steel or iron pipes or fittings are connected with copper tube or brass fittings. ANNEX 6 Supporting information on radionuclides A6.1 Guidance levels for radionuclides in drinking-water Table A6.1 Guidance levels for radionuclides in drinking-water Radio-Guidance Radio-Guidance Radio-Guidance Radio-Guidance nuclide level (Bq/l)a nuclide level (Bq/l)a nuclide level (Bq/l)a nuclide level (Bq/l)a 3H 10 000 71Ge 10 000 105Rh 1 000 129Cs 1 000 7Be 10 000 73As 1 000 103Pd 1 000 131Cs 1 000 14C 100 74As 100 105Ag 100 132Cs 100 22Na 100 76As 100 110mAg 100 134Cs 10 32P 100 77As 1 000 111Ag 100 135Cs 100 33P 1 000 75Se 100 109Cd 100 136Cs 100 35S 100 82Br 100 115Cd 100 137Cs 10 36Cl 100 86Rb 100 115mCd 100 131Ba 1 000 45Ca 100 85Sr 100 111In 1 000 140Ba 100 47Ca 100 89Sr 100 114mIn 100 140La 100 46Sc 100 90Sr 10 113Sn 100 139Ce 1 000 47Sc 100 90Y 100 125Sn 100 141Ce 100 48Sc 100 91Y 100 122Sb 100 143Ce 100 48V 100 93Zr 100 124Sb 100 144Ce 10 51Cr 10 000 95Zr 100 125Sb 100 143Pr 100 52Mn 100 93mNb 1 000 123mTe 100 147Nd 100 53Mn 10 000 94Nb 100 127Te 1 000 147Pm 1 000 54Mn 100 95Nb 100 127mTe 100 149Pm 100 55Fe 1 000 93Mo 100 129Te 1 000 151Sm 1 000 59Fe 100 99Mo 100 129mTe 100 153Sm 100 56Co 100 96Tc 100 131Te 1 000 152Eu 100 57Co 1 000 97Tc 1 000 131mTe 100 154Eu 100 58Co 100 97mTc 100 132Te 100 155Eu 1 000 60Co 100 99Tc 100 125I 10 153Gd 1 000 59Ni 1 000 97Ru 1 000 126I 10 160Tb 100 63Ni 1 000 103Ru 100 129I 1 169Er 1 000 65Zn 100 106Ru 10 131I 10 171Tm 1 000 Table A6.1 (continued) Radio-Guidance Radio-Guidance Radio-Guidance Radio-Guidance nuclide level (Bq/l)a nuclide level (Bq/l)a nuclide level (Bq/l)a nuclide level (Bq/l)a 175Yb 210Pbb 231U 243Am1 000 0.1 1 000 1 182Ta 206Bi 232U 242Cm100 100 110 181W 207Bi 233U 243Cm1 000100 1 1 185W 210Bib 234Ub 244Cm1 000100 1 1 186Re 210Pob 235Ub 245Cm100 0.1 11 185Os 223Rab 236Ub 246Cm100 1 11 191Os 224Rab 237U 247Cm100 1100 1 193Os 225Ra 238Ub,c 248Cm100 1 100.1 190Ir 226Rab 237Np 249Bk100 1 1100 192Ir 228Rab 239Np 246Cf100 0.1 100 100 191Pt 227Thb 236Pu 248Cf1 000 10 110 193mPt 228Thb 237Pu 249Cf1 000 1 1 000 1 198Au 229Th 238Pu 250Cf100 0.1 11 199Au 230Thb 239Pu 251Cf1 000 1 11 197Hg 231Thb 240Pu 252Cf1 000 1 000 1 1 203Hg 232Thb 241Pu 253Cf100 1 10100 200Tl 234Thb 242Pu 254Cf1 000100 1 1 201Tl 230Pa 244Pu 253Es1 000 100 1 10 202Tl 231Pab 241Am 254Es1 000 0.1 1 10 204Tl 233Pa 242Am 254mEs100 100 1 000 100 203Pb 230U 242mAm1 000 1 1 a Guidance levels were rounded to the nearest order of magnitude by averaging the log scale values (to 10n if the calculated value was below 3 × 10n and to 10n+1 if the value was 3 × 10n or above). For example, if the calculated value was 2 Bq/L (i.e. 2 × 100), the guidance level was rounded to 100 (i.e. = 1) whereas, if the calculated value was 3 Bq /L (i.e. 3 × 100 or above), the guidance level was rounded to 101 (i.e. = 10). b Natural radionuclides. The provisional guideline value for uranium in drinking‑water is 30 μg/l based on its chemical toxicity for the kidney (see section 8.5). A6.2 References for further information about radionuclides ICRP (1989) Individual monitoring for intakes of radionuclides by workers. ICRP Publication 54. Annals of the ICRP, 19(1–3). ICRP (2006) Human alimentary tract model for radiological protection. ICRP Publication 100. Annals of the ICRP, 36(2). ICRP (2008) Nuclear decay data for dosimetric calculations. ICRP Publication 107. Annals of the ICRP, Volume 38(3). A6.3 References for further information about analytical methods and treatment technologies for radionuclides Annanmäki M, ed. (2000) Treatment techniques for removing natural radionuclides from drinking water. Final report of the TENAWA Project. Helsinki, Radiation and Nuclear Safety Authority (STUK-A169). APHA, AWWA, WEF (2005) Standard methods for the examination of water and wastewater, 21st ed. Washington, DC, American Public Health Association, American Water Works Association and Water Environment Federation, pp. 7–15. ASTM (1998) ASTM annual book of standards. Vol. 11.02. Philadelphia, PA, American Society for Testing and Materials. Bring R, Miller AG (1992) Direct detection of trace levels of uranium by laser induced kinetic phosphor¬imetry. Analytical Chemistry, 64:1413–1418. Chiu NW, Dean JR (1986) Radioanalytical methods manual. Ottawa, Ontario, Canadian Government Publishing Centre, Canadian Centre for Mineral and Energy Technology, National Uranium Tailings Program (CANMET Report 78-22). Crawford-Brown DJ (1989) The biokinetics and dosimetry of radon-222 in the human body following ingestion of groundwater. Environmental Geochemistry and Health, 11:10–17. Department of National Health and Welfare (1977) Chemical procedures for the determination of 89Sr, 90Sr, and 137Cs in surface waters, fresh-water algae and fresh-water fish. Ottawa, Ontario, Department of National Health and Welfare (Report 77-EHD-14). Health Canada (2000) Environmental radioactivity in Canada 1989–1996. Available from Environmental Radiation Hazards Division, Radiation Protection Bureau, Health Canada, Ottawa, Ontario [see also earlier editions of Environmental radioactivity in Canada]. Health Canada (2004) Point-of-use and point-of-entry treatment technologies for the removal of lead-210 and uranium from drinking water. Richmond Hill, Ontario, Senes Consultants Ltd. Igarashi Y, Kawamura H, Shiraishi K (1989) Determination of thorium and uranium in biological samples by inductively coupled plasma mass spectrometry using internal standardization. Journal of Analytical Atomic Spectrometry, 4:571–576. ISO (2003) Standard ISO 5667 3: Water quality—Sampling—Part 3: Guidance on the preservation and handling of water samples. Geneva, International Organization for Standardization. ISO (2006) Standard ISO 5667 1: Water quality—Sampling—Part 1: Guidance on the design of sampling programmes and sampling techniques. Geneva, International Organization for Standardization. ISO (2006) Standard ISO 5667-5: Water quality—Sampling—Part 5: Guidance on sampling of drinking water from treatment works and piped distribution systems. Geneva, International Organization for Standardization. ISO (2007) Standard ISO 9696: Water quality—Measurement of gross alpha activity in non-saline water—Thick source method. Geneva, International Organization for Standardization. ISO (2007) Standard ISO 10703: Water quality—Determination of the activity concentration of radionuclides—Method by high resolution gamma-ray spectrometry. Geneva, International Organization for Standardization. ISO (2008) Standard ISO 9697: Water quality—Measurement of gross beta activity in non-saline water—Thick source method. Geneva, International Organization for Standardization. ISO (2009) Standard ISO 5667-11: Water quality—Sampling—Part 11: Guidance on sampling of groundwaters. Geneva, International Organization for Standardization. ISO (2009) Standard ISO 10704: Water quality—Measurement of gross alpha and gross beta activity in non-saline water—Thin source deposit method. Geneva, International Organization for Standardization. ISO (2010) ISO 969: Water quality—Determination of tritium activity concentration—Liquid scintillation counting method. Geneva, International Organization for Standardization. ISO (2010) Standard ISO 11704: Water quality—Measurement of gross alpha and beta activity concentration in non-saline water—Liquid scintillation counting method. Geneva, International Organization for Standardization. ISO (in preparation) Standard ISO 13160: Water quality—Measurement of strontium 90 and strontium 89. Geneva, International Organization for Standardization. ISO (in preparation) Standard ISO 13161: Water quality—Measurement of polonium 210 activity concentration in water by alpha spectrometry. Geneva, International Organization for Standardization. ISO (in preparation) Standard ISO 13162: Water quality—Determination of carbon 14 activity— Liquid scintillation counting method. Geneva, International Organization for Standardization. ISO (in preparation) Standard ISO 13163-1: Water quality—Measurement of lead 210 activity concentration—Part 1: Liquid scintillation counting method. Geneva, International Organization for Standardization. ISO (in preparation) Standard ISO 13164-1: Water quality—Measurement of the activity concentration of radon-222 and its short-lived decay products—Part 1: Radon origins and measurement methods. Geneva, International Organization for Standardization. ISO (in preparation) Standard ISO 13164-2: Water quality—Measurement of the activity concentration of radon-222 and its short-lived decay products—Part 2: Direct measurement by gamma spectrometry. Geneva, International Organization for Standardization. ISO (in preparation) Standard ISO 13164-3: Water quality—Measurement of the activity concentration of radon-222 and its short-lived decay products—Part 3: Indirect measurement with degassing. Geneva, International Organization for Standardization. ISO (in preparation) Standard ISO 13165-1: Water quality—Measurement of radium 226 activity concentration—Part 1: Liquid scintillation counting method. Geneva, International Organization for Standardization. Lariviere D et al. (2009) Rapid and automated analytical technologies for radiological nuclear emergency preparedness. In: Koskinen AN, ed. Nuclear chemistry: New research. Nova Science Publishers, Inc., pp. 99–154. NSF International (2005) Contaminant guide. Ann Arbor, MI, NSF International (http://www. nsf.org/consumer/drinking_water/dw_contaminant_guide.asp?program=WaterTre). NSF International (2005) Contaminant testing protocols. Ann Arbor, MI, NSF International (http://www.nsf.org/consumer/drinking_water/dw_contaminant_protocols.asp). Prichard HM, Gesell TF (1977) Rapid measurements of 222Rn concentrations in water with a commercial liquid scintillations counter. Health Physics, 33:577–581. Prichard HM, Venso EA, Dodson CL (1991) Liquid scintillation analysis of 222Rn in water by alpha/beta discrimination. Radioactivity and Radiochemistry, 3:28–26. USEPA (1980) Prescribed procedures for measurement of radioactivity in drinking water. Washington, DC, United States Environmental Protection Agency (EPA 600/4-80-032). USEPA (1987) Two test procedures for radon in drinking water. Appendix D. Analytical test procedure. Washington, DC, United States Environmental Protection Agency, p. 22 (EPA/600/287/082). USEPA (1999) National primary drinking water regulations; radon-222. Washington, DC, United States Environmental Protection Agency. Federal Register, 64(211). USEPA (2000) National primary drinking water regulations; radionuclides; final rule. Washington, DC, United States Environmental Protection Agency (40 Code of Federal Regulations Parts 9, 141 and 142). USEPA (2000) Radionuclides notice of data availability technical support document. Prepared by Office of Groundwater and Drinking Water, United States Environmental Protection Agency, in collaboration with Office of Indoor Air and Radiation, USEPA, and United States Geological Survey. USEPA (2008) Approved methods for radionuclides. Washington, DC, United States Environmental Protection Agency (http://www.epa.gov/ogwdw/methods/pdfs/methods/methods_ radionuclides.pdf). Vitz E (1991) Toward a standard method for determining waterborne radon. Health Physics, 60:817–829. Volchok HL, de Planque G, eds (1983) EML procedures manual, 26th ed. New York, NY, United States Department of Energy, Environmental Measurements Laboratory (HASL-300). WHO (2002) Establishing a dialogue on risks from electromagnetic fields. Geneva, World Health Organization. WHO (2009) WHO handbook on indoor radon: A public health perspective. Geneva, World Health Organization. ANNEX 7 Contributors to the development of the Guidelines for drinking-water quality: fourth edition incorporating the first and second addenda TThis annex lists the names of those who have contributed to the development of the fourth edition of the Guidelines for drinking-water quality and to the first and second addenda to the fourth edition, through participation at relevant meetings, authorship or peer review of text in the Guidelines themselves or its supporting documents, or through provision of intellectual advice. The list of contributors begins with the first meeting at which the fourth edition was discussed, held in Berlin, Germany, in 2007. All those who contributed to the third edition of the Guidelines as well as the first and second addenda to the third edition, which constitute a major portion of this fourth edition, are listed in Annex 2 of the third edition incorporating the first and second addenda, available on the WHO web site at: https://www.who.int/ publications/i/item/9789241547611. Sincere apologies are extended to any contributors whose names have inadvertently been omitted from these lists. C. Abbot, United Utilities, United Kingdom H. Abouzaid, WHO, Egypt L. Achene, Istituto Superiore di Sanità, Italy J. Adams, Liverpool School of Tropical Medicine, United Kingdom E. Addai, United Nations Children’s Fund, Senegal A. Adin, Hebrew University of Jerusalem, Israel S. Adrian, Environmental Protection Agency, USA R. Aertgeerts, formerly WHO Regional Office for Europe, Germany P. Aggett, Emeritus University of Central Lancashire, United Kingdom E. Agus, East Bay Municipal Utility District, USA A. Ahmed, WHO, Bangladesh F. Ahmed, Bangladesh University of Engineering and Technology, Bangladesh K.M. Ahmed, University of Dhaka, Bangladesh A. Aitio, WHO, Switzerland H. Al-Hasni, Public Authority for Electricity and Water, Oman 563 N. Al-Hmoud, Princess Sumaya University for Technology, Jordan L. Ali, WHO, Maldives T. Alimamedova, WHO, Tajikistan D. Allély-Fermé, WHO, Switzerland G. Allgood, The Proctor & Gamble Company, USA B.M. Altura, New York Downstate Medical Center, USA B.T. Altura, New York Downstate Medical Center, USA L. Alves Campos, National Sanitary Control Agency (ANVISA), Brazil S. Al-Wahaibi, Ministry of Health, Oman C. Alzamora, International Manganese Institute, France M. Amazonas, The Coca-Cola Company, USA R. Anderson, WHO, Switzerland E. Andrés, Ministerio de Vivienda, Ordenamiento Territorial y Medio Ambiente, Uruguay K.B. Andrus, Air Transport Association of America, Inc., USA R.W. Angelotti, Upper Occoquan Service Authority, USA S. Appleyard, Department of Environment Regulation of Western Australia, Australia G. Ardon, Ministry of Housing, Spatial Planning and Environment, the Netherlands F. Arellano, Maynilad Water, Philippines T. Ariyananda, Lanka Rain Water Harvesting Forum, Sri Lanka Y. Asada, National Institute of Public Health, Japan M. Asami, National Institute of Public Health, Japan N. Ashbolt, School of Public Health, University of Alberta, Canada S. Atkinson, McMaster University, Canada S.M.F.O. Azevedo, Universidade Federal do Rio de Janeiro, Brazil B. Baer, WHO Regional Office for the Western Pacific, Philippines D. Baguma, African Rural University, Uganda C.D. Baker, Centre for Affordable Water and Sanitation Technology (CAWST), Canada H. Bakir, WHO Regional Centre for Environmental Health Activities, Jordan A. Ballot, Norwegian Institute for Water Research, Norway E. Barrenberg, formerly WHO, Switzerland P.R.G. Barrocas, National School of Public Health, Brazil L. Barrott, MWH, United Kingdom J. Barrow, Centers for Disease Control and Prevention, USA J. Bartram, formerly University of North Carolina, USA R. Bastian, Environmental Protection Agency, USA R. Bastos, Universidade Federal de Vicosa, Brazil H.K. Bates, Research Association, USA G. Bateman, Environment Agency, United Kingdom A. Bathija, Environmental Protection Agency, USA J. Baumgartner, University of Wisconsin, USA A.S. Baweja, Health Canada, Canada P. A. Bawono, Ministry of Health, Indonesia M.R. Bayer, Environmental Protection Agency, USA D. Bennitz, Health Canada, Canada M.J. Benoliel. Empresa Portuguesa das Águas Livres, SA (EPAL), Portugal M.W. Berg, Texas Commission on Environmental Quality, USA M. Berglund, Karolinska Institute of Environmental Medicine, Sweden C. Bernard, Museum National d’Histoire Naturelle, France R. Bernardi, Ministerio de Vivienda, Ordenamiento Territorial y Medio Ambiente, Uruguay R.J. Bevan, Independent Consultant (formerly Cranfield University), United Kingdom J. Bhagwan, Water Research Commission, South Africa S. Bickel, United Nations Children’s Fund, USA V. Bhat, formerly NSF International, USA A. Bhushan, WHO Regional Office for South-East Asia, India C. Bianchessi, formerly WHO, Switzerland K. Billington, Natural Logic, Australia S. Bish, United Nations Children’s Fund, USA L. Bláha, Masaryk University, Czechia E. Blanton, PATH, USA M. Blokker, Kiwa Water Research, the Netherlands F. Bochicchio, National Center for Radiation Protection and Computational Physics, Italy C.P. Boepple, Upper Occoquan Service Authority, USA S. Boisson, WHO, Switzerland V.L. Bombeta, Local Water Utilities Administration (LWUA), Philippines L. Bonadonna, Istituto Superiore di Sanità, Italy A. Boobis, Imperial College London, United Kingdom M. Bormans, Université de Rennes, France R. Bos, WHO, Switzerland (currently International Water Association, the Netherlands) J. Bourdon-Lacombe, Health Canada, Canada A.F. Bouwman, PBL Netherlands Environmental Assessment Agency, the Netherlands M. Bowman, Water Corporation, USA J. Bradley, Public Health England, United Kingdom C. Brandão, University of Brasilia, Brazil P. Brandhuber, Brandhuber Water Quality & Treatment, USA B. Breach, Independent Consultant, United Kingdom B. Brena, Universidad de la República de Uruguay, Uruguay E. Briand, Ministère du Travail, de l’Emploi et de la Santé, France R.B. Brobst, Environmental Protection Agency, USA J.D. Brookes, The University of Adelaide, Australia T. Brooks, Health Canada, Canada J. Brown, Georgia Institute of Technology, USA (formerly London School of Hygiene & Tropical Medicine, United Kingdom) J. Brown, International Atomic Energy Agency, Austria (formerly Independent Consultant, United Kingdom) R. Brown, WHO, Switzerland C. Browne, Ministry of Health, Barbados G. Brundrett, Brundrett Associates, United Kingdom T. Bruursema, NSF International, USA M. Burch, SA Water, Australia J. Burgess, Water Research Commission, South Africa P. Byleveld, New South Wales Department of Health, Australia E. Calderon, Sanitary Engineering Institute, Argentina M.E. Calderon, Peru R. Calderon, Environmental Protection Agency, USA P. Callan, Independent Consultant, Australia D. Calmet, International Organization for Standardization and Nuclear Advisor of the Permanent Mission of France at United Nations, Austria D. Campbell-Lendrum, WHO, Switzerland E. Carden, Department of Health and Human Services, Australia W.W. Carmichael, Wright State University, USA Z. Carr, WHO, Switzerland N. Carrard, ISF-UTS, Australia R. Carrier, Health Canada, Canada V. Casey, WaterAid, United Kingdom C. Castell-Exner, German Technical and Scientific Association for Gas and Water, Germany D. Chad, ToxStrategies, USA C Chaffey, Health Canada, Canada R. Chalmers, Public Health Wales Microbiology, United Kingdom F. Charalambous, Australian Radiation Protection and Nuclear Safety Agency, Australia K. Charles, University of Oxford, United Kingdom P. Charles, Centre International de Recherche Sur l’eau et l’Environnement – Suez Environnement, France R. Charron, Health Canada, Canada Y. Chartier, formerly WHO, Switzerland P. Chave, Pollution Control, United Kingdom J. Chen, Health Canada, Canada N. Chernoff, Environmental Protection Agency, USA A. Cherry, Health Canada, Canada C.K. Chew, formerly WHO, Switzerland (currently PUB, the national water agency, Singapore) T. Chhoden, Ministry of Works and Human Settlement, Bhutan M.L. Chong, formerly WHO, Philippines (currently PUB, the national water agency, Singapore) I. Chorus, formerly Federal Environment Agency, Germany D. Chuckman, International Flight Services Association, Canada G. Cissé, Swiss Tropical and Public Health Institute, Switzerland T. Clasen, Emory University, USA L. Coccagna, Independent Consultant, Italy G.A. Codd, University of Dundee, United Kingdom R. Coelho, Aguas do Algarve, Portugal G. Colarullo, Federation of Italian Municipal Utilities, Italy J. Colbourne, Drinking Water Inspectorate, United Kingdom T. Colgan, formerly International Atomic Energy Agency, Austria G. Combs, United States Department of Agriculture, USA C.J. Coomans, Chris Swartz Water Utilization Engineers, South Africa J. Cooper, Health Canada, Canada L. Corrales, Centers for Disease Control and Prevention, USA O. Corrales, ACM-2020, Spain H. Costa, Departamento da Qualidade, Portugal R. Costello, National Institutes of Health, USA R. Costlow, R. Costlow Consulting, USA J. Cotruvo, Joseph Cotruvo & Associates/NSF International Collaborating Centre, USA M. Couper, formerly WHO, Switzerland D Court Marques, European Food Safety Authority, Italy C. Cox, Caribbean Environmental Health Institute, St Lucia P. Cox, Sydney Water, Australia A. Cronin, University of Surrey, United Kingdom D. Crump, Cranfield University D. Cunliffe, South Australian Department of Health, Australia J. Dadakis, Orange County Water District, USA E. Dalgalarrondo, Ministerio de Vivienda, Ordenamiento Territorial y Medio Ambiente, Uruguay F. Dangendorf, University of Bonn, Germany L. D’Anglada, Environmental Protection Agency, USA T. Darlow, MWH, United Kingdom D. Davidson, Center for Food Safety and Applied Nutrition, Food and Drug Administration, USA M. Davidson, Public Health England, United Kingdom A. Davison, Risk Edge, Australia C. de Bazigan, Antenna, Switzerland L.R. de Dios, Department of Health, Philippines D. Deere, Water Futures Pty Ltd, Australia J. De France, WHO, Switzerland D. de Jager, Ministry of Health, New Zealand R. del Valle Bazán, Universidad Nacional de Córdoba, Argentina J. Dennis, Environmental Health Consulting, New Zealand J. Dennis, Thames Water Utilities, United Kingdom A.M. de Roda Husman, National Institute for Public Health and the Environment, the Netherlands P. de Souza, Emanti Management, South Africa K. de Vette, International Water Association, the Netherlands D. Deere, Water Futures Pty. Ltd., Australia V. Delgermaa, WHO, Mongolia C. Di Carlo, National Center for Radiation Protection and Computational Physics, Italy H. Dieter, formerly Federal Environment Agency, Germany D. Dietrich, University of Konstanz, Germany E. Dittmann, University of Potsdam, Germany P. Donlon, Water Services Association, Australia J. Donohue, Environmental Protection Agency, USA T. Dooley, United Nations Children’s Fund, USA C. Dorea, University of Victoria, Canada F. Douchin, DASS de Seine Maritime, France N. Dowdall, British Airways, United Kingdom P. Drechsel, International Water Management Institute, Sri Lanka J. Drewes, Technical University of Munich, Germany (formerly Colorado School of Mines, USA) A.A. Drozd, Universidad Nacional de Avellaneda, Argentina D. Drury, Independent Consultant, United Kingdom I. Dublineau, Institut de Radioprotection et de Sûreté Nucléaire, France B. Duncan, Environmental Protection Agency, USA N.T. Duong, Vietnam Water Supply and Sewerage Association (VWSA), Viet Nam P. Du Pisani, formerly City of Windhoek, Namibia L. Düster, Federal Institute of Hydrology, Germany K. Dziekan, Federal Environment Agency, Germany K. Ebi, University of Washington, USA A. Eckhardt, Federal Environment Agency, Germany C. Edgar, Cranfield University, United Kingdom P. Edmondson, Medentech Ltd, United Kingdom C. Eidhin, Environmental Protection Agency, Ireland A. Eleveld, Safe Water and AIDS Project, Kenya R. Elin, University of Louisville, USA T. Endo, Ministry of Health, Labour and Welfare, Japan S. Enkhtsetseg, WHO Regional Office for Europe, Germany J. Escamilla, WHO, Panama S. Estier, Federal Office of Public Health, Switzerland M. Exner, Institute for Hygiene and Public Health, University of Bonn, Germany A. Eyring, Philadelphia Water Department, USA E.J. Faassen, Wageningen University & Research, the Netherlands I. Falconer, University of Adelaide, Australia J. Falkinham, Virginia Tech, USA Z. Fang, Department of Health Quarantine, General Administration of Quality Supervision, Inspection and Quarantine of the People’s Republic of China, China J. Fastner, Federal Environment Agency, Germany J. Fawell, Cranfield University, United Kingdom D. Fayzieva, Uzbekistan Academy of Science, Uzbekistan T. Fengthong, Ministry of Health, Lao People’s Democratic Republic L. Ferenc, Institute for Water Pollution Control, Water Resources Research Centre, Hungary C. Fergusson, Department of Primary Industries, Australia A.M.C. Fernández, University of Sevilla, Spain E. Ferretti, Istituto Superiore di Sanità, Italy V. Fessard, BioAgroPolis, France I. Feuerpfeil, formerly Federal Environment Agency, Germany L. Fewtrell, Aberystwyth University, United Kingdom K. Fielding, University of Queensland, Australia M. Fisher, University of North Carolina, USA M. Foran, Centre for Affordable Water and Sanitation Technology (CAWST), Canada K. Ford, Rio Tinto, United Kingdom M. Forson, United Nations Children’s Fund, USA A. Foss, Greenwater Laboratories, USA P. Fosselard, European Federation of Bottled Water, Belgium C. Frambøl, Danish Water and Waste Water Association (DANVA), Denmark M.R. Franklin, Institute of Radiation Protection and Dosimetry, Brazil M. Frobel, IM System, Germany D. Frost, Aqua Focus Ltd, United Kingdom D. Fuente, University of South Carolina, USA D. Fujise Waterworks Bureau Kawasaki City, Japan N. Funamizu, Hokkaido University, Japan A. Furey, Cork Institute of Technology, Ireland A. Galizia, Environmental Protection Agency, USA K. Galladant, London School of Hygiene and Tropical Medicine, United Kingdom R. Gangaraju, Health Canada, Canada C. García, Ministerio de Vivienda, Ordenamiento Territorial y Medio Ambiente, Uruguay D. Gatel, EUREAU, Belgium M. Gately, Medentech, Ireland J. Geere, University of East Anglia, United Kingdom K. Gehrcke, Federal Office for Radiation Protection, Germany R.J. Gelting, Centers for Disease Control and Prevention, USA C. Gerba, University of Arizona, USA C. Gibbons, Environmental Protection Agency, USA M. Giddings, Health Canada, Canada K. Gin, Nanyang Technological University, Singapore S. Gitahi, United Nations Children’s Fund, Ghana M. Gleizes, Institute for Radiological Protection and Nuclear Safety, France S. Godfrey, Water and Sanitation Expert, Ethiopia C. Gollnisch, Akkreditierte Hygieneinspektionsstelle für Trinkwassersysteme, Germany R. Goossens, Compagnie Intercommunale Bruxelloise des Eaux, Belgium B. Gordon, WHO, Switzerland F. Gore, WHO, Switzerland E. Goslan, Cranfield University, United Kingdom W. Grabow, Health-related Water Microbiology (formerly of University of Pretoria), South Africa J. Grace, Air Safety, Health and Security Department, Association of Flight Attendants-CWA, USA J. Graham, University of Berkeley California, USA A.C. Grandjean, Center for Human Nutrition, USA L. Green, Australian Radiation Protection and Nuclear Safety Agency, Australia O. Green, WCA Environment Limited, United Kingdom S. Greene, Environmental Protection Agency, USA P. Greiner, NSF International, USA R. Grey-Gardner, formerly Centre for Appropriate Technology, Australia J.C. Grigoletto, Ministry of Health, Brazil T.J. Grizzard, Virginia Tech, USA H.-J. Grummt, Federal Environment Agency, Germany G. Grützmacher, Berliner Wasserbetriebe, Germany C. Güler, Hacettepe University, Turkey M. Gunnarsdóttir, University of Iceland, Iceland P. Günther, Prentec Technical Services (Pty) Ltd, South Africa R. Gupta, Visvesvaraya National Institute of Technology, India G. Gurbanova, Ministry of Ecology and Natural Resources, Azerbaijan K. Guyton, International Agency for Research on Cancer, France L.T. Ha, National Institute of Occupational and Environmental Health, Viet Nam C. Hadjichristodoulou, University of Thessaly, Greece L. Hamilton, formerly Ministry of Health, New Zealand M. Hammond, Independent Consultant, Switzerland S. Hansen, TIB Chemicals, Germany S. Harris, International Life Sciences Institute, USA P. Hartemann, Faculté de Médicine de Nancy, France T. Hasan, Pacific Islands Applied Geoscience Commission, Fiji N. Hassan, WHO South Pacific Office, Fiji S. Hauswirth, Public Health Service, Germany R.P. Heaney, Creighton University, USA J. Hearn, ALS Water Resources Group, Australia P. Heaton, formerly Power and Water Corporation, Australia H. Heijnen, Consultant, c/o United Nations Children’s Fund, Lao People’s Democratic Republic J. Hejzlar, Czech Academy of Sciences, Czechia K. Hellier, Melbourne Water, Australia S. Herbst, Institute for Hygiene and Public Health, University of Bonn, Germany O. Hernandez, FHI 360, USA R.C. Hertzberg, Biomathematics Consulting, USA A. Hill, Vestergaard Frandsen, Switzerland D.J. Hill, Environmental Protection Agency, USA M.R. Hipsey, The University of Western Australia, Australia A. Hirose, National Institute of Health Sciences, Japan A. Hiskia, NCSR Demokritos, Greece L. Ho, Allwater, Australia E.J. Hoekstra, Institute for Health and Consumer Protection, Italy P. Hofmann, Federal Office for Radiation Protection, Germany C. Hollister, Air Transport Association of Canada, Canada H. Holstag, 300 in 6, the Netherlands L. Hope, WHO, Switzerland R. Hossain, WHO, Switzerland G. Howard, University of Bristol (formerly Department for International Development), United Kingdom S. Hrudey, University of Alberta, Canada L.C. Hsiang, National Environment Agency, Singapore J. Hu, National University of Singapore, Singapore J. Huff, National Institute of Environmental Health Sciences, USA B. Hultquist, formerly California Department of Public Health, USA J.-F. Humbert, Sorbonne Université, France A. Humpage, South Australian Water Corporation, Australia J. Hunt, Drinking-Water Inspectorate, United Kingdom P. Hunter, University of East Anglia, United Kingdom A. Hussain, Ministry of Housing and Environment, Maldives F. Husson, Solar Solutions LLC, USA A. Hyskaj, Eötvös Loránd University, Hungary B.W. Ibelings, Université de Genève, Switzerland S.A. Ibrahim, Ministry of Housing and Environment, Maldives S. Iddings, formerly WHO South Pacific Office, Fiji H. Igarashi, WHO, Switzerland P. Illig, Centers for Disease Control and Prevention, USA A. Illmer, United Nations Children’s Fund, USA T. Inoue, Japan Water Forum, Japan F. Istace, Euoprean Food Safety Authority (EFSA), Italy M. Itoh, National Institute of Public Health, Japan S. Itoh, Kyoto University, Japan D. Jackson, Independent Consultant, United Kingdom (formerly Nepal) P. Jackson WRc-NSF Ltd, United Kingdom C. Jacobsson, Swedish University of Agricultural Sciences, Sweden P. Jagals, University of Queensland, Australia N. Jara, Ministerio de Vivienda, Ordenamiento Territorial y Medio Ambiente, Uruguay F.A. Jardin, COPASA, Brazil P. Jarvis, Cranfield University, United Kingdom A. Jayaratne, Yarra Valley Water, Australia E. Jesuthasan, WHO, Switzerland H.E. Jianzhong, National University of Singapore, Singapore B. Jiménez Cisneros, Institute of Engineering, Mexico R. Johnston, WHO, Switzerland G. Jones, University of Canberra, Australia H. Jones, Loughborough University, United Kingdom K. Jones, Public Health England, United Kingdom T.T. Jorge, formerly WHO, Switzerland C. Jørgensen, DHI, Denmark P.S. Joshi, National Environment Agency, Singapore D. Jovanovic, Institute of Public Health, Serbia R. Junek, Federal Environment Agency, Germany T. Jung, German Federal Office for Radiation Protection, Germany A. Kabir, WHO, Bangladesh A. Kabirizi, Ministry of Water and Environment, Uganda Mihály Kádár, formerly National Institute of Environmental Health, Hungary S. Kalandarov, WHO, Tajikistan N. Kalebaila, Water Research Commission, South Africa T. Kaloudis, EYDAP - Athens Water Supply and Sewerage Company, Greece A. Kämpfe, Federal Environment Agency, Germany M. Kanazawa, Ministry of Health, Labour and Welfare, Japan B.P. Kandel, Amarapuri Water Utility, Nepal C. Kanyesigye, National Water and Sewerage Corporation (NWSC), Uganda P. Karani, African Development Bank, Tunisia E. Kardinaal, KWR Watercycle Research Institute, the Netherlands G.Y.-H. Karina, National University of Singapore, Singapore H. Kasan, Rand Water, South Africa A. Kasuya, Ministry of Health Labour and Welfare, Japan H. Katayama, University of Tokyo, Japan D. Kay, Aberystwyth University, United Kingdom K. Kelleher, formerly International Atomic Energy Agency, Austria S. Khan, The University of New South Wales, Australia K. Khatri, WHO, Fiji N.R. Khatri, formerly WHO, Nepal R. Khush, Aquaya, USA S. Kilani, Ministry of Water and Irrigation, Jordan P. Kirch, Enwor-Energie & Wasser Vor Ort GmbH, Germany N. Kishida, National Institute of Public Health, Japan T. Kistemann, University of Bonn, Germany S. Klitzke, Federal Environment Agency, Germany W. Kloas, Leibniz-Institute of Freshwater Ecology and Inland Fisheries, Germany W.R. Knocke, Virginia Tech, USA N. Kobayashi, National Institute of Health Sciences, Japan B.J. Kobelski, Environmental Protection Agency, USA M. Kochubovski, Institute of Public Health, Republic of North Macedonia K. Komatsu, National Institute for Environmental Studies, Japan K. Kosaka, National Institute of Public Health, Japan N.O. Kotei, Public Utilities Regulatory Commission, Ghana A. Kovacs, International Commission for the Protection of the Danube River, Austria J. Lalung, Universiti Sains Malaysia, Malaysia P. Kozarsky, Centers for Disease Control and Prevention, USA F. Kozisek, National Institute of Public Health, Czechia R. Kryschi, Germany Y. Kubo, Ministry of Health, Labour and Welfare, Japan P. Kubon, Federal Environment Agency, Germany Y. Kudo, Japan Water Works Association, Japan S. Kumar, University of Malaya, Malaysia S. Kunikane, University of Shizuoka, Japan S. Kurebayashi, Ministry of the Environment, Japan R. Kurmayer, University of Innsbruck, Austria W. Kutane, WHO, Ethiopia P. Labhasetwar, National Environmental Engineering Research Institute, India S. Labib, Health Canada, Canada J. Lafontaine, Health Canada, Canada H. Lahav, Makhshim Chemical Works Ltd, USA J. Lahnsteiner, VA TECH WABAG, Austria K.C. Lai, PUB, the national water agency, Singapore B. Lampe, NSF International, USA D. Lantagne, Tufts University, USA L. Laraki, Office National de l’eau Potable, Morocco I. Law, IBL Solutions, Australia L.A. Lawton, Robert Gordon University, United Kingdom M.W. LeChevallier, American Water, USA D. Lee, PUB, the national water agency, Singapore H. Lee, PUB, the national water agency, Singapore H.K. Lee, National University of Singapore, Singapore J. Lee, Health Protection Agency, United Kingdom L. Lejon, Flemish Agency for Care and Health, Belgium R. Lemen, retired (formerly with United States Public Health Service, USA) F. Lemieux, Health Canada, Canada P. Lennon, PATH, USA G. Leslie, University of New South Wales, Australia F. Leusch, Griffith University, Australia K. Levy, Rollins School of Public Health, USA R. Lieberman, Environmental Protection Agency, USA M.H. Lim, PUB, the national water agency, Singapore K. Linden, University of Colorado Boulder, USA O. Loebel, EurEau, Belgium J.-F. Loret, Centre International de Recherche Sur l’eau et l’Environnement – Suez Environnement, France P. Lotz, MINTEK, South Africa A. Lovell, Water Services Association of Australia, Australia S. Luby, International Centre for Diarrhoeal Disease Research, Bangladesh L. Lucentini, National Institute of Health, Italy C. Lucks, Federal Office for Radiation Protection, Germany J. Luh, formerly University of North Carolina, USA J. MacAulay, Health Canada, Canada D. MacChesney, Environmental Protection Agency, USA L. Macpherson, New Water ReSources, USA S.N. Madjunarova, International Atomic Energy Agency, Austria Y. Magara, Hokkaido University, Japan B. Magtibay, WHO, Philippines S.G. Mahmud, WHO, Bangladesh D. Maison, WHO, Switzerland B. Majuru, WHO, Switzerland H.-J. Mälzer, IWW Water Centre, Germany J. Mankiewicz-Boczek, United Nations Educational, Scientific and Cultural Organization, Poland E. Mantzouki, Université de Genève, Switzerland D. Mara, University of Leeds, United Kingdom K.J. Marienau, Centers for Disease Control and Prevention, USA M. Markus, Orange County Water District, USA B. Maršálek, Masarykova Univerzita, Czechia P. Marsden, Drinking Water Inspectorate, United Kingdom M.G. Martí, Sociedad General de Aguas de Barcelona (AGBAR), Spain C. Martinho, Acquawise, Portugal T. Matsuda, Ministry of Health, Labour and Welfare, Japan Y. Matsui, Hokkaido University, Japan K. Matsumoto, Ministry of Health, Labour and Welfare, Japan A. Mavridou, Technological Educational Institute of Athens, Greece A. May, Drinking Water Inspectorate, United Kingdom N. McColl, Public Health England, United Kingdom A. McCoy, Health Canada, Canada A. McDonald, Population Services International (PSI), USA S. McFadyen, Health Canada, Canada M.J. McGuire, Michael J. McGuire, Inc., USA K. McHugh, Population Services International (PSI), USA R.M. McKeown, WHO, Switzerland D. Medeiros, Health Canada, Canada G. Medema, KWR Watercycle Research Institute and Delft University of Technology, the Netherlands K. Medlicott, WHO, Switzerland E. Medlin, Centers for Disease Control and Prevention, USA M.E. Meek, University of Ottawa, Canada R. Meierhofer, Eawag, Switzerland K. Meme, Lifewater International, USA R. Mendes, Acquawise, Portugal J. Menge, formerly City of Windhoek, Namibia D.L. Menucci, WHO, France J. Mercer, Health Canada, Canada J. Meriluoto, Åbo Akademi University, Finland B.J. Merkel, Freiberg University of Mining and Technology, Germany W. Merkel, IWW, Germany J.S. Metcalfe, Institute of Ethnomedicine, USA R. Meyerhoff, Lilly Research Laboratories, Eli Lilly and Company, USA C. Michelena, Ministerio de Vivienda, Ordenamiento Territorial y Medio Ambiente, Uruguay G. Miller, Environmental Protection Agency, USA H. Michel, Carollo Engineers, USA M. Millan, Data Instincts, USA R. Miller, Water Corporation, Australia F. Miranda da Rocha, National Sanitary Control Agency, Brazil R. Mitchell, WRc, United Kingdom I. Moffat, Health Canada, Canada Z.A. Mohamed, Sohag University, Egypt H.G.H. Mohammad, Ministry of Health, Kuwait N. Mohlala, National Nuclear Regulator, South Africa D. Moir, Health Canada, Canada D. Mokadam, Association of Flight Attendants-CWA, USA A. Molinari, Ente Regulador de Agua y Sanemiento (ERAS), Argentina M. Mons, Kiwa Water Research, the Netherlands T. Monteiro, formerly WHO Pan-American Health Organization, Peru M. Montgomery, WHO, Switzerland A. Mooijman, Independent Consultant, the NetherlandsC. Morais, Águas do Cávado, Portugal H. Morii, Osaka City University, Japan V. Morisset, Health Canada, Canada T. Morita, Japan Water Forum, Japan N. Moritani, Ministry of Health, Labour and Welfare, Japan R. Morris, University College London, United Kingdom J. Mosher, Water Environment & Reuse Foundation, USA B. Mouchtouri, University of Thessal, Greece M. Moussif, Mohamed V Airport, Morocco F.H. Mughal, Independent Consultant, Pakistan (formerly Independent Researcher, India) H. Muller, Independent Consultant, South Africa C. Munoz-Trochez, formerly International Water Association, United Kingdom C.M. Murray, Fairfax Water, USA M. Muse, previously with Environmental Protection Agency, USA J. Nadeau, Health Canada, Canada B. Nancarrow, Syme and Nancarrow Water, Australia S. Nappier, Environmental Protection Agency, USA N.O. Nascimento, Federal University of Minas Gerais, Brazil K.J. Nath, Institution of Public Health Engineers, India M.I.J. Navarro, Universidad Nacional Autónoma de México [UNAM], Mexico M. Ncube, Johannesburg Water, South Africa R. Neipp, Ministry of Health and Social Policy, Spain J.C. Neto, Universidade Federal do Ceará, Brazil T. Neville, Vestergaard Frandsen, Zambia G. Newcombe, formerly SA Water, Australia T. Ngai, Centre for Affordable Water and Sanitation Technology (CAWST), Canada M.S. Ngon, WHO, Myanmar A.V.F. Ngowi, Muhimbili University of Health and Allied Sciences, United Republic of Tanzania C. Nicholson, Sydney Water, Australia F.H. Nielsen, United States Department of Agriculture, USA J.W. Nieves, Columbia University, USA Y. Nijdam, Waternet, the Netherlands C. Nishida, WHO, Switzerland I.K. Njoya, Ministry of Water and Energy, Cameroon A. Nocker, IWW Water Centre, Germany (formerly Cranfield University, United Kingdom) C. Nokes, Environmental Science and Research Ltd, New Zealand V.J. Novotny, Professor Emeritus, Marquette University, USA, and Northeastern University, USA N. O’Connor, Ecos Environmental Consulting, Australia O. Odediran, United Nations Children’s Fund, USA B. Odugbemi, Consultant at Lagos State University Teaching Hospital, Nigeria O. Oenema, Wageningen University and Research Center, the Netherlands G. Offringa, formerly Water Research Commission, South Africa P.-Y. Oger, United Nations Children’s Fund, USA M. Ogoshi, National Institute for Land and Infrastructure Management, Japan J.-E. Oh, Pusan National University, Republic of Korea E. Ohanian, Environmental Protection Agency, USA K. Ohno, formerly National Institute of Public Health, Japan S. Okamoto, Public Works Research Institute, JapanS. Ólafsdóttir, Icelandic Food and Veterinary Authority, Iceland G. Oliver, Australian Water Secure Innovations (formerly Australian Water Recycling Centre of Excellence), Australia C.N. Ong, National University of Singapore, Singapore S.L. Ong, National University of Singapore, Singapore L. Onyon, WHO, Switzerland C. Ortiz, Laguna Madre Water District, USA P. Osborn, 300in6 initiative, the Netherlands N.J. Osborne, University of Queensland, Australia P.S. Oshida, formerly Environmental Protection Agency, USA W. Oswald, Rollins School of Public Health, USA J. O’Toole, Monash University, Australia S. Ou, Public Health South, New Zealand A. Overbo, University of Minnesota (formerly University of North Carolina), USA M. Overmars, formerly Pacific Islands Applied Geoscience Commission, Fiji J. Padisák, Pannon Egyetem, Hungary S.R. Panthi, WHO, Nepal A. Paoli, Atkins Limited, United Kingdom J.M. Parra Morte, European Food Safety Authority, Italy T. Paux, Ministère de la santé, de la jeunesse et des sports, France Payden, WHO Regional Office for South East Asia, India G.L. Peralta, WHO, Philippines J.P. Peregalli, Ministerio de Vivienda, Ordenamiento Territorial y Medio Ambiente, Uruguay S. Perry, State of Washington Office of Drinking Water, USA M.D.R. Perez, WHO, Switzerland S. Petterson, Water & Health Pty Ltd, Australia S. Phan, WHO, Cambodia C. Pickl, Federal Environment Agency, Germany J. Pietersen, Midvaal Water Company, South Africa B. Pilon, International Air Transport Association, Switzerland G. Pinotti, Ministerio de Vivienda, Ordenamiento Territorial y Medio Ambiente, Uruguay O. Pintos, Asociación Federal de Entes Reguladores de Agua y Saneamiento de Argentina, Argentina W. Piyasena, formerly Ministry of Water Supply and Drainage, Sri Lanka M. Plemp, Centre for Infectious Disease Control, the Netherlands B. Plotkin, WHO, Switzerland M. Podeprel, Helioz Research and Development, Austria T. Pohle, Air Transport Association, USA C. Pollard, Drinking Water Inspectorate, United Kingdom S. Pollard, Cranfield University, United Kingdom K. Pond, University of Surrey, United Kingdom D. Poulin, Health Canada, Canada H.L. Pound, University of Tennessee, USA J. Pratt, Veolia Water Central, United Kingdom F. Properzi, WHO, Switzerland (currently UN-Water, Switzerland) J. Puddick, Cawthron Institute, New Zealand T. Pule, WHO, Republic of Congo D. Purkiss, NSF International, USA W. Qu, Fudan University, China A. Queste, University of Bonn, Germany C. Quiblier, Museum National d’Histoire Naturelle, France H. Quiñones, Scientific and Technical Translator, Spain M. Rafatullah, Universiti Sains Malaysia, Malaysia R. Rainey, United States Agency for International Development (USAID), USA S. Ramasamy, Environmental Protection Agency, USA V. Ramnath, National Environment Agency, Singapore A. Rannou, Institute for Radiological Protection and Nuclear Safety, France T. Rapp, Federal Environment Agency, Germany H. Raymond, Ohio State University, USA S. Regli, Environmental Protection Agency, USA P. Regunathan, Regunathan & Associates Inc., USA D. Reid, Alberta Environment and Parks, Canada B. Rickert, Federal Environment Agency, Germany A. Rinehold, WHO, Switzerland U. Ringelband, formerly Federal Environment Agency, Germany J. Ringo, Bio-Cide International, Inc., USA S. Risica, National Institute of Health, Italy M. Rivett, University of Birmingham, United Kingdom W. Robertson, Watermicrobe Consultancy (formerly Health Canada), Canada C. Robertson-Kellie, Drinking Water Quality Regulator for Scotland, United Kingdom C. Robillot, HeadStart Development Pty Ltd, Australia C. Rockey, South West Water, United Kingdom G. Rodier, WHO, Switzerland A.L.G. Rodrigues, Sabesp, Brazil C. Rodriguez, Western Australia Department of Health, Australia L. Rogers, WHO, Switzerland J. Rose, Michigan State University, USA J.W. Rosenboom, Water and Sanitation Program of the World Bank (WSP), Cambodia (currently Bill & Melinda Gates Foundation) K. Ross, ISF-UTS, Australia S. Rostron, Ministry of Health, New Zealand K. Rotert, Environmental Protection Agency, USA N. Roth, Swiss Centre for Applied Human Toxicology, Switzerland M. Rouse, University of Oxford, United Kingdom R. Rowe, The Water Institute, University of North Carolina, USA P. Rzymski, Poznan University of Medical Sciences, Poland M. Sagehashi, National Institute of Public Health, Japan H.J. Salas, Pan American Center for Sanitary Engineering and Environmental Sciences, Lima, Peru L.R. Sally, formerly International Water Management Institute, Sri Lanka N. Salmaso, Fondazione Mach-Istituto Agrario di S. Michele all’Adige Hydrobiology, Italy A. Salveson, Carollo Engineers, USA P. Samuels, HR Wallingford, United Kingdom M. Samwell, Women in Europe for a Common Future, the NetherlandsR. Sancho, Águas do Algarve, Portugal H. Sanderson, Danish National Environmental Research Institute, Denmark A. Sargaonkar, National Environmental Engineering Research Institute, India A. Sasso, Environmental Protection Agency, USA B. Schaefer, Federal Environment Agency, Germany B.A. Schaeffer, Environmental Protection Agency, USA S. Schaub, formerly Environmental Protection Agency, USA C. Schets, National Institute for Public Health and the Environment, the Netherlands J. Schijven, National Institute for Public Health and the Environment and Utrecht University, the Netherlands S. Schira, Liquitech, USA O. Schmoll, WHO Regional Office for Europe, Germany B. Schnabel, formerly Federal Environment Agency, Germany M. Schubauer-Berigan, International Agency for Research on Cancer, France S. Seki, Ministry of the Environment, Japan C. Sevenich, Hamburg Port Health Center, Germany F. Shafeeqa, Live & Learn Environmental Education, Maldives G. Shaghaghi, Ministry of Health, Islamic Republic of Iran N. Shah, Unilever R & D Laboratory, India F. Shannoun, WHO, Switzerland R.K. Sharma, Rural Water Supply and Sanitation Fund Development Board, Nepal N. Shaw, International Shipping Federation, United Kingdom D. Sheehan, Coliban Water, Australia M. Sheffer, Editor, Canada E. Sheward, University of Central Lancashire, United Kingdom P. Shodmonov, State Sanitary Epidemiological Surveillance Service, Tajikistan K. Sholtes, Centers for Disease Control and Prevention, USA D. Shrestha, United Nations High Commissioner for Refugees, Switzerland R.R.P. Shrestha, WHO, Nepal L. Siegel, Safe Water International, USA S. Silma, International Organization of Migration, Switzerland L. Simas, ERSAR, the Water and Waste Services Regulation Authority, Portugal D. Simazaki, National Institute of Public Health, Japan J. Simmons, Environmental Protection Agency, USA J. Sims, retired (formerly WHO, Switzerland) M. Sinclair, Monash University, Australia O. Sinitsyna, Ministry of Health, Russian Federation C. Skak, Danish Toxicology Centre, Denmark K. Skeppström, formerly Swedish Radiation Safety Authority, Sweden P. Smeets, KWR Watercycle Research Institute, the Netherlands B. Smith, Independent Consultant, United Kingdom D. Smith, Melbourne Water, Australia J. Smith, Independent Consultant, United Kingdom S. Smith, Wessex Water, United Kingdom K. Snead, Environmental Protection Agency, USA S. Snyder, University of Arizona, USA M. Sobsey, University of North Carolina, USA J. Soller, Soller Environmental, LLC, USA B. Sontia, University of Ottawa, Canada B. Stanford, Hazen and Sawyer, USA E. Steinle-Darling, Carollo Engineers, USA P. Steinmann, Federal Office of Public Health, Switzerland T.-A. Stenstrom, Swedish Institute for Infectious Disease Control, Sweden (currently Durban University of Technology, South Africa) M. Stevens, Melbourne Water, Australia M. Stevenson, Cascade Designs, USA I. Stewart, Food and Water Toxicology Consulting, Australia N. Stewart, Carnival UK, United Kingdom J. Strandberg, formerly WHO, Sweden V. Straškrábová, Czech Academy of Sciences, Czechia S. Sturm, German Technical and Scientific Association for Gas and Water – Technologiezentrum Wasser, Germany K. Sudo, Japan International Cooperation Agency, Japan M. Suffet, University of California Los Angeles, USA A. Sufiev, State Sanitary Epidemiological Surveillance Service, Tajikistan J. Suhaimi, Ranhill Utilities, Malaysia A. Sukenik, Israel Oceanographic and Limnological Research, Israel S. Sumanaweera, National Water Supply and Drainage Board, Sri Lanka C. Summerill, Cranfield University, United Kingdom S. Surman-Lee, Health Protection Agency, United Kingdom D. Susau, Live & Learn Environmental Education, Fiji D. Sutherland, WHO Regional Office for South East Asia, India (formerly Independent Consultant, United Kingdom) A. Suzuki, Ministry of the Environment, Japan K. Suzuki, TMWW, Japan M. Swart, Rand Water (formerly Department of Water Affairs), South Africa C. Swartz, Chris Swartz Water Utilization Engineers, South Africa D. Swiderski, Aquor/Global Water Council, USA L. Taghizade, Ministry of Health, Azerbaijan K. Takahashi, Asbestos Diseases Research Institute, Australia M. Takahashi, formerly National Institute of Health Sciences, Japan A. Tamas, Swiss Federal Institute of Environmental Science and Technology/ Department of Water and Sanitation in Developing Countries, Switzerland H. Tanaka, Kyoto University, Japan K Tanaka, formerly Waterworks Bureau Kawasaki City, Japan R. Tanner, Independent Consultant, Belgium M. Taylor, formerly Ministry of Health, New Zealand C. Teaf, Florida State University, USA E. Testai, National Institute of Health, Italy P. Teixeira, WHO, USA M. Templeton, Imperial College London, United Kingdom A. Terazono, National Institute for Environmental Studies, Japan P. Teunis, National Institute for Public Health and the Environment, the Netherlands, and Rollins School of Public Health, Emory University, USA C. Thibeault, International Air Transport Association, Canada P. Thompson, ToxStrategies, USA T. Thompson, formerly WHO, Philippines B. Thunholm, Geological Survey of Sweden, Sweden S.M. Tibatemwa, International Water Association, Kenya D. Till, Independent Consultant, New Zealand R. Tinker, Australian Radiation Protection and Nuclear Safety Agency, Australia J. Tobiason, University of Massachusetts Amherst, USA J. Todd, Environmental Protection Agency, USA I. Toh, PUB, the national water agency, Singapore S. Toh, PUB, the national water agency, Singapore R. Tomisaka, Ministry of the Environment, Japan N. Ton Tuan, WHO, Viet Nam A. Törökné, National Accreditation Authority, Hungary A. Torres, International Organization of Migration, Switzerland R. Torres, WHO, Plurinational State of Bolivia R.M. Touyz, University of Ottawa, Canada B. Tracy, Health Canada, Canada A. Trevett, WHO, Bangladesh (currently United Nations Children’s Fund, Kenya) Y. Trihadiningrum, Institut Teknologi Sepuluh Nopember, Indonesia D.M. Trindade, Centre for Disease Control and Prevention, Macao Special Administrative Region, China A. Tritscher, formerly WHO, Switzerland M. Troussellier, Museum National d’Histoire Naturelle, France R.R. Trussell, Trussell Technologies, USA A. Tugulea, Health Canada, Canada S. Tuite, Health Canada, Canada T.M. Ua-Cookson, Ministry of Health, New Zealand T. Udagawa, Japan Water Works Association, Japan P. Undesser, Water Quality Association, USA E. Urquhart, National Aeronautics and Space Administration, USA S. Vajpeyee, Government Medical College and New Civil Hospital, India M. Valcke, Institut National de Santé Publique du Québec, Canada K. van den Belt, Flander Environment Agency, Belgium J.P. van der Hoek, Amsterdam Water Supply, the Netherlands B. van der Merwe, Environmental Engineering Services, Namibia E. van Deventer, WHO, Switzerland P. Van Maanen, United Nations Children’s Fund, USA J. Van Zyl, University of Cape Town, South Africa L. Varadi, President of the Hungarian Aquaculture Association, Hungary G. Vartanian, National Water Research Institute, USA L. Veiga, formerly Institute of Radiation Protection and Dosimetry, Brazil G. Velo, University of Verona, Italy F. Venter, University of Pretoria, South Africa P. Verger, WHO, Switzerland E. Veschetti, Istituto Superiore di Sanità, Italy E.Viau, Bioenergy Frontiers, USA C. Vickers, WHO, Switzerland L. Vidal, Consultant, Uruguay J.M.P. Vieira, University of Minho, Portugal L. Vijselaar, Danish Committee for Aid to Afghan Refugees (DACAAR), Afghanistan C. Viljoen, Rand Water, South Africa E. Villalobos Prats, WHO, Switzerland D. Viola, International Association of Plumbing and Mechanical Officials and World Plumbing Council, USA N. Virabouth, Ministry of Public Works and Transport, Lao People’s Democratic Republic G. Vivas, WHO, Barbados A. von Hildebrand, formerly WHO Regional Office for the Western Pacific, Philippines M. von Sperling, Federal University of Minas Gerais, Brazil T. Wade, Environmental Protection Agency, USA R. Wahabi, Ministry of Health, Morocco R. Walker, Water Corporation, Australia C. Wallace, United Nations University, Canada N. Walmsley, HR Wallingford, United Kingdom T. Waters, Environmental Protection Agency, USA C. Weaver, Purdue University, USA S. Webster, MWH, New Zealand W. Weglicki, George Washington University Medical Center, USA M. Wehner, Orange County Water District, USA R. Weisman, Environmental Protection Agency, USA M. Welker, Independent Consultant, Germany S. Weragoda, National Water Supply and Drainage Board, Sri Lanka S. Westacott, Southampton City Council, United Kingdom K. White, representing collective view from American Chemistry Council, USA I. Wienand, University of Bonn, Germany S. Wijesekara, United Nations Children’s Fund, USA A. Wiklund, DG Energy, European Commission, Luxembourg S.W. Wilhelm, University of Tennessee, USA J. Willetts, ISF-UTS, Australia A. Williams, CF (formerly University of North Carolina), USA T. Williams, International Water Association, the Netherlands H. Willmitzer, Technische Universität Dresden, Germany D. Wilusz, Department of State, USA K. Winterford, ISF-UTS, Australia C. Witkowski, Association of Flight Attendants-CWA, USA C. Wittwer, Federal Office for Radiation Protection, Germany K.-M. Wollin, Niedersächsisches Landesgesundheitsamt, Germany K.W. Wong, PUB, the national water agency, Singapore C.H. Woo, PUB, the national water agency, Singapore S.A. Wood, Cawthron Institute, New Zealand G. Woolhouse, HR Wallingford, United Kingdom P. Xie, Institute of Hydrobiology - Chinese Academy of Sciences, China M. Yadav, Rural Water Supply and Sanitation Fund Development Board, Nepal I. Yamaguchi, National Institute of Public Health, Japan J. Yap, National Environment Agency, Singapore G. Yasvinski, Health Canada, Canada O. Yiha, WHO, Ethiopia M. Young, Environmental Protection Agency, USA R. Yuen, International Water Association, Singapore M. N. M. Yunus, Atomic Energy Licensing Board Member, Malaysia T. Zabel, WRc, United Kingdom Y. Zaki, International Organization of Migration, Switzerland A. Zamyadi, University of Montreal, Canada I. Zastenskaya, WHO, GermanyB. Žegura, National Institute of Biology, Slovenia M. Zemlyanova, Federal Scientific Center for Medical and Preventive Health Risk Management Technologies, Russian Federation M. Zessner, Technische Universität Wien, Austria R. Zhang, National Center for Rural Water Supply Technical Guidance, Center for Disease Control and Prevention, China K. Ziegler-Skylakakis, Technical University of Munich, Germany G. Ziglio, University of Trento, Italy K. Zoschke, Technische Universität Dresden, Germany G. Zwolsman, KWR Watercycle Research Institute, the Netherlands This fourth editionincorporating the first and second addenda,of the World Health Organization's Guidelines for Drinking-water Qualitybuilds on over 60 years of guidance by WHO on drinking-water quality,which has formed an authoritative basis for the setting of national regulations and standards for water safety in support of public health.lt is the product of significant revisions to clarify and elaborateonwaysofimplementingitsrecom-mendations of contextual hazard identification and risk management,through the establishment of health-based targets,catchment-to-consumer water safety plansandindependentsurveillance.ltreflectsthe renewed focus on primary prevention.Significant additional guidance on good practice is presented,incorporating changes introduced in the third edition.Emergingwatermanagementissuesare comprehensivelyaddressedforarangeof circumstances,from household water treatment and safe storage and the bulk supply of water over long distances to the implications of climate change.Additional risk assessments are presented for a number of new chemical and microbial hazards and applied to a suite of pesticides used for public health purposes.Existingreviewsonchemicalsandwaterborne pathogenshavebeenrevisedtoaccountfornew scientificinformation.Thechapteronradiological aspectsofdrinking-waterqualityhasbeen comprehensively updated.Even more than the previous edition,this new edition incorporating the first and second addenda,emphasizes achievable practices and the formulation of sound regulations,applicable to low-income,middle-income and industrialized countries alike,that aim to prevent a potential health crisis caused by the consumption of unsafe drinking-water,against the backdrop of rapid urbanization,water scarcity and climate change.This fourth editionincorporating the first and second addenda,of the World Health Organization's Guidelines for Drinking-water Qualitybuilds on over 60 years of guidance by WHO on drinking-water quality,which has formed an authoritative basis for the setting of national regulations and standards for water safety in support of public health.lt is the product of significant revisions to clarify and elaborateonwaysofimplementingitsrecom-mendations of contextual hazard identification and risk management,through the establishment of health-based targets,catchment-to-consumer water safety plansandindependentsurveillance.ltreflectsthe renewed focus on primary prevention.Significant additional guidance on good practice is presented,incorporating changes introduced in the third edition.Emergingwatermanagementissuesare comprehensivelyaddressedforarangeof circumstances,from household water treatment and safe storage and the bulk supply of water over long distances to the implications of climate change.Additional risk assessments are presented for a number of new chemical and microbial hazards and applied to a suite of pesticides used for public health purposes.Existingreviewsonchemicalsandwaterborne pathogenshavebeenrevisedtoaccountfornew scientificinformation.Thechapteronradiological aspectsofdrinking-waterqualityhasbeen comprehensively updated.Even more than the previous edition,this new edition incorporating the first and second addenda,emphasizes achievable practices and the formulation of sound regulations,applicable to low-income,middle-income and industrialized countries alike,that aim to prevent a potential health crisis caused by the consumption of unsafe drinking-water,against the backdrop of rapid urbanization,water scarcity and climate change.97892400450649 789240 045064